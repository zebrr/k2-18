"""
LLM Client –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å OpenAI Responses API.

–ú–æ–¥—É–ª—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∫–ª–∞—Å—Å—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å OpenAI API —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π:
- –ö–æ–Ω—Ç—Ä–æ–ª—è –ª–∏–º–∏—Ç–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤ —á–µ—Ä–µ–∑ response headers
- –¶–µ–ø–æ—á–µ–∫ –æ—Ç–≤–µ—Ç–æ–≤ —á–µ—Ä–µ–∑ previous_response_id
- Reasoning –º–æ–¥–µ–ª–µ–π (o*)
- Exponential backoff retry –ª–æ–≥–∏–∫–∏
- –û–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫ –∏ –æ—Ç–∫–∞–∑–æ–≤ –º–æ–¥–µ–ª–∏

–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
    >>> config = {
    ...     'api_key': 'sk-...',
    ...     'model': 'gpt-4o',
    ...     'tpm_limit': 120000,
    ...     'tpm_safety_margin': 0.15,
    ...     'max_completion': 4096,
    ...     'max_retries': 6
    ... }
    >>> client = OpenAIClient(config)
    >>> response, response_id, usage = client.create_response(
    ...     "You are a helpful assistant",
    ...     "What is the capital of France?"
    ... )
"""

import time
import logging
import json
from typing import Optional, Dict, Any, List, Tuple
from dataclasses import dataclass
from datetime import datetime, timedelta

import openai
from openai import OpenAI
import tiktoken


@dataclass
class ResponseUsage:
    """
    –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è —É—á–µ—Ç–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤.
    
    Attributes:
        input_tokens (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤–æ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        output_tokens (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ –º–æ–¥–µ–ª–∏
        total_tokens (int): –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ (input + output)
        reasoning_tokens (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ reasoning (–¥–ª—è o* –º–æ–¥–µ–ª–µ–π)
        
    Example:
        >>> usage = ResponseUsage(
        ...     input_tokens=150,
        ...     output_tokens=50, 
        ...     total_tokens=200,
        ...     reasoning_tokens=30
        ... )
        >>> print(f"Total cost: {usage.total_tokens} tokens")
        >>> if usage.reasoning_tokens > 0:
        ...     print(f"Including {usage.reasoning_tokens} reasoning tokens")
    """
    input_tokens: int
    output_tokens: int
    total_tokens: int
    reasoning_tokens: int = 0


class TPMBucket:
    """
    –ö–æ–Ω—Ç—Ä–æ–ª—å –ª–∏–º–∏—Ç–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –º–∏–Ω—É—Ç—É (TPM) —á–µ—Ä–µ–∑ response headers.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç headers –æ—Ç OpenAI API –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è
    –ª–∏–º–∏—Ç–æ–≤. Headers –ø—Ä–∏—Ö–æ–¥—è—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ:
    - x-ratelimit-remaining-tokens: —á–∏—Å–ª–æ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è —Ç–æ–∫–µ–Ω–æ–≤
    - x-ratelimit-reset-tokens: –≤—Ä–µ–º—è —Å–±—Ä–æ—Å–∞ –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, "820ms")
    
    Attributes:
        initial_limit (int): –ù–∞—á–∞–ª—å–Ω—ã–π –ª–∏–º–∏—Ç –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        remaining_tokens (int): –û—Å—Ç–∞–≤—à–∏–µ—Å—è —Ç–æ–∫–µ–Ω—ã –∏–∑ headers
        reset_time (int): Unix timestamp —Å–±—Ä–æ—Å–∞ –ª–∏–º–∏—Ç–∞
        
    Example:
        >>> bucket = TPMBucket(120000)
        >>> # –ü–æ—Å–ª–µ –∑–∞–ø—Ä–æ—Å–∞ –æ–±–Ω–æ–≤–ª—è–µ–º –∏–∑ headers
        >>> bucket.update_from_headers(response_headers)
        >>> # –ü–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–∏–º –∑–∞–ø—Ä–æ—Å–æ–º –ø—Ä–æ–≤–µ—Ä—è–µ–º
        >>> bucket.wait_if_needed(5000, safety_margin=0.15)
    """
    
    def __init__(self, initial_limit: int):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è TPM bucket.
        
        Args:
            initial_limit: –ù–∞—á–∞–ª—å–Ω—ã–π –ª–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤ –≤ –º–∏–Ω—É—Ç—É –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        """
        self.initial_limit = initial_limit
        self.remaining_tokens = initial_limit
        self.reset_time = None
        self.logger = logging.getLogger(__name__)
        
    def update_from_headers(self, headers: Dict[str, str]) -> None:
        """
        –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏–∑ response headers.
        
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç headers –≤ —Ñ–æ—Ä–º–∞—Ç–µ OpenAI:
        - remaining —Ç–æ–∫–µ–Ω—ã –∫–∞–∫ —á–∏—Å–ª–æ
        - reset –≤—Ä–µ–º—è –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö —Å —Å—É—Ñ—Ñ–∏–∫—Å–æ–º "ms"
        
        Args:
            headers: Headers –æ—Ç OpenAI API response –∏–ª–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π dict
            
        Note:
            OpenAI API –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Ä–µ–º—è —Å–±—Ä–æ—Å–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ "XXXms" (–º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥—ã).
            –ù–∞–ø—Ä–∏–º–µ—Ä: "820ms" –æ–∑–Ω–∞—á–∞–µ—Ç —Å–±—Ä–æ—Å —á–µ—Ä–µ–∑ 820 –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥.
            
            –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –≤—ã–ø–æ–ª–Ω—è–µ—Ç —Ä–æ–ª—å "reimburse" –≤ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–π bucket –ª–æ–≥–∏–∫–µ -
            —Å–æ—Å—Ç–æ—è–Ω–∏–µ –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –æ—Ç —Å–µ—Ä–≤–µ—Ä–∞.
        """
        # –û–±–Ω–æ–≤–ª—è–µ–º remaining tokens (consume —ç—Ñ—Ñ–µ–∫—Ç)
        if 'x-ratelimit-remaining-tokens' in headers:
            remaining = headers.get('x-ratelimit-remaining-tokens')
            if remaining:
                try:
                    old_remaining = self.remaining_tokens
                    self.remaining_tokens = int(remaining)
                    
                    # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π
                    tokens_consumed = old_remaining - self.remaining_tokens
                    if tokens_consumed > 0:
                        self.logger.debug(
                            f"TPM consumed: {tokens_consumed} tokens "
                            f"({old_remaining} ‚Üí {self.remaining_tokens})"
                        )
                    elif tokens_consumed < 0:
                        self.logger.debug(
                            f"TPM reimbursed: {-tokens_consumed} tokens "
                            f"({old_remaining} ‚Üí {self.remaining_tokens})"
                        )
                except (ValueError, TypeError) as e:
                    self.logger.warning(f"Failed to parse remaining tokens '{remaining}': {e}")
            
        # –û–±–Ω–æ–≤–ª—è–µ–º reset time
        if 'x-ratelimit-reset-tokens' in headers:
            reset_value = headers.get('x-ratelimit-reset-tokens')
            if reset_value and reset_value != '0ms':
                try:
                    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
                    if isinstance(reset_value, str):
                        if reset_value.endswith('ms'):
                            # –§–æ—Ä–º–∞—Ç "XXXms" - –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥—ã –¥–æ —Å–±—Ä–æ—Å–∞
                            reset_ms = int(reset_value.rstrip('ms'))
                            self.reset_time = int(time.time() + (reset_ms / 1000.0))
                            
                            # –õ–æ–≥–∏—Ä—É–µ–º –≤ —á–∏—Ç–∞–µ–º–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
                            reset_datetime = datetime.fromtimestamp(self.reset_time)
                            reset_in_seconds = reset_ms / 1000.0
                            self.logger.debug(
                                f"TPM reset in {reset_in_seconds:.1f}s "
                                f"(at {reset_datetime.strftime('%H:%M:%S')})"
                            )
                        elif reset_value.endswith('s'):
                            # –§–æ—Ä–º–∞—Ç "X.XXXs" - —Å–µ–∫—É–Ω–¥—ã –¥–æ —Å–±—Ä–æ—Å–∞
                            reset_seconds = float(reset_value.rstrip('s'))
                            self.reset_time = int(time.time() + reset_seconds)
                            
                            # –õ–æ–≥–∏—Ä—É–µ–º –≤ —á–∏—Ç–∞–µ–º–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
                            reset_datetime = datetime.fromtimestamp(self.reset_time)
                            self.logger.debug(
                                f"TPM reset in {reset_seconds:.1f}s "
                                f"(at {reset_datetime.strftime('%H:%M:%S')})"
                            )
                        else:
                            # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º unix timestamp
                            self.reset_time = int(float(reset_value))
                            reset_datetime = datetime.fromtimestamp(self.reset_time)
                            self.logger.debug(f"TPM reset time: {reset_datetime.strftime('%H:%M:%S')}")
                    else:
                        # –ï—Å–ª–∏ —É–∂–µ —á–∏—Å–ª–æ
                        self.reset_time = int(reset_value)
                        reset_datetime = datetime.fromtimestamp(self.reset_time)
                        self.logger.debug(f"TPM reset time: {reset_datetime.strftime('%H:%M:%S')}")
                        
                except (ValueError, TypeError) as e:
                    self.logger.warning(f"Failed to parse reset time '{reset_value}': {e}")
                    self.reset_time = None
    
    def wait_if_needed(self, required_tokens: int, safety_margin: float = 0.15) -> None:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤ –∏ –æ–∂–∏–¥–∞–Ω–∏–µ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏.
        
        –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –≤—ã–ø–æ–ª–Ω—è–µ—Ç —Ä–æ–ª—å "consume" –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–π bucket –ª–æ–≥–∏–∫–µ -
        –ø—Ä–æ–≤–µ—Ä—è–µ—Ç, –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ª–∏ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞, –∏ –∂–¥—ë—Ç –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–∏
        –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏.
        
        Args:
            required_tokens: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–µ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            safety_margin: –ó–∞–ø–∞—Å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 15%)
            
        Note:
            –ü–æ—Å–ª–µ –æ–∂–∏–¥–∞–Ω–∏—è —Å–±—Ä–æ—Å–∞ –ª–∏–º–∏—Ç–∞, remaining_tokens —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è –≤
            initial_limit, –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞—è –ø–æ–ª–Ω–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ bucket.
        """
        required_with_margin = int(required_tokens * (1 + safety_margin))
        
        self.logger.debug(
            f"TPM check: need {required_with_margin} tokens "
            f"(base: {required_tokens} + margin: {int(required_tokens * safety_margin)}), "
            f"have {self.remaining_tokens}"
        )
        
        if self.remaining_tokens >= required_with_margin:
            self.logger.debug("TPM check passed: sufficient tokens available")
            return
            
        # –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤
        if self.reset_time:
            current_time = time.time()
            wait_time = self.reset_time - current_time
            
            if wait_time > 0:
                # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
                if wait_time < 60:
                    wait_str = f"{wait_time:.1f}s"
                else:
                    minutes = int(wait_time // 60)
                    seconds = int(wait_time % 60)
                    wait_str = f"{minutes}m {seconds}s"
                    
                self.logger.info(
                    f"TPM limit reached: {self.remaining_tokens}/{required_with_margin} tokens. "
                    f"Waiting {wait_str} until reset..."
                )
                
                # –ñ–¥—ë–º —Å –Ω–µ–±–æ–ª—å—à–∏–º –∑–∞–ø–∞—Å–æ–º –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏
                time.sleep(wait_time + 0.1)
                
                # –ü–æ—Å–ª–µ –æ–∂–∏–¥–∞–Ω–∏—è –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–π –ª–∏–º–∏—Ç
                old_remaining = self.remaining_tokens
                self.remaining_tokens = self.initial_limit
                self.logger.info(
                    f"TPM limit reset: tokens restored "
                    f"({old_remaining} ‚Üí {self.remaining_tokens})"
                )
                # –î–æ–±–∞–≤–ª—è–µ–º –≤—ã–≤–æ–¥ –≤ –∫–æ–Ω—Å–æ–ª—å
                current_time = datetime.now().strftime("%H:%M:%S")
                print(f"[{current_time}] INFO     | ‚úÖ TPM limit reset, continuing...")
            else:
                # Reset time —É–∂–µ –ø—Ä–æ—à–ª–æ, –º–æ–∂–Ω–æ —Å—á–∏—Ç–∞—Ç—å —á—Ç–æ –ª–∏–º–∏—Ç –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
                self.logger.debug("TPM reset time already passed, assuming limit restored")
                self.remaining_tokens = self.initial_limit
        else:
            # –ù–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ reset time
            self.logger.warning(
                f"TPM limit low ({self.remaining_tokens}/{required_with_margin}), "
                f"but no reset time available. Proceeding anyway..."
            )


class IncompleteResponseError(Exception):
    """Raised when response generation is incomplete (hit token limit)."""
    pass


class OpenAIClient:
    """
    –ö–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å OpenAI Responses API.
    
    –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç:
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ previous_response_id –¥–ª—è —Ü–µ–ø–æ—á–µ–∫
    - –ü–æ–¥–¥–µ—Ä–∂–∫—É reasoning –º–æ–¥–µ–ª–µ–π —Å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    - Exponential backoff retry –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö
    - –ö–æ–Ω—Ç—Ä–æ–ª—å –ª–∏–º–∏—Ç–æ–≤ —á–µ—Ä–µ–∑ response headers (x-ratelimit-*)
    
    Attributes:
        config (dict): –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞
        client (OpenAI): –≠–∫–∑–µ–º–ø–ª—è—Ä OpenAI –∫–ª–∏–µ–Ω—Ç–∞
        tpm_bucket (TPMBucket): –ö–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä –ª–∏–º–∏—Ç–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤
        last_response_id (str): ID –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —É—Å–ø–µ—à–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
        is_reasoning_model (bool): True –¥–ª—è o* –º–æ–¥–µ–ª–µ–π
        encoder: –¢–æ–∫–µ–Ω–∞–π–∑–µ—Ä –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–¥—Å—á—ë—Ç–∞
        
    Example:
        >>> config = {
        ...     'api_key': 'sk-...',
        ...     'model': 'gpt-4o',
        ...     'tpm_limit': 120000,
        ...     'tpm_safety_margin': 0.15,
        ...     'max_completion': 4096,
        ...     'timeout': 45,
        ...     'max_retries': 6,
        ...     'temperature': 1.0
        ... }
        >>> client = OpenAIClient(config)
        >>> 
        >>> # –ü–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å
        >>> text1, id1, usage1 = client.create_response(
        ...     "You are a helpful assistant",
        ...     "Hello!"
        ... )
        >>> 
        >>> # –í—Ç–æ—Ä–æ–π –∑–∞–ø—Ä–æ—Å —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –ø–µ—Ä–≤–æ–≥–æ
        >>> text2, id2, usage2 = client.create_response(
        ...     "Continue being helpful",
        ...     "What's the weather?"
        ... )
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OpenAI –∫–ª–∏–µ–Ω—Ç–∞.
        
        Args:
            config: –°–ª–æ–≤–∞—Ä—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å–æ —Å–ª–µ–¥—É—é—â–∏–º–∏ –∫–ª—é—á–∞–º–∏:
                - api_key (str): –ö–ª—é—á OpenAI API
                - model (str): –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ (gpt-4o, o1-preview –∏ —Ç.–¥.)
                - tpm_limit (int): –õ–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤ –≤ –º–∏–Ω—É—Ç—É
                - tpm_safety_margin (float): –ó–∞–ø–∞—Å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ (default 0.15)
                - max_completion (int): –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                - timeout (int): –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
                - max_retries (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫
                - temperature (float, optional): –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –æ–±—ã—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
                - reasoning_effort (str, optional): –£—Ä–æ–≤–µ–Ω—å –¥–ª—è reasoning –º–æ–¥–µ–ª–µ–π
                - reasoning_summary (str, optional): –¢–∏–ø summary –¥–ª—è reasoning
        """
        self.config = config
        self.client = OpenAI(
            api_key=config['api_key'],
            timeout=config.get('timeout', 60.0)
        )
        self.tpm_bucket = TPMBucket(config['tpm_limit'])
        self.logger = logging.getLogger(__name__)
        
        # –ü–æ—Å–ª–µ–¥–Ω–∏–π response_id –¥–ª—è chain of responses
        self.last_response_id: Optional[str] = None
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –º–æ–¥–µ–ª—å reasoning (o*)
        self.is_reasoning_model = config['model'].startswith('o')
        self.logger.info(f"Initialized OpenAI client: model={config['model']}, reasoning={self.is_reasoning_model}")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–¥—Å—á—ë—Ç–∞
        self.encoder = tiktoken.get_encoding("o200k_base")
    
    def _prepare_request_params(self, instructions: str, input_data: str, 
                              previous_response_id: Optional[str] = None) -> Dict[str, Any]:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è Responses API.
        
        Args:
            instructions: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç (–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è –º–æ–¥–µ–ª–∏)
            input_data: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –≤–≤–æ–¥
            previous_response_id: ID –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            
        Returns:
            dict: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è responses.create()
        """
        params = {
            'model': self.config['model'],
            'instructions': instructions,
            'input': input_data,
            'max_output_tokens': self.config['max_completion'],
            'store': True,  # –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–ª—è Responses API
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º previous_response_id –µ—Å–ª–∏ –µ—Å—Ç—å
        if previous_response_id:
            params['previous_response_id'] = previous_response_id
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è reasoning –º–æ–¥–µ–ª–µ–π (o*)
        if self.is_reasoning_model:
            params['reasoning'] = {
                'effort': self.config.get('reasoning_effort', 'medium'),
                'summary': self.config.get('reasoning_summary', 'auto')
            }
            # –î–ª—è reasoning –º–æ–¥–µ–ª–µ–π –ù–ï —É–∫–∞–∑—ã–≤–∞–µ–º temperature
        else:
            # –î–ª—è –æ–±—ã—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —É–∫–∞–∑—ã–≤–∞–µ–º temperature
            params['temperature'] = self.config.get('temperature', 1.0)
        
        return params
    
    def _clean_json_response(self, response_text: str) -> str:
        """
        –û—á–∏—Å—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞ –æ—Ç markdown –æ–±–µ—Ä—Ç–æ–∫ –∏ –¥—Ä—É–≥–∏—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤.
        
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —É–¥–∞–ª—è–µ—Ç:
        - ```json...``` –æ–±–µ—Ä—Ç–∫–∏
        - ```...``` –æ–±–µ—Ä—Ç–∫–∏ –±–µ–∑ —è–∑—ã–∫–∞
        - –õ–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –≤ –Ω–∞—á–∞–ª–µ/–∫–æ–Ω—Ü–µ
        
        Args:
            response_text: –°—ã—Ä–æ–π —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç –º–æ–¥–µ–ª–∏
            
        Returns:
            str: –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        text = response_text.strip()
        
        # –£–¥–∞–ª—è–µ–º ```json...``` –æ–±–µ—Ä—Ç–∫–∏
        if text.startswith('```json') and text.endswith('```'):
            text = text[7:-3].strip()  # –£–±–∏—Ä–∞–µ–º ```json –≤ –Ω–∞—á–∞–ª–µ –∏ ``` –≤ –∫–æ–Ω—Ü–µ
            self.logger.debug("Removed ```json wrapper from response")
        
        # –£–¥–∞–ª—è–µ–º –æ–±—ã—á–Ω—ã–µ ``` –æ–±–µ—Ä—Ç–∫–∏
        elif text.startswith('```') and text.endswith('```'):
            text = text[3:-3].strip()  # –£–±–∏—Ä–∞–µ–º ``` –≤ –Ω–∞—á–∞–ª–µ –∏ –≤ –∫–æ–Ω—Ü–µ
            self.logger.debug("Removed ``` wrapper from response")
        
        return text

    def _extract_response_content(self, response) -> str:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –æ—Ç–≤–µ—Ç–∞ –∏–∑ Responses API.
        
        –î–ª—è reasoning –º–æ–¥–µ–ª–µ–π –æ–∂–∏–¥–∞–µ—Ç—Å—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞:
        - output[0]: reasoning
        - output[1]: message
        
        –î–ª—è –æ–±—ã—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π:
        - output[0]: message
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –æ—Ç–≤–µ—Ç–∞
            if hasattr(response, 'status'):
                # –†–∞–∑—Ä–µ—à–∞–µ–º –∫–∞–∫ completed, —Ç–∞–∫ –∏ incomplete —Å—Ç–∞—Ç—É—Å—ã
                if response.status not in ['completed', 'incomplete']:
                    raise ValueError(f"Response has unexpected status: {response.status}")
                
                # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –¥–ª—è incomplete
                if response.status == 'incomplete':
                    reason = None
                    if hasattr(response, 'incomplete_details') and response.incomplete_details:
                        reason = getattr(response.incomplete_details, 'reason', 'unknown')
                    self.logger.warning(
                        f"Response is incomplete (reason: {reason}). "
                        f"This may happen with low max_output_tokens limits."
                    )
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ output
            if not response.output or len(response.output) == 0:
                raise ValueError("Response has empty output")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–Ω–¥–µ–∫—Å message –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –º–æ–¥–µ–ª–∏
            if self.is_reasoning_model:
                # –£ reasoning –º–æ–¥–µ–ª–µ–π –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –º–∏–Ω–∏–º—É–º 2 —ç–ª–µ–º–µ–Ω—Ç–∞
                # output[0] - reasoning, output[1] - message
                if len(response.output) < 2:
                    raise ValueError(f"Reasoning model returned insufficient output items: {len(response.output)}")
                message_output = response.output[1]
            else:
                # –£ –æ–±—ã—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π message –≤ –ø–µ—Ä–≤–æ–º —ç–ª–µ–º–µ–Ω—Ç–µ
                message_output = response.output[0]
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø output —ç–ª–µ–º–µ–Ω—Ç–∞
            if not hasattr(message_output, 'type') or message_output.type != 'message':
                raise ValueError(f"Expected message output, got type: {getattr(message_output, 'type', 'unknown')}")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ content
            if not hasattr(message_output, 'content') or not message_output.content:
                raise ValueError("Message has no content")
            
            # content - —ç—Ç–æ –º–∞—Å—Å–∏–≤, –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç —Å —Ç–∏–ø–æ–º output_text
            text_content = None
            refusal_content = None

            for content_item in message_output.content:
                if hasattr(content_item, 'type'):
                    if content_item.type == 'output_text':
                        if hasattr(content_item, 'text'):
                            text_content = content_item.text
                            break
                    elif content_item.type == 'refusal':
                        if hasattr(content_item, 'refusal'):
                            refusal_content = content_item.refusal

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º refusal –ø–µ—Ä–≤—ã–º
            if refusal_content is not None:
                raise ValueError(f"Model refused to respond: {refusal_content}")
                
            if text_content is None:
                raise ValueError("No text content found in message")
            
            # –û—á–∏—â–∞–µ–º –æ—Ç –≤–æ–∑–º–æ–∂–Ω—ã—Ö markdown –æ–±–µ—Ä—Ç–æ–∫
            cleaned_text = self._clean_json_response(text_content)
            
            return cleaned_text
            
        except Exception as e:
            self.logger.error(f"Failed to extract response content: {e}")
            self.logger.debug(f"Response structure: {response}")
            raise

    def _extract_usage_info(self, response) -> ResponseUsage:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∞—Ö.
        
        Args:
            response: –û–±—ä–µ–∫—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç OpenAI API
            
        Returns:
            ResponseUsage: –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ç–æ–∫–µ–Ω–∞—Ö
        """
        try:
            usage = response.usage
            reasoning_tokens = 0
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º reasoning —Ç–æ–∫–µ–Ω—ã –µ—Å–ª–∏ –µ—Å—Ç—å
            if hasattr(usage, 'output_tokens_details') and usage.output_tokens_details:
                reasoning_tokens = getattr(usage.output_tokens_details, 'reasoning_tokens', 0)
            
            return ResponseUsage(
                input_tokens=usage.input_tokens,
                output_tokens=usage.output_tokens,
                total_tokens=usage.total_tokens,
                reasoning_tokens=reasoning_tokens
            )
        except AttributeError as e:
            self.logger.warning(f"Could not extract usage info: {e}")
            return ResponseUsage(0, 0, 0, 0)

    def _update_tpm_via_probe(self) -> None:
            """
            –ü–æ–ª—É—á–µ–Ω–∏–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö rate limit –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ probe –∑–∞–ø—Ä–æ—Å.
            
            –í—ã–ø–æ–ª–Ω—è–µ—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∫ —Å–∞–º–æ–π –¥–µ—à–µ–≤–æ–π –º–æ–¥–µ–ª–∏
            –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è response headers —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ª–∏–º–∏—Ç–∞—Ö.
            
            Note:
                –í background —Ä–µ–∂–∏–º–µ (async) OpenAI –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç rate limit headers,
                –ø–æ—ç—Ç–æ–º—É –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–µ probe –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –∞–∫—Ç—É–∞–ª–∏–∑–∞—Ü–∏–∏.
            """
            try:
                self.logger.debug("Executing TPM probe request")
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º with_raw_response –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ headers
                raw = self.client.responses.with_raw_response.create(
                    model="gpt-4.1-nano-2025-04-14",
                    input="2+2=?",  # –ü—Ä–æ—Å—Ç–æ–π –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å
                    max_output_tokens=20,  # –ú–∏–Ω–∏–º—É–º 16, —Å—Ç–∞–≤–∏–º 20 —Å –∑–∞–ø–∞—Å–æ–º
                    temperature=0.1,  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏
                    background=False  # –í–ê–ñ–ù–û: —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Ä–µ–∂–∏–º!
                )
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º headers
                headers = dict(raw.headers)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º TPM bucket
                self.tpm_bucket.update_from_headers(headers)
                
                self.logger.debug(
                    f"TPM probe successful: remaining={self.tpm_bucket.remaining_tokens}, "
                    f"reset_time={self.tpm_bucket.reset_time}"
                )
                
            except Exception as e:
                self.logger.warning(f"TPM probe failed: {e}")
                # –ù–µ –∫—Ä–∏—Ç–∏—á–Ω–æ - –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å —Ç–µ–∫—É—â–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏

    def _create_response_async(self, instructions: str, input_data: str, 
                             previous_response_id: Optional[str] = None) -> Tuple[str, str, ResponseUsage]:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ response –≤ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–º —Ä–µ–∂–∏–º–µ —Å polling.
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç background=True –¥–ª—è –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –ø–æ–ª—É—á–µ–Ω–∏—è response_id,
        –∑–∞—Ç–µ–º –≤—ã–ø–æ–ª–Ω—è–µ—Ç polling –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.
        
        Args:
            instructions: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç (–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è –º–æ–¥–µ–ª–∏)
            input_data: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ (—Ç–µ–∫—Å—Ç –∏–ª–∏ JSON)
            previous_response_id: ID –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            
        Returns:
            tuple: (response_text, response_id, usage_info)
            
        Raises:
            TimeoutError: –ü—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏ –æ–±—â–µ–≥–æ timeout
            IncompleteResponseError: –ü—Ä–∏ incomplete —Å—Ç–∞—Ç—É—Å–µ
            ValueError: –ü—Ä–∏ failed —Å—Ç–∞—Ç—É—Å–µ –∏–ª–∏ –æ—Ç–∫–∞–∑–µ –º–æ–¥–µ–ª–∏
        """
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∫–∞–∫ –æ–±—ã—á–Ω–æ
        params = self._prepare_request_params(instructions, input_data, previous_response_id)
        
        # –í–ê–ñ–ù–û: –≤–∫–ª—é—á–∞–µ–º background —Ä–µ–∂–∏–º
        params['background'] = True
        
        # –¢–æ—á–Ω—ã–π –ø–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ª–∏–º–∏—Ç–æ–≤
        full_prompt = instructions + "\n\n" + input_data
        estimated_input_tokens = len(self.encoder.encode(full_prompt))
        required_tokens = estimated_input_tokens + self.config['max_completion']

        # –û–±–Ω–æ–≤–ª—è–µ–º TPM –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ probe –ø–µ—Ä–µ–¥ –æ—Å–Ω–æ–≤–Ω—ã–º –∑–∞–ø—Ä–æ—Å–æ–º
        # –í async —Ä–µ–∂–∏–º–µ OpenAI –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç rate limit headers,
        # –ø–æ—ç—Ç–æ–º—É –∏—Å–ø–æ–ª—å–∑—É–µ–º probe –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∞–∫—Ç—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        self._update_tpm_via_probe()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤ —Å safety margin
        safety_margin = self.config.get('tpm_safety_margin', 0.15)
        self.tpm_bucket.wait_if_needed(required_tokens, safety_margin)
        
        retry_count = 0
        last_exception = None
        
        while retry_count <= self.config['max_retries']:
            try:
                self.logger.debug(f"Async API request attempt {retry_count + 1}: model={params['model']}, "
                                f"estimated_tokens={required_tokens}, previous_response_id={params.get('previous_response_id')}")
                
                # –®–∞–≥ 1: –°–æ–∑–¥–∞–µ–º background –∑–∞–ø—Ä–æ—Å
                raw_initial = self.client.responses.with_raw_response.create(**params)
                initial_headers = raw_initial.headers
                initial_response = raw_initial.parse()
                response_id = initial_response.id
                
                self.logger.info(f"Background response created: {response_id[:8]}...")
                
                # –û–±–Ω–æ–≤–ª—è–µ–º TPM –∏–∑ –Ω–∞—á–∞–ª—å–Ω—ã—Ö headers
                # –ó–ê–ö–û–ú–ú–ï–ù–¢–ò–†–û–í–ê–ù–û: –≤ async —Ä–µ–∂–∏–º–µ headers –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç rate limit info
                # if initial_headers:
                #    self.tpm_bucket.update_from_headers(initial_headers)
                
                # –®–∞–≥ 2: Polling loop
                start_time = time.time()
                poll_count = 0
                poll_interval = self.config.get('poll_interval', 5)
                
                while True:
                    elapsed = time.time() - start_time
                    
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ hard timeout
                    if elapsed > self.config['timeout']:
                        self.logger.warning(f"Response generation exceeded timeout ({self.config['timeout']}s), cancelling...")
                        try:
                            self.client.responses.cancel(response_id)
                            self.logger.info(f"Successfully cancelled response {response_id[:12]}")
                        except Exception as e:
                            # Race condition: response –º–æ–∂–µ—Ç —É–∂–µ –±—ã—Ç—å completed/failed
                            self.logger.debug(f"Could not cancel {response_id[:12]}: {e}")
                        
                        raise TimeoutError(f"Response generation exceeded {self.config['timeout']}s timeout")
                    
                    # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å
                    try:
                        raw_status = self.client.responses.with_raw_response.retrieve(response_id)
                        status_headers = raw_status.headers
                        response = raw_status.parse()
                    except Exception as e:
                        self.logger.error(f"Failed to retrieve response status: {e}")
                        raise
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º TPM –∏–∑ headers —Å—Ç–∞—Ç—É—Å–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
                    # –ó–ê–ö–û–ú–ú–ï–ù–¢–ò–†–û–í–ê–ù–û: –≤ async —Ä–µ–∂–∏–º–µ headers –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç rate limit info
                    # if status_headers:
                    #     self.tpm_bucket.update_from_headers(status_headers)
                    
                    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—É—Å–æ–≤
                    status = response.status
                    
                    if status == 'completed':

                        # –£—Å–ø–µ—à–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ
                        self.logger.info(f"Response {response_id[:12]} completed successfully")
                        
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                        response_text = self._extract_response_content(response)
                        usage_info = self._extract_usage_info(response)
                        
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º response_id –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –≤—ã–∑–æ–≤–∞
                        self.last_response_id = response_id
                        
                        self.logger.debug(f"Async response success: id={response_id}, "
                                        f"tokens={usage_info.total_tokens} "
                                        f"(reasoning: {usage_info.reasoning_tokens})")
                        
                        return response_text, response_id, usage_info
                    
                    elif status == 'incomplete':
                        # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ - –æ—Ç–≤–µ—Ç –æ–±—Ä–µ–∑–∞–Ω
                        reason = "unknown"
                        if hasattr(response, 'incomplete_details') and response.incomplete_details:
                            reason = getattr(response.incomplete_details, 'reason', 'unknown')
                        
                        # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å —á–∞—Å—Ç–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
                        partial_tokens = 0
                        if hasattr(response, 'usage') and response.usage:
                            partial_tokens = response.usage.output_tokens
                        
                        self.logger.error(
                            f"Response {response_id[:12]} incomplete: {reason}. "
                            f"Generated {partial_tokens} tokens before hitting limit."
                        )
                        
                        # –í—ã–≤–æ–¥–∏–º –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª
                        current_time = datetime.now().strftime("%H:%M:%S")
                        print(f"[{current_time}] ERROR    | ‚ùå Response incomplete: {reason}")
                        print(f"[{current_time}] ERROR    |    Generated only {partial_tokens} tokens")
                        
                        # –î–ª—è reasoning –º–æ–¥–µ–ª–µ–π —ç—Ç–æ –æ—Å–æ–±–µ–Ω–Ω–æ –∫—Ä–∏—Ç–∏—á–Ω–æ
                        if self.is_reasoning_model and reason == "max_output_tokens":
                            current_time = datetime.now().strftime("%H:%M:%S")
                            print(f"[{current_time}] HINT     | üí° Reasoning model needs more tokens. "
                                f"Current limit: {self.config['max_completion']}")
                        
                        # –ë—Ä–æ—Å–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ retry –ª–æ–≥–∏–∫–µ
                        raise IncompleteResponseError(
                            f"Response generation incomplete ({reason}). "
                            f"Generated only {partial_tokens} tokens."
                        )
                    
                    elif status == 'failed':
                        # –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                        error_msg = "Unknown error"
                        if hasattr(response, 'error') and response.error:
                            error_msg = getattr(response.error, 'message', str(response.error))
                        
                        self.logger.error(f"Response {response_id[:12]} failed: {error_msg}")
                        
                        raise ValueError(f"Response generation failed: {error_msg}")
                    
                    elif status == 'cancelled':
                        # –ù–µ –¥–æ–ª–∂–Ω–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç—å, –µ—Å–ª–∏ –º—ã —Å–∞–º–∏ –Ω–µ –æ—Ç–º–µ–Ω–∏–ª–∏
                        self.logger.error(f"Response {response_id[:12]} was cancelled unexpectedly")
                        raise ValueError("Response was cancelled")

                    elif status == 'queued':
                        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–π —Å—Ç–∞—Ç—É—Å —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ä–∞–∑
                        if poll_count == 0:
                            current_time = datetime.now().strftime("%H:%M:%S")
                            print(f"[{current_time}] QUEUE    | ‚è≥ Response {response_id[:12]}... in progress")
                        
                        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 3 –ø—Ä–æ–≤–µ—Ä–∫–∏ (~21 —Å–µ–∫ –ø—Ä–∏ poll_interval=7)
                        elif poll_count > 0 and poll_count % 3 == 0:
                            elapsed_time = int(time.time() - start_time)
                            current_time = datetime.now().strftime("%H:%M:%S")
                            
                            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –≤—Ä–µ–º—è
                            if elapsed_time < 60:
                                time_str = f"{elapsed_time}s"
                            else:
                                minutes = elapsed_time // 60
                                seconds = elapsed_time % 60
                                time_str = f"{minutes}m {seconds}s"
                            
                            print(f"[{current_time}] PROGRESS | ‚è≥ Elapsed: {time_str}")
                        
                        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª polling
                        if poll_count < 3:
                            time.sleep(2)  # –ü–µ—Ä–≤—ã–µ 3 —Ä–∞–∑–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –±—ã—Å—Ç—Ä–æ
                        else:
                            time.sleep(poll_interval)  # –ü–æ—Ç–æ–º –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª
                        
                        poll_count += 1
                        continue
                    
                    else:
                        # –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å—Ç–∞—Ç—É—Å
                        self.logger.error(f"Unknown response status: {status}")
                        raise ValueError(f"Unknown response status: {status}")

            except (openai.RateLimitError, IncompleteResponseError) as e:
                # –≠—Ç–∏ –æ—à–∏–±–∫–∏ –º–æ–∂–µ–º retry
                retry_count += 1
                if retry_count > self.config['max_retries']:
                    self.logger.error(f"Error after all retries: {type(e).__name__}: {e}")
                    raise e
                
                # –î–ª—è IncompleteResponseError —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ª–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤
                if isinstance(e, IncompleteResponseError):
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π –ª–∏–º–∏—Ç –ø–µ—Ä–µ–¥ –∏–∑–º–µ–Ω–µ–Ω–∏–µ–º
                    old_limit = params.get('max_output_tokens', self.config['max_completion'])
                    if retry_count == 1:
                        params['max_output_tokens'] = int(old_limit * 1.5)
                    elif retry_count == 2:
                        params['max_output_tokens'] = int(old_limit * 2.0)
                    else:
                        # –ë–æ–ª—å—à–µ –Ω–µ —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º
                        raise ValueError(
                            f"Response still incomplete after {retry_count} retries. "
                            f"Max tokens tried: {params['max_output_tokens']}"
                        )
                    
                    self.logger.info(
                        f"Retrying with increased token limit: {old_limit} ‚Üí {params['max_output_tokens']}"
                    )
                    current_time = datetime.now().strftime("%H:%M:%S")
                    print(f"[{current_time}] RETRY    | üîÑ Increasing token limit: {old_limit} ‚Üí {params['max_output_tokens']}")
                
                wait_time = 20 * (2 ** (retry_count - 1))
                self.logger.warning(f"{type(e).__name__}, retry {retry_count}/{self.config['max_retries']} in {wait_time}s")
                current_time = datetime.now().strftime("%H:%M:%S")
                print(f"[{current_time}] RETRY    | ‚è≥ Waiting {wait_time}s before retry {retry_count}/{self.config['max_retries']}...")
                time.sleep(wait_time)
                last_exception = e
                
            except TimeoutError as e:
                # Timeout –Ω–µ retry - —Å—Ä–∞–∑—É –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ–º
                raise e
                
            except Exception as e:
                # –î—Ä—É–≥–∏–µ –æ—à–∏–±–∫–∏ —Ç–æ–∂–µ –º–æ–∂–µ–º –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å retry
                retry_count += 1
                if retry_count > self.config['max_retries']:
                    self.logger.error(f"Error after all retries: {type(e).__name__}: {e}")
                    raise e
                
                wait_time = 20 * (2 ** (retry_count - 1))
                self.logger.warning(f"{type(e).__name__}, retry {retry_count}/{self.config['max_retries']} in {wait_time}s: {e}")
                time.sleep(wait_time)
                last_exception = e
        
        # –ï—Å–ª–∏ –¥–æ—à–ª–∏ —Å—é–¥–∞ - –∏—Å—á–µ—Ä–ø–∞–Ω—ã –≤—Å–µ retry
        raise last_exception or Exception("Max retries exceeded")

    def create_response(self, instructions: str, input_data: str, 
                       previous_response_id: Optional[str] = None) -> Tuple[str, str, ResponseUsage]:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ response —á–µ—Ä–µ–∑ OpenAI Responses API.
        
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏:
        - –£–ø—Ä–∞–≤–ª—è–µ—Ç previous_response_id –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        - –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ª–∏–º–∏—Ç—ã —á–µ—Ä–µ–∑ headers —Å safety margin
        - –í—ã–ø–æ–ª–Ω—è–µ—Ç retry —Å exponential backoff –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö
        - –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–ø–µ—Ü–∏—Ñ–∏–∫—É reasoning –º–æ–¥–µ–ª–µ–π
        - –û–±–Ω–æ–≤–ª—è–µ—Ç TPM bucket –∏–∑ response headers
        
        Args:
            instructions: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç (–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è –º–æ–¥–µ–ª–∏)
            input_data: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ (—Ç–µ–∫—Å—Ç –∏–ª–∏ JSON)
            previous_response_id: ID –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –æ—Ç–≤–µ—Ç–∞ (–µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω,
                –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π last_response_id)
            
        Returns:
            tuple: (response_text, response_id, usage_info)
                - response_text (str): –¢–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏
                - response_id (str): ID –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ —Ü–µ–ø–æ—á–∫–µ
                - usage_info (ResponseUsage): –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∞—Ö
                
        Raises:
            openai.RateLimitError: –ü—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏ rate limit –ø–æ—Å–ª–µ –≤—Å–µ—Ö retry
            openai.APIError: –ü—Ä–∏ –æ—à–∏–±–∫–∞—Ö API –ø–æ—Å–ª–µ –≤—Å–µ—Ö retry
            ValueError: –ü—Ä–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–º –æ—Ç–≤–µ—Ç–µ –∏–ª–∏ –æ—Ç–∫–∞–∑–µ –º–æ–¥–µ–ª–∏
            
        Example:
            >>> # –ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å
            >>> text, resp_id, usage = client.create_response(
            ...     "You are a helpful assistant",
            ...     "What is 2+2?"
            ... )
            >>> print(f"Response: {text}")
            >>> print(f"Used {usage.total_tokens} tokens")
            >>> 
            >>> # –¶–µ–ø–æ—á–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
            >>> text2, resp_id2, usage2 = client.create_response(
            ...     "Continue the conversation",
            ...     "And what is 3+3?",
            ...     previous_response_id=resp_id
            ... )
        """
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π previous_response_id –∏–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π
        if previous_response_id is None:
            previous_response_id = self.last_response_id
            
        # –í—ã–∑—ã–≤–∞–µ–º –Ω–æ–≤—ã–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –º–µ—Ç–æ–¥
        return self._create_response_async(instructions, input_data, previous_response_id)

    def repair_response(self, instructions: str, input_data: str) -> Tuple[str, str, ResponseUsage]:
        """
        Repair –∑–∞–ø—Ä–æ—Å —Å —Ç–µ–º –∂–µ previous_response_id.
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ–≥–¥–∞ LLM –≤–µ—Ä–Ω—É–ª –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç (–Ω–∞–ø—Ä–∏–º–µ—Ä,
        –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON). –î–æ–±–∞–≤–ª—è–µ—Ç –∫ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ
        –≤–µ—Ä–Ω—É—Ç—å —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–π JSON –∏ –ø–æ–≤—Ç–æ—Ä—è–µ—Ç –∑–∞–ø—Ä–æ—Å —Å —Ç–µ–º –∂–µ
        –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º.
        
        Args:
            instructions: –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            input_data: –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
            
        Returns:
            tuple: (response_text, response_id, usage_info)
                –°–º. create_response() –¥–ª—è –¥–µ—Ç–∞–ª–µ–π
                
        Example:
            >>> try:
            ...     text, resp_id, usage = client.create_response(prompt, data)
            ...     result = json.loads(text)  # –ú–æ–∂–µ—Ç —É–ø–∞—Å—Ç—å
            ... except json.JSONDecodeError:
            ...     # –ü—Ä–æ–±—É–µ–º repair
            ...     text, resp_id, usage = client.repair_response(prompt, data)
            ...     result = json.loads(text)  # –î–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤–∞–ª–∏–¥–Ω—ã–π JSON
        """
        repair_instructions = (
            instructions + 
            "\n\nIMPORTANT: Return **valid JSON** only. "
            "Do NOT use markdown formatting like ```json```. "
            "Do NOT include any explanatory text. "
            "Return ONLY the raw JSON object."
        )
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ previous_response_id —á—Ç–æ –∏ –≤ –ø—Ä–µ–¥—ã–¥—É—â–µ–º –∑–∞–ø—Ä–æ—Å–µ
        return self.create_response(repair_instructions, input_data, self.last_response_id)