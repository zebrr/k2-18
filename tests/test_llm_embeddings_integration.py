"""
–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã –¥–ª—è LLM Embeddings.

–¢—Ä–µ–±—É—é—Ç —Ä–µ–∞–ª—å–Ω–æ–≥–æ API –∫–ª—é—á–∞ OpenAI. –ó–∞–ø—É—Å–∫–∞—Ç—å —Å –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è:
OPENAI_API_KEY=sk-...
python -m pytest tests/test_llm_embeddings_integration.py -v

Windows:
set OPENAI_API_KEY=sk-proj-...
echo %OPENAI_API_KEY%

–ò–ª–∏ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –µ—Å–ª–∏ –∫–ª—é—á–∞ –Ω–µ—Ç:
python -m pytest tests/test_llm_embeddings_integration.py -v -m "not integration"
"""

import os
import sys
import time
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ src –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –º–æ–¥—É–ª–µ–π
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

import numpy as np
import pytest

from utils.llm_embeddings import EmbeddingsClient, cosine_similarity_batch

# –ü–æ–º–µ—Ç–∫–∞ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤
pytestmark = pytest.mark.integration


@pytest.fixture
def api_key():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ API –∫–ª—é—á–∞ –∏–∑ –æ–∫—Ä—É–∂–µ–Ω–∏—è."""
    key = os.environ.get("OPENAI_API_KEY")
    if not key:
        pytest.skip("OPENAI_API_KEY not set")
    return key


@pytest.fixture
def config(api_key):
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–æ–≤."""
    return {
        "embedding_api_key": api_key,
        "embedding_model": "text-embedding-3-small",
        "embedding_tpm_limit": 5000000,
        "max_retries": 3,
        "max_batch_tokens": 100000,
        "max_texts_per_batch": 2048,
        "truncate_tokens": 8000,
    }


@pytest.fixture
def client(config):
    """–°–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∏–µ–Ω—Ç–∞ –¥–ª—è —Ç–µ—Å—Ç–æ–≤."""
    return EmbeddingsClient(config)


@pytest.fixture
def test_texts():
    """–¢–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö."""
    return {
        "english_short": "The quick brown fox jumps over the lazy dog",
        "russian_short": "–í –ª–µ—Å—É —Ä–æ–¥–∏–ª–∞—Å—å —ë–ª–æ—á–∫–∞, –≤ –ª–µ—Å—É –æ–Ω–∞ —Ä–æ—Å–ª–∞",
        "english_medium": (
            "Machine learning is a subset of artificial intelligence that focuses on "
            "the use of data and algorithms to imitate the way that humans learn, "
            "gradually improving its accuracy."
        ),
        "russian_medium": (
            "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ ‚Äî —ç—Ç–æ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –∫–æ—Ç–æ—Ä–æ–µ "
            "—Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ "
            "—Å–ø–æ—Å–æ–±–∞ –æ–±—É—á–µ–Ω–∏—è —á–µ–ª–æ–≤–µ–∫–∞, –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É–ª—É—á—à–∞—è —Å–≤–æ—é —Ç–æ—á–Ω–æ—Å—Ç—å."
        ),
        "mixed": (
            "Python —è–≤–ª—è–µ—Ç—Å—è –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–º —è–∑—ã–∫–æ–º –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è. "
            "It is widely used in data science and –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏."
        ),
        "code": "def hello_world():\n    print('Hello, World!')\n    return 42",
        "empty": "",
        "special_chars": "–°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã: @#$%^&*()_+{}[]|\\:;\"'<>,.?/~`¬±¬ß",
        "unicode": "–≠–º–æ–¥–∑–∏: üöÄ üåü üî• | –°–∏–º–≤–æ–ª—ã: Œ± Œ≤ Œ≥ Œ¥ | –ö–∏—Ç–∞–π—Å–∫–∏–π: ‰Ω†Â•Ω‰∏ñÁïå",
    }


class TestSingleEmbedding:
    """–¢–µ—Å—Ç—ã –ø–æ–ª—É—á–µ–Ω–∏—è embedding –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."""

    def test_single_english_text(self, client, test_texts):
        """–¢–µ—Å—Ç –ø–æ–ª—É—á–µ–Ω–∏—è embedding –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."""
        text = test_texts["english_short"]
        embeddings = client.get_embeddings([text])

        assert isinstance(embeddings, np.ndarray)
        assert embeddings.shape == (1, 1536)
        assert embeddings.dtype == np.float32

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤–µ–∫—Ç–æ—Ä –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω
        norm = np.linalg.norm(embeddings[0])
        assert np.isclose(norm, 1.0, rtol=1e-5)

    def test_single_russian_text(self, client, test_texts):
        """–¢–µ—Å—Ç –ø–æ–ª—É—á–µ–Ω–∏—è embedding –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."""
        text = test_texts["russian_short"]
        embeddings = client.get_embeddings([text])

        assert embeddings.shape == (1, 1536)
        assert embeddings.dtype == np.float32

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤–µ–∫—Ç–æ—Ä –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω
        norm = np.linalg.norm(embeddings[0])
        assert np.isclose(norm, 1.0, rtol=1e-5)


class TestBatchProcessing:
    """–¢–µ—Å—Ç—ã –±–∞—Ç—á–µ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤."""

    def test_small_batch(self, client, test_texts):
        """–¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–µ–±–æ–ª—å—à–æ–≥–æ –±–∞—Ç—á–∞ (2-10 —Ç–µ–∫—Å—Ç–æ–≤)."""
        texts = [
            test_texts["english_short"],
            test_texts["russian_short"],
            test_texts["mixed"],
            test_texts["code"],
        ]

        embeddings = client.get_embeddings(texts)

        assert embeddings.shape == (4, 1536)
        assert embeddings.dtype == np.float32

        # –í—Å–µ –≤–µ–∫—Ç–æ—Ä—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã
        for i in range(4):
            norm = np.linalg.norm(embeddings[i])
            assert np.isclose(norm, 1.0, rtol=1e-5)

    @pytest.mark.timeout(120)
    def test_large_batch(self, client):
        """–¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª—å—à–æ–≥–æ –±–∞—Ç—á–∞ (50 —Ç–µ–∫—Å—Ç–æ–≤)."""
        # –°–æ–∑–¥–∞–µ–º 50 —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä—ã—Ö —Ç–µ—Å—Ç–æ–≤
        texts = []
        for i in range(50):
            texts.append(f"This is test text number {i}. It contains unique information: {i * 17}")

        start_time = time.time()
        embeddings = client.get_embeddings(texts)
        _ = time.time() - start_time  # duration

        assert embeddings.shape == (50, 1536)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤–µ–∫—Ç–æ—Ä—ã —Ä–∞–∑–ª–∏—á–Ω—ã
        # –ë–µ—Ä–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ª—É—á–∞–π–Ω—ã—Ö –ø–∞—Ä
        for _ in range(5):
            i, j = np.random.randint(0, 50, 2)
            if i != j:
                similarity = np.dot(embeddings[i], embeddings[j])
                assert similarity < 0.99  # –ù–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–º–∏

    @pytest.mark.slow
    @pytest.mark.timeout(180)
    def test_very_large_batch(self, client):
        """–¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª—å—à–æ–≥–æ –±–∞—Ç—á–∞ (100 —Ç–µ–∫—Å—Ç–æ–≤)."""
        # –°–æ–∑–¥–∞–µ–º 100 —Ç–µ–∫—Å—Ç–æ–≤ - 2500 –±—ã–ª–æ —á—Ä–µ–∑–º–µ—Ä–Ω–æ –¥–ª—è —Ç–µ—Å—Ç–æ–≤
        texts = [f"Document {i}: Some content that varies by number {i}" for i in range(100)]

        embeddings = client.get_embeddings(texts)

        assert embeddings.shape == (100, 1536)
        # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –±–∞—Ç—á–∞—Ö


class TestLongTexts:
    """–¢–µ—Å—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤."""

    def test_text_near_limit(self, client):
        """–¢–µ—Å—Ç —Ç–µ–∫—Å—Ç–∞ –±–ª–∏–∑–∫–æ–≥–æ –∫ –ª–∏–º–∏—Ç—É 8192 —Ç–æ–∫–µ–Ω–æ–≤."""
        # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç –ø—Ä–∏–º–µ—Ä–Ω–æ –Ω–∞ 8000 —Ç–æ–∫–µ–Ω–æ–≤
        long_text = "This is a test sentence. " * 1600  # ~8000 —Ç–æ–∫–µ–Ω–æ–≤

        embeddings = client.get_embeddings([long_text])
        assert embeddings.shape == (1, 1536)

    def test_text_over_limit(self, client):
        """–¢–µ—Å—Ç –æ–±—Ä–µ–∑–∫–∏ —Ç–µ–∫—Å—Ç–∞ –ø—Ä–µ–≤—ã—à–∞—é—â–µ–≥–æ 8192 —Ç–æ–∫–µ–Ω–∞."""
        # –°–æ–∑–¥–∞–µ–º –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        very_long_text = "This is a test sentence with more words to increase token count. " * 2000

        # –ù–µ –¥–æ–ª–∂–Ω–æ –≤—ã–±—Ä–æ—Å–∏—Ç—å –∏—Å–∫–ª—é—á–µ–Ω–∏–µ - —Ç–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ–±—Ä–µ–∑–∞–Ω
        embeddings = client.get_embeddings([very_long_text])
        assert embeddings.shape == (1, 1536)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤ –ª–æ–≥–∞—Ö, —á—Ç–æ —Ç–µ–∫—Å—Ç –±—ã–ª –æ–±—Ä–µ–∑–∞–Ω
        # (–≤ —Ä–µ–∞–ª—å–Ω–æ–º –∫–æ–¥–µ –¥–æ–ª–∂–Ω–æ –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å—Å—è –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ)

    def test_mixed_length_batch(self, client, test_texts):
        """–¢–µ—Å—Ç –±–∞—Ç—á–∞ —Å —Ç–µ–∫—Å—Ç–∞–º–∏ —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã."""
        texts = [
            test_texts["english_short"],  # –ö–æ—Ä–æ—Ç–∫–∏–π
            "Medium text. " * 100,  # –°—Ä–µ–¥–Ω–∏–π
            "Long text. " * 1500,  # –î–ª–∏–Ω–Ω—ã–π
            "Not empty text",  # –ó–∞–º–µ–Ω—è–µ–º –ø—É—Å—Ç–æ–π –Ω–∞ –Ω–µ–ø—É—Å—Ç–æ–π
        ]

        embeddings = client.get_embeddings(texts)
        assert embeddings.shape == (4, 1536)

        # –í—Å–µ –≤–µ–∫—Ç–æ—Ä—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –Ω–µ–Ω—É–ª–µ–≤—ã–º–∏
        for i in range(4):
            assert not np.allclose(embeddings[i], 0.0)


class TestCosineSimilarity:
    """–¢–µ—Å—Ç—ã –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –±–ª–∏–∑–æ—Å—Ç–∏ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö embeddings."""

    def test_identical_texts(self, client):
        """–¢–µ—Å—Ç –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –±–ª–∏–∑–æ—Å—Ç–∏ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤."""
        text = "Machine learning is fascinating"
        embeddings = client.get_embeddings([text, text])

        similarity = cosine_similarity_batch(
            embeddings[0:1],  # –ü–µ—Ä–≤—ã–π —Ç–µ–∫—Å—Ç
            embeddings[1:2],  # –í—Ç–æ—Ä–æ–π —Ç–µ–∫—Å—Ç (–∏–¥–µ–Ω—Ç–∏—á–Ω—ã–π)
        )

        assert similarity.shape == (1, 1)
        assert np.isclose(similarity[0, 0], 1.0, rtol=1e-5)

    def test_similar_texts(self, client):
        """–¢–µ—Å—Ç –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –±–ª–∏–∑–æ—Å—Ç–∏ –ø–æ—Ö–æ–∂–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤."""
        texts = [
            "Machine learning is a subset of artificial intelligence",
            "AI includes machine learning as one of its branches",
            "Deep learning is part of machine learning",
        ]

        embeddings = client.get_embeddings(texts)

        # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ–ø–∞—Ä–Ω—ã–µ —Å—Ö–æ–¥—Å—Ç–≤–∞
        similarity_matrix = cosine_similarity_batch(embeddings, embeddings)

        assert similarity_matrix.shape == (3, 3)

        # –î–∏–∞–≥–æ–Ω–∞–ª—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å ~1.0 (—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —Å–∞–º–∏–º —Å–æ–±–æ–π)
        for i in range(3):
            assert np.isclose(similarity_matrix[i, i], 1.0, rtol=1e-5)

        # –ü–æ—Ö–æ–∂–∏–µ —Ç–µ–∫—Å—Ç—ã –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å –∑–∞–º–µ—Ç–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        # –°–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥ —Å 0.7 –¥–æ 0.5 - —Ä–µ–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∏–∂–µ
        assert similarity_matrix[0, 1] > 0.5  # –ë—ã–ª–æ 0.7
        assert similarity_matrix[0, 2] > 0.5  # –ë—ã–ª–æ 0.7
        assert similarity_matrix[1, 2] > 0.5  # –ë—ã–ª–æ 0.7

    def test_different_texts(self, client, test_texts):
        """–¢–µ—Å—Ç –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –±–ª–∏–∑–æ—Å—Ç–∏ —Ä–∞–∑–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤."""
        texts = [
            test_texts["english_short"],  # –ü—Ä–æ –ª–∏—Å—É
            test_texts["code"],  # –ö–æ–¥ Python
            test_texts["russian_medium"],  # –ü—Ä–æ ML –Ω–∞ —Ä—É—Å—Å–∫–æ–º
        ]

        embeddings = client.get_embeddings(texts)
        similarity_matrix = cosine_similarity_batch(embeddings, embeddings)

        # –¢–µ–∫—Å—Ç—ã –Ω–∞ —Ä–∞–∑–Ω—ã–µ —Ç–µ–º—ã –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å –Ω–∏–∑–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        assert similarity_matrix[0, 1] < 0.5  # –õ–∏—Å–∞ vs –∫–æ–¥

        # –ù–æ —Ç–µ–∫—Å—Ç—ã –ø—Ä–æ ML –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö –º–æ–≥—É—Ç –±—ã—Ç—å –ø–æ—Ö–æ–∂–∏
        # (–Ω–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ, —Ç.–∫. –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –º–æ–¥–µ–ª–∏)

    def test_multilingual_similarity(self, client):
        """–¢–µ—Å—Ç –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –±–ª–∏–∑–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–æ–≤ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö."""
        texts = [
            "The cat is sleeping on the sofa",
            "–ö–æ—Ç —Å–ø–∏—Ç –Ω–∞ –¥–∏–≤–∞–Ω–µ",
            "Le chat dort sur le canap√©",
        ]

        embeddings = client.get_embeddings(texts)
        similarity_matrix = cosine_similarity_batch(embeddings, embeddings)

        # –û–¥–∏–Ω–∞–∫–æ–≤—ã–π —Å–º—ã—Å–ª –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö –¥–æ–ª–∂–µ–Ω –¥–∞–≤–∞—Ç—å –≤—ã—Å–æ–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        # –°–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥ —Å 0.8 –¥–æ 0.6 - —Ä–µ–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è ~0.6
        assert similarity_matrix[0, 1] > 0.55  # EN vs RU (–±—ã–ª–æ 0.8)
        assert similarity_matrix[0, 2] > 0.55  # EN vs FR (–±—ã–ª–æ 0.8)
        assert similarity_matrix[1, 2] > 0.55  # RU vs FR (–±—ã–ª–æ 0.8)


class TestEdgeCases:
    """–¢–µ—Å—Ç—ã –≥—Ä–∞–Ω–∏—á–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤."""

    def test_empty_input(self, client):
        """–¢–µ—Å—Ç –ø—É—Å—Ç–æ–≥–æ –º–∞—Å—Å–∏–≤–∞ —Ç–µ–∫—Å—Ç–æ–≤."""
        embeddings = client.get_embeddings([])
        assert isinstance(embeddings, np.ndarray)
        assert embeddings.shape == (0,)

    def test_empty_string(self, client):
        """–¢–µ—Å—Ç –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–∏."""
        embeddings = client.get_embeddings([""])
        assert embeddings.shape == (1, 1536)

        # –î–ª—è –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–∏ –¥–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å—Å—è –Ω—É–ª–µ–≤–æ–π –≤–µ–∫—Ç–æ—Ä
        assert np.allclose(embeddings[0], 0.0)

    def test_mixed_empty_and_non_empty(self, client):
        """–¢–µ—Å—Ç —Å–º–µ—Å–∏ –ø—É—Å—Ç—ã—Ö –∏ –Ω–µ–ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫."""
        texts = ["Hello world", "", "Test text", "   ", "Final text"]
        embeddings = client.get_embeddings(texts)

        assert embeddings.shape == (5, 1536)

        # –ù–µ–ø—É—Å—Ç—ã–µ —Ç–µ–∫—Å—Ç—ã –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å –Ω–µ–Ω—É–ª–µ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã
        assert not np.allclose(embeddings[0], 0.0)  # "Hello world"
        assert not np.allclose(embeddings[2], 0.0)  # "Test text"
        assert not np.allclose(embeddings[4], 0.0)  # "Final text"

        # –ü—É—Å—Ç—ã–µ —Ç–µ–∫—Å—Ç—ã –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å –Ω—É–ª–µ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã
        assert np.allclose(embeddings[1], 0.0)  # ""
        assert np.allclose(embeddings[3], 0.0)  # "   "

    def test_special_characters(self, client, test_texts):
        """–¢–µ—Å—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤."""
        embeddings = client.get_embeddings([test_texts["special_chars"]])
        assert embeddings.shape == (1, 1536)

    def test_unicode_and_emoji(self, client, test_texts):
        """–¢–µ—Å—Ç Unicode —Å–∏–º–≤–æ–ª–æ–≤ –∏ —ç–º–æ–¥–∑–∏."""
        embeddings = client.get_embeddings([test_texts["unicode"]])
        assert embeddings.shape == (1, 1536)


class TestTPMLimits:
    """–¢–µ—Å—Ç—ã —Å–æ–±–ª—é–¥–µ–Ω–∏—è TPM –ª–∏–º–∏—Ç–æ–≤."""

    def test_tpm_tracking(self, client):
        """–¢–µ—Å—Ç –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤."""
        initial_remaining = client.remaining_tokens

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤
        texts = ["Test text"] * 10
        client.get_embeddings(texts)

        # –û—Å—Ç–∞—Ç–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤ –¥–æ–ª–∂–µ–Ω —É–º–µ–Ω—å—à–∏—Ç—å—Å—è
        assert client.remaining_tokens < initial_remaining

        # reset_time –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
        assert client.reset_time > time.time()

    @pytest.mark.slow
    @pytest.mark.timeout(120)
    def test_large_volume_processing(self, client):
        """–¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª—å—à–æ–≥–æ –æ–±—ä–µ–º–∞ —Å —É—á–µ—Ç–æ–º –ª–∏–º–∏—Ç–æ–≤."""
        # –°–æ–∑–¥–∞–µ–º –º–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤ (–Ω–æ –Ω–µ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –¥–ª—è —Ç–µ—Å—Ç–∞)
        texts = []
        for i in range(50):
            texts.append(
                f"This is a longer test text number {i} with more content to consume tokens."
            )

        start_time = time.time()
        embeddings = client.get_embeddings(texts)
        _ = time.time() - start_time  # duration

        assert embeddings.shape == (50, 1536)


class TestErrorHandling:
    """–¢–µ—Å—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫ API."""

    def test_invalid_api_key(self):
        """–¢–µ—Å—Ç —Å –Ω–µ–≤–µ—Ä–Ω—ã–º API –∫–ª—é—á–æ–º."""
        bad_config = {
            "embedding_api_key": "sk-invalid-key-12345",
            "embedding_model": "text-embedding-3-small",
            "max_retries": 1,  # –£–º–µ–Ω—å—à–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏ —Ç–µ—Å—Ç–∞
        }

        client = EmbeddingsClient(bad_config)

        with pytest.raises(Exception) as exc_info:
            client.get_embeddings(["Test text"])

        # –î–æ–ª–∂–Ω–∞ –±—ã—Ç—å –æ—à–∏–±–∫–∞ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏
        assert (
            "invalid" in str(exc_info.value).lower()
            or "unauthorized" in str(exc_info.value).lower()
        )

    def test_network_timeout(self, client):
        """–¢–µ—Å—Ç —Ç–∞–π–º–∞—É—Ç–∞ —Å–µ—Ç–∏ (—Å–∏–º—É–ª—è—Ü–∏—è)."""
        # –í —Ä–µ–∞–ª—å–Ω–æ–º —Ç–µ—Å—Ç–µ —Å–ª–æ–∂–Ω–æ —Å–∏–º—É–ª–∏—Ä–æ–≤–∞—Ç—å —Ç–∞–π–º–∞—É—Ç
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–ª–∏–µ–Ω—Ç –∏–º–µ–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ retry
        assert client.max_retries > 0
        assert hasattr(client, "_wait_for_tokens")


class TestVectorProperties:
    """–¢–µ—Å—Ç—ã —Å–≤–æ–π—Å—Ç–≤ –≤–µ–∫—Ç–æ—Ä–æ–≤."""

    def test_vector_normalization(self, client, test_texts):
        """–¢–µ—Å—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–µ–∫—Ç–æ—Ä–æ–≤."""
        texts = list(test_texts.values())[:5]
        embeddings = client.get_embeddings(texts)

        for i in range(len(texts)):
            norm = np.linalg.norm(embeddings[i])
            assert np.isclose(norm, 1.0, rtol=1e-5), f"Vector {i} norm = {norm}"

    def test_vector_dimensions(self, client):
        """–¢–µ—Å—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –≤–µ–∫—Ç–æ—Ä–æ–≤."""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –≤—Ö–æ–¥–∞
        test_cases = [
            ["Single text"],
            ["Text 1", "Text 2"],
            ["Non-empty"] * 10,  # –ó–∞–º–µ–Ω—è–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
            ["Very long text " * 1000],
        ]

        for texts in test_cases:
            embeddings = client.get_embeddings(texts)
            assert embeddings.shape == (len(texts), 1536)
            assert embeddings.dtype == np.float32


class TestPerformance:
    """–¢–µ—Å—Ç—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏."""

    def test_embedding_speed(self, client):
        """–¢–µ—Å—Ç —Å–∫–æ—Ä–æ—Å—Ç–∏ –ø–æ–ª—É—á–µ–Ω–∏—è embeddings."""
        texts = ["Test text for performance measurement"] * 20

        start_time = time.time()
        embeddings = client.get_embeddings(texts)
        duration = time.time() - start_time

        assert embeddings.shape == (20, 1536)

        # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±—ã—Å—Ç—Ä–æ (< 5 —Å–µ–∫—É–Ω–¥ –¥–ª—è 20 —Ç–µ–∫—Å—Ç–æ–≤)
        assert duration < 5.0

        _ = 20 / duration  # texts_per_second


class TestHeadersTracking:
    """–¢–µ—Å—Ç—ã –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è headers –æ—Ç API."""

    def test_real_headers_tracking(self, client):
        """–¢–µ—Å—Ç —Ä–µ–∞–ª—å–Ω–æ–≥–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è TPM —á–µ—Ä–µ–∑ headers."""
        # –ó–∞–ø–æ–º–∏–Ω–∞–µ–º –Ω–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        initial_remaining = client.remaining_tokens

        # –î–µ–ª–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–∞–ø—Ä–æ—Å–æ–≤
        texts_batch1 = ["First batch text"] * 5
        embeddings1 = client.get_embeddings(texts_batch1)

        # –ü–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ remaining_tokens –¥–æ–ª–∂–µ–Ω —É–º–µ–Ω—å—à–∏—Ç—å—Å—è
        assert client.remaining_tokens < initial_remaining
        first_remaining = client.remaining_tokens

        # –î–µ–ª–∞–µ–º –≤—Ç–æ—Ä–æ–π –∑–∞–ø—Ä–æ—Å
        texts_batch2 = ["Second batch text"] * 5
        embeddings2 = client.get_embeddings(texts_batch2)

        # –ü–æ—Å–ª–µ –≤—Ç–æ—Ä–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –¥–æ–ª–∂–µ–Ω –µ—â–µ —É–º–µ–Ω—å—à–∏—Ç—å—Å—è
        assert client.remaining_tokens < first_remaining

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ embeddings –ø–æ–ª—É—á–µ–Ω—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
        assert embeddings1.shape == (5, 1536)
        assert embeddings2.shape == (5, 1536)

    def test_tpm_reset_time_from_headers(self, client):
        """–¢–µ—Å—Ç —É—Å—Ç–∞–Ω–æ–≤–∫–∏ reset_time –∏–∑ headers."""
        # –î–µ–ª–∞–µ–º –∑–∞–ø—Ä–æ—Å
        texts = ["Test text for reset time"]
        embeddings = client.get_embeddings(texts)

        # reset_time –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏–∑ headers (–≤ –±—É–¥—É—â–µ–º)
        assert client.reset_time > time.time()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ embeddings –ø–æ–ª—É—á–µ–Ω—ã
        assert embeddings.shape == (1, 1536)

    def test_exactly_8192_tokens(self, client):
        """–¢–µ—Å—Ç —Ç–µ–∫—Å—Ç–∞ —Å —Ä–æ–≤–Ω–æ 8192 —Ç–æ–∫–µ–Ω–∞–º–∏."""
        # –°–æ–∑–¥–∞–µ–º –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        # "test " = –ø—Ä–∏–º–µ—Ä–Ω–æ 1 —Ç–æ–∫–µ–Ω, –Ω—É–∂–Ω–æ ~8192 –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π
        long_text = "test " * 8200  # –ù–µ–º–Ω–æ–≥–æ –±–æ–ª—å—à–µ –¥–ª—è –≥–∞—Ä–∞–Ω—Ç–∏–∏

        # –û–±—Ä–µ–∑–∞–µ–º –¥–æ —Ç–æ—á–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤
        tokens = client.encoding.encode(long_text)
        if len(tokens) > 8192:
            tokens = tokens[:8192]
            long_text = client.encoding.decode(tokens)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ç–µ–∫—Å—Ç –Ω–µ –æ–±—Ä–µ–∑–∞–µ—Ç—Å—è –ø—Ä–∏ —Ä–æ–≤–Ω–æ 8192 —Ç–æ–∫–µ–Ω–∞—Ö
        embeddings = client.get_embeddings([long_text])
        assert embeddings.shape == (1, 1536)

    def test_correct_model_usage(self, config):
        """–¢–µ—Å—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞."""
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å –¥—Ä—É–≥–æ–π –º–æ–¥–µ–ª—å—é –µ—Å–ª–∏ –æ–Ω–∞ –¥–æ—Å—Ç—É–ø–Ω–∞
        config["embedding_model"] = "text-embedding-3-small"  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç—É –∂–µ –º–æ–¥–µ–ª—å
        client = EmbeddingsClient(config)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –º–æ–¥–µ–ª—å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ
        assert client.model == "text-embedding-3-small"

        # –î–µ–ª–∞–µ–º –∑–∞–ø—Ä–æ—Å —á—Ç–æ–±—ã —É–±–µ–¥–∏—Ç—å—Å—è —á—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç
        embeddings = client.get_embeddings(["Test model usage"])
        assert embeddings.shape == (1, 1536)

    def test_batch_with_new_config_params(self, api_key):
        """–¢–µ—Å—Ç –±–∞—Ç—á–∏–Ω–≥–∞ —Å –Ω–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏."""
        config = {
            "embedding_api_key": api_key,
            "embedding_model": "text-embedding-3-small",
            "embedding_tpm_limit": 5000000,
            "max_retries": 3,
            "max_batch_tokens": 50000,  # –£–º–µ–Ω—å—à–µ–Ω–Ω—ã–π –ª–∏–º–∏—Ç –¥–ª—è —Ç–µ—Å—Ç–∞
            "max_texts_per_batch": 100,  # –£–º–µ–Ω—å—à–µ–Ω–Ω—ã–π –ª–∏–º–∏—Ç –¥–ª—è —Ç–µ—Å—Ç–∞
            "truncate_tokens": 5000,  # –£–º–µ–Ω—å—à–µ–Ω–Ω—ã–π –ª–∏–º–∏—Ç –¥–ª—è —Ç–µ—Å—Ç–∞
        }

        client = EmbeddingsClient(config)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø—Ä–∏–º–µ–Ω–∏–ª–∏—Å—å
        assert client.max_batch_tokens == 50000
        assert client.max_texts_per_batch == 100
        assert client.truncate_tokens == 5000

        # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç—ã –∫–æ—Ç–æ—Ä—ã–µ –ø–æ—Ç—Ä–µ–±—É—é—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –±–∞—Ç—á–µ–π
        texts = ["Test text for batching"] * 150  # –ë–æ–ª—å—à–µ —á–µ–º max_texts_per_batch

        embeddings = client.get_embeddings(texts)

        # –î–æ–ª–∂–Ω—ã –ø–æ–ª—É—á–∏—Ç—å –≤—Å–µ embeddings –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –±–∞—Ç—á–∏–Ω–≥
        assert embeddings.shape == (150, 1536)


if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤
    pytest.main([__file__, "-v"])
