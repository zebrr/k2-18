[slicer]
max_tokens              = 5000
overlap                 = 0
soft_boundary           = true
soft_boundary_max_shift = 500
tokenizer               = "o200k_base"
allowed_extensions      = ["json", "txt", "md", "html"]

[itext2kg]
is_reasoning        = true        # ОБЯЗАТЕЛЬНЫЙ! Иначе падаем с ошибкой. Проверь модель!

# НОВОЕ: Управление контекстом
# truncation = "auto"           # "auto" | "disabled" | закомментировано = не передается
# response_chain_depth = 2      # Глубина цепочки (1-N) | закомментировано = без ограничений | 0 = независимые запросы

# Основная модель
model 				= "gpt-5-2025-08-07" # Новый ризонер: 400,000 / 128,000
# model               = "gpt-4.1-2025-04-14" # Не ризонер: 1,047,576 / 32,768 
max_context_tokens  = 400000
tpm_limit           = 450000
max_completion      = 100000

# Тестовая модель (только эти 4 параметра уникальные)
# Тестовая модель по типу должна быть такой же, как и основная (или обе ризонеры или нет)
model_test              = "gpt-5-mini-2025-08-07"  # Новый ризонер: 400,000 / 128,000
# model_test              = "gpt-4.1-mini-2025-04-14" # Не ризонер: 1,047,576 / 32,768 
max_context_tokens_test = 400000
tpm_limit_test          = 450000
max_completion_test     = 100000

# Общие параметры настройки для ОБЕИХ моделей
# temperature         = 0.6
reasoning_effort    = "high"
reasoning_summary   = "auto"
# verbosity         = "medium"      # Закомментирован = null

# Остальные параметры утилиты (НЕ УДАЛЯТЬ! Копировать текущие значения)
api_key             = "sk-..."  # Set via OPENAI_API_KEY env variable
tpm_safety_margin   = 0.15
log_level           = "info"
timeout             = 360
max_retries         = 3
poll_interval       = 7

[dedup]
embedding_model     = "text-embedding-3-small"
embedding_api_key   = ""  # Optional, uses OPENAI_API_KEY if not set
embedding_tpm_limit = 350000                    # лимит для embedding моделей
sim_threshold       = 0.97
len_ratio_min       = 0.8
faiss_M             = 32
faiss_efC           = 200
faiss_metric        = "INNER_PRODUCT"
k_neighbors         = 5

[refiner]
is_reasoning        = true        # ОБЯЗАТЕЛЬНЫЙ! Иначе падаем с ошибкой. Проверь модель!

# НОВОЕ: Управление контекстом
# truncation = "auto"           # "auto" | "disabled" | закомментировано = не передается
# response_chain_depth = 0      # Для refiner рекомендуется 0 (независимые запросы)

# Основная модель
model 				= "gpt-5-2025-08-07" # Новый ризонер: 400,000 / 128,000
# model               = "gpt-4.1-2025-04-14" # Не ризонер: 1,047,576 / 32,768 
max_context_tokens  = 400000
tpm_limit           = 450000
max_completion      = 100000

# Тестовая модель (только эти 4 параметра уникальные)
# Тестовая модель по типу должна быть такой же, как и основная (или обе ризонеры или нет)
model_test              = "gpt-5-mini-2025-08-07"  # Новый ризонер: 400,000 / 128,000
# model_test              = "gpt-4.1-mini-2025-04-14" # Не ризонер: 1,047,576 / 32,768 
max_context_tokens_test = 400000
tpm_limit_test          = 450000
max_completion_test     = 100000

# Общие параметры настройки для ОБЕИХ моделей
# temperature         = 0.6
reasoning_effort    = "high"
reasoning_summary   = "auto"
# verbosity         = "medium"      # Закомментирован = null

# Параметры для embeddings и поиска пар (НЕ УДАЛЯТЬ!)
run                 = true
embedding_model     = "text-embedding-3-small"
embedding_api_key   = ""  # Optional, uses OPENAI_API_KEY if not set
embedding_tpm_limit = 350000
sim_threshold       = 0.80
max_pairs_per_node  = 20
faiss_M             = 32
faiss_metric        = "INNER_PRODUCT"

# Остальные параметры утилиты (НЕ УДАЛЯТЬ!)
api_key             = "sk-..."  # Set via OPENAI_API_KEY env variable
tpm_safety_margin   = 0.15
log_level           = "info"
timeout             = 360
max_retries         = 3
poll_interval       = 7
weight_low          = 0.3
weight_mid          = 0.6
weight_high         = 0.9