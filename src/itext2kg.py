#!/usr/bin/env python3
"""
iText2KG - –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π –∏–∑ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤.

–£—Ç–∏–ª–∏—Ç–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–ª–∞–π—Å—ã –∏–∑ staging, –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∏—Ö –≤ LLM
—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ previous_response_id, –∏ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ —Å—Ç—Ä–æ–∏—Ç
ConceptDictionary –∏ LearningChunkGraph.
"""

import json
import logging
import sys
import time
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any, Set
from dataclasses import dataclass, field

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ PYTHONPATH –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –∏–º–ø–æ—Ä—Ç–æ–≤
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.utils.config import load_config
from src.utils.exit_codes import (
    EXIT_SUCCESS, EXIT_CONFIG_ERROR, EXIT_INPUT_ERROR,
    EXIT_RUNTIME_ERROR, EXIT_API_LIMIT_ERROR, EXIT_IO_ERROR
)
from src.utils.llm_client import OpenAIClient, ResponseUsage
from src.utils.validation import (
    validate_json, validate_graph_invariants, 
    validate_graph_invariants_intermediate,
    validate_concept_dictionary_invariants,
    ValidationError, GraphInvariantError
)

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ UTF-8 –∫–æ–¥–∏—Ä–æ–≤–∫–∏ –¥–ª—è Windows –∫–æ–Ω—Å–æ–ª–∏
from src.utils.console_encoding import setup_console_encoding
setup_console_encoding()

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
CONFIG_PATH = Path(__file__).parent / "config.toml"
PROMPTS_DIR = Path(__file__).parent / "prompts"
SCHEMAS_DIR = Path(__file__).parent / "schemas"
STAGING_DIR = Path(__file__).parent.parent / "data" / "staging"
OUTPUT_DIR = Path(__file__).parent.parent / "data" / "out"
LOGS_DIR = Path(__file__).parent.parent / "logs"

EXTRACTION_PROMPT_FILE = "itext2kg_extraction.md"
MAX_REPAIR_ATTEMPTS = 1


@dataclass
class ProcessingStats:
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–ª–∞–π—Å–æ–≤."""
    total_slices: int = 0
    processed_slices: int = 0
    failed_slices: int = 0
    total_concepts: int = 0
    total_nodes: int = 0
    total_edges: int = 0
    total_tokens_used: int = 0
    start_time: datetime = field(default_factory=lambda: datetime.now())


@dataclass
class SliceData:
    """–î–∞–Ω–Ω—ã–µ –æ–¥–Ω–æ–≥–æ —Å–ª–∞–π—Å–∞."""
    id: str
    order: int
    source_file: str
    slug: str
    text: str
    slice_token_start: int
    slice_token_end: int


class SliceProcessor:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–ª–∞–π—Å–æ–≤ –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π."""
    
    def __init__(self, config: Dict[str, Any]):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞.
        
        Args:
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–∑ config.toml
        """
        self.config = config['itext2kg']
        self.llm_client = OpenAIClient(self.config)
        self.logger = self._setup_logger()
        self.stats = ProcessingStats()
        
        # –ù–∞–∫–æ–ø–∏—Ç–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö
        self.concept_dictionary: Dict[str, List[Dict]] = {"concepts": []}
        self.learning_graph: Dict[str, List[Dict]] = {"nodes": [], "edges": []}
        self.known_node_ids: Set[str] = set()  # –î–ª—è –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è
        self.concept_id_map: Dict[str, int] = {}  # concept_id -> index –≤ concepts
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ–º–ø—Ç–∞ –∏ —Å—Ö–µ–º
        self.extraction_prompt = self._load_extraction_prompt()

    def _format_tokens(self, tokens: int) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —á–∏—Ç–∞–µ–º—ã–π –≤–∏–¥.
        
        Args:
            tokens: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            
        Returns:
            –°—Ç—Ä–æ–∫–∞ –≤–∏–¥–∞ "123", "45.61k", "1.22M"
        """
        if tokens < 1000:
            return str(tokens)
        elif tokens < 1_000_000:
            # –¢—ã—Å—è—á–∏ —Å –æ–¥–Ω–∏–º –∑–Ω–∞–∫–æ–º –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç–æ–π
            return f"{tokens / 1000:.2f}k"
        else:
            # –ú–∏–ª–ª–∏–æ–Ω—ã —Å –æ–¥–Ω–∏–º –∑–Ω–∞–∫–æ–º –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç–æ–π
            return f"{tokens / 1_000_000:.2f}M"

    def _setup_logger(self) -> logging.Logger:
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞ —Å –≤—ã–≤–æ–¥–æ–º –≤ —Ñ–∞–π–ª –∏ –∫–æ–Ω—Å–æ–ª—å."""
        logger = logging.getLogger('itext2kg')
        logger.setLevel(getattr(logging, self.config['log_level'].upper()))
        
        # –§–∞–π–ª–æ–≤—ã–π handler
        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        log_file = LOGS_DIR / f"itext2kg_{timestamp}.log"
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setFormatter(
            logging.Formatter('%(message)s')  # JSON Lines format
        )
        logger.addHandler(file_handler)
        
        # –ö–æ–Ω—Å–æ–ª—å–Ω—ã–π handler –¥–ª—è –æ—à–∏–±–æ–∫
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.WARNING)
        console_handler.setFormatter(
            logging.Formatter('[%(asctime)s] %(levelname)-8s | %(message)s', datefmt='%H:%M:%S')
        )
        logger.addHandler(console_handler)
        
        return logger
    
    def _load_extraction_prompt(self) -> str:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ–º–ø—Ç–∞ —Å –ø–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–æ–π —Å—Ö–µ–º."""
        prompt_path = PROMPTS_DIR / EXTRACTION_PROMPT_FILE
        if not prompt_path.exists():
            raise FileNotFoundError(f"Prompt file not found: {prompt_path}")
        
        prompt_content = prompt_path.read_text(encoding='utf-8')
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ö–µ–º –¥–ª—è –ø–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–∏
        concept_schema_path = SCHEMAS_DIR / "ConceptDictionary.schema.json"
        graph_schema_path = SCHEMAS_DIR / "LearningChunkGraph.schema.json"
        
        concept_schema = json.loads(concept_schema_path.read_text(encoding='utf-8'))
        graph_schema = json.loads(graph_schema_path.read_text(encoding='utf-8'))
        
        # –ü–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å—Ö–µ–º –≤ –ø—Ä–æ–º–ø—Ç
        prompt_content = prompt_content.replace(
            "{concept_dictionary_schema}", 
            json.dumps(concept_schema, indent=2)
        )
        prompt_content = prompt_content.replace(
            "{learning_chunk_graph_schema}", 
            json.dumps(graph_schema, indent=2)
        )
        
        return prompt_content
    
    def _load_slice(self, slice_file: Path) -> SliceData:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å–ª–∞–π—Å–∞ –∏–∑ —Ñ–∞–π–ª–∞.
        
        Args:
            slice_file: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å–ª–∞–π—Å–∞
            
        Returns:
            SliceData –æ–±—ä–µ–∫—Ç
            
        Raises:
            json.JSONDecodeError: –ï—Å–ª–∏ —Ñ–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON
        """
        try:
            data = json.loads(slice_file.read_text(encoding='utf-8'))
            return SliceData(
                id=data['id'],
                order=data['order'],
                source_file=data['source_file'],
                slug=data['slug'],
                text=data['text'],
                slice_token_start=data['slice_token_start'],
                slice_token_end=data['slice_token_end']
            )
        except (json.JSONDecodeError, KeyError) as e:
            raise ValueError(f"Invalid slice file {slice_file}: {e}")
    
    def _format_slice_input(self, slice_data: SliceData) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LLM.
        
        Args:
            slice_data: –î–∞–Ω–Ω—ã–µ —Å–ª–∞–π—Å–∞
            
        Returns:
            JSON —Å—Ç—Ä–æ–∫–∞ —Å ConceptDictionary –∏ Slice
        """
        input_data = {
            "ConceptDictionary": self.concept_dictionary,
            "Slice": {
                "id": slice_data.id,
                "order": slice_data.order,
                "source_file": slice_data.source_file,
                "slug": slice_data.slug,
                "text": slice_data.text,
                "slice_token_start": slice_data.slice_token_start,
                "slice_token_end": slice_data.slice_token_end
            }
        }
        
        return json.dumps(input_data, ensure_ascii=False, indent=2)
    
    def _update_concept_dictionary(self, concepts_added: List[Dict]) -> None:
        """
        –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ ConceptDictionary.
        
        Args:
            concepts_added: –°–ø–∏—Å–æ–∫ –Ω–æ–≤—ã—Ö/–æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤ –∏–∑ LLM –æ—Ç–≤–µ—Ç–∞
        """
        for new_concept in concepts_added:
            concept_id = new_concept['concept_id']
            
            if concept_id in self.concept_id_map:
                # –ö–æ–Ω—Ü–µ–ø—Ç —Å—É—â–µ—Å—Ç–≤—É–µ—Ç - –æ–±–Ω–æ–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ aliases
                idx = self.concept_id_map[concept_id]
                existing_concept = self.concept_dictionary['concepts'][idx]
                
                # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö aliases (lowercase -> original)
                existing_aliases = existing_concept['term'].get('aliases', [])
                existing_lower_map = {alias.lower(): alias for alias in existing_aliases}
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–æ–≤—ã–µ aliases
                new_aliases = new_concept['term'].get('aliases', [])
                added_aliases = []
                
                for new_alias in new_aliases:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º case-insensitive
                    if new_alias.lower() not in existing_lower_map:
                        existing_lower_map[new_alias.lower()] = new_alias
                        added_aliases.append(new_alias)
                
                if added_aliases:
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ aliases (–±–µ—Ä–µ–º values - –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏)
                    existing_concept['term']['aliases'] = sorted(existing_lower_map.values())
                    
                    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
                    self.logger.debug(json.dumps({
                        "timestamp": datetime.now().isoformat(),
                        "level": "DEBUG",
                        "event": "concept_update",
                        "concept_id": concept_id,
                        "action": "added_aliases",
                        "new_aliases": sorted(added_aliases)
                    }))
            else:
                # –ù–æ–≤—ã–π –∫–æ–Ω—Ü–µ–ø—Ç - —á–∏—Å—Ç–∏–º aliases –æ—Ç case-insensitive –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                aliases = new_concept.get('term', {}).get('aliases', [])
                if aliases:
                    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–µ—Ä–≤–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ
                    seen_lower = {}
                    unique_aliases = []
                    for alias in aliases:
                        alias_lower = alias.lower()
                        if alias_lower not in seen_lower:
                            seen_lower[alias_lower] = True
                            unique_aliases.append(alias)
                    
                    new_concept['term']['aliases'] = unique_aliases
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ü–µ–ø—Ç
                self.concept_dictionary['concepts'].append(new_concept)
                self.concept_id_map[concept_id] = len(self.concept_dictionary['concepts']) - 1
                self.stats.total_concepts += 1
                
                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è
                self.logger.debug(json.dumps({
                    "timestamp": datetime.now().isoformat(),
                    "level": "DEBUG",
                    "event": "concept_added",
                    "concept_id": concept_id
                }))
    
    def _process_chunk_nodes(self, new_nodes: List[Dict]) -> List[Dict]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ —É–∑–ª–æ–≤ —Ç–∏–ø–∞ Chunk –∏ Assessment —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–π.
        
        Args:
            new_nodes: –°–ø–∏—Å–æ–∫ –Ω–æ–≤—ã—Ö —É–∑–ª–æ–≤ –∏–∑ –ø–∞—Ç—á–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ —É–∑–ª–æ–≤ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ –≥—Ä–∞—Ñ
        """
        nodes_to_add = []
        
        for node in new_nodes:
            node_type = node.get('type')
            node_id = node.get('id')
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ Chunk –∏ Assessment —É–∑–ª–æ–≤ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ ID
            if node_type in ('Chunk', 'Assessment') and node_id in self.known_node_ids:
                # –ù–∞—Ö–æ–¥–∏–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —É–∑–µ–ª
                existing_node = None
                for idx, existing in enumerate(self.learning_graph['nodes']):
                    if existing['id'] == node_id:
                        existing_node = existing
                        existing_idx = idx
                        break
                
                if existing_node:
                    # –î–ª—è Chunk —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞
                    if node_type == 'Chunk':
                        if len(node.get('text', '')) > len(existing_node.get('text', '')):
                            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —É–∑–µ–ª
                            self.learning_graph['nodes'][existing_idx] = node
                            
                            self.logger.debug(json.dumps({
                                "timestamp": datetime.now().isoformat(),
                                "level": "DEBUG",
                                "event": "chunk_updated",
                                "node_id": node_id,
                                "old_length": len(existing_node.get('text', '')),
                                "new_length": len(node.get('text', ''))
                            }))
                        else:
                            # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –±–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–∏–π –≤–∞—Ä–∏–∞–Ω—Ç
                            self.logger.debug(json.dumps({
                                "timestamp": datetime.now().isoformat(),
                                "level": "DEBUG",
                                "event": "chunk_ignored",
                                "node_id": node_id,
                                "reason": "shorter_duplicate"
                            }))
                    else:
                        # –î–ª—è Assessment –ø—Ä–æ—Å—Ç–æ –ª–æ–≥–∏—Ä—É–µ–º –∏ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç
                        self.logger.warning(json.dumps({
                            "timestamp": datetime.now().isoformat(),
                            "level": "WARN",
                            "event": "assessment_duplicate_ignored",
                            "node_id": node_id
                        }))
            else:
                # –ù–æ–≤—ã–π —É–∑–µ–ª - –¥–æ–±–∞–≤–ª—è–µ–º
                nodes_to_add.append(node)
                if node_id:  # –ó–∞—â–∏—Ç–∞ –æ—Ç –ø—É—Å—Ç—ã—Ö ID
                    self.known_node_ids.add(node_id)
        
        return nodes_to_add
    
    def _validate_edges(self, edges: List[Dict]) -> List[Dict]:
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è —Ä—ë–±–µ—Ä —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —É–∑–ª–æ–≤ –∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤.
        
        Args:
            edges: –°–ø–∏—Å–æ–∫ —Ä—ë–±–µ—Ä –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ –≤–∞–ª–∏–¥–Ω—ã—Ö —Ä—ë–±–µ—Ä
        """
        valid_edges = []
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ ID (—É–∑–ª—ã + –∫–æ–Ω—Ü–µ–ø—Ç—ã)
        all_known_ids = self.known_node_ids.copy()
        all_known_ids.update(self.concept_id_map.keys())
        
        # –°–æ–±–∏—Ä–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä—ë–±—Ä–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        existing_edges = set()
        for edge in self.learning_graph.get('edges', []):
            existing_edges.add((edge['source'], edge['target'], edge['type']))
        
        # –¢–∞–∫–∂–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º —Ä—ë–±—Ä–∞ –≤–Ω—É—Ç—Ä–∏ —Ç–µ–∫—É—â–µ–≥–æ –ø–∞—Ç—á–∞
        patch_edges = set()
        
        for edge in edges:
            source = edge.get('source')
            target = edge.get('target')
            edge_type = edge.get('type')
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —É–∑–ª–æ–≤
            if source not in all_known_ids or target not in all_known_ids:
                self.logger.warning(json.dumps({
                    "timestamp": datetime.now().isoformat(),
                    "level": "WARN",
                    "event": "edge_dropped",
                    "reason": "invalid_reference",
                    "source": source,
                    "target": target
                }))
                continue
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ PREREQUISITE self-loops
            if edge_type == 'PREREQUISITE' and source == target:
                self.logger.warning(json.dumps({
                    "timestamp": datetime.now().isoformat(),
                    "level": "WARN",
                    "event": "edge_dropped",
                    "reason": "prerequisite_self_loop",
                    "node_id": source
                }))
                continue
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–µ—Å–∞
            weight = edge.get('weight', 0.5)
            if not (0 <= weight <= 1):
                self.logger.warning(json.dumps({
                    "timestamp": datetime.now().isoformat(),
                    "level": "WARN",
                    "event": "edge_dropped",
                    "reason": "invalid_weight",
                    "weight": weight
                }))
                continue
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç
            edge_key = (source, target, edge_type)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ—Ç–∏–≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ä—ë–±–µ—Ä –≤ –≥—Ä–∞—Ñ–µ
            if edge_key in existing_edges:
                self.logger.info(json.dumps({
                    "timestamp": datetime.now().isoformat(),
                    "level": "INFO",
                    "event": "edge_dropped",
                    "reason": "duplicate_edge",
                    "source": source,
                    "target": target,
                    "type": edge_type
                }))
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ—Ç–∏–≤ —Ä—ë–±–µ—Ä –≤ —Ç–µ–∫—É—â–µ–º –ø–∞—Ç—á–µ
            if edge_key in patch_edges:
                self.logger.info(json.dumps({
                    "timestamp": datetime.now().isoformat(),
                    "level": "INFO",
                    "event": "edge_dropped",
                    "reason": "duplicate_in_patch",
                    "source": source,
                    "target": target,
                    "type": edge_type
                }))
                continue
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞–ª–∏–¥–Ω–æ–µ —Ä–µ–±—Ä–æ
            valid_edges.append(edge)
            patch_edges.add(edge_key)
        
        return valid_edges

    def _add_mentions_edges(self, chunk_nodes: List[Dict]) -> int:
            """
            –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–æ–±–∞–≤–ª—è–µ—Ç MENTIONS edges –æ—Ç Chunks –∫ Concepts.
            
            –ò—â–µ—Ç —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤ (primary term –∏ aliases) –≤ —Ç–µ–∫—Å—Ç–µ —á–∞–Ω–∫–æ–≤
            —Å–æ–≥–ª–∞—Å–Ω–æ –ø—Ä–∞–≤–∏–ª–∞–º:
            - Full word matches only (–Ω–µ –ø–æ–¥—Å—Ç—Ä–æ–∫–∏)
            - Case-insensitive
            - Exact forms only (–±–µ–∑ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–∏)
            
            Args:
                chunk_nodes: –°–ø–∏—Å–æ–∫ —É–∑–ª–æ–≤ —Ç–∏–ø–∞ Chunk –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
                
            Returns:
                –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö MENTIONS edges
            """
            if not self.concept_dictionary.get('concepts'):
                return 0
                
            edges_added = 0
            
            # –°–æ–±–∏—Ä–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ MENTIONS edges —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
            existing_mentions = set()
            for edge in self.learning_graph.get('edges', []):
                if edge.get('type') == 'MENTIONS':
                    existing_mentions.add((edge['source'], edge['target']))
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π Chunk —É–∑–µ–ª
            for chunk in chunk_nodes:
                if chunk.get('type') != 'Chunk':
                    continue
                    
                chunk_text = chunk.get('text', '')
                if not chunk_text:
                    continue
                    
                chunk_id = chunk['id']
                chunk_text_lower = chunk_text.lower()
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–π –∫–æ–Ω—Ü–µ–ø—Ç
                for concept in self.concept_dictionary['concepts']:
                    concept_id = concept['concept_id']
                    
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ MENTIONS edge —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
                    if (chunk_id, concept_id) in existing_mentions:
                        continue
                    
                    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ç–µ—Ä–º–∏–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ (primary + aliases)
                    terms_to_search = []
                    
                    primary_term = concept.get('term', {}).get('primary')
                    if primary_term:
                        terms_to_search.append(primary_term)
                        
                    aliases = concept.get('term', {}).get('aliases', [])
                    terms_to_search.extend(aliases)
                    
                    # –ò—â–µ–º –∫–∞–∂–¥—ã–π —Ç–µ—Ä–º–∏–Ω
                    found = False
                    for term in terms_to_search:
                        if not term:
                            continue
                            
                        # –°–æ–∑–¥–∞–µ–º —Ä–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è full word match
                        # \b - –≥—Ä–∞–Ω–∏—Ü–∞ —Å–ª–æ–≤–∞, —Ä–∞–±–æ—Ç–∞–µ—Ç —Å Unicode
                        pattern = r'\b' + re.escape(term.lower()) + r'\b'
                        
                        if re.search(pattern, chunk_text_lower):
                            found = True
                            break
                    
                    # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ - –¥–æ–±–∞–≤–ª—è–µ–º MENTIONS edge
                    if found:
                        mentions_edge = {
                            'source': chunk_id,
                            'target': concept_id,
                            'type': 'MENTIONS',
                            'weight': 1.0
                        }
                        
                        self.learning_graph['edges'].append(mentions_edge)
                        existing_mentions.add((chunk_id, concept_id))
                        edges_added += 1
                        
                        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ DEBUG —Ä–µ–∂–∏–º–µ
                        if self.config['log_level'].lower() == 'debug':
                            self.logger.debug(json.dumps({
                                "timestamp": datetime.now().isoformat(),
                                "level": "DEBUG",
                                "event": "mentions_edge_added",
                                "source": chunk_id,
                                "target": concept_id,
                                "found_term": term
                            }))
            
            if edges_added > 0:
                self.stats.total_edges += edges_added
                
                # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
                self.logger.info(json.dumps({
                    "timestamp": datetime.now().isoformat(),
                    "level": "INFO",
                    "event": "mentions_edges_added",
                    "count": edges_added,
                    "chunks_processed": len(chunk_nodes)
                }))
            
            return edges_added
    
    def _process_llm_response(self, response_text: str, slice_id: str) -> Tuple[bool, Optional[Dict]]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ LLM.
        
        Args:
            response_text: –¢–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM
            slice_id: ID —Ç–µ–∫—É—â–µ–≥–æ —Å–ª–∞–π—Å–∞
            
        Returns:
            (success, parsed_data) - —É—Å–ø–µ—Ö –∏ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–ª–∏ None
        """
        try:
            # –ü—Ä–æ–∞–∫—Ç–∏–≤–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º –ø–µ—Ä–µ–¥ –ø–∞—Ä—Å–∏–Ω–≥–æ–º
            cleaned_text = response_text
            
            # 1. –ò—Å–ø—Ä–∞–≤–ª—è–µ–º HTML –∞—Ç—Ä–∏–±—É—Ç—ã —Å –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –∫–∞–≤—ã—á–∫–∞–º–∏
            # –ü–∞—Ç—Ç–µ—Ä–Ω: attr='\"value\"' -> attr="value"
            cleaned_text = re.sub(
                r'(\b(?:href|src|target|action|name|frameborder|width|height|align))=\'\"([^\"]*?)\"\'', 
                r'\1="\2"', 
                cleaned_text
            )
            
            # 2. –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –æ–±—Ä–∞—Ç–Ω—ã–π —Å–ª—É—á–∞–π: attr="'value'"  -> attr="value"
            cleaned_text = re.sub(
                r'(\b(?:href|src|target|action|name|frameborder|width|height|align))="\'([^\']*?)\'"', 
                r'\1="\2"', 
                cleaned_text
            )
            
            # –õ–æ–≥–∏—Ä—É–µ–º –µ—Å–ª–∏ –±—ã–ª–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è
            if cleaned_text != response_text:
                self.logger.debug(json.dumps({
                    "timestamp": datetime.now().isoformat(),
                    "level": "DEBUG",
                    "event": "response_cleaned",
                    "slice_id": slice_id,
                    "message": "Applied HTML attribute cleanup"
                }))
            
            # –ü–∞—Ä—Å–∏–Ω–≥ JSON
            response_data = json.loads(cleaned_text)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            if 'concepts_added' not in response_data or 'chunk_graph_patch' not in response_data:
                raise ValueError("Missing required fields in response")
            
            concepts_added = response_data['concepts_added'].get('concepts', [])
            patch = response_data['chunk_graph_patch']
            
            # –ë–∞–∑–æ–≤–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ —Å—Ö–µ–º–∞–º (—Ç–æ–ª—å–∫–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞)
            validate_json({'concepts': concepts_added}, 'ConceptDictionary')
            validate_json(patch, 'LearningChunkGraph')
            
            return True, response_data
            
        except (json.JSONDecodeError, ValueError, ValidationError) as e:
            self.logger.error(json.dumps({
                "timestamp": datetime.now().isoformat(),
                "level": "ERROR",
                "event": "response_validation_failed",
                "slice_id": slice_id,
                "error": str(e)
            }))
            return False, None

    def _apply_patch(self, patch_data: Dict) -> Tuple[int, int]:
            """
            –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø–∞—Ç—á–∞ –∫ –≥—Ä–∞—Ñ—É –∑–Ω–∞–Ω–∏–π.
            
            Args:
                patch_data: –î–∞–Ω–Ω—ã–µ –ø–∞—Ç—á–∞ —Å concepts_added –∏ chunk_graph_patch
                
            Returns:
                (nodes_added, edges_added) - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö —É–∑–ª–æ–≤ –∏ —Ä—ë–±–µ—Ä
            """
            nodes_added = 0
            edges_added = 0
            
            # –û–±–Ω–æ–≤–ª—è–µ–º ConceptDictionary
            concepts_to_add = patch_data['concepts_added'].get('concepts', [])
            self._update_concept_dictionary(concepts_to_add)
            
            # –°–æ–∑–¥–∞–µ–º —É–∑–ª—ã —Ç–∏–ø–∞ Concept –¥–ª—è –Ω–æ–≤—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤ –∏–∑ concepts_added
            for concept in concepts_to_add:
                concept_id = concept['concept_id']
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ —Ç–∞–∫–æ–π —É–∑–µ–ª
                if concept_id not in self.known_node_ids:
                    concept_node = {
                        "id": concept_id,
                        "type": "Concept",
                        "text": concept['term']['primary'],
                        "definition": concept['definition'],
                        "local_start": 0  # –ö–æ–Ω—Ü–µ–ø—Ç—ã –Ω–µ –∏–º–µ—é—Ç –ø–æ–∑–∏—Ü–∏–∏ –≤ —Ç–µ–∫—Å—Ç–µ, —Å—Ç–∞–≤–∏–º 0
                    }
                    self.learning_graph['nodes'].append(concept_node)
                    self.known_node_ids.add(concept_id)
                    nodes_added += 1
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —É–∑–ª—ã –∏–∑ –ø–∞—Ç—á–∞
            new_nodes = patch_data['chunk_graph_patch'].get('nodes', [])
            nodes_to_add = self._process_chunk_nodes(new_nodes)
            self.learning_graph['nodes'].extend(nodes_to_add)
            nodes_added += len(nodes_to_add)
            self.stats.total_nodes += nodes_added
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä—ë–±—Ä–∞
            new_edges = patch_data['chunk_graph_patch'].get('edges', [])
            valid_edges = self._validate_edges(new_edges)
            self.learning_graph['edges'].extend(valid_edges)
            edges_added = len(valid_edges)
            self.stats.total_edges += edges_added
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ MENTIONS edges
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ –Ω–æ–≤—ã–µ —É–∑–ª—ã, —Ç–∞–∫ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ
            chunk_nodes_to_process = []
            
            # –ù–æ–≤—ã–µ —É–∑–ª—ã —Ç–∏–ø–∞ Chunk
            for node in nodes_to_add:
                if node.get('type') == 'Chunk':
                    chunk_nodes_to_process.append(node)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ —É–∑–ª—ã (–∏–∑ _process_chunk_nodes)
            for node in new_nodes:
                if node.get('type') == 'Chunk' and node['id'] in self.known_node_ids:
                    # –ù–∞—Ö–æ–¥–∏–º —É–∑–µ–ª –≤ –≥—Ä–∞—Ñ–µ (–æ–Ω –º–æ–≥ –±—ã—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω)
                    for graph_node in self.learning_graph['nodes']:
                        if graph_node['id'] == node['id']:
                            chunk_nodes_to_process.append(graph_node)
                            break
            
            # –î–æ–±–∞–≤–ª—è–µ–º MENTIONS edges
            mentions_added = self._add_mentions_edges(chunk_nodes_to_process)
            edges_added += mentions_added
            
            return nodes_added, edges_added
    
    def _save_bad_response(self, slice_id: str, original_response: str, 
                          error: str, repair_response: Optional[str] = None) -> None:
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.
        
        Args:
            slice_id: ID —Å–ª–∞–π—Å–∞
            original_response: –ü–µ—Ä–≤—ã–π –æ—Ç–≤–µ—Ç LLM
            error: –û–ø–∏—Å–∞–Ω–∏–µ –æ—à–∏–±–∫–∏
            repair_response: –û—Ç–≤–µ—Ç –ø–æ—Å–ª–µ repair (–µ—Å–ª–∏ –±—ã–ª)
        """
        bad_response_file = LOGS_DIR / f"{slice_id}_bad.json"
        bad_data = {
            "slice_id": slice_id,
            "timestamp": datetime.now().isoformat(),
            "original_response": original_response,
            "validation_error": error,
            "repair_response": repair_response
        }
        
        bad_response_file.write_text(
            json.dumps(bad_data, ensure_ascii=False, indent=2),
            encoding='utf-8'
        )
    
    def _save_temp_dumps(self, reason: str) -> None:
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–∞–º–ø–æ–≤ –ø—Ä–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–∫–∞—Ö.
        
        Args:
            reason: –ü—Ä–∏—á–∏–Ω–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (validation_failed, io_error, etc.)
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # –ü—É—Ç–∏ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        temp_concept_path = LOGS_DIR / f"ConceptDictionary_temp_{reason}_{timestamp}.json"
        temp_graph_path = LOGS_DIR / f"LearningChunkGraph_temp_{reason}_{timestamp}.json"
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º ConceptDictionary
        if self.concept_dictionary and self.concept_dictionary.get('concepts'):
            temp_concept_path.write_text(
                json.dumps(self.concept_dictionary, ensure_ascii=False, indent=2),
                encoding='utf-8'
            )
            print(f"Temporary ConceptDictionary saved to: {temp_concept_path}", file=sys.stderr)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º LearningChunkGraph
        if self.learning_graph and (self.learning_graph.get('nodes') or self.learning_graph.get('edges')):
            temp_graph_path.write_text(
                json.dumps(self.learning_graph, ensure_ascii=False, indent=2),
                encoding='utf-8'
            )
            print(f"Temporary LearningChunkGraph saved to: {temp_graph_path}", file=sys.stderr)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–±—Ä–∞–±–æ—Ç–∫–∏
        stats_path = LOGS_DIR / f"processing_stats_{reason}_{timestamp}.json"
        stats_data = {
            "timestamp": datetime.now().isoformat(),
            "reason": reason,
            "stats": {
                "total_slices": self.stats.total_slices,
                "processed_slices": self.stats.processed_slices,
                "failed_slices": self.stats.failed_slices,
                "total_concepts": self.stats.total_concepts,
                "total_nodes": self.stats.total_nodes,
                "total_edges": self.stats.total_edges,
                "total_tokens_used": self.stats.total_tokens_used,
                "processing_time": str(datetime.now() - self.stats.start_time)
            }
        }
        stats_path.write_text(
            json.dumps(stats_data, ensure_ascii=False, indent=2),
            encoding='utf-8'
        )
        print(f"Processing stats saved to: {stats_path}", file=sys.stderr)
    
    def _process_single_slice(self, slice_file: Path) -> bool:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —Å–ª–∞–π—Å–∞.
        
        Args:
            slice_file: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å–ª–∞–π—Å–∞
            
        Returns:
            True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ, False –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ª–∞–π—Å–∞
            slice_data = self._load_slice(slice_file)
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞—á–∞–ª–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
            self.logger.info(json.dumps({
                "timestamp": datetime.now().isoformat(),
                "level": "INFO",
                "event": "slice_start",
                "slice_id": slice_data.id,
                "order": slice_data.order,
                "total": self.stats.total_slices
            }))
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            input_data = self._format_slice_input(slice_data)
            
            # –í—ã–∑–æ–≤ LLM
            start_time = time.time()
            
            # DEBUG –ª–æ–≥ –ø—Ä–æ–º–ø—Ç–∞
            if self.config['log_level'].lower() == 'debug':
                self.logger.debug(json.dumps({
                    "timestamp": datetime.now().isoformat(),
                    "level": "DEBUG",
                    "event": "llm_request",
                    "slice_id": slice_data.id,
                    "prompt": self.extraction_prompt,
                    "input_data": json.loads(input_data)
                }))
            
            try:
                response_text, response_id, usage = self.llm_client.create_response(
                    instructions=self.extraction_prompt,
                    input_data=input_data
                )
                
                # DEBUG –ª–æ–≥ –æ—Ç–≤–µ—Ç–∞
                if self.config['log_level'].lower() == 'debug':
                    self.logger.debug(json.dumps({
                        "timestamp": datetime.now().isoformat(),
                        "level": "DEBUG",
                        "event": "llm_response",
                        "slice_id": slice_data.id,
                        "response": response_text,
                        "response_id": response_id,
                        "usage": {
                            "input_tokens": usage.input_tokens,
                            "output_tokens": usage.output_tokens,
                            "reasoning_tokens": usage.reasoning_tokens
                        }
                    }))
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞
                success, parsed_data = self._process_llm_response(response_text, slice_data.id)
                
                if not success:
                    # –ü–æ–ø—ã—Ç–∫–∞ repair —Å —É—Ç–æ—á–Ω—è—é—â–∏–º –ø—Ä–æ–º–ø—Ç–æ–º
                    self.logger.info(json.dumps({
                        "timestamp": datetime.now().isoformat(),
                        "level": "INFO",
                        "event": "repair_attempt",
                        "slice_id": slice_data.id
                    }))

                    # –î–æ–±–∞–≤–ª—è–µ–º –≤—ã–≤–æ–¥ –≤ –∫–æ–Ω—Å–æ–ª—å
                    current_time = datetime.now().strftime("%H:%M:%S")
                    print(f"[{current_time}] REPAIR   | üîß Attempting to fix JSON validation error...")
                    print(f"[{current_time}] REPAIR   | üìù Adding clarification to prompt and retrying...")

                    # –§–æ—Ä–º–∏—Ä—É–µ–º repair –ø—Ä–æ–º–ø—Ç —Å —É—Ç–æ—á–Ω–µ–Ω–∏–µ–º
                    repair_instructions = (
                        f"{self.extraction_prompt}\n\n"
                        "IMPORTANT: Your previous response was not valid JSON or did not match the required schema. "
                        "Please ensure your response is EXACTLY one valid JSON object with the structure shown above. "
                        "Do not include any text before or after the JSON object."
                    )
                    
                    # repair_response –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π previous_response_id
                    repair_text, repair_id, repair_usage = self.llm_client.repair_response(
                        instructions=repair_instructions,
                        input_data=input_data
                    )
                    
                    success, parsed_data = self._process_llm_response(repair_text, slice_data.id)
                    
                    if success:
                        # Repair —É—Å–ø–µ—à–µ–Ω
                        current_time = datetime.now().strftime("%H:%M:%S")
                        print(f"[{current_time}] REPAIR   | ‚úÖ JSON validation fixed successfully!")
                    else:
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–ª–æ—Ö–∏–µ –æ—Ç–≤–µ—Ç—ã
                        self._save_bad_response(
                            slice_data.id, 
                            response_text,
                            "JSON validation failed after repair",
                            repair_text
                        )
                        
                        self.logger.error(json.dumps({
                            "timestamp": datetime.now().isoformat(),
                            "level": "ERROR",
                            "event": "slice_failed",
                            "slice_id": slice_data.id,
                            "error": "JSON validation failed after repair"
                        }))
                        
                        # –í—ã–≤–æ–¥ –æ—à–∏–±–∫–∏ –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª
                        current_time = datetime.now().strftime("%H:%M:%S")
                        print(f"[{current_time}] ERROR    | ‚ùå {slice_data.order:03d}/{self.stats.total_slices:03d} | "
                              f"{slice_data.id} | JSON validation failed after repair")
                        
                        return False
                    
                    # Repair —É—Å–ø–µ—à–µ–Ω - –∏—Å–ø–æ–ª—å–∑—É–µ–º repair usage
                    usage = repair_usage
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–∞—Ç—á
                nodes_added, edges_added = self._apply_patch(parsed_data)

                # –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –ø–∞—Ç—á–∞
                try:
                    validate_graph_invariants_intermediate(self.learning_graph)
                    validate_concept_dictionary_invariants(self.concept_dictionary)
                except (ValidationError, GraphInvariantError) as e:
                    self.logger.error(json.dumps({
                        "timestamp": datetime.now().isoformat(),
                        "level": "ERROR", 
                        "event": "incremental_validation_failed",
                        "slice_id": slice_data.id,
                        "error": str(e)
                    }))
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                    self._save_temp_dumps(f"validation_error_slice_{slice_data.id}")
                    
                    # –í—ã–≤–æ–¥ –≤ –∫–æ–Ω—Å–æ–ª—å
                    current_time = datetime.now().strftime("%H:%M:%S")
                    print(f"[{current_time}] ERROR    | ‚ùå Incremental validation failed for {slice_data.id}")
                    print(f"[{current_time}] ERROR    | üìã Error: {str(e)[:100]}...")
                    
                    # –ù–ï –ø–∞–¥–∞–µ–º —Å—Ä–∞–∑—É, –ø–æ–º–µ—á–∞–µ–º slice –∫–∞–∫ failed
                    return False
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                self.stats.total_tokens_used += usage.total_tokens
                duration_sec = round(time.time() - start_time, 0)
                duration_ms = int((time.time() - start_time) * 1000)
                
                # –í—ã–≤–æ–¥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª
                current_time = datetime.now().strftime("%H:%M:%S")

                # –§–æ—Ä–º–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–∫–µ–Ω–∞—Ö
                tokens_info = f"tokens_used={self._format_tokens(self.stats.total_tokens_used)} | tokens_current={self._format_tokens(usage.total_tokens)}"
                if usage.reasoning_tokens > 0:
                    tokens_info += f" incl. reasoning={self._format_tokens(usage.reasoning_tokens)}"

                print(f"[{current_time}] SLICE    | ‚úÖ {slice_data.order:03d}/{self.stats.total_slices:03d} | "
                      f"{tokens_info} | {duration_sec}s | "
                      f"concepts={len(self.concept_dictionary['concepts'])} | "
                      f"nodes={self.stats.total_nodes} | edges={self.stats.total_edges}")
                
                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —É—Å–ø–µ—Ö–∞
                self.logger.info(json.dumps({
                    "timestamp": datetime.now().isoformat(),
                    "level": "INFO",
                    "event": "slice_success",
                    "slice_id": slice_data.id,
                    "tokens_used": usage.total_tokens,
                    "duration_ms": duration_ms,
                    "concepts_total": len(self.concept_dictionary['concepts']),
                    "nodes_added": nodes_added,
                    "edges_added": edges_added
                }))
                
                return True
                
            except Exception as e:
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ API
                error_type = type(e).__name__

                # –í–ê–ñ–ù–û: –û–±–Ω—É–ª—è–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ undefined
                response_text = None
                response_id = None
                usage = None
                
                # –í—ã–≤–æ–¥ –æ—à–∏–±–∫–∏ –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª
                current_time = datetime.now().strftime("%H:%M:%S")
                
                # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è rate limit
                if "rate" in str(e).lower() or error_type == "RateLimitError":
                    # LLM –∫–ª–∏–µ–Ω—Ç —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–µ—Ç retry —Å backoff
                    print(f"[{current_time}] ERROR    | ‚ö†Ô∏è {error_type} | waiting for retry...")
                else:
                    print(f"[{current_time}] ERROR    | ‚ö†Ô∏è {error_type} | slice {slice_data.id}")
                
                self.logger.error(json.dumps({
                    "timestamp": datetime.now().isoformat(),
                    "level": "ERROR",
                    "event": "api_error",
                    "slice_id": slice_data.id,
                    "error_type": error_type,
                    "error": str(e)
                }))
                
                # –ï—Å–ª–∏ –≤—Å–µ retry –∏—Å—á–µ—Ä–ø–∞–Ω—ã, —Å—á–∏—Ç–∞–µ–º —Å–ª–∞–π—Å failed
                return False
                
        except Exception as e:
            # –û–±—â–∞—è –æ—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
            self.logger.error(json.dumps({
                "timestamp": datetime.now().isoformat(),
                "level": "ERROR",
                "event": "slice_processing_error",
                "slice_file": str(slice_file),
                "error": str(e)
            }))
            return False
    
    def run(self) -> int:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∑–∞–ø—É—Å–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏.
        
        Returns:
            –ö–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–∞–º–º—ã
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Å–ª–∞–π—Å–æ–≤
            slice_files = sorted(STAGING_DIR.glob("*.slice.json"))
            if not slice_files:
                self.logger.error("No slice files found in staging directory")
                return EXIT_INPUT_ERROR
            
            self.stats.total_slices = len(slice_files)
            
            # –í—ã–≤–æ–¥ –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ —Å—Ç–∞—Ç—É—Å–∞
            self._print_start_status()
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ª–∞–π—Å–æ–≤
            for slice_file in slice_files:
                try:
                    success = self._process_single_slice(slice_file)
                    if success:
                        self.stats.processed_slices += 1
                    else:
                        self.stats.failed_slices += 1
                        
                    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
                    if self.stats.processed_slices % 10 == 0 and self.stats.processed_slices > 0:
                        self.logger.info(json.dumps({
                            "timestamp": datetime.now().isoformat(),
                            "level": "INFO",
                            "event": "progress_checkpoint",
                            "processed": self.stats.processed_slices,
                            "failed": self.stats.failed_slices,
                            "total": self.stats.total_slices
                        }))
                        
                except KeyboardInterrupt:
                    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º
                    self.logger.warning("Processing interrupted by user")
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    if self.stats.processed_slices > 0:
                        self.logger.info(f"Processed {self.stats.processed_slices}/{self.stats.total_slices} slices before interruption")
                        try:
                            self._save_temp_dumps("interrupted")
                            self.logger.info("Partial results saved to logs directory")
                        except Exception as e:
                            self.logger.error(f"Failed to save partial results: {e}")
                    
                    return EXIT_RUNTIME_ERROR
                    
                except Exception as e:
                    # –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å–ª–∞–π—Å–∞
                    self.logger.error(f"Unexpected error processing {slice_file}: {e}")
                    self.stats.failed_slices += 1
                    # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Å–ª–∞–π—Å–æ–≤
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ—Ö —Å–ª–∞–π—Å–æ–≤
            if self.stats.processed_slices == 0:
                self.logger.error("All slices failed processing")
                
                # –í—ã–≤–æ–¥ —Å—Ç–∞—Ç—É—Å–∞ –æ—à–∏–±–∫–∏
                current_time = datetime.now().strftime("%H:%M:%S")
                print(f"[{current_time}] FAILED   | ‚ùå All slices failed processing")
                print(f"[{current_time}] SAVING   | üíæ Attempting to save empty structures...")

                # –ü—ã—Ç–∞–µ–º—Å—è —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ö–æ—Ç—è –±—ã –ø—É—Å—Ç—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
                try:
                    self._save_temp_dumps("all_failed")
                    print(f"[{current_time}] INFO     | Check /logs/ for temporary files and diagnostics")
                except Exception as dump_error:
                    current_time = datetime.now().strftime("%H:%M:%S")
                    print(f"[{current_time}] ERROR    | ‚ö†Ô∏è Failed to save temp dumps: {dump_error}", file=sys.stderr)
                
                return EXIT_RUNTIME_ERROR
            
            # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –µ—Å–ª–∏ —á–∞—Å—Ç—å —Å–ª–∞–π—Å–æ–≤ failed
            if self.stats.failed_slices > 0:
                failure_rate = self.stats.failed_slices / self.stats.total_slices
                self.logger.warning(f"Partial failure: {self.stats.failed_slices}/{self.stats.total_slices} slices failed ({failure_rate:.1%})")
                
                # –ï—Å–ª–∏ –±–æ–ª—å—à–µ 50% failed - –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–∞–µ–º
                if failure_rate > 0.5:
                    self.logger.warning(f"High failure rate ({failure_rate:.1%}) - results may be incomplete")
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            return self._finalize_and_save()
            
        except Exception as e:
            # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞
            self.logger.error(f"Critical error in run(): {e}")
            
            # –í—ã–≤–æ–¥ —Å—Ç–∞—Ç—É—Å–∞ –æ—à–∏–±–∫–∏
            current_time = datetime.now().strftime("%H:%M:%S")
            print(f"[{current_time}] FAILED   | ‚ùå Critical error: {str(e)[:50]}...")
            print(f"[{current_time}] SAVING   | üíæ Emergency dump of current state...")
            
            # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ
            try:
                self._save_temp_dumps("critical_error")
                print(f"[{current_time}] INFO     | Check /logs/ for temporary files and diagnostics")
            except Exception as dump_error:
                current_time = datetime.now().strftime("%H:%M:%S")
                print(f"[{current_time}] ERROR    | ‚ö†Ô∏è Failed to save emergency dumps: {dump_error}", file=sys.stderr)
                
            return EXIT_RUNTIME_ERROR
    
    def _print_start_status(self):
        """–í—ã–≤–æ–¥ –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ —Å—Ç–∞—Ç—É—Å–∞ –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª."""
        current_time = datetime.now().strftime("%H:%M:%S")
        print(f"[{current_time}] START    | {self.stats.total_slices} slices | "
              f"model={self.config['model']} | tpm={self.config['tpm_limit']//1000}k")
    
    def _finalize_and_save(self) -> int:
        """
        –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
        
        Returns:
            –ö–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        """
        try:
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ —Å—Ö–µ–º–∞–º
            validate_json(self.concept_dictionary, "ConceptDictionary")
            validate_json(self.learning_graph, "LearningChunkGraph")
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏–Ω–≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—É—é –≤–∞–ª–∏–¥–∞—Ü–∏—é, —Ç–∞–∫ –∫–∞–∫ –º–æ–≥—É—Ç –±—ã—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤
            validate_concept_dictionary_invariants(self.concept_dictionary)
            validate_graph_invariants_intermediate(self.learning_graph)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤
            concept_path = OUTPUT_DIR / "ConceptDictionary.json"
            graph_path = OUTPUT_DIR / "LearningChunkGraph_raw.json"
            
            concept_path.write_text(
                json.dumps(self.concept_dictionary, ensure_ascii=False, indent=2),
                encoding='utf-8'
            )
            graph_path.write_text(
                json.dumps(self.learning_graph, ensure_ascii=False, indent=2),
                encoding='utf-8'
            )
            
            # –í—ã–≤–æ–¥ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Å—Ç–∞—Ç—É—Å–∞
            self._print_end_status()

            # –í—ã–≤–æ–¥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–∞—Ö
            current_time = datetime.now().strftime("%H:%M:%S")
            print(f"[{current_time}] SUCCESS  | ‚úÖ Results saved to /data/out/")
            print(f"                           | - ConceptDictionary.json")
            print(f"                           | - LearningChunkGraph_raw.json")
            
            return EXIT_SUCCESS
            
        except (ValidationError, GraphInvariantError) as e:
            self.logger.error(f"Validation failed: {e}")
            
            # –í—ã–≤–æ–¥ —Å—Ç–∞—Ç—É—Å–∞ –æ—à–∏–±–∫–∏
            current_time = datetime.now().strftime("%H:%M:%S")
            print(f"[{current_time}] FAILED   | ‚ùå Validation failed: {str(e)[:50]}...")
            print(f"[{current_time}] SAVING   | üíæ Attempting to save partial results...")
            
            # –ü–æ–ø—ã—Ç–∫–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
            try:
                self._save_temp_dumps("validation_failed")
                print(f"[{current_time}] INFO     | Check /logs/ for temporary files and diagnostics")
            except Exception as dump_error:
                current_time = datetime.now().strftime("%H:%M:%S")
                print(f"[{current_time}] ERROR    | ‚ö†Ô∏è Failed to save temp dumps: {dump_error}", file=sys.stderr)
                
            return EXIT_RUNTIME_ERROR
            
        except Exception as e:
            self.logger.error(f"Failed to save output files: {e}")
            
            # –ü–æ–ø—ã—Ç–∫–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
            try:
                self._save_temp_dumps("io_error")
            except Exception as dump_error:
                current_time = datetime.now().strftime("%H:%M:%S")
                print(f"[{current_time}] ERROR    | ‚ö†Ô∏è Failed to save temp dumps: {dump_error}", file=sys.stderr)
                
            return EXIT_IO_ERROR
    
    def _print_end_status(self):
        """–í—ã–≤–æ–¥ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Å—Ç–∞—Ç—É—Å–∞ –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª."""
        current_time = datetime.now().strftime("%H:%M:%S")
        duration = datetime.now() - self.stats.start_time
        minutes, seconds = divmod(int(duration.total_seconds()), 60)
        
        print(f"[{current_time}] END      | Done | slices={self.stats.processed_slices} | "
            f"time={minutes}m {seconds}s")


def main():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –≤ –ø—Ä–æ–≥—Ä–∞–º–º—É."""
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        config = load_config(CONFIG_PATH)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        processor = SliceProcessor(config)
        return processor.run()
        
    except FileNotFoundError as e:
        return EXIT_CONFIG_ERROR
    except Exception as e:
        return EXIT_CONFIG_ERROR


if __name__ == "__main__":
    sys.exit(main())