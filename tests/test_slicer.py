"""
Unit —Ç–µ—Å—Ç—ã –¥–ª—è –º–æ–¥—É–ª—è slicer.py

–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –æ—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞:
- create_slug()
- preprocess_text()
- validate_config_parameters()
"""

# –î–æ–±–∞–≤–ª—è–µ–º src –≤ path –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
import sys
import unicodedata
from pathlib import Path

import pytest

sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from slicer import (create_slug, preprocess_text, slice_text_with_window,
                    validate_config_parameters)
from utils.tokenizer import count_tokens


class TestCreateSlug:
    """–¢–µ—Å—Ç—ã –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ create_slug."""

    def test_cyrillic_transliteration(self):
        """–¢–µ—Å—Ç —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏–∏ –∫–∏—Ä–∏–ª–ª–∏—Ü—ã."""
        result = create_slug("–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –°—Ç—Ä—É–∫—Ç—É—Ä—ã.txt")
        assert result == "algoritmy_i_struktury"

    def test_english_with_spaces(self):
        """–¢–µ—Å—Ç –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å –ø—Ä–æ–±–µ–ª–∞–º–∏."""
        result = create_slug("My Course Chapter 1.md")
        assert result == "my_course_chapter_1"

    def test_hyphens_preserved(self):
        """–¢–µ—Å—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–µ—Ñ–∏—Å–æ–≤."""
        result = create_slug("python-basics.html")
        assert result == "python-basics"

    def test_extension_removal(self):
        """–¢–µ—Å—Ç —É–¥–∞–ª–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–π."""
        assert create_slug("test.json") == "test"
        assert create_slug("file.HTML") == "file"
        assert create_slug("document.MD") == "document"

    def test_complex_filename(self):
        """–¢–µ—Å—Ç —Å–ª–æ–∂–Ω–æ–≥–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞."""
        result = create_slug("–ì–ª–∞–≤–∞ 2. –û—Å–Ω–æ–≤—ã Python (—á–∞—Å—Ç—å-1).txt")
        assert result == "glava_2._osnovy_python_(chast'-1)"

    def test_special_characters(self):
        """–¢–µ—Å—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤."""
        result = create_slug("File@#$%^&*()Name.md")
        assert result == "file@#$%^&*()name"


class TestPreprocessText:
    """–¢–µ—Å—Ç—ã –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ preprocess_text."""

    def test_unicode_normalization(self):
        """–¢–µ—Å—Ç Unicode –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ NFC."""
        # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç —Å –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Å–∏–º–≤–æ–ª–∞–º–∏
        text_with_combining = "caf√©"  # e + ÃÅ (combining acute accent)
        result = preprocess_text(text_with_combining)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∞
        expected = unicodedata.normalize("NFC", text_with_combining)
        assert result == expected

    def test_script_tag_removal(self):
        """–¢–µ—Å—Ç —É–¥–∞–ª–µ–Ω–∏—è script —Ç–µ–≥–æ–≤."""
        html_with_script = """
        <html>
        <head>
        <script>alert('dangerous code');</script>
        </head>
        <body>
        <p>–ü–æ–ª–µ–∑–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç</p>
        </body>
        </html>
        """

        result = preprocess_text(html_with_script)

        # Script —Ç–µ–≥ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —É–¥–∞–ª–µ–Ω
        assert "<script>" not in result
        assert "alert('dangerous code');" not in result
        # –ü–æ–ª–µ–∑–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –æ—Å—Ç–∞—Ç—å—Å—è
        assert "–ü–æ–ª–µ–∑–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç" in result

    def test_style_tag_removal(self):
        """–¢–µ—Å—Ç —É–¥–∞–ª–µ–Ω–∏—è style —Ç–µ–≥–æ–≤."""
        html_with_style = """
        <html>
        <head>
        <style>
        body { background: red; }
        .danger { color: evil; }
        </style>
        </head>
        <body>
        <h1>–ó–∞–≥–æ–ª–æ–≤–æ–∫</h1>
        </body>
        </html>
        """

        result = preprocess_text(html_with_style)

        # Style —Ç–µ–≥ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —É–¥–∞–ª–µ–Ω
        assert "<style>" not in result
        assert "background: red" not in result
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –¥–æ–ª–∂–µ–Ω –æ—Å—Ç–∞—Ç—å—Å—è
        assert "–ó–∞–≥–æ–ª–æ–≤–æ–∫" in result

    def test_both_script_and_style_removal(self):
        """–¢–µ—Å—Ç —É–¥–∞–ª–µ–Ω–∏—è –∏ script, –∏ style —Ç–µ–≥–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ."""
        html_mixed = """
        <html>
        <head>
        <script src="evil.js"></script>
        <style>body { display: none; }</style>
        </head>
        <body>
        <h1>–ö–æ–Ω—Ç–µ–Ω—Ç</h1>
        <script>alert('popup');</script>
        </body>
        </html>
        """

        result = preprocess_text(html_mixed)

        assert "<script" not in result
        assert "<style>" not in result
        assert "evil.js" not in result
        assert "display: none" not in result
        assert "alert(" not in result
        assert "–ö–æ–Ω—Ç–µ–Ω—Ç" in result

    def test_plain_text_unchanged(self):
        """–¢–µ—Å—Ç —á—Ç–æ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç –æ—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π."""
        plain_text = """
        –≠—Ç–æ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç –±–µ–∑ HTML —Ç–µ–≥–æ–≤.
        
        –ì–ª–∞–≤–∞ 1. –í–≤–µ–¥–µ–Ω–∏–µ
        
        –ó–¥–µ—Å—å –µ—Å—Ç—å —Ä–∞–∑–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã: 123, abc, –∫–∏—Ä–∏–ª–ª–∏—Ü–∞.
        """

        result = preprocess_text(plain_text)

        # –¢–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –æ—Å—Ç–∞—Ç—å—Å—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ —Ç–µ–º –∂–µ (—Ç–æ–ª—å–∫–æ Unicode –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è)
        normalized_expected = unicodedata.normalize("NFC", plain_text)
        assert result == normalized_expected

    def test_html_without_script_style(self):
        """–¢–µ—Å—Ç HTML –±–µ–∑ script/style —Ç–µ–≥–æ–≤."""
        clean_html = """
        <html>
        <body>
        <h1>–ó–∞–≥–æ–ª–æ–≤–æ–∫</h1>
        <p>–ü–∞—Ä–∞–≥—Ä–∞—Ñ —Å <a href="link">—Å—Å—ã–ª–∫–æ–π</a></p>
        <div class="content">–ö–æ–Ω—Ç–µ–Ω—Ç</div>
        </body>
        </html>
        """

        result = preprocess_text(clean_html)

        # HTML —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–æ–ª–∂–Ω–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å—Å—è
        assert "<h1>" in result
        assert '<a href="link">' in result
        assert "–ó–∞–≥–æ–ª–æ–≤–æ–∫" in result
        assert "—Å—Å—ã–ª–∫–æ–π" in result

    def test_case_insensitive_tag_detection(self):
        """–¢–µ—Å—Ç –Ω–µ—á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫ —Ä–µ–≥–∏—Å—Ç—Ä—É —Ç–µ–≥–æ–≤."""
        html_upper = """
        <HTML>
        <SCRIPT>bad code</SCRIPT>
        <STYLE>bad style</STYLE>
        <BODY>Good content</BODY>
        </HTML>
        """

        result = preprocess_text(html_upper)

        assert "bad code" not in result
        assert "bad style" not in result
        assert "Good content" in result

    def test_invalid_input(self):
        """–¢–µ—Å—Ç –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –≤—Ö–æ–¥–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞."""
        with pytest.raises(ValueError, match="Input parameter must be a string"):
            preprocess_text(123)

        with pytest.raises(ValueError, match="Input parameter must be a string"):
            preprocess_text(None)


class TestValidateConfigParameters:
    """–¢–µ—Å—Ç—ã –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ validate_config_parameters."""

    def test_valid_config(self):
        """–¢–µ—Å—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏."""
        valid_config = {
            "slicer": {
                "max_tokens": 15000,
                "overlap": 0,
                "soft_boundary_max_shift": 500,
                "allowed_extensions": ["json", "txt", "md", "html"],
            }
        }

        # –ù–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –∏—Å–∫–ª—é—á–µ–Ω–∏–π
        validate_config_parameters(valid_config)

    def test_missing_slicer_section(self):
        """–¢–µ—Å—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è —Å–µ–∫—Ü–∏–∏ slicer."""
        config = {"other": {}}

        with pytest.raises(
            ValueError, match="Missing required parameter slicer.max_tokens"
        ):
            validate_config_parameters(config)

    def test_missing_required_param(self):
        """–¢–µ—Å—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞."""
        config = {
            "slicer": {
                "max_tokens": 15000,
                # overlap –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
                "soft_boundary_max_shift": 500,
                "allowed_extensions": ["txt"],
            }
        }

        with pytest.raises(
            ValueError, match="Missing required parameter slicer.overlap"
        ):
            validate_config_parameters(config)

    def test_invalid_max_tokens(self):
        """–¢–µ—Å—Ç –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ max_tokens."""
        config = {
            "slicer": {
                "max_tokens": -100,  # –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                "overlap": 0,
                "soft_boundary_max_shift": 500,
                "allowed_extensions": ["txt"],
            }
        }

        with pytest.raises(ValueError, match="must be a positive integer"):
            validate_config_parameters(config)

    def test_invalid_overlap(self):
        """–¢–µ—Å—Ç –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ overlap."""
        config = {
            "slicer": {
                "max_tokens": 15000,
                "overlap": -5,  # –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                "soft_boundary_max_shift": 500,
                "allowed_extensions": ["txt"],
            }
        }

        with pytest.raises(
            ValueError, match="must be a non-negative integer"
        ):
            validate_config_parameters(config)

    def test_overlap_greater_than_max_tokens(self):
        """–¢–µ—Å—Ç overlap –±–æ–ª—å—à–µ max_tokens."""
        config = {
            "slicer": {
                "max_tokens": 1000,
                "overlap": 1500,  # –ë–æ–ª—å—à–µ max_tokens
                "soft_boundary_max_shift": 500,
                "allowed_extensions": ["txt"],
            }
        }

        with pytest.raises(ValueError, match="must be less than max_tokens"):
            validate_config_parameters(config)

    def test_overlap_validation_with_shift(self):
        """–¢–µ—Å—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–ª—è overlap > 0."""
        config = {
            "slicer": {
                "max_tokens": 15000,
                "overlap": 1000,
                "soft_boundary_max_shift": 900,  # –ë–æ–ª—å—à–µ overlap*0.8 (800)
                "allowed_extensions": ["txt"],
            }
        }

        with pytest.raises(ValueError, match="must not exceed overlap\\*0\\.8"):
            validate_config_parameters(config)

    def test_empty_allowed_extensions(self):
        """–¢–µ—Å—Ç –ø—É—Å—Ç–æ–≥–æ —Å–ø–∏—Å–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–π."""
        config = {
            "slicer": {
                "max_tokens": 15000,
                "overlap": 0,
                "soft_boundary_max_shift": 500,
                "allowed_extensions": [],  # –ü—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
            }
        }

        with pytest.raises(ValueError, match="must be a non-empty list"):
            validate_config_parameters(config)


class TestSliceTextWithWindow:
    """–¢–µ—Å—Ç—ã –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ slice_text_with_window."""

    def test_empty_text(self):
        """–¢–µ—Å—Ç –ø—É—Å—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."""
        result = slice_text_with_window("", 1000, 0, True, 100)
        assert result == []

        result = slice_text_with_window("   ", 1000, 0, True, 100)
        assert result == []

    def test_text_fits_in_one_slice(self):
        """–¢–µ—Å—Ç —Ç–µ–∫—Å—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–µ—â–∞–µ—Ç—Å—è –≤ –æ–¥–∏–Ω —Å–ª–∞–π—Å."""
        text = "–ö–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–ª–æ–≤."
        max_tokens = 1000

        result = slice_text_with_window(text, max_tokens, 0, True, 100)

        assert len(result) == 1
        slice_text, start, end = result[0]
        assert slice_text == text
        assert start == 0
        assert end == count_tokens(text)

    def test_multiple_slices_no_overlap(self):
        """–¢–µ—Å—Ç –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–ª–∞–π—Å–æ–≤ –±–µ–∑ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–π (overlap=0)."""
        # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π —Ç–æ—á–Ω–æ —Ä–∞–∑–¥–µ–ª–∏—Ç—Å—è –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ª–∞–π—Å–æ–≤
        text = "–°–ª–æ–≤–æ. " * 50  # –û–∫–æ–ª–æ 100 —Ç–æ–∫–µ–Ω–æ–≤
        max_tokens = 30

        result = slice_text_with_window(text, max_tokens, 0, False, 0)

        # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ª–∞–π—Å–æ–≤
        assert len(result) > 1

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥—Ä–∞–Ω–∏—Ü—ã –∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤/–ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–π
        for i in range(len(result) - 1):
            current_slice = result[i]
            next_slice = result[i + 1]

            # –ö–æ–Ω–µ—Ü —Ç–µ–∫—É—â–µ–≥–æ —Å–ª–∞–π—Å–∞ = –Ω–∞—á–∞–ª–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ (overlap=0)
            assert current_slice[2] == next_slice[1]

        # –ü–µ—Ä–≤—ã–π —Å–ª–∞–π—Å –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å 0
        assert result[0][1] == 0

        # –ü–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–∞–π—Å –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –Ω–∞ –æ–±—â–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ —Ç–æ–∫–µ–Ω–æ–≤
        total_tokens = count_tokens(text)
        assert result[-1][2] == total_tokens

    def test_multiple_slices_with_overlap(self):
        """–¢–µ—Å—Ç –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–ª–∞–π—Å–æ–≤ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è–º–∏ (overlap>0)."""
        text = "–¢–æ–∫–µ–Ω " * 50  # –û–∫–æ–ª–æ 50 —Ç–æ–∫–µ–Ω–æ–≤
        max_tokens = 20
        overlap = 5

        result = slice_text_with_window(text, max_tokens, overlap, False, 0)

        assert len(result) > 1

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
        for i in range(len(result) - 1):
            current_slice = result[i]
            next_slice = result[i + 1]

            # –ù–∞—á–∞–ª–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ = –∫–æ–Ω–µ—Ü —Ç–µ–∫—É—â–µ–≥–æ - overlap
            expected_next_start = current_slice[2] - overlap
            assert next_slice[1] == expected_next_start

    def test_no_token_loss(self):
        """–¢–µ—Å—Ç —á—Ç–æ —Ç–æ–∫–µ–Ω—ã –Ω–µ —Ç–µ—Ä—è—é—Ç—Å—è –ø—Ä–∏ —Ä–∞–∑–±–∏–µ–Ω–∏–∏."""
        text = "–¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —á—Ç–æ –≤—Å–µ —Ç–æ–∫–µ–Ω—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –ø—Ä–∏ —Ä–∞–∑–±–∏–µ–Ω–∏–∏ –Ω–∞ —Å–ª–∞–π—Å—ã."
        max_tokens = 15

        # –¢–µ—Å—Ç –¥–ª—è overlap=0
        result_no_overlap = slice_text_with_window(text, max_tokens, 0, True, 10)

        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ç–æ–∫–µ–Ω—ã –∏–∑ —Å–ª–∞–π—Å–æ–≤
        total_tokens_from_slices = 0
        for slice_text, start, end in result_no_overlap:
            total_tokens_from_slices += end - start

        original_tokens = count_tokens(text)
        assert total_tokens_from_slices == original_tokens

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –Ω–∞—á–∞–ª–æ –ø–µ—Ä–≤–æ–≥–æ —Å–ª–∞–π—Å–∞ = 0
        assert result_no_overlap[0][1] == 0

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∫–æ–Ω–µ—Ü –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–∞–π—Å–∞ = –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
        assert result_no_overlap[-1][2] == original_tokens

    def test_token_positions_correctness(self):
        """–¢–µ—Å—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –ø–æ–∑–∏—Ü–∏–π —Ç–æ–∫–µ–Ω–æ–≤."""
        text = "–û–¥–∏–Ω –¥–≤–∞ —Ç—Ä–∏ —á–µ—Ç—ã—Ä–µ –ø—è—Ç—å —à–µ—Å—Ç—å —Å–µ–º—å –≤–æ—Å–µ–º—å –¥–µ–≤—è—Ç—å –¥–µ—Å—è—Ç—å."
        max_tokens = 8

        result = slice_text_with_window(text, max_tokens, 0, False, 0)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ slice_token_start inclusive, slice_token_end exclusive
        import tiktoken

        encoding = tiktoken.get_encoding("o200k_base")
        all_tokens = encoding.encode(text)

        for slice_text, start, end in result:
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω—ã –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ
            expected_slice_tokens = all_tokens[start:end]
            expected_slice_text = encoding.decode(expected_slice_tokens)

            assert slice_text == expected_slice_text

    def test_soft_boundaries_applied(self):
        """–¢–µ—Å—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è soft boundaries."""
        # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç —Å —è–≤–Ω—ã–º–∏ –≥—Ä–∞–Ω–∏—Ü–∞–º–∏
        text = """–ì–ª–∞–≤–∞ 1. –ü–µ—Ä–≤–∞—è –≥–ª–∞–≤–∞
        
        –≠—Ç–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–µ—Ä–≤–æ–π –≥–ª–∞–≤—ã —Å –¥–ª–∏–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º.
        
        –ì–ª–∞–≤–∞ 2. –í—Ç–æ—Ä–∞—è –≥–ª–∞–≤–∞
        
        –≠—Ç–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤—Ç–æ—Ä–æ–π –≥–ª–∞–≤—ã."""

        max_tokens = 30

        # –ë–µ–∑ soft boundaries
        result_hard = slice_text_with_window(text, max_tokens, 0, False, 0)

        # –° soft boundaries
        result_soft = slice_text_with_window(text, max_tokens, 0, True, 20)

        # Soft boundaries –º–æ–≥—É—Ç –¥–∞—Ç—å –¥—Ä—É–≥–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ
        # (–Ω–µ –≤—Å–µ–≥–¥–∞, –Ω–æ –≤ –æ–±—â–µ–º —Å–ª—É—á–∞–µ –≤–æ–∑–º–æ–∂–Ω–æ)
        # –ì–ª–∞–≤–Ω–æ–µ —á—Ç–æ –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è

        def get_total_tokens_from_slices(slices):
            return sum(slice_info[2] - slice_info[1] for slice_info in slices)

        total_original = count_tokens(text)
        assert get_total_tokens_from_slices(result_hard) == total_original
        assert get_total_tokens_from_slices(result_soft) == total_original

    def test_unicode_handling(self):
        """–¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ Unicode —Å–∏–º–≤–æ–ª–æ–≤."""
        text = "–ü—Ä–∏–≤–µ—Ç –º–∏—Ä! üåç –≠—Ç–æ —Ç–µ—Å—Ç —ç–º–æ–¥–∑–∏ –∏ —é–Ω–∏–∫–æ–¥–∞: √±√°√©√≠√≥√∫ √ß√±"
        max_tokens = 20

        result = slice_text_with_window(text, max_tokens, 0, True, 5)

        # –û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - –Ω–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –∏—Å–∫–ª—é—á–µ–Ω–∏–π
        assert len(result) >= 1

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ Unicode —Å–∏–º–≤–æ–ª—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è
        combined_text = "".join(slice_text for slice_text, _, _ in result)

        # –í—Å–µ —Å–∏–º–≤–æ–ª—ã –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞ –¥–æ–ª–∂–Ω—ã –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å
        # (–ø–æ—Ä—è–¥–æ–∫ –º–æ–∂–µ—Ç —Å–ª–µ–≥–∫–∞ –æ—Ç–ª–∏—á–∞—Ç—å—Å—è –∏–∑-–∑–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏, –Ω–æ –æ—Å–Ω–æ–≤–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å)
        assert "–ü—Ä–∏–≤–µ—Ç" in combined_text
        assert "üåç" in combined_text
        assert "—ç–º–æ–¥–∑–∏" in combined_text
