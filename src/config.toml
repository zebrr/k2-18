[slicer]
max_tokens              = 5000
overlap                 = 0
soft_boundary           = true
soft_boundary_max_shift = 500
tokenizer               = "o200k_base"
allowed_extensions      = ["txt", "md", "html"]

# gpt-5-2025-08-07 - дорогая
# gpt-5-mini-2025-08-07 - дешевая

[itext2kg]
is_reasoning        = true        # ОБЯЗАТЕЛЬНЫЙ! Иначе падаем с ошибкой. Проверь модель!

# Управление контекстом
truncation = "auto"           # "auto" | "disabled" | закомментировано = не передается
response_chain_depth = 1      # Глубина цепочки (1-N) | закомментировано = без ограничений | 0 = независимые запросы

# Основная модель
model 				= "gpt-5-mini-2025-08-07" # Новый ризонер: 400,000 / 128,000
# model               = "gpt-4.1-2025-04-14" # Не ризонер: 1,047,576 / 32,768 
max_context_tokens  = 400000
tpm_limit           = 450000
max_completion      = 100000

# Тестовая модель (только эти 4 параметра уникальные)
# Тестовая модель по типу должна быть такой же, как и основная (или обе ризонеры или нет)
model_test              = "gpt-5-mini-2025-08-07"  # Новый ризонер: 400,000 / 128,000
# model_test              = "gpt-4.1-mini-2025-04-14" # Не ризонер: 1,047,576 / 32,768 
max_context_tokens_test = 400000
tpm_limit_test          = 450000
max_completion_test     = 100000

# Общие параметры настройки для ОБЕИХ моделей
# temperature         = 0.6
reasoning_effort    = "medium"
reasoning_summary   = "auto"
# verbosity         = "medium"      # Закомментирован = null

# Model for TPM probe requests (optional, defaults to cheapest available)
probe_model = "gpt-4.1-nano-2025-04-14"

# Weight for automatic MENTIONS edges (itext2kg_graph)
auto_mentions_weight = 0.35  # Weak connection, can be strengthened by refiner

# Остальные параметры утилиты (НЕ УДАЛЯТЬ! Копировать текущие значения)
api_key             = "sk-..."  # Set via OPENAI_API_KEY env variable
tpm_safety_margin   = 0.15
log_level           = "info"
timeout             = 1200
max_retries         = 3
poll_interval       = 10

[dedup]
embedding_model     = "text-embedding-3-small"
embedding_api_key   = ""  # Optional, uses OPENAI_API_KEY if not set
embedding_tpm_limit = 1000000                   # Реальный лимит для embedding моделей
max_batch_tokens    = 100000                    # Мягкий лимит токенов на батч
max_texts_per_batch = 2048                      # Максимум текстов в одном запросе
truncate_tokens     = 8000                      # Обрезка текстов (API лимит 8192)
sim_threshold       = 0.85
len_ratio_min       = 0.8
faiss_M             = 32
faiss_efC           = 200
faiss_metric        = "INNER_PRODUCT"
k_neighbors         = 5

[refiner]
is_reasoning        = true        # ОБЯЗАТЕЛЬНЫЙ! Иначе падаем с ошибкой. Проверь модель!

# Управление контекстом
# truncation = "auto"           # "auto" | "disabled" | закомментировано = не передается
response_chain_depth = 0      # Глубина цепочки (1-N) | закомментировано = без ограничений | 0 = независимые запросы

# Основная модель
model 				= "gpt-5-2025-08-07" # Новый ризонер: 400,000 / 128,000
# model               = "gpt-4.1-2025-04-14" # Не ризонер: 1,047,576 / 32,768 
max_context_tokens  = 400000
tpm_limit           = 450000
max_completion      = 100000

# Тестовая модель (только эти 4 параметра уникальные)
# Тестовая модель по типу должна быть такой же, как и основная (или обе ризонеры или нет)
model_test              = "gpt-5-mini-2025-08-07"  # Новый ризонер: 400,000 / 128,000
# model_test              = "gpt-4.1-mini-2025-04-14" # Не ризонер: 1,047,576 / 32,768 
max_context_tokens_test = 400000
tpm_limit_test          = 450000
max_completion_test     = 100000

# Общие параметры настройки для ОБЕИХ моделей
# temperature         = 0.6
reasoning_effort    = "medium"
reasoning_summary   = "auto"
# verbosity         = "medium"      # Закомментирован = null

# Параметры для embeddings и поиска пар
run                 = true
embedding_model     = "text-embedding-3-small"
embedding_api_key   = ""  # Optional, uses OPENAI_API_KEY if not set
embedding_tpm_limit = 1000000                   # Реальный лимит для embedding моделей
max_batch_tokens    = 100000                    # Мягкий лимит токенов на батч
max_texts_per_batch = 2048                      # Максимум текстов в одном запросе
truncate_tokens     = 8000                      # Обрезка текстов (API лимит 8192)
sim_threshold       = 0.65
max_pairs_per_node  = 20
faiss_M             = 32
faiss_metric        = "INNER_PRODUCT"

# НОВЫЕ параметры для двух проходов
enable_backward_pass = true          # Включить второй проход (по умолчанию true)
backward_sim_threshold = 0.70        # Порог схожести для backward (может быть выше)

# УДАЛЕНО: веса теперь прописаны в промптах fw/bw
# weight_low = 0.3
# weight_mid = 0.6
# weight_high = 0.9

# Остальные параметры утилиты
api_key             = "sk-..."  # Set via OPENAI_API_KEY env variable
tpm_safety_margin   = 0.15
log_level           = "info"
timeout             = 1200
max_retries         = 3
poll_interval       = 7