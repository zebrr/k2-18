- 
  ## path

  /handbook/ml/article/about

  ## content

  ## Об этой книге
Эта книга написана коллективом добрых людей, состоящим из преподавателей и выпускников Школы анализа данных. Своим появлением она обязана двум замечательным курсам. Во-первых, это [курс Константина Вячеславовича Воронцова](http://www.machinelearning.ru/wiki/index.php?title=%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5_%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5_%28%D0%BA%D1%83%D1%80%D1%81_%D0%BB%D0%B5%D0%BA%D1%86%D0%B8%D0%B9%2C_%D0%9A.%D0%92.%D0%92%D0%BE%D1%80%D0%BE%D0%BD%D1%86%D0%BE%D0%B2%29), на котором выросло подавляющее большинство авторов книги, да и вообще ML-специалистов в России. Во-вторых, это курс [NLP Course | For You](https://lena-voita.github.io/nlp_course.html) Лены Войта, благодаря которому мы поняли, как должен выглядеть современный учебник, и на который мы будем регулярно ссылаться в частях, связанных с анализом текстов.

Идея была такая: записать сложившийся в ШАДе курс машинного обучения в виде книги, при этом избежав каких-либо компромиссов: нигде ничего не упрощать чрезмерно, дать необходимую теорию, описать и исторически важные алгоритмы, и применяющиеся сегодня, вместе с теорией рассказывать и практические вопросы о реализации алгоритмов и работе с данными. 

Математика — это один из языков, на котором написан учебник. Мы будем стараться давать необходимые пояснения, но всё же уверенное владение линейной алгеброй, математическим анализом и теорией вероятностей будет большим плюсом. Знания статистики и методов выпуклой оптимизации не обязательны, хотя сделают чтение комфортнее.

Читая книгу, вы, возможно, заметите в ней ошибки, неточности и плохо объяснённые детали. В таком случае, пожалуйста, дайте нам знать об этом, написав ([сюда](https://forms.yandex.ru/surveys/academy/?proekt=handbooks)) — так вы поможете и другим читателям. 

Итак, приступим. 


  ## handbook

  Учебник по машинному обучению

  ## title

  Об этой книге

  ## description

  Эта книга написана коллективом добрых людей, состоящим из преподавателей и выпускников Школы анализа данных

- 
  ## path

  /handbook/ml/article/mashinnoye-obucheniye

  ## content

  **Машинное обучение** — это наука, изучающая алгоритмы, автоматически улучшающиеся благодаря опыту.

Когда Алан Тьюринг работал над первыми ([компьютерами](https://ru.wikipedia.org/wiki/Bombe)), он пытался расшифровать сообщения немецких военных, закодированные машиной Энигма. Поиск расшифровки требовал перебора массы вариантов. Люди с этой задачей справлялись плохо, зато машина могла решить её сравнительно быстро. Очевидно, далеко не для каждой задачи, с которой люди справляются с трудом, можно написать программу для эффективного поиска решения. Более того, есть целый класс задач (так называемые NP-трудные задачи), которые нельзя решить за разумное время. Можно даже явно доказать, что никакой компьютер здесь чуда тоже не совершит. Самое интересное это то, что бывают и задачи, которые для людей особенного труда не составляют, но которые почему-то крайне трудно запрограммировать, например:

* перевести текст с одного языка на другой;

* диагностировать болезнь по симптомам;

* сравнить, какой из двух документов в интернете лучше подходит под данный поисковый запрос;

* сказать, что изображено на картинке;

* оценить, по какой цене удастся продать квартиру.

У всех этих задач есть много общего. Во-первых, их решение можно записать как функцию, которая отображает **объекты** или **примеры** (**samples**) в **предсказания** (**targets**). Например, больных надо отобразить в диагнозы, а документы в оценку релевантности. Во-вторых, вряд ли у этих задач есть единственно верное, идеальное решение. Даже профессиональные переводчики могут по-разному перевести один и тот же текст, и оба перевода будут верными. Так что лучшее в этих задачах — враг хорошего. В конце концов, и доктора иногда делают ошибки в диагнозах, и вы не всегда можете сказать, что же именно изображено на картинке. В-третьих, у нас есть много примеров правильных ответов (скажем, переводов предложения на другой язык или подписей к заданной картинке), а примеры неправильных ответов (если они нужны), как правило, не составляет труда сконструировать. Мы назовём функцию, отображающую объекты в предсказания, — **моделью**, а имеющийся у нас набор примеров — **обучающей выборкой** или **датасетом**. Обучающая выборка состоит из:

* **объектов** (к примеру, скачанные из интернета картинки, истории больных, активность пользователей сервиса и так далее);

* и **ответов** (подписи к картинкам, диагнозы, информация об уходе пользователей с сервиса), которые мы также будем иногда называть **таргетами**.

### Постановка задачи

Описанные выше задачи являются примерами задач **обучения с учителем** (**supervised learning**), так как правильные ответы для каждого объекта обучающей выборки заранее известны. Задачи обучения с учителем делятся на следующие виды в зависимости от того, каким может быть множество $\mathbb{Y}$ всех возможных ответов (таргетов):

1. $\mathbb{Y} = \mathbb{R}$ или $\mathbb{Y} = \mathbb{R}^M$ — **регрессия**. Примерами задач регрессии является предсказание продолжительности поездки на каршеринге, спрос на конкретный товар в конкретный день или погода в вашем городе на завтра (температура, влажность и давление — это несколько вещественных чисел, которые формируют вектор нашего предсказания).
2. $\mathbb{Y} = {0, 1}$ — **бинарная классификация**. Например, мы можем предсказывать, кликнет ли пользователь по рекламному объявлению, вернёт ли клиент кредит в установленный срок, сдаст ли студент сессию, случится ли определённое заболевание у пациента, есть ли на картинке банан.
3. $\mathbb{Y} = {1, \dots, K}$ — **многоклассовая (multiclass) классификация**. Например, определение предметной области для научной статьи (математика, биология, психология и т. д.).
4. $\mathbb{Y} = {0, 1}^K$ — **многоклассовая классификация с пересекающимися классами (multilabel classification)**. Например, задача автоматического проставления тегов для ресторанов (логично, что ресторан может одновременно иметь несколько тегов).
5. $\mathbb{Y}$ — конечное упорядоченное множество — **ранжирование**. Основным примером является задача ранжирования поисковой выдачи, где для любого запроса нужно отсортировать все возможные документы по релевантности этому запросу; при этом оценка релевантности имеет смысл только в контексте сравнения двух документов между собой, её абсолютное значение информации не несёт.

Ответ может быть и более сложным. Так, в задаче сегментации изображения требуется для каждого пикселя предсказать, к какому объекту или типу объектов он относится, а в задаче машинного перевода мы должны сгенерировать предложение (или целый текст), являющееся переводом исходного. Интерес представляют и задачи **порождения новых объектов**, то есть генерации правдоподобных объектов, из ничего или на основе уже существующих. С помощью такой модели также можно научиться увеличивать разрешение изображения и применять любимые всеми маски в Snapchat или Instagram.

Есть и относительно небольшой класс задач, относящихся к **обучению без учителя** (**unsupervised learning**), — это задачи, для которых нам известны только данные, а ответы неизвестны или вообще не существуют. Более того, часто поиск "правильных" ответов не является самоцелью. Классическим примером обучения без учителя является кластеризация — задача разделения объектов на группы, обладающие некоторыми неизвестными нам, но, как мы в глубине души надеемся, интерпретируемыми свойствами. Примером может служить кластеризация документов из электронной библиотеки по темам или кластеризация новостей с целью выделения крупных сюжетов.

Бывают и другие виды (и даже парадигмы) машинного обучения, так что если вы встретите задачу, которую никак не получается отнести к одному из перечисленных выше типов, не расстраивайтесь и знайте, что где-то дальше в учебнике вас ждёт рассказ про такие задачи.

**Вопрос на подумать.** Определите тип следующих задач. По возможности попробуйте отнести их к более узким видам задач.

1. Предсказание курса евро к доллару на следующий день.

2. Стилизация текста. Например, перевод на бюрократический язык: «Пиппина и Мерри похитили!» $\mapsto$ «Граждане Тук, Перегрин Паладинович, 2990 года рождения, и Брендибак, Мериадок Сарадокович, 2982 года рождения, были похищены неустановленными лицами».

3. Детектирование котиков на изображении.

4. Обучение робокота запрыгивать на стол из произвольной позы.

5. Поиск наборов товаров, которые посетители супермаркета часто покупают вместе.

{% cut "Ответ (не открывайте сразу; сначала подумайте сами!)" %}

1. Это задача регрессии. Модель предсказывает вещественное число, пусть и с небольшим количеством знаков после запятой.

2. Это задача генерации новых объектов на основе уже существующих.

3. В зависимости от того, для чего мы детектируем котиков, это может быть задача регрессии (предсказание координат вершин прямоугольника, в котором находится котик) или классификации (если нас просто интересует, есть котик или нет).

4. Эту задачу можно решать по-разному. Например, создав физическую модель движения робокота и рассчитав оптимальную последовательность движений. Если мы всё-таки хотим решать её с помощью машинного обучения, то можно поступить следующим образом. Создадим компьютерную симуляцию (чтобы не ломать настоящего робота) и модель, которая будет в каждый момент на основе конфигурации сочленений, высоты от пола, расстояния до стола, фазы Луны и других важных параметров предсказывать, как нужно дальше поворачивать лапы, изгибать спину кота и так далее. Эту модель будем прогонять в симуляции, так или иначе меняя её в зависимости от того, насколько удачно робот справляется со своей задачей. Такая парадигма называется **обучением с подкреплением** (**reinforcement learning**), и о ней мы поговорим в [отдельном параграфе](https://academy.yandex.ru/handbook/ml/article/obuchenie-s-podkrepleniem).

Вы можете спросить: а почему это не обучение с учителем? Ведь у нас есть объекты — последовательности движений и ответы — запрыгнул кот на стол или нет. Проблема в том, что перебрать кучу траекторий (ввиду сложности задачи — действительно огромную кучу) и для каждой получить ответ — это очень долго и сложно; кроме того, нам хотелось бы иметь фреймворк, в котором можно было бы относительно легко адаптироваться, скажем, к изменению высоты стола.

5. Это задача обучения без учителя.

{% endcut %}

**Вопрос на подумать.** Ранжирование — это задача с таргетом из конечного упорядоченного множества $(1,\ldots,K)$. Казалось бы, её запросто можно было бы рассматривать как задачу классификации на $K$ классов или задачу регрессии. В чём же проблема? Почему так не делают?

{% cut "Ответ (не открывайте сразу; сначала подумайте сами!)" %}

Для решения задач ранжирования обычно строят модель, предсказывающую некоторое вещественное число, по которому затем сортируют объекты, — так почему бы не считать её регрессией? Дело в том, что функции потерь и метрики в этой задаче совсем другие. Нам неважно, какие именно вещественные числа мы предсказываем. Мы просто хотим, чтобы более релевантным объектам сопоставлялись числа побольше.

Задача «предскажите 10 самых релевантных объектов» не похожа на задачу классификации. Мир меняется, появляются новые объекты, и если к нам в руки попадёт объект более релевантный, чем текущий топ-1, все номера позиций поедут, и выученное нами соответствие объектов и номеров можно будет выкидывать на помойку.

{% endcut %}

## Критерии качества

По обучающей выборке мы хотим построить модель, предсказания которой достаточно хороши. Что вообще значит «достаточно хороши»? Не понимая, чего мы хотим добиться, мы не предложим хорошего решения, поэтому нужно внимательно отнестись к выбору **метрик качества**.

Возможно, вы уже участвовали в соревнованиях по анализу данных. На таких соревнованиях метрику организатор выбирает за вас, и она, как правило, непосредственным образом связана с результатами предсказаний. Но на практике всё бывает намного сложнее.

Например, мы хотим:

* решить, сколько коробок с бананами нужно завтра привезти в конкретный магазин, чтобы минимизировать количество товара, который не будет выкуплен, и минимизировать вероятность того, что покупатель к концу дня не найдёт желаемый продукт на полке;
* увеличить счастье пользователей от работы с нашим сервисом, чтобы пользователи стали лояльнее, а сервис мог получать стабильный прогнозируемый доход;
* решить, нужно ли направить пациента на дополнительное медицинское обследование.

В каждом конкретном случае может возникать целая иерархия метрик.

1. Самый верхний уровень – это **бизнес-метрики**, например, будущий доход сервиса. Их трудно измерить в моменте, они сложным образом зависят от совокупности всех наших усилий, не только связанных с машинным обучением.

2. **Онлайн** (**online**) **метрики** – это характеристики работающей системы, с помощью которых мы надеемся оценить, что будет с бизнес-метриками. Например, это может быть:
   – Медианная длина сессии в онлайн-игре. Можно предположить, что пользователь, который долго сидит в игре – это довольный пользователь.
   – Среднее количество бананов на полках во всех магазинах торговой сети в конце дня.

3. Не всегда плоды наших трудов оцениваются числами. Многое может зависеть от субъективного восприятия людей, и для того, чтобы оценить их реакцию до выпуска в продакшен, применяется оценка специально нанятыми людьми – асессорами. Например, так можно оценивать, получилось ли у нас улучшить качество машинного перевода или релевантность выдачи в поисковой системе.

4. **Офлайн** (**offline**) **метрики** могут быть измерены до введения модели в эксплуатацию, например, по историческим данным. В задачах, в которых нужно предсказывать какой-то конкретный таргет, офлайн метрики обычно оценивают отклонение предсказаний модели от истинных значений таргета. Например, это может быть точность предсказания, то есть число верно угаданных значений, или среднеквадратичное отклонение.

Асессорскую оценку тоже можно считать офлайн-метрикой

В этой книге речь в основном пойдёт об офлайновых метриках и о функциях потерь. И прежде, чем вы начнёте знакомиться с методами решения задач обучения с учителем, полезно посмотреть, какими бывают метрики качества. Вот несколько примеров:

* для задачи постановки диагноза хорошими метриками могут быть, например, доля правильно поставленных диагнозов или доля больных, которым удалось поставить правильный диагноз (а вы поняли разницу?);

* для задачи предсказания цены квартиры метрикой качества может быть доля квартир, для которых разница между предсказанным и истинным значением цены не превысила какого-то порога, или средний модуль разницы между предсказанным и истинным значением;

* для задачи ранжирования поисковых документов по запросу — доля пар документов, которые мы упорядочили неправильно.

Цель обычно в том, чтобы найти модель, для которой значение метрики будет оптимальным.

**Вопрос на подумать.** Важно помнить, что разные нужды заказчика могут диктовать самые разные метрики. Вернёмся к задаче постановки диагноза пациентам больницы. Какие метрики вы предложили бы использовать в каждом из следующих случаев:

* обычный год в обычном терапевтическом отделении обычной больницы;

* определение очень неприятной болезни, которая жутким клеймом падёт на каждого, кому поставили такой диагноз;

* определение опасной и очень заразной болезни.

{% cut "Ответ (не открывайте сразу; сначала подумайте сами!)" %}

Конечно, даже в каждом из этих довольно частных случаев могут быть разные ситуации и разные метрики, но вот как, например, можно было бы ответить:

* Обычный год в обычном терапевтическом отделении обычной больницы — тогда главного врача вполне устроит, если доля правильно поставленных диагнозов будет высокой (эта метрика называется **accuracy**).

* Определение очень неприятной болезни, которая жутким клеймом падёт на каждого, кому поставили такой диагноз, — тогда нам важно максимизировать долю действительно больных среди тех, кому мы имели несчастье поставить этот диагноз (эта метрика называется точностью, или **precision**).

* Определение опасной и очень заразной болезни — тогда нам важно не пропустить ни одного заражённого, и метрика будет иметь вид доли правильно определённых носителей (эта метрика называется полнотой, или **recall**).

Разумеется, это самые простые метрики, и в реальной жизни вам придётся работать с более сложной иерархией метрик; немного подробнее мы поговорим об этом в параграфе про измерение качества моделей.

{% endcut %}

**Вопрос на подумать.** Рассмотрим задачу детектирования людей на изображении. Чаще всего под детектированием понимают указание местоположения человека на картинке. Например, модель пытается выделить прямоугольник, в котором, по её мнению, есть человеческая фигура. Подумайте, какие метрики можно было бы использовать в различных ситуациях для измерения качества решения этой задачи. Не забудьте, что метрики — это способ численно измерить то, насколько модель помогает нам в жизни, так что важно думать о том, зачем нам вообще детектировать людей.

{% cut "Ответ (не открывайте сразу; сначала подумайте сами!)" %}

Вот несколько вариантов, которые можно было бы придумать:

* Мы разрабатываем программу для проведения видеоконференций и хотим добавить эффект, который облачает участника в рыцарские доспехи, — в этом случае нам важно корректно определять местоположение и в качестве метрики мы могли бы брать среднеквадратичное отклонение координат каких-нибудь опорных точек тела от истинных.

* Мы строим систему безопасности вокруг какого-то важного объекта, и нам важно обнаруживать вторжение — в этом случае нам не очень принципиально, насколько точно отмечено местоположение человека в кадре, но все люди должны быть обнаружены. Таким образом, в качестве метрики можно рассмотреть полноту: на какой доле кадров, где действительно были люди, наша модель отметила их наличие.

* Мы строим систему, определяющую, не превышает ли количество людей в помещении некоторый порог (например, в рамках борьбы с пандемией), — в этом случае метрикой может быть, скажем, среднеквадратичное отклонение числа детектированных моделью людей от истинного их количества.

{% endcut %}

Критерии качества не всегда сводятся к метрикам. Бизнес или общество могут накладывать и другие требования, например:

* Модель может выдавать предсказания в режиме реального времени. Заметим, что это требование не только к модели, но и к её реализации, а также к тому железу или к тем серверам, на которых она работает.

* Модель достаточно компактна, чтобы помещаться на мобильном телефоне или другом устройстве.

* Можно объяснить, на основании чего модель сделала то или иное предсказание для конкретного объекта. Это может быть важным в случае, если модель решает что-то важное в жизни человека, например, дадут ли кредит или будет ли согласовано дорогостоящее лечение. Такое требование является частным случаем более общего понятия интерпретируемости модели.

* Предсказания модели не дискриминируют какую-либо категорию пользователей. Например, если двум людям с одинаковой и достаточно длинной историей просмотров онлайн-кинотеатр рекомендует разные фильмы только из-за того, что у них разный пол, то это не здорово.

## Данные

Машинное обучение начинается с данных. Важно, чтобы их было достаточно много и чтобы они были достаточно качественными. Некоторые проекты приходится откладывать на неопределённый срок из-за того, что просто невозможно собрать данные.

Чем сложнее задача, тем больше данных нужно, чтобы её решить. Например, существенные успехи в задачах распознавания изображений были достигнуты лишь с появлением очень больших датасетов (и, стоит добавить, вычислительных мощностей). Вычислительные ресурсы продолжают совершенствовать, но во многих ситуациях размеченных данных (то есть объектов, которым кто-то сопоставил ответ) было бы по-прежнему слишком мало: например, для решения задачи аннотирования изображений (image captioning) потребовалось бы огромное количество пар (изображение, описание). В некоторых случаях можно воспользоваться открытыми датасетами. Сейчас их доступно довольно много и некоторые весьма велики, но чаще всего они создаются для довольно простых задач, например, для задачи классификации изображений. Иногда датасет можно купить. Но для каких-то задач вы нигде не найдёте данных. Скажем, авторам неизвестно больших и качественных корпусов телефонных разговоров с расшифровками – в том числе и по причинам конфиденциальности таких данных.

Бороться с проблемой нехватки данных можно двумя способами.

Первый – использование **краудсорсинга**, то есть привлечение людей, готовых разметить много данных. Во многих ситуациях (например, когда речь заходит об оценке поисковой выдачи) без дополнительной разметки никак не обойтись. Мы расскажем про краудсорсинг подробнее в соответствующем параграфе. Некоторые проекты, в первую очередь научные и социальные, используют также **citizen science** – разметку данных волонтёрами без какого-либо вознаграждения, просто за чувство причастности к доброму делу исследования животных Африки или формы галактик.

Второй же способ состоит в использовании неразмеченных данных. К примеру, в задаче аннотирования изображений у нас есть огромное количество никак не связанных друг с другом изображений и текстов. Однако, мы можем использовать их для того, чтобы помочь компьютеру понять, какие слова в принципе могут стоять рядом в предложении. Подходы, связанные с использованием неразмеченных данных для решения задач обучения с учителем, объединяются термином **self-supervised learning** и очень активно используются сейчас. Важной составляющей является **обучение представлений** (**representation learning**) — задача построения компактных векторов небольшой размерности из сложных по структуре данных, например, изображений, звука, текстов, графов, так, чтобы близкие по структуре или семантике данные получали метрически близкие представления. Делать это можно разными способами — например, используя фрагменты моделей, обученных для решения какой-либо другой задачи, или строя модель, предсказывающую скрытую часть объекта по оставшейся его части — например, пропущенное слово в предложении. Этому будет посвящен [отдельный параграф](https://academy.yandex.ru/handbook/ml/article/obuchenie-predstavlenij) нашего учебника.

Но кроме количества данных важно ещё и то, насколько они хороши и удобны для анализа. Давайте разберёмся, что это значит и какие с этим бывают проблемы.

Для работы с объектом модель должна опираться на какие-то его свойства, например, доход человека, цвет левого верхнего пикселя на изображении или частоту встречаемости слова «интеграл» в тексте. Эти свойства обычно называются **признаками**, а совокупность свойств, которые мы выделили у объекта – его **признаковым описанием**.

Вот несколько простых и распространённых разновидностей признаков:

* **Численные** – например, рост или доход. Иногда отдельно выделяют вещественные и целочисленные признаки.

* **Категориальные** признаки принимают значения из некоторого дискретного множества. Например, профессия человека или день недели.

* **Бинарные признаки** принимают два значения: $0$ и $1$ или «да» и «нет». С ними можно работать и как с численными, и как с категориальными.

* Среди категориальных признаков иногда выделяют **ординальные**. Они принимают значения из некоторого *упорядоченного* дискретного множества. Например, класс опасности химического вещества (бывает от 1-го до 4-го) или год обучения для студента являются ординальными.

Приходится иметь дело и с более сложно устроенными признаками. Например, описание ресторана может содержать тексты отзывов или фотографии, а профиль человека в социальной сети – список его друзей. Для многих однородных типов данных, таких как изображения, видео, тексты, звук, графы, разработано большое количество методов извлечения признаков – сейчас в первую очередь нейросетевых. О них вы сможете прочитать в разделах про нейросетевые архитектуры для соответствующих типов данных. Если же попадаются какие-то более сложно устроенные данные, могут потребоваться дополнительные усилия для извлечения из них признаков – этот процесс называют **feature engineering**.

Удобно бывает записать данные в виде таблицы, строки которой соответствуют объектам, а столбцы – признакам. Например:

#|
||

**Объекты**

|

Возраст

|

Оценка по ML

||
||

**Наташа**

|

21

|

отл

||
||

**Вася**

|

N/A

|

удовл

||
||

**Игорь**

|

47

|

хор

||
|#

Данные, представленные в таком виде, называются **табличными**. Табличные данные – один из самых удобных для анализа форматов. Свои успешные пайплайны работы есть также для уже упомянутых текстов, звука, изображений, видео, графов.

Лучше всего, если все признаки являются численными. Тогда с таблицей можно работать как с объектом линейной алгебры – **матрицей объекты-признаки**.

Создание информативного признакового описания очень важно для дальнейшего анализа. Но нужно также следить за качеством полученных данных. Вам могут встретиться, например, следующие проблемы:

* **Пропуски** (пропущенные значения). Так, в примере табличных данных выше нам неизвестен возраст Васи. Объекты или признаки, в которых есть пропуски, можно удалять из выборки, но если пропусков довольно много, мы можем потерять таким образом слишком много информации. Кроме того, наличие пропуска само по себе может нести информацию: скажем, это может говорить о систематической проблеме в сборе данных для того или иного сегмента выборки. Некоторые модели, например, решающие деревья, обладают собственными средствами для работы с пропусками, другие же – например, линейные модели или нейросети – требуют, чтобы пропуски были вычищены или заменены на что-то.

* **Выбросы**, то есть объекты, которые резко отличаются от большинства остальных. Например, в датасете с информацией о клиентах банка 140-летний человек, очевидно, будет весьма нетипичным. Выбросы могут возникать из-за ошибок при сборе данных или представлять собой реально существующие аномалии. Обычно выбросы лучше удалять, но в некоторых случаях выбросами могут быть важные объекты (например, очень богатые клиенты банка), и тогда их, возможно, стоит отлавливать и обрабатывать отдельно.

* **Ошибки разметки**. Если, например, вы собираете данные с помощью разметчиков-людей, то вы должны быть готовы к тому, что часть таргетов будет отмечена неправильно. Даже если не думать о том, что не все из разметчиков совершенно честные и старательные, задача может оказаться для них сложной.

* **Data drift**. С течением времени данные могут меняться. Например, может измениться схема сбора данных, и они начнут приходить в формате, который вообще не обрабатывается моделью. Или же может поменяться распределение данных: скажем, если вы делали образовательный сервис для студентов, а к вам стали приходить и более зрелые люди. Data drift – это суровая реальность для любой системы, которая решает не сиюминутную задачу, поэтому нужно уметь мониторить распределение данных и, если нужно, обновлять модель.

Встречаются и другие проблемы. Нередко существенную часть данных приходится выкидывать, потому что в процессе сбора что-нибудь сломалось или потому, что полгода назад в сервисе изменили систему логирования и более старые данные невозможно склеить с более новыми.

## Модель и алгоритм обучения

Модель — это некоторый способ описания мира. Например, «Земля плоская» — это модель, и не такая плохая, как вам может показаться. Ей активно пользуются, когда всё происходит в масштабах одного города и кривизной поверхности можно пренебрегать. С другой стороны, если мы попробуем рассчитать кратчайший путь из Парижа в Лос-Анджелес, модель плоской Земли выдаст неадекватный ответ, она войдёт в противоречие с имеющимися данными, и её придётся заменить на «Земля круглая», «Земля имеет форму эллипсоида» и так далее — в той мере, в которой нам важна точность и в какой нам это позволяет (не)совершенство измерительной техники. Так, модель «Земля — это похожая на геоид с шершавостями на месте горных хребтов» очень точная и замечательная, но, возможно, будет избыточно сложной для большинства практических задач и при этом слишком тяжёлой в плане вычислений.

В первых параграфах мы будем рассматривать в основном предсказательные модели, то есть модели вида $y = f(x)$, которые пытаются уловить зависимость между признаковым описанием $x$ объекта и таргетом $y$. Но порой мы будем иметь дело и с моделями данных: например, «такой-то признак имеет нормальное распределение».

Чаще всего предсказательные модели мы будем брать из некоторого параметрического семейства $y = f_w(x)$, где $w$ — параметры, которые мы будем подбирать по данным.

Для примера давайте возьмём задачу предсказания цены квартиры. В качестве класса моделей выберем константные функции $f(x) = c$ (то есть будем для всех квартир предсказывать одно и то же значение цены). Поскольку значение не зависит от $x$, нам не очень важно, в каком виде получено признаковое описание: это может быть набор совершенно любых сведений о квартире. Не забудем зафиксировать метрику качества — **среднее абсолютное отклонение** (mean absolute error, она же **MAE**).

$$MAE(f, X, y) = L(f, X, y) = \frac1N\sum\limits_{i=1}^N \vert f(x_i) - y_i\vert \rightarrow \min\limits_f,
$$

где $f$ — это модель (та самая, $f(x) = c$), $X = (x_1,\ldots,x_N)$ — обучающие примеры (данные о квартирах, которые мы смогли достать), $y = (y_1,\ldots,y_N)$ — правильные ответы (то есть цены на известные нам квартиры). Чтобы найти минимум MAE, возьмём производную от выражения

$$\frac1N\sum\limits_{i=1}^N \vert c - y_i\vert \rightarrow \min\limits_f,
$$

и приравняем её к нулю:

$$\nabla_cL(f, X, y) = \frac1N\sum\limits_{i=1}^N sign(c - y_i) = 0
$$

$$\#\left\{i\mid y_i < c\right\} - \#\left\{i\mid y_i > c\right\} = 0
$$

Нам подходят точки $c$, для которых число $y_i$, строго меньших $c$, равно числу $y_i$, строго больших $c$. Таким образом, нам подходит [медиана](https://en.wikipedia.org/wiki/Median) набора $(y_1,\ldots,y_N)$:

$$f(x) = \mathrm{median}(y).
$$

**Вопрос на подумать.** Давайте теперь в задаче предсказания цены квартиры рассмотрим метрику **среднеквадратичное отклонение** (**MSE**):

$$MSE(f, X, y) = \frac1N\sum_{i=1}^N(f(x_i) - y_i)^2
$$

Каким будет оптимальное значение параметра $c$ для константной модели $f(x) = c$?

{% cut "Ответ (не открывайте сразу; сначала подумайте сами!)" %}

Это будет среднее значение:

$$\overline{y} = \frac1N\sum_{i=1}^Ny_i
$$

{% endcut %}

Прекрасно, значит, в классе константных функций мы можем найти оптимальную модель. Может быть, это можно сделать и в каком-нибудь более интересном классе? Этому вопросу и будет посвящена большая часть нашей книги. Классический курс ML состоит из описания классов моделей и способов работы с ними. Несмотря на то что для решения большинства практических задач на сегодня достаточно знать только два типа моделей — **градиентный бустинг на решающих деревьях** и **нейросетевые модели** — мы постараемся рассказать и про другие, чтобы развить у вас глубинное понимание предмета и дать возможность не только использовать лучшие сложившиеся практики, но и, при вашем желании, участвовать в проработке новых идей и поиске новых методов — уже в роли исследователя, а не просто инженера.

Кроме выбора модели важен также выбор **алгоритма обучения**. Алгоритм обучения — это процедура, которая превращает обучающую выборку в обученную модель. Скажем, в примере выше для константной модели мы в качестве алгоритма обучения использовали поиск нуля градиента. Как мы увидим дальше, градиентные методы используются для обучения многих моделей, и это очень богатый класс методов оптимизации, из которого порой не так просто выбрать лучший.

В качестве примера рассмотрим задачу бинарной классификации точек на плоскости, для которой выберем линейную модель:

![linear2](https://yastatic.net/s3/education-portal/media/linear2_9f742c7f00_ab782a0d4d.webp)

Метрикой будет accuracy, то есть доля верных предсказаний.

Теперь нам нужно по обучающей выборке подобрать оптимальную разделяющую прямую $y = w_1x + w_0$. Числа $w_1$ и $w_0$ являются **настраиваемыми** (**обучаемыми**) **параметрами** модели, именно их будет по выборке восстанавливать алгоритм обучения. Но есть проблема: метрика accuracy не дифференцируема. Поэтому мы должны подобрать другую дифференцируемую функцию $\mathcal{L}(X, y, w)$, минимизация которой будет более или менее соответствовать оптимизации вероятности. Такая функция называется **функцией потерь**, **лоссом** (от слова **loss**) или **лосс-функцией**. О том, как могут выглядеть лосс-функции для бинарной линейной классификации, вы можете почитать в параграфе про [линейные модели](https://education.yandex.ru/handbook/ml/article/linear-models).

В качестве алгоритма обучения мы можем взять теперь градиентный спуск:

$$w_{t+1} = w_t - \alpha\nabla_w\mathcal{L},
$$

где $\alpha$ — шаг оптимизации — коэффициент, влияющий на скорость и устойчивость алгоритма. Отметим, что разный выбор коэффициента $\alpha$, вообще говоря, даёт разные алгоритмы обучения, которые могут приводить к разным результатам: если $\alpha$ слишком мал, то спуск может не дойти до оптимума, а если слишком велик, то алгоритм будет «скакать» вокруг оптимума и никогда туда не попадёт. Мы видим, что важен не только выбор модели, но и выбор алгоритма обучения.

Число $\alpha$ является **гиперпараметром** алгоритма, то есть задаётся до начала обучения — но его тоже можно подбирать по данным. Более подробно о подборе гиперпараметров вы можете узнать в соответствующем [параграфе](https://academy.yandex.ru/handbook/ml/article/podbor-giperparametrov).

### Выбор модели, переобучение

Может показаться, что мы вас обманули, когда пугали сложностями: очевидно, что для любой задачи машинного обучения можно построить идеальную модель, надо всего лишь запомнить всю обучающую выборку с ответами. Такая модель может достичь идеального качества по любой метрике, но радости от неё довольно мало, ведь мы хотим, чтобы она выявила какие-то закономерности в данных и помогла нам с ответами там, где мы их не знаем. Важно понимать, какая у построенной модели **обобщающая способность**, то есть насколько она способна выучить общие закономерности, присущие не только обучающей выборке, и давать адекватные предсказания на новых данных. Для того чтобы предохранить себя от конфуза, поступают обычно так: делят выборку с данными на две части: **обучающую выборку** и **тестовую выборку** (**train** и **test**). Обучающую выборку используют для собственно обучения модели, а метрики считают на тестовой.

Такой подход позволяет отделить модели, которые просто удачно подстроились к обучающим данным, от моделей, в которых произошла **генерализация** (**generalization**), то есть от таких, которые на самом деле кое-что поняли о том, как устроены данные, и могут выдавать полезные предсказания для объектов, которых не видели.

Например, рассмотрим три модели регрессионной зависимости, построенные на одном и том же синтетическом датасете с одним-единственным признаком. Жёлтым нарисованы точки обучающей выборки. Здесь мы представим, что есть «истинная» закономерность (пунктир), которая искажена шумом (погрешности измерения, влияние других факторов и т.д.).

![three](https://yastatic.net/s3/education-portal/media/three_regression_models_b2744f22fb_0183cd8b68.webp)

Левая, линейная модель недостаточно хороша: она сделала, что могла, но плохо приближает зависимость, особенно при маленьких и при больших $x$. Правая «запомнила» всю обучающую выборку (и в самом деле, чтобы вычислить значение этой функции, нам надо знать координаты всех исходных точек) вместо того, чтобы моделировать исходную зависимость. Наконец, центральная, хоть и не проходит через точки обучающей выборки, довольно неплохо моделирует истинную зависимость.

Алгоритм, избыточно подстроившийся под данные, называют **переобученным**.

С увеличением сложности модели ошибка на обучающей выборке падает. Во многих задачах очень сложная модель будет работать примерно так же, как модель, «просто запомнившая всю обучающую выборку», но с генерализацией всё будет плохо: ведь выученные закономерности будут слишком специфическими, подогнанными под то, что происходит на обучающей выборке. Мы видим это на трёх графиках сверху: линейная функция очень проста, но и закономерность приближает лишь очень грубо; на правом же графике мы видим довольно хитрую функцию, которая точно подобрана под значения из обучающей выборки, но явно слишком эксцентрична, чтобы соответствовать какой-то природной зависимости. Оптимальная же генерализация достигается на модели не слишком сложной и не слишком простой.

В качестве иллюстрации для того же самого датасета рассмотрим модели вида

$$y\ =\ \text{ многочлен степени }D
$$

Ясно, что с ростом $D$ сложность модели растёт, и она достигает всё лучшего качества на обучающей выборке. А что, если у нас есть ещё тестовая выборка? Каким будет качество на ней? Вот так могут выглядеть графики среднеквадратичного отклонения (MSE) для обучающей и тестовой выборок:

![train](https://yastatic.net/s3/education-portal/media/train_vs_test_095bd91e5c_b09bf85938.webp)

Мы видим здесь типичную для классических моделей картину: MSE на обучающей выборке падает (может быть, даже до нуля), а на тестовой сперва падает, а затем начинает снова расти.

**Замечание**. Для моделей глубинного обучения всё немного интереснее: в некоторых ситуациях есть грань, за которой метрика на тестовой выборке снова начинает падать. Но об этом в своё время. Пока давайте запомним, что слишком сложная модель — это вредно, а переобучение — боль.

Точный способ выбрать алгоритм оптимальной сложности по данной задаче нам пока неизвестен, хотя какую-то теоретическую базу имеющимся философским наблюдениям мы дадим в главе про теорию обучения; при этом есть хорошо продуманная методология сравнения разных моделей и выбора среди них оптимальной — об этом мы обязательно расскажем вам в следующих главах. А пока дадим самый простой и неизменно ценный совет: не забывайте считать метрики на тестовой выборке и никогда не смешивайте её с обучающей!

**Вопрос на подумать.** Обсуждая переобучение, мы упоминали про сложность модели, но не сказали, что это такое. Как бы вы её определили? Как описать / сравнить сложность моделей для двух приведённых ниже задач? Почему, кстати, мы решили, что средняя модель ОК, а правая переобученная?

![regression](https://yastatic.net/s3/education-portal/media/regression_48dddf92a9_72b138a079.webp)

![classification](https://yastatic.net/s3/education-portal/media/classification_ee0f2f481d_9f063ccac1.webp)

{% cut "Ответ (не открывайте сразу; сначала подумайте сами!)" %}

Сложность модели можно очень грубо охарактеризовать числом настраиваемых параметров модели, то есть тех, которые мы можем определить по данным в процессе обучения. Это не имеет никакого математического смысла, и о каких-то более серьёзных оценках мы поговорим в главе про теорию машинного обучения, но никто не бросит в вас камень, если вы скажете, что модель с 10 тысячами параметров сложнее, чем модель с 1000 параметров.

В первой задаче левая модель — это, судя по всему, линейная функция, у неё два параметра, вторая — наверное, квадратичная с тремя параметрами, а правая — многочлен какой-то высокой степени (на самом деле 11-й), у неё параметров намного больше. Центральная модель явно лучше, чем левая, справляется с тем, чтобы приблизить истинную закономерность; правая тоже вроде неплохо справляется с тем, чтобы приблизить её для обучающих данных, но вот два резких провала и крутое пике слева никак не объясняются имеющимися данными, и на двух тестовых точках в районе $0,5$ модель отчаянно врёт — так что есть причины считать, что она переобучилась.

Со второй задачей ситуация во многом похожая. Центральная модель явно лучше разделяет жёлтые и серые точки. На правой же картинке мы видим довольно неестественные выпячивания жёлтой и серой областей: например, к серой точке в центре картинки (которая наверняка была выбросом) протянулось серое «щупальце», захватившее и несколько тестовых (и даже обучающих) точек другого класса. В целом можно поспорить о том, плох ли правый классификатор, но он явно рисует слишком сложные границы, чтобы можно было поверить, что они отражают что-то из реальной жизни.

{% endcut %}

### После обучения

В момент, когда подобраны все обучаемые параметры и гиперпараметры модели, работа специалиста по машинному обучению не заканчивается.

Во-первых, модель чаще всего создают для того, чтобы она работала в некотором продакшене. И чтобы она там оказалась, нужно эффективно её закодить, научить работать параллельно и подружить с используемыми вами фреймворками. Процесс выкатки в продакшен называется словом **деплой** или **деплоймент** (от **deploy**). После деплоя можно посчитать онлайн-метрики. Также имеет смысл провести **АБ-тестирование**, то есть сравнение с предыдущей версией модели на случайно выбранных подмножествах пользователей или сессий. Более подробно об АБ-тестировании вы сможете почитать в соответствующем параграфе. Если новая модель работает не очень здорово, должна быть возможность откатиться к старой.

После деплоймента модели важно продолжать дообучать или переобучать её при поступлении новых данных, а также мониторить качество. Мы уже обсуждали data drift, но бывает также и **concept drift** — изменение зависимости между признаками и таргетом. Например, если вы делаете музыкальные рекомендации, вам нужно будет учитывать и появление новых треков, и изменение вкусов аудитории. О мониторинге качества моделей мы подробнее расскажем в соответствующем параграфе.

--------

Теперь предлагаем вам потренировать изученный материал на практике. Скачайте [ноутбук](https://yastatic.net/s3/ml-handbook/admin/autohw_intro_ML_92e1d33a4d.ipynb?updated_at=2024-03-07T13:21:15.515Z) с лабораторной работой. В нём вы найдете описания заданий и дополнительные материалы. Задания из лабораторной прикреплены к этому параграфу в виде задач в системе Яндекс Контест. Чтобы проверить себя, отправляйте решения по соответствующим задачам в систему. Успехов в практике!

  ## handbook

  Учебник по машинному обучению

  ## title

  Машинное обучение

  ## description

  Что такое машинное обучение и каким оно бывает. Основные понятия машинного обучения: признаки, таргеты, метрики, переобучение


- 
  ## path

  /handbook/ml/article/linear-models

  ## content

  Мы начнем с самых простых и понятных моделей машинного обучения: линейных. В этом параграфе мы разберёмся, что это такое, почему они работают и в каких случаях их стоит использовать. Так как это первый класс моделей, с которым вы столкнётесь, мы постараемся подробно проговорить все важные моменты. Заодно объясним, как работает машинное обучение, на сравнительно простых примерах.

## Почему модели линейные?

Представьте, что у вас есть множество объектов $\mathbb{X}$, а вы хотели бы каждому объекту сопоставить какое-то значение. К примеру, у вас есть набор операций по банковской карте, а вы бы хотели, понять, какие из этих операций сделали мошенники. Если вы разделите все операции на два класса и нулём обозначите законные действия, а единицей мошеннические, то у вас получится простейшая задача классификации. Представьте другую ситуацию: у вас есть данные геологоразведки, по которым вы хотели бы оценить перспективы разных месторождений. В данном случае по набору геологических данных ваша модель будет, к примеру, оценивать потенциальную годовую доходность шахты. Это пример задачи регрессии. Числа, которым мы хотим сопоставить объекты из нашего множества иногда называют таргетами (от английского **target**).

Таким образом, задачи классификации и регрессии можно сформулировать как поиск отображения из множества объектов $\mathbb{X}$ в множество возможных таргетов.

Математически задачи можно описать так:

- **классификация**: $\mathbb{X}  \to \{0,1,\ldots,K\}$, где $0, \ldots, K$ – номера классов,
- **регрессия**: $\mathbb{X} \to \mathbb{R}$.

Очевидно, что просто сопоставить какие-то объекты каким-то числам — дело довольно бессмысленное. Мы же хотим быстро обнаруживать мошенников или принимать решение, где строить шахту. Значит нам нужен какой-то критерий качества. Мы бы хотели найти такое отображение, которое лучше всего приближает истинное соответствие между объектами и таргетами. Что значит «лучше всего» – вопрос сложный. Мы к нему будем много раз возвращаться. Однако, есть более простой вопрос: среди каких отображений мы будем искать самое лучшее? Возможных отображений может быть много, но мы можем упростить себе задачу и договориться, что хотим искать решение только в каком-то заранее заданном параметризированном семействе функций. Весь этот параграф будет посвящен самому простому такому семейству — линейным функциям вида

$$y = w_1 x_1 + \ldots + w_D x_D + w_0,
$$

где $y$ – целевая переменная (**таргет**), $(x_1, \ldots, x_D)$ – вектор, соответствующий объекту выборки (**вектор признаков**), а $w_1, \ldots, w_D, w_0$ – параметры модели. Признаки ещё называют **фичами** (от английского **features**). Вектор $w = (w_1,\ldots,w_D)$ часто называют вектором весов, так как на предсказание модели можно смотреть как на взвешенную сумму признаков объекта, а число $w_0$ – свободным коэффициентом, или **сдвигом** (**bias**). Более компактно линейную модель можно записать в виде

$$y = \langle x, w\rangle + w_0
$$

Теперь, когда мы выбрали семейство функций, в котором будем искать решение, задача стала существенно проще. Мы теперь ищем не какое-то абстрактное отображение, а конкретный вектор $(w_0,w_1,\ldots,w_D)\in\mathbb{R}^{D+1}$.

**Замечание**. Чтобы применять линейную модель, нужно, чтобы каждый объект уже был представлен вектором численных признаков $x_1,\ldots,x_D$. Конечно, просто текст или граф в линейную модель не положить, придётся сначала придумать для него численные фичи. Модель называют линейной, если она является линейной по этим численным признакам.

Разберёмся, как будет работать такая модель в случае, если $D = 1$. То есть у наших объектов есть ровно один численный признак, по которому они отличаются. Теперь наша линейная модель будет выглядеть совсем просто: $y = w_1 x_1 + w_0$. Для задачи регрессии мы теперь пытаемся приблизить значение игрек какой-то линейной функцией от переменной икс. А что будет значить линейность для задачи классификации? Давайте вспомним про пример с поиском мошеннических транзакций по картам. Допустим, нам известна ровно одна численная переменная — объём транзакции. Для бинарной классификации транзакций на законные и потенциально мошеннические мы будем искать так называемое **разделяющее правило**: там, где значение функции положительно, мы будем предсказывать один класс, где отрицательно – другой. В нашем примере простейшим правилом будет какое-то пороговое значение объёма транзакций, после которого есть смысл пометить транзакцию как подозрительную.

![1](https://yastatic.net/s3/education-portal/media/1_1_02255c591c_b62e3f69e1.webp)

В случае более высоких размерностей вместо прямой будет гиперплоскость с аналогичным смыслом.

**Вопрос на подумать**. Если вы посмотрите содержание учебника, то не найдёте в нём ни «полиномиальных» моделей, ни каких-нибудь «логарифмических», хотя, казалось бы, зависимости бывают довольно сложными. Почему так?

{% cut "Ответ (не открывайте сразу; сначала подумайте сами!)" %}

Линейные зависимости не так просты, как кажется. Пусть мы решаем задачу регрессии. Если мы подозреваем, что целевая переменная $y$ не выражается через $x_1, x_2$ как линейная функция, а зависит ещё от логарифма $x_1$ и ещё как-нибудь от того, разные ли знаки у признаков, то мы можем ввести дополнительные слагаемые в нашу линейную зависимость, просто объявим эти слагаемые новыми переменными и добавив перед ними соответствующие регрессионные коэффициенты

$$y \approx w_1 x_1 + w_2 x_2 + w_3\log{x_1} + w_4\text{sgn}(x_1x_2) + w_0,
$$

и в итоге из двумерной нелинейной задачи мы получили четырёхмерную линейную регрессию.

{% endcut %}

**Вопрос на подумать**. А как быть, если одна из фичей является *категориальной*, то есть принимает значения из (обычно конечного числа) значений, не являющихся числами? Например, это может быть время года, уровень образования, марка машины и так далее. Как правило, с такими значениями невозможно производить арифметические операции или же результаты их применения не имеют смысла.

{% cut "Ответ (не открывайте сразу; сначала подумайте сами!)" %}

В линейную модель можно подать только численные признаки, так что категориальную фичу придётся как-то закодировать. Рассмотрим для примера вот такой датасет

![1](https://yastatic.net/s3/education-portal/media/1_2_db8db4914e_a9b7006dcb.webp)

Здесь два категориальных признака – `pet_type` и `color`. Первый принимает четыре различных значения, второй – пять.

Самый простой способ – использовать **one-hot кодирование** (**one-hot encoding**). Пусть исходный признак мог принимать $M$ значений $c_1,\ldots, c_M$. Давайте заменим категориальный признак на $M$ признаков, которые принимают значения $0$ и $1$: $i$-й будет отвечать на вопрос «принимает ли признак значение $c_i$?». Иными словами, вместо ячейки со значением $c_i$ у объекта появляется строка нулей и единиц, в которой единица стоит только на $i$-м месте.

В нашем примере получится вот такая табличка:

![1](https://yastatic.net/s3/education-portal/media/1_3_e1e9cb25e4_2f97300934.webp)

Можно было бы на этом остановиться, но добавленные признаки обладают одним неприятным свойством: в каждом из них ровно одна единица, так что сумма соответствующих столбцов равна столбцу из единиц. А это уже плохо. Представьте, что у нас есть линейная модель

$$y \sim w_1x_1 + \ldots + w_{D-1}x_{d-1} + w_{c_1}x_{c_1} + \ldots + w_{c_M}x_{c_M} + w_0
$$

Преобразуем немного правую часть:

$$y\sim w_1x_1 + \ldots + w_{D-1}x_{d-1} + \underbrace{(w_{c_1} - w_{c_M})}_{=:w'_{c_1}}x_{c_1} + \ldots + \underbrace{(w_{c_{M-1}} - w_{c_M})}_{=:w'_{C_{M-1}}}x_{c_{M-1}} + w_{c_M}\underbrace{(x_{c_1} + \ldots + x_{c_M})}_{=1} + w_0 =
$$

$$= w_1x_1 + \ldots + w_{D-1}x_{d-1} + w'_{c_1}x_{c_1} + \ldots + w'_{c_{M-1}}x_{c_{M-1}} + \underbrace{(w_{c_M} + w_0)}_{=w'_{0}}
$$

Как видим, от одного из новых признаков можно избавиться, не меняя модель. Более того, это стоит сделать, потому что наличие «лишних» признаков ведёт к переобучению или вовсе ломает модель – подробнее об этом мы поговорим в разделе про регуляризацию. Поэтому при использовании one-hot-encoding обычно выкидывают признак, соответствующий одному из значений. Например, в нашем примере итоговая матрица объекты-признаки будет иметь вид:

![1](https://yastatic.net/s3/education-portal/media/1_4_c3df35f737_7f968599c9.webp)

Конечно, one-hot кодирование – это самый наивный способ работы с категориальными признаками, и для более сложных фичей или фичей с большим количеством значений оно плохо подходит. С рядом более продвинутых техник вы познакомитесь в разделе про обучение представлений.

{% endcut %}

Помимо простоты, у линейных моделей есть несколько других достоинств. К примеру, мы можем достаточно легко судить, как влияют на результат те или иные признаки. Скажем, если вес $w_i$ положителен, то с ростом $i$-го признака таргет в случае регрессии будет увеличиваться, а в случае классификации наш выбор будет сдвигаться в пользу одного из классов. Значение весов тоже имеет прозрачную интерпретацию: чем вес $w_i$ больше, тем «важнее» $i$-й признак для итогового предсказания. То есть, если вы построили линейную модель, вы неплохо можете объяснить заказчику те или иные её результаты. Это качество моделей называют **интерпретируемостью**. Оно особенно ценится в индустриальных задачах, цена ошибки в которых высока. Если от работы вашей модели может зависеть жизнь человека, то очень важно понимать, как модель принимает те или иные решения и какими принципами руководствуется. При этом не все методы машинного обучения хорошо интерпретируемы, к примеру, поведение искусственных нейронных сетей или градиентного бустинга интерпретировать довольно сложно.

В то же время слепо доверять весам линейных моделей тоже не стоит по целому ряду причин:

- Линейные модели всё-таки довольно узкий класс функций, они неплохо работают для небольших датасетов и простых задач. Однако, если вы решаете линейной моделью более сложную задачу, то вам, скорее всего, придётся выдумывать дополнительные признаки, являющиеся сложными функциями от исходных. Поиск таких дополнительных признаков называется **feature engineering**, технически он устроен примерно так, как мы описали в вопросе про "полиномиальные модели". Вот только поиском таких искусственных фичей можно сильно увлечься, так что осмысленность интерпретации будет сильно зависеть от здравого смысла эксперта, строившего модель.
- Если между признаками есть приближённая линейная зависимость, коэффициенты в линейной модели могут совершенно потерять физический смысл (об этой проблеме и о том, как с ней бороться, мы поговорим дальше, когда будем обсуждать регуляризацию).
- Особенно осторожно стоит верить в утверждения вида «этот коэффициент маленький, значит, этот признак не важен». Во-первых, всё зависит от масштаба признака: вдруг коэффициент мал, чтобы скомпенсировать его. Во-вторых, зависимость действительно может быть слабой, но кто знает, в какой ситуации она окажется важна. Такие решения принимаются на основе данных, например, путём проверки статистического критерия (об этом мы коротко упомянем в разделе про вероятностные модели).
- Конкретные значения весов могут меняться в зависимости от обучающей выборки, хотя с ростом её размера они будут потихоньку сходиться к весам «наилучшей» линейной модели, которую можно было бы построить по всем-всем-всем данным на свете.

Обсудив немного общие свойства линейных моделей, перейдём к тому, как их всё-таки обучать. Сначала разберёмся с регрессией, а затем настанет черёд классификации.

## Линейная регрессия и метод наименьших квадратов (МНК)

Мы начнём с использования линейных моделей для решения задачи регрессии. Простейшим примером постановки задачи линейной регрессии является **метод наименьших квадратов** (Ordinary least squares).

Пусть у нас задан датасет $(X, y)$, где $y=(y_i)_{i=1}^N \in \mathbb{R}^N$ – вектор значений целевой переменной, а $X=(x_i)_{i = 1}^N \in \mathbb{R}^{N \times D}, x_i \in \mathbb{R}^D$ – матрица объекты-признаки, в которой $i$-я строка – это вектор признаков $i$-го объекта выборки. Мы хотим моделировать зависимость $y_i$ от $x_i$ как линейную функцию со свободным членом. Общий вид такой функции из $\mathbb{R}^D$ в $\mathbb{R}$ выглядит следующим образом:

$$\color{#348FEA}{f_w(x_i) = \langle w, x_i \rangle + w_0}
$$

Свободный член $w_0$ часто опускают, потому что такого же результата можно добиться, добавив ко всем $x_i$ признак, тождественно равный единице; тогда роль свободного члена будет играть соответствующий ему вес:

$$\begin{pmatrix}x_{i1} & \ldots & x_{iD} \end{pmatrix}\cdot\begin{pmatrix}w_1\\ \vdots \\ w_D\end{pmatrix} + w_0 =
\begin{pmatrix}1 &  x_{i1} & \ldots & x_{iD} \end{pmatrix}\cdot\begin{pmatrix}w_0 \\ w_1\\ \vdots \\ w_D \end{pmatrix}
$$

Поскольку это сильно упрощает запись, в дальнейшем мы будем считать, что это уже сделано и зависимость имеет вид просто $f_w(x_i) = \langle w, x_i \rangle$.

### Сведение к задаче оптимизации

Мы хотим, чтобы на нашем датасете (то есть на парах $(x_i, y_i)$ из обучающей выборки) функция $f_w$ как можно лучше приближала нашу зависимость.

![1](https://yastatic.net/s3/education-portal/media/1_5_66d8287b36_26d8e60f74.webp)

Для того, чтобы чётко сформулировать задачу, нам осталось только одно: на математическом языке выразить желание «приблизить $f_w(x)$ к $y$». Говоря простым языком, мы должны научиться измерять качество модели и минимизировать её ошибку, как-то меняя обучаемые параметры. В нашем примере обучаемые параметры — это веса $w$. Функция, оценивающая то, как часто модель ошибается, традиционно называется **функцией потерь**, **функционалом качества** или просто **лоссом** (**loss function**). Важно, чтобы её было легко оптимизировать: скажем, гладкая функция потерь – это хорошо, а кусочно постоянная – просто ужасно.

Функции потерь бывают разными. От их выбора зависит то, насколько задачу в дальнейшем легко решать, и то, в каком смысле у нас получится приблизить предсказание модели к целевым значениям. Интуитивно понятно, что для нашей текущей задачи нам нужно взять вектор $y$ и вектор предсказаний модели и как-то сравнить, насколько они похожи. Так как эти вектора «живут» в одном векторном пространстве, расстояние между ними вполне может быть функцией потерь. Более того, положительная непрерывная функция от этого расстояния тоже подойдёт в качестве функции потерь. При этом способов задать расстояние между векторами тоже довольно много. От всего этого разнообразия глаза разбегаются, но мы обязательно поговорим про это позже. Сейчас давайте в качестве лосса возьмём квадрат $L^2$-нормы вектора разницы предсказаний модели и $y$. Во-первых, как мы увидим дальше, так задачу будет нетрудно решить, а во-вторых, у этого лосса есть ещё несколько дополнительных свойств:

- $L^2$-норма разницы – это евклидово расстояние $\|y - f_w(x)\|_2$ между вектором таргетов и вектором ответов модели, то есть мы их приближаем в смысле самого простого и понятного «расстояния».

- Как мы увидим в разделе про вероятностные модели, с точки зрения статистики это соответствует гипотезе о том, что наши данные состоят из линейного «сигнала» и нормально распределенного «шума».

Так вот, наша функция потерь выглядит так:

$$L(f, X, y) = \|y - f(X)\|_2^2 =
$$

$$= \|y - Xw\|_2^2 = \sum_{i=1}^N(y_i - \langle x_i, w \rangle)^2
$$

Такой функционал ошибки не очень хорош для сравнения поведения моделей на выборках разного размера. Представьте, что вы хотите понять, насколько качество модели на тестовой выборке из $2500$ объектов хуже, чем на обучающей из $5000$ объектов. Вы измерили $L^2$-норму ошибки и получили в одном случае $300$, а в другом $500$. Эти числа не очень интерпретируемы. Гораздо лучше посмотреть на среднеквадратичное отклонение

$$L(f, X, y) = \frac1N\sum_{i=1}^N(y_i - \langle x_i, w \rangle)^2
$$

По этой метрике на тестовой выборке получаем $0,12$, а на обучающей $0,1$.

Функция потерь $\frac1N\sum_{i=1}^N(y_i - \langle x_i, w \rangle)^2$ называется **Mean Squared Error**, **MSE** или **среднеквадратическим отклонением**. Разница с $L^2$-нормой чисто косметическая, на алгоритм решения задачи она не влияет:

$$\color{#348FEA}{\text{MSE}(f, X, y) =  \frac{1}{N}\|y - X w\|_2^2}
$$

В самом широком смысле, функции работают с объектами множеств: берут какой-то входящий объект из одного множества и выдают на выходе соответствующий ему объект из другого. Если мы имеем дело с отображением, которое на вход принимает функции, а на выходе выдаёт число, то такое отображение называют **функционалом**. Если вы посмотрите на нашу функцию потерь, то увидите, что это именно функционал. Для каждой конкретной линейной функции, которую задают веса $w_i$, мы получаем число, которое оценивает, насколько точно эта функция приближает наши значения $y$. Чем меньше это число, тем точнее наше решение, значит для того, чтобы найти лучшую модель, этот функционал нам надо минимизировать по $w$:

$$\color{#348FEA}{\|y - Xw\|_2^2 \longrightarrow \min_w}
$$

Эту задачу можно решать разными способами. В этом параграфе мы сначала решим эту задачу аналитически, а потом приближенно. Сравнение двух этих решений позволит нам проиллюстрировать преимущества того подхода, которому посвящена эта книга. На наш взгляд, это самый простой способ "на пальцах" показать суть машинного обучения.

### МНК: точный аналитический метод

Точку минимума можно найти разными способами. Если вам интересно аналитическое решение, вы можете найти его в параграфе про матричные дифференцирования (раздел «[Примеры вычисления производных сложных функций](https://education.yandex.ru/handbook/ml/article/matrichnoe-differencirovanie#primery-vychisleniya-proizvodnyh-slozhnyh-funkczij)»). Здесь же мы воспользуемся геометрическим подходом.

Пусть $x^{(1)},\ldots,x^{(D)}$ – столбцы матрицы $X$, то есть столбцы признаков. Тогда

$$Xw = w_1x^{(1)}+\ldots+w_Dx^{(D)},
$$

и задачу регрессии можно сформулировать следующим образом: *найти линейную комбинацию столбцов* $x^{(1)},\ldots,x^{(D)}$, которая наилучшим способом приближает столбец $y$ по евклидовой норме – то есть *найти **проекцию** вектора* $y$ на подпространство, образованное векторами $x^{(1)},\ldots,x^{(D)}$.

Разложим $y = y_{\parallel} + y_{\perp}$, где $y_{\parallel} = Xw$ – та самая проекция, а $y_{\perp}$ – ортогональная составляющая, то есть $y_{\perp} = y - Xw\perp x^{(1)},\ldots,x^{(D)}$. Как это можно выразить в матричном виде? Оказывается, очень просто:

$$X^T(y - Xw) = 0
$$

В самом деле, каждый элемент столбца $X^T(y - Xw)$ – это скалярное произведение строки $X^T$ (=столбца $X$ = одного из $x^{(i)}$) на $y - Xw$. Из уравнения $X^T(y - Xw) = 0$ уже очень легко выразить $w$:

$$w = (X^TX)^{-1}X^Ty
$$

**Вопрос на подумать** Для вычисления $w_{\ast}$ нам приходится обращать (квадратную) матрицу $X^TX$, что возможно, только если она невырождена. Что это значит с точки зрения анализа данных? Почему мы верим, что это выполняется во всех разумных ситуациях?

{% cut "Ответ (не открывайте сразу; сначала подумайте сами!)" %}

Как известно из линейной алгебры, для вещественной матрицы $X$ ранги матриц $X$ и $X^TX$ совпадают. Матрица $X^TX$ невырождена тогда и только тогда, когда её ранг равен числу её столбцов, что равно числу столбцов матрицы $X$. Иными словами, формула регрессии поломается, только если столбцы матрицы $X$ линейно зависимы. Столбцы матрицы $X$ – это признаки. А если наши признаки линейно зависимы, то, наверное, что-то идёт не так и мы должны выкинуть часть из них, чтобы остались только линейно независимые.

Другое дело, что зачастую признаки могут быть *приближённо* линейно зависимы, особенно если их много. Тогда матрица $X^TX$ будет близка к вырожденной, и это, как мы дальше увидим, будет вести к разным, в том числе вычислительным проблемам.

{% endcut %}

Вычислительная сложность аналитического решения — $O(D^2N + D^3)$, где $N$ — длина выборки, $D$ — число признаков у одного объекта. Слагаемое $ND^2$ отвечает за сложность перемножения матриц $X^T$ и $X$, а слагаемое $D^3$ — за сложность обращения их произведения. Перемножать матрицы $(X^TX)^{-1}$ и $X^T$ не стоит. Гораздо лучше сначала умножить $y$ на $X^T$, а затем полученный вектор на $(X^TX)^{-1}$: так будет быстрее и, кроме того, не нужно будет хранить матрицу $(X^TX)^{-1}X^T$.

Вычисление можно ускорить, используя продвинутые алгоритмы перемножения матриц или итерационные методы поиска обратной матрицы.

#### Проблемы «точного» решения

Заметим, что для получения ответа нам нужно обратить матрицу $X^TX$. Это создает множество проблем:

1. Основная проблема в обращении матрицы — это то, что вычислительно обращать большие матрицы дело сложное, а мы бы хотели работать с датасетами, в которых у нас могут быть миллионы точек,
2. Матрица $X^TX$, хотя почти всегда обратима в разумных задачах машинного обучения, зачастую плохо обусловлена. Особенно если признаков много, между ними может появляться приближённая линейная зависимость, которую мы можем упустить на этапе формулировки задачи. В подобных случаях погрешность нахождения $w$ будет зависеть от квадрата [числа обусловленности](https://ru.wikipedia.org/wiki/%D0%A7%D0%B8%D1%81%D0%BB%D0%BE_%D0%BE%D0%B1%D1%83%D1%81%D0%BB%D0%BE%D0%B2%D0%BB%D0%B5%D0%BD%D0%BD%D0%BE%D1%81%D1%82%D0%B8) матрицы $X$, что очень плохо. Это делает полученное таким образом решение численно неустойчивым: малые возмущения $y$ могут приводить к катастрофическим изменениям $w$.

{% cut "Пара слов про число обусловленности." %}

Пожертвовав математической строгостью, мы можем считать, что число обусловленности матрицы $X$ – это корень из отношения наибольшего и наименьшего из собственных чисел матрицы $X^TX$. Грубо говоря, оно показывает, насколько разного масштаба бывают собственные значения $X^TX$. Если рассмотреть $L^2$-норму ошибки предсказания, как функцию от $w$, то её линии уровня будут эллипсоидами, форма которых определяется квадратичной формой с матрицей $X^TX$ (проверьте это!). Таким образом, число обусловленности говорит о том, насколько вытянутыми являются эти эллипсоиды.

{% endcut %}

{% cut "Подробнее" %}

Данные проблемы не являются поводом выбросить решение на помойку. Существует как минимум два способа улучшить его численные свойства, однако если вы не знаете про сингулярное разложение, то лучше вернитесь сюда, когда узнаете.

1. Построим $QR$-разложение матрицы $X$. Напомним, что это разложение, в котором матрица $Q$ ортогональна по столбцам (то есть её столбцы ортогональны и имеют длину 1; в частности, $Q^TQ=E$), а $R$ квадратная и верхнетреугольная. Подставив его в формулу, получим

$$w = ((QR)^TQR)^{-1}(QR)^T y = (R^T\underbrace{Q^TQ}_{=E}R)^{-1}R^TQ^Ty = R^{-1}R^{-T}R^TQ^Ty = R^{-1}Q^Ty
$$

Отметим, что написать $(R^TR)^{-1} = R^{-1}R^{-T}$ мы имеем право благодаря тому, что $R$ квадратная. Полученная формула намного проще, обращение верхнетреугольной матрицы (=решение системы с верхнетреугольной левой частью) производится быстро и хорошо, погрешность вычисления $w$ будет зависеть просто от числа обусловленности матрицы $X$, а поскольку нахождение $QR$-разложения является достаточно стабильной операцией, мы получаем решение с более хорошими, чем у исходной формулы, численными свойствами.

2. Также можно использовать псевдообратную матрицу, построенную с помощью сингулярного разложения, о котором подробно написано в разделе про матричные разложения. А именно, пусть

$$A = U\underbrace{\mathrm{diag}(\sigma_1,\ldots,\sigma_r)}_{=\Sigma}V^T
$$

– это усечённое сингулярное разложение, где $r$ – это ранг $A$. В таком случае диагональная матрица посередине является квадратной, $U$ и $V$ ортогональны по столбцам: $U^TU = E$, $V^TV = E$. Тогда

$$w = (V\Sigma \underbrace{U^TU}_{=E}\Sigma V^T)^{-1}V\Sigma U^Ty
$$

Заметим, что $V\Sigma^{-2}V^T\cdot V\Sigma^2V^T = E = V\Sigma^2V^T\cdot V\Sigma^{-2}V^T$, так что $(V\Sigma^2 V^T)^{-1} = V\Sigma^{-2}V^T$, откуда

$$w = V\Sigma^{-2}\underbrace{V^TV}_{=E}\Sigma U^Ty = V\Sigma^{-1}Uy
$$

Хорошие численные свойства сингулярного разложения позволяют утверждать, что и это решение ведёт себя довольно неплохо.

Тем не менее, вычисление всё равно остаётся довольно долгим и будет по-прежнему страдать (хоть и не так сильно) в случае плохой обусловленности матрицы $X$.

{% endcut %}

Полностью вылечить проблемы мы не сможем, но никто и не обязывает нас останавливаться на «точном» решении (которое всё равно никогда не будет вполне точным). Поэтому ниже мы познакомим вас с совершенно другим методом.

### МНК: приближенный численный метод

Минимизируемый функционал является гладким и выпуклым, а это значит, что можно эффективно искать точку его минимума с помощью итеративных градиентных методов. Более подробно вы можете прочитать о них в разделе про методы оптимизации, а здесь мы лишь коротко расскажем об одном самом базовом подходе.

Как известно, градиент функции в точке направлен в сторону её наискорейшего роста, а антиградиент (противоположный градиенту вектор) в сторону наискорейшего убывания. То есть имея какое-то приближение оптимального значения параметра $w$, мы можем его улучшить, посчитав градиент функции потерь в точке и немного сдвинув вектор весов в направлении антиградиента:

$$w_j \mapsto w_j - \alpha \frac{d}{d{w_j}} L(f_w, X, y)
$$

где $\alpha$ – это параметр алгоритма (**«темп обучения»**), который контролирует величину шага в направлении антиградиента. Описанный алгоритм называется **градиентным спуском**.

Посмотрим, как будет выглядеть градиентный спуск для функции потерь $L(f_w, X, y) = \frac1N\vert\vert Xw - y\vert\vert^2$. Градиент квадрата евклидовой нормы мы уже считали; соответственно,

$$\nabla_wL = \frac2{N} X^T (Xw - y)
$$

Следовательно, стартовав из какого-то начального приближения, мы можем итеративно уменьшать значение функции, пока не сойдёмся (по крайней мере в теории) к минимуму (вообще говоря, локальному, но в данном случае глобальному).

**Алгоритм градиентного спуска**

```python
w = random_normal()             # можно пробовать и другие виды инициализации
repeat S times:                 # другой вариант: while abs(err) > tolerance
   f = X.dot(w)                 # посчитать предсказание
   err = f - y                  # посчитать ошибку
   grad = 2 * X.T.dot(err) / N  # посчитать градиент
   w -= alpha * grad            # обновить веса
```

С теоретическими результатами о скорости и гарантиях сходимости градиентного спуска вы можете познакомиться в параграфе про [методы оптимизации](https://education.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning). Мы позволим себе лишь несколько общих замечаний:

- Поскольку задача выпуклая, выбор начальной точки влияет на скорость сходимости, но не настолько сильно, чтобы на практике нельзя было стартовать всегда из нуля или из любой другой приятной вам точки;
- Число обусловленности матрицы $X$ существенно влияет на скорость сходимости градиентного спуска: чем более вытянуты эллипсоиды уровня функции потерь, тем хуже;
- Темп обучения $\alpha$ тоже сильно влияет на поведение градиентного спуска; вообще говоря, он является гиперпараметром алгоритма, и его, возможно, придётся подбирать отдельно. Другими гиперпараметрами являются максимальное число итераций $S$ и/или порог tolerance.

{% cut "Иллюстрация." %}

Рассмотрим три задачи регрессии, для которых матрица $X$ имеет соответственно маленькое, среднее и большое числа обусловленности. Будем строить для них модели вида $y=w_1x_1 + w_2x_2$. Раскрасим плоскость $(w_1, w_2)$ в соответствии со значениями $\|X_{\text{train}}w - y_{\text{train}}\|^2$. Тёмная область содержит минимум этой функции – оптимальное значение $w_{\ast}$. Также запустим из двух точек градиентный спуск с разными значениями темпа обучения $\alpha$ и посмотрим, что получится:

![1](https://yastatic.net/s3/education-portal/media/1_6_becfd30d9a_03ada551b4.webp)

Заголовки графиков ("Round", "Elliptic", "Stripe-like") относятся к форме линий уровня потерь (чем более они вытянуты, тем хуже обусловлена задача и тем хуже может вести себя градиентный спуск).

Итог: при неудачном выборе $\alpha$ алгоритм не сходится или идёт вразнос, а для плохо обусловленной задачи он сходится абы куда.

{% endcut %}

Вычислительная сложность градиентного спуска – $O(NDS)$, где, как и выше, $N$ – длина выборки, $D$ – число признаков у одного объекта. Сравните с оценкой $O(D^2N+D^3)$ для «наивного» вычисления аналитического решения.

Сложность по памяти – $O(ND)$ на хранение выборки. В памяти мы держим и выборку, и градиент, но в большинстве реалистичных сценариев доминирует выборка.

### Стохастический градиентный спуск

На каждом шаге градиентного спуска нам требуется выполнить потенциально дорогую операцию вычисления градиента по всей выборке (сложность $O(ND)$). Возникает идея заменить градиент его оценкой на подвыборке (в английской литературе такую подвыборку обычно именуют **batch** или **mini-batch**; в русской разговорной терминологии тоже часто встречается слово **батч** или **мини-батч**).

А именно, если функция потерь имеет вид суммы по отдельным парам объект-таргет

$$L(w, X, y) = \frac1N\sum_{i=1}^NL(w, x_i, y_i),
$$

а градиент, соответственно, записывается в виде

$$\nabla_wL(w, X, y) = \frac1N\sum_{i=1}^N\nabla_wL(w, x_i, y_i),
$$

то предлагается брать оценку

$$\nabla_wL(w, X, y) \approx \frac1B\sum_{t=1}^B\nabla_wL(w, x_{i_t}, y_{i_t})
$$

для некоторого подмножества этих пар $(x_{i_t}, y_{i_t})_{t=1}^B$. Обратите внимание на множители $\frac1N$ и $\frac1B$ перед суммами. Почему они нужны? Полный градиент $\nabla_wL(w, X, y)$ можно воспринимать как среднее градиентов по всем объектам, то есть как оценку матожидания $\mathbb{E}\nabla_wL(w, x, y)$; тогда, конечно, оценка матожидания по меньшей подвыборке тоже будет иметь вид среднего градиентов по объектам этой подвыборки.

Как делить выборку на батчи? Ясно, что можно было бы случайным образом сэмплировать их из полного датасета, но даже если использовать быстрый алгоритм вроде резервуарного сэмплирования, сложность этой операции не самая оптимальная. Поэтому используют линейный проход по выборке (которую перед этим лучше всё-таки случайным образом перемешать). Давайте введём ещё один параметр нашего алгоритма: размер батча, который мы обозначим $B$. Теперь на $B$ очередных примерах вычислим градиент и обновим веса модели. При этом вместо количества шагов алгоритма обычно задают количество **эпох** $E$. Это ещё один гиперпараметр. Одна эпоха – это один полный проход нашего сэмплера по выборке. Заметим, что если выборка очень большая, а модель компактная, то даже первый проход бывает можно не заканчивать.

**Алгоритм:**

```python
 w = normal(0, 1)
 repeat E times:
   for i = B, i <= n, i += B
      X_batch = X[i-B : i]
      y_batch = y[i-B : i]
      f = X_batch.dot(w)                 # посчитать предсказание
      err = f - y_batch                  # посчитать ошибку
      grad = 2 * X_batch.T.dot(err) / B  # посчитать градиент
      w -= alpha * grad

```

Сложность по времени – $O(NDE)$. На первый взгляд, она такая же, как и у обычного градиентного спуска, но заметим, что мы сделали в $N / B$ раз больше шагов, то есть веса модели претерпели намного больше обновлений.

Сложность по памяти можно довести до $O(BD)$: ведь теперь всю выборку не надо держать в памяти, а достаточно загружать лишь текущий батч (а остальная выборка может лежать на диске, что удобно, так как в реальности задачи, в которых выборка целиком не влезает в оперативную память, встречаются сплошь и рядом). Заметим, впрочем, что при этом лучше бы $B$ взять побольше: ведь чтение с диска – намного более затратная по времени операция, чем чтение из оперативной памяти.

В целом, разницу между алгоритмами можно представлять как-то так:

![1](https://yastatic.net/s3/education-portal/media/1_7_4b8031112e_852be73f9d.webp)

Шаги стохастического градиентного спуска заметно более шумные, но считать их получается значительно быстрее. В итоге они тоже сходятся к оптимальному значению из-за того, что матожидание оценки градиента на батче равно самому градиенту. По крайней мере, сходимость можно получить при хорошо подобранных коэффициентах темпа обучения в случае выпуклого функционала качества. Подробнее мы об этом поговорим в [параграфе про оптимизацию](https://education.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning). Для сложных моделей и лоссов стохастический градиентный спуск может сходиться плохо или застревать в локальных минимумах, поэтому придумано множество его улучшений. О некоторых из них также рассказано в [параграфе про оптимизацию](https://education.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning).

Существует определённая терминологическая путаница, иногда стохастическим градиентным спуском называют версию алгоритма, в которой размер батча равен единице (то есть максимально шумная и быстрая версия алгоритма), а версии с бОльшим размером батча называют **batch gradient descent**. В книгах, которые, возможно, старше вас, такая процедура иногда ещё называется incremental gradient descent. Это не очень принципиально, но вы будьте готовы, если что.

**Вопрос на подумать**. Вообще говоря, если объём данных не слишком велик и позволяет это сделать, объекты лучше случайным образом перемешивать перед тем, как подавать их в алгоритм стохастического градиентного спуска. Как вам кажется, почему?

Также можно использовать различные стратегии отбора объектов. Например, чаще брать объекты, на которых ошибка больше. Какие ещё стратегии вы могли бы придумать?

{% cut "Ответ (не открывайте сразу; сначала подумайте сами!)" %}

Легко представить себе ситуацию, в которой объекты как-нибудь неудачно упорядочены, скажем, по возрастанию таргета. Тогда модель будет попеременно то запоминать, что все таргеты маленькие, то – что все таргеты большие. Это может и не повлиять на качество итоговой модели, но может привести и к довольно печальным последствиям. И вообще, чем более разнообразные батчи модель увидит в процессе обучения, тем лучше.

Стратегий можно придумать много. Например, не брать объекты, на которых ошибка слишком большая (возможно, это выбросы – зачем на них учиться), или вообще не брать те, на которых ошибка достаточно мала (они «ничему не учат»). Рекомендуем, впрочем, прибегать к этим эвристикам, только если вы понимаете, зачем они вам нужны и почему есть надежда, что они помогут.

{% endcut %}

### Неградиентные методы

После прочтения этой главы у вас может сложиться ощущение, что приближённые способы решения ML задач и градиентные методы – это одно и тоже, но вы будете правы в этом только на 98%. В принципе, существуют и другие способы численно решать эти задачи, но в общем случае они работают гораздо хуже, чем градиентный спуск, и не обладают таким хорошим теоретическим обоснованием. Мы не будем рассказывать про них подробно, но можете на досуге почитать, скажем, про Stepwise regression, Orthogonal matching pursuit или LARS. У LARS, кстати, есть довольно интересное свойство: он может эффективно работать на выборках, в которых число признаков больше числа примеров. С алгоритмом LARS вы можете познакомиться в [параграфе про оптимизацию](https://education.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning).

## Регуляризация

Всегда ли решение задачи регрессии единственно? Вообще говоря, нет. Так, если в выборке два признака будут линейно зависимы (и следовательно, ранг матрицы будет меньше $D$), то гарантировано найдётся такой вектор весов $\nu$ что $\langle\nu, x_i\rangle = 0\ \ \forall x_i$. В этом случае, если какой-то $w$ является решением оптимизационной задачи, то и $w + \alpha \nu$ тоже является решением для любого $\alpha$. То есть решение не только не обязано быть уникальным, так ещё может быть сколь угодно большим по модулю. Это создаёт вычислительные трудности. Малые погрешности признаков сильно возрастают при предсказании ответа, а в градиентном спуске накапливается погрешность из-за операций со слишком большими числами.

Конечно, в жизни редко бывает так, что признаки строго линейно зависимы, а вот быть приближённо линейно зависимыми они вполне могут быть. Такая ситуация называется **мультиколлинеарностью**. В этом случае у нас, всё равно, возникают проблемы, близкие к описанным выше. Дело в том, что $X\nu\sim 0$ для вектора $\nu$, состоящего из коэффициентов приближённой линейной зависимости, и, соответственно, $X^TX\nu\approx 0$, то есть матрица $X^TX$ снова будет близка к вырожденной. Как и любая симметричная матрица, она диагонализуется в некотором ортонормированном базисе, и некоторые из собственных значений $\lambda_i$ близки к нулю. Если вектор $X^Ty$ в выражении $(X^TX)^{-1}X^Ty$ будет близким к соответствующему собственному вектору, то он будет умножаться на $1 /{\lambda_i}$, что опять же приведёт к появлению у $w$ очень больших по модулю компонент (при этом $w$ ещё и будет вычислен с большой погрешностью из-за деления на маленькое число). И, конечно же, все ошибки и весь шум, которые имелись в матрице $X$, при вычислении $y\sim Xw$ будут умножаться на эти большие и неточные числа и возрастать во много-много раз, что приведёт к проблемам, от которых нас не спасёт никакое сингулярное разложение.

Важно ещё отметить, что в случае, когда несколько признаков линейно зависимы, веса $w_i$ при них теряют физический смысл. Может даже оказаться, что вес признака, с ростом которого таргет, казалось бы, должен увеличиваться, станет отрицательным. Это делает модель не только неточной, но и принципиально не интерпретируемой. Вообще, неадекватность знаков или величины весов – хорошее указание на мультиколлинеарность.

Для того, чтобы справиться с этой проблемой, задачу обычно **регуляризуют**, то есть добавляют к ней дополнительное ограничение на вектор весов. Это ограничение можно, как и исходный лосс, задавать по-разному, но, как правило, ничего сложнее, чем $L^1$- и $L^2$-нормы, не требуется.

Вместо исходной задачи теперь предлагается решить такую:

$$\color{#348FEA}{\min_w L(f, X, y) = \min_w(\|X w - y\|_2^2 + \lambda \|w\|^k_k )}
$$

$\lambda$ – это очередной параметр, а $\|w\|\^k_k$ – это один из двух вариантов:

$$\color{#348FEA}{\|w\|^2_2 = w^2_1 + \ldots + w^2_D}
$$

или

$$\color{#348FEA}{\|w\|_1^1 = \vert w_1 \vert + \ldots + \vert w_D \vert}
$$

Добавка $\lambda\|w\|^k_k$ называется **регуляризационным членом** или **регуляризатором**, а число $\lambda$ – **коэффициентом регуляризации**.

Коэффициент $\lambda$ является гиперпараметром модели и достаточно сильно влияет на качество итогового решения. Его подбирают по логарифмической шкале (скажем, от `1e-2` до `1e+2`), используя для сравнения моделей с разными значениями $\lambda$ дополнительную валидационную выборку. При этом качество модели с подобранным коэффициентом регуляризации уже проверяют на тестовой выборке, чтобы исключить переобучение. Более подробно о том, как нужно подбирать гиперпараметры, вы можете почитать в [соответствующем параграфе](https://education.yandex.ru/handbook/ml/article/podbor-giperparametrov).

Отдельно надо договориться о том, что вес $w_0$, соответствующий отступу от начала координат (то есть признаку из всех единичек), мы регуляризовать не будем, потому что это не имеет смысла: если даже все значения $y$ равномерно велики, это не должно портить качество обучения. Обычно это не отображают в формулах, но если придираться к деталям, то стоило бы написать сумму по всем весам, кроме $w_0$:

$$\|w\|^2_2 = \sum_{\color{red}{j=1}}^{D}w_j^2,
$$

$$\|w\|_1 = \sum_{\color{red}{j=1}}^{D} \vert w_j \vert
$$

В случае $L^2$-регуляризации решение задачи изменяется не очень сильно. Например, продифференцировав новый лосс по $w$, легко получить, что «точное» решение имеет вид:

$$w = (X^TX + \lambda I)^{-1}X^Ty
$$

Отметим, что за этой формулой стоит и понятная численная интуиция: раз матрица $X^TX$ близка к вырожденной, то обращать её сродни самоубийству. Мы лучше слегка исказим её добавкой $\lambda I$, которая увеличит все собственные значения на $\lambda$, отодвинув их от нуля. Да, аналитическое решение перестаёт быть «точным», но за счёт снижения численных проблем мы получим более качественное решение, чем при использовании «точной» формулы.

В свою очередь, градиент функции потерь

$$L(f_w, X, y) = \|Xw - y\|^2 + \lambda\|w\|^2
$$

по весам теперь выглядит так:

$$\nabla_wL(f_w, X, y) = 2X^T(Xw - y) + 2\lambda w
$$

Подставив этот градиент в алгоритм стохастического градиентного спуска, мы получаем обновлённую версию приближенного алгоритма, отличающуюся от старой только наличием дополнительного слагаемого.

**Вопрос на подумать**. Рассмотрим стохастический градиентный спуск для $L^2$-регуляризованной линейной регрессии с батчами размера $1$. Выберите правильный вариант шага SGD:

(а) $w_i\mapsto w_i - 2\alpha(\langle w, x_j\rangle - y_j)x_{ji} - \frac{2\alpha\lambda}N w_i,\quad i=1,\ldots,D$;

(б) $w_i\mapsto w_i - 2\alpha(\langle w, x_j\rangle - y_j)x_{ji} - 2\alpha\lambda w_i,\quad i=1,\ldots,D$;

(в) $w_i\mapsto w_i - 2\alpha(\langle w, x_j\rangle - y_j)x_{ji} - 2\lambda N w_i,\quad i=1,\ldots D$.

{% cut "Ответ (не открывайте сразу; сначала подумайте сами!)" %}

Не регуляризованная функция потерь имеет вид $\mathcal{L}(X, y, w) = \frac1N\sum_{i=1}^N\mathcal{L}(x_i, y_i, w)$, и её можно воспринимать, как оценку по выборке $(x_i, y_i)_{i=1}^N$ идеальной функции потерь

$$\mathcal{L}(w) = \mathbb{E}_{x, y}\mathcal{L}(x, y, w)
$$

Регуляризационный член не зависит от выборки и добавляется отдельно:

$$\mathcal{L}_{\text{reg}}(w) = \mathbb{E}_{x, y}\mathcal{L}(x, y, w) + \lambda\|w\|^2
$$

Соответственно, идеальный градиент регуляризованной функции потерь имеет вид

$$\nabla_w\mathcal{L}_{\text{reg}}(w) = \mathbb{E}_{x, y}\nabla_w\mathcal{L}(x, y, w) + 2\lambda w,
$$

Градиент по батчу – это тоже оценка градиента идеальной функции потерь, только не на выборке $(X, y)$, а на батче $(x_{t_i}, y_{t_i})_{i=1}^B$ размера $B$. Он будет выглядеть так:

$$\nabla_w\mathcal{L}_{\text{reg}}(w) = \frac1B\sum_{i=1}^B\nabla_w\mathcal{L}(x_{t_i}, y_{t_i}, w) + 2\lambda w.
$$

Как видите, коэффициентов, связанных с числом объектов в батче или в исходной выборке, во втором слагаемом нет. Так что верным является второй вариант. Кстати, обратите внимание, что в третьем ещё и нет коэффициента $\alpha$ перед производной регуляризационного слагаемого, это тоже ошибка.

{% endcut %}

**Вопрос на подумать**. Распишите процедуру стохастического градиентного спуска для $L^1$-регуляризованной линейной регрессии. Как вам кажется, почему никого не волнует, что функция потерь, строго говоря, не дифференцируема?

{% cut "Ответ (не открывайте сразу; сначала подумайте сами!)" %}

Распишем для случая батча размера 1:

$$w_i\mapsto w_i - \alpha(\langle w, x_j\rangle - y_j)x_{ji} - \frac{\lambda}N \text{sign}(w_i),\quad i=1,\ldots,D
$$

Функция потерь не дифференцируема лишь в одной точке. Так как в машинном обучении чаще всего мы имеем дело с данными вероятностного характера, это не влечёт каких-то особых проблем. Дело в том, что попадание прямо в ноль очень маловероятно из-за численных погрешностей в данных, так что мы можем просто доопределить производную в одной точке, а если даже пару раз попадём в неё за время обучения, это не приведёт к каким-то значительным изменениям результатов.

{% endcut %}

Отметим, что $L^1$- и $L^2$-регуляризацию можно определять для любой функции потерь $L(w, X, y)$ (и не только в задаче регрессии, а и, например, в задаче классификации тоже). Новая функция потерь будет соответственно равна

$$\widetilde{L}(w, X, y) = L(w, X, y) + \lambda\|w\|_1
$$

или

$$\widetilde{L}(w, X, y) = L(w, X, y) + \lambda\|w\|_2^2
$$

### Разреживание весов в $L^1$-регуляризации

$L^2$-регуляризация работает прекрасно и используется в большинстве случаев, но есть одна полезная особенность $L^1$-регуляризации: её применение приводит к тому, что у признаков, которые не оказывают большого влияния на ответ, вес в результате оптимизации получается равным $0$. Это позволяет удобным образом удалять признаки, слабо влияющие на таргет. Кроме того, это даёт возможность автоматически избавляться от признаков, которые участвуют в соотношениях приближённой линейной зависимости, соответственно, спасает от проблем, связанных с мультиколлинеарностью, о которых мы писали выше.

Не очень строгим, но довольно интуитивным образом это можно объяснить так:

1. В точке оптимума линии уровня регуляризационного члена касаются линий уровня основного лосса, потому что, во-первых, и те, и другие выпуклые, а во-вторых, если они пересекаются трансверсально, то существует более оптимальная точка:

![1](https://yastatic.net/s3/education-portal/media/1_8_bb012dea47_1dd4105626.webp)

2. Линии уровня $L^1$-нормы – это $N$-мерные октаэдры. Точки их касания с линиями уровня лосса, скорее всего, лежат на грани размерности, меньшей $N-1$, то есть как раз в области, где часть координат равна нулю:

![1](https://yastatic.net/s3/education-portal/media/1_9_c157936d62_f29bca1e43.webp)

Заметим, что данное построение говорит о том, как выглядит оптимальное решение задачи, но ничего не говорит о способе, которым это решение можно найти. На самом деле, найти такой оптимум непросто: у $L^1$ меры довольно плохая производная. Однако, способы есть. Можете на досуге прочитать, например, [вот эту статью](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41159.pdf) о том, как работало предсказание CTR в google в 2012 году. Там этой теме посвящается довольно много места. Кроме того, рекомендуем посмотреть про проксимальные методы в разделе этой книги про оптимизацию в ML.

Заметим также, что вообще-то оптимизация любой нормы $L_x, \ 0  < x \leq 1$, приведёт к появлению разреженных векторов весов, просто если c $L^1$ ещё хоть как-то можно работать, то с остальными всё будет ещё сложнее.

## Другие лоссы

Стохастический градиентный спуск можно очевидным образом обобщить для решения задачи линейной регрессии с любой другой функцией потерь, не только квадратичной: ведь всё, что нам нужно от неё, – это чтобы у функции потерь был градиент. На практике это делают редко, но тем не менее рассмотрим ещё пару вариантов.

### MAE

**Mean absolute error**, абсолютная ошибка, появляется при замене $L^2$ нормы в MSE на $L^1$:

$$\color{#348FEA}{MAE(y, \widehat{y}) = \frac1N\sum_{i=1}^N \vert y_i - \widehat{y}_i\vert}
$$

Можно заметить, что в MAE по сравнению с MSE существенно меньший вклад в ошибку будут вносить примеры, сильно удалённые от ответов модели. Дело тут в том, что в MAE мы считаем модуль расстояния, а не квадрат, соответственно, вклад больших ошибок в MSE получается существенно больше. Такая функция потерь уместна в случаях, когда вы пытаетесь обучить регрессию на данных с большим количеством выбросов в таргете.

Иначе на эту разницу можно посмотреть так: MSE приближает матожидание условного распределения $y \mid x$, а MAE – медиану.

### MAPE

**Mean absolute percentage error**, относительная ошибка.

$$MAPE(y, \widehat{y}) = \frac1N\sum_{i=1}^N \left|\frac{\widehat{y}_i-y_i}{y_i}\right|
$$

Часто используется в задачах прогнозирования (например, погоды, загруженности дорог, кассовых сборов фильмов, цен), когда ответы могут быть различными по порядку величины, и при этом мы бы хотели верно угадать порядок, то есть мы не хотим штрафовать модель за предсказание 2000 вместо 1000 в разы сильней, чем за предсказание 2 вместо 1.

**Вопрос на подумать**. Кроме описанных выше в задаче линейной регрессии можно использовать и другие функции потерь, например, **Huber loss**:

$$\mathcal{L}(f, X, y)=\sum_{i=1}^N h_\delta\left(y_i-\left\langle w_i, x\right\rangle\right), \text { где } h_\delta(z)=\left\{\begin{array}{l}
\frac{1}{2} z^2,|z| \leqslant \delta \\
\delta\left(|z|-\frac{1}{2} \delta\right),|z|>\delta
\end{array}\right.
$$

Число $\delta$ является гиперпараметром. Сложная формула при $\vert z\vert > \delta$ нужна, чтобы функция $h_{\delta}(z)$ была непрерывной. Попробуйте объяснить, зачем может быть нужна такая функция потерь.

{% cut "Ответ (не открывайте сразу; сначала подумайте сами!)" %}

Часто требования формулируют в духе «функция потерь должна слабее штрафовать то-то и сильней штрафовать вот это». Например, $L^2$-регуляризованный лосс штрафует за большие по модулю веса. В данном случае можно заметить, что при небольших значениях ошибки берётся просто MSE, а при больших мы начинаем штрафовать нашу модель менее сурово. Например, это может быть полезно для того, чтобы выбросы не так сильно влияли на результат обучения.

{% endcut %}

## Линейная классификация

Теперь давайте поговорим про задачу классификации. Для начала будем говорить про бинарную классификацию на два класса. Обобщить эту задачу до задачи классификации на $K$ классов не составит большого труда. Пусть теперь наши таргеты $y$ кодируют принадлежность к положительному или отрицательному классу, то есть принадлежность множеству $\{-1,1\}$ (в этом параграфе договоримся именно так обозначать классы, хотя в жизни вам будут нередко встречаться и метки $\{0,1\}$), а $x$ – по-прежнему векторы из $\mathbb{R}^D$. Мы хотим обучить линейную модель так, чтобы плоскость, которую она задаёт, как можно лучше отделяла объекты одного класса от другого.

![1](https://yastatic.net/s3/education-portal/media/1_10_22eb9c5128_115a8caa4e.webp)

В идеальной ситуации найдётся плоскость, которая разделит классы: положительный окажется с одной стороны от неё, а отрицательный с другой. Выборка, для которой это возможно, называется **линейно разделимой**. Увы, в реальной жизни такое встречается крайне редко.

Как обучить линейную модель классификации, нам ещё предстоит понять, но уже ясно, что итоговое предсказание можно будет вычислить по формуле

$$y = \text{sign} \langle w, x_i\rangle
$$

{% cut "Почему бы не решать, как задачу регрессии?" %}

Мы можем попробовать предсказывать числа $-1$ и $1$, минимизируя для этого, например, MSE с последующим взятием знака, но ничего хорошего не получится. Во-первых, регрессия почти не штрафует за ошибки на объектах, которые лежат близко к *разделяющей плоскости*, но не с той стороны. Во вторых, ошибкой будет считаться предсказание, например, $5$ вместо $1$, хотя нам-то на самом деле не важно, какой у числа модуль, лишь бы знак был правильным. Если визуализировать такое решение, то проблемы тоже вполне заметны:

![1](https://yastatic.net/s3/education-portal/media/1_11_0fe41ca811_dca2d9e4b3.webp)

Нам нужна прямая, которая разделяет эти точки, а не проходит через них!

{% endcut %}

Сконструируем теперь функционал ошибки так, чтобы он вышеперечисленными проблемами не обладал. Мы хотим минимизировать число ошибок классификатора, то есть

$$\sum_i \mathbb{I}[y_i \neq sign \langle w, x_i\rangle]\longrightarrow \min_w
$$

Домножим обе части на $y_i$ и немного упростим

$$\sum_i \mathbb{I}[y_i \langle w, x_i\rangle < 0]\longrightarrow \min_w
$$

Величина $M = y_i \langle w, x_i\rangle$ называется **отступом** (**margin**) классификатора. Такая фунция потерь называется **misclassification loss**. Легко видеть, что

- отступ положителен, когда $sign(y_i) = sign(\langle w, x_i\rangle)$, то есть класс угадан верно; при этом чем больше отступ, тем больше расстояние от $x_i$ до разделяющей гиперплоскости, то есть «уверенность классификатора»;

- отступ отрицателен, когда $sign(y_i) \ne sign(\langle w, x_i\rangle)$, то есть класс угадан неверно; при этом чем больше по модулю отступ, тем более сокрушительно ошибается классификатор.

От каждого из отступов мы вычисляем функцию

$$F(M) = \mathbb{I}[M < 0] = \begin{cases}1,\ M < 0,\\ 0,\ M\geqslant 0\end{cases}
$$

Она кусочно-постоянная, и из-за этого всю сумму невозможно оптимизировать градиентными методами: ведь её производная равна нулю во всех точках, где она существует. Но мы можем мажорировать её какой-нибудь более гладкой функцией, и тогда задачу можно будет решить. Функции можно использовать разные, у них свои достоинства и недостатки, давайте рассмотрим несколько примеров:

![1](https://yastatic.net/s3/education-portal/media/1_12_7fddcb49c6_ff86ee2509.webp)

**Вопрос на подумать**. Допустим, мы как-то обучили классификатор, и подавляющее большинство отступов оказались отрицательными. Правда ли нас постигла катастрофа?

{% cut "Ответ (не открывайте сразу; сначала подумайте сами!)" %}

Наверное, мы что-то сделали не так, но ситуацию можно локально выправить, если предсказывать классы, противоположные тем, которые выдаёт наша модель.

{% endcut %}

**Вопрос на подумать**. Предположим, что у нас есть два классификатора с примерно одинаковыми и достаточно приемлемыми значениями интересующей нас метрики. При этом одна почти всегда выдаёт предсказания с большими по модулю отступами, а вторая – с относительно маленькими. Верно ли, что первая модель лучше, чем вторая?

{% cut "Ответ (не открывайте сразу; сначала подумайте сами!)" %}

На первый взгляд кажется, что первая модель действительно лучше: ведь она предсказывает «увереннее», но на самом деле всё не так однозначно: во многих случаях модель, которая умеет «честно признать, что не очень уверена в ответе», может быть предпочтительней модели, которая врёт с той же непотопляемой уверенностью, что и говорит правду. В некоторых случаях лучше может оказаться модель, которая, по сути, просто отказывается от классификации на каких-то объектах.

{% endcut %}

### Ошибка перцептрона

Реализуем простейшую идею: давайте считать отступы только на неправильно классифицированных объектах и учитывать их не бинарно, а линейно, пропорционально их размеру. Получается такая функция:

$$F(M) = \max(0, -M)
$$

Давайте запишем такой лосс с $L^2$-регуляризацией:

$$L(w, x, y) = \lambda\vert\vert w\vert\vert^2_2 + \sum_i \max(0, -y_i \langle w, x_i\rangle)
$$

Найдём градиент:

$$\nabla_w L(w, x, y) = 2 \lambda w + \sum_i
\begin{cases}
0,            & y_i \langle w, x_i \rangle > 0 \\
- y_i x_i,  & y_i \langle w, x_i \rangle \leq 0
\end{cases}
$$

Имея аналитическую формулу для градиента, мы теперь можем так же, как и раньше, применить стохастический градиентный спуск, и задача будет решена.

Данная функция потерь впервые была предложена для перцептрона Розенблатта, первой вычислительной модели нейросети, которая в итоге привела к появлению глубокого обучения.

Она решает задачу линейной классификации, но у неё есть одна особенность: её решение не единственно и сильно зависит от начальных параметров. Например, все изображённые ниже классификаторы имеют одинаковый нулевой лосс:

![1](https://yastatic.net/s3/education-portal/media/1_13_ef3e9fca2c_6eb38e5a25.webp)

### Hinge loss, SVM

Для таких случаев, как на картинке выше, возникает логичное желание не только найти разделяющую прямую, но и постараться провести её на одинаковом удалении от обоих классов, то есть максимизировать минимальный отступ:

![1](https://yastatic.net/s3/education-portal/media/1_14_34f6185216_69b91a68a8.webp)

Это можно сделать, слегка поменяв функцию ошибки, а именно положив её равной:

$$F(M) = \max(0, 1-M)
$$

$$L(w, x, y) = \lambda\||w\||^2_2 + \sum_i \max(0, 1-y_i \langle w, x_i\rangle)
$$

$$\nabla_w L(w, x, y) = 2 \lambda w + \sum_i
        \begin{cases}
            0,           & 1 - y_i \langle w, x_i \rangle \leq 0 \\
            - y_i x_i,   & 1 - y_i \langle w, x_i \rangle > 0
        \end{cases}
$$

Почему же добавленная единичка приводит к желаемому результату?

Интуитивно это можно объяснить так: объекты, которые проклассифицированы правильно, но не очень "уверенно" (то есть $0 \leq y_i \langle w, x_i\rangle < 1$), продолжают вносить свой вклад в градиент и пытаются "отодвинуть" от себя разделяющую плоскость как можно дальше.

К данному выводу можно прийти и чуть более строго; для этого надо совершенно по-другому взглянуть на выражение, которое мы минимизируем. Поможет вот эта картинка:

![1](https://yastatic.net/s3/education-portal/media/1_15_583bd074b0_283e6ecbec.webp)

Если мы максимизируем минимальный отступ, то надо максимизировать $\frac{2}{\|w\|_2}$, то есть ширину полосы при условии того, что большинство объектов лежат с правильной стороны, что эквивалентно решению нашей исходной задачи:

$$\lambda\|w\|^2_2 + \sum_i \max(0, 1-y_i \langle w, x_i\rangle) \longrightarrow\min\limits_{w}
$$

Отметим, что первое слагаемое у нас обратно пропорционально ширине полосы, но мы и максимизацию заменили на минимизацию, так что тут всё в порядке. Второе слагаемое – это штраф за то, что некоторые объекты неправильно расположены относительно разделительной полосы. В конце концов, никто нам не обещал, что классы наши линейно разделимы и можно провести оптимальную плоскость вообще без ошибок.

Итоговое положение плоскости задаётся всего несколькими обучающими примерами. Это ближайшие к плоскости правильно классифицированные объекты, которые называют **опорными векторами** или **support vectors**. Весь метод, соответственно, зовётся методом **опорных векторов**, или **support vector machine**, или сокращённо **SVM**. Начиная с шестидесятых годов это был сильнейший из известных методов машинного обучения. В девяностые его сменили методы, основанные на деревьях решений, которые, в свою очередь, недавно передали «пальму первенства» нейросетям.

Почему же SVM был столь популярен? Из-за небольшого количества параметров и доказуемой оптимальности. Сейчас для нас нормально выбирать специальный алгоритм под задачу и подбирать оптимальные гиперпараметры для этого алгоритма перебором, а когда-то трава была зеленее, а компьютеры медленнее, и такой роскоши у людей не было. Поэтому им нужны были модели, которые гарантированно неплохо работали бы в любой ситуации. Такой моделью и был SVM.

Другие замечательные свойства SVM: существование уникального решения и доказуемо минимальная склонность к переобучению среди всех популярных классов линейных классификаторов. Кроме того, несложная модификация алгоритма, ядровый SVM, позволяет проводить нелинейные разделяющие поверхности.

Строгий вывод постановки задачи SVM можно прочитать [тут](https://www.mit.edu/~9.520/spring08/Classes/class05.pdf) или [в лекции К.В. Воронцова](http://machinelearning.ru/wiki/images/archive/a/a0/20150316112120!Voron-ML-Lin-SVM.pdf).

### Логистическая регрессия

В этом параграфе мы будем обозначать классы нулём и единицей.

Ещё один интересный метод появляется из желания посмотреть на классификацию как на задачу предсказания вероятностей. Хороший пример – предсказание кликов в интернете (например, в рекламе и поиске). Наличие клика в обучающем логе не означает, что, если повторить полностью условия эксперимента, пользователь обязательно кликнет по объекту опять. Скорее у объектов есть какая-то "кликабельность", то есть истинная вероятность клика по данному объекту. Клик на каждом обучающем примере является реализацией этой случайной величины, и мы считаем, что в пределе в каждой точке отношение положительных и отрицательных примеров должно сходиться к этой вероятности.

Проблема состоит в том, что вероятность, по определению, величина от 0 до 1, а простого способа обучить линейную модель так, чтобы это ограничение соблюдалось, нет. Из этой ситуации можно выйти так: научить линейную модель правильно предсказывать какой-то объект, связанный с вероятностью, но с диапазоном значений $(-\infty,\infty)$, и преобразовать ответы модели в вероятность. Таким объектом является **logit** или **log odds** – логарифм отношения вероятности положительного события к отрицательному $\log\left(\frac{p}{1-p}\right)$.

Если ответом нашей модели является $\log\left(\frac{p}{1-p}\right)$, то искомую вероятность посчитать не трудно:

$$\langle w, x_i\rangle = \log\left(\frac{p}{1-p}\right)
$$

$$e^{\langle w, x_i\rangle} = \frac{p}{1-p}
$$

$$p=\frac{1}{1 + e^{-\langle w, x_i\rangle}}
$$

Функция в правой части называется **сигмоидой** и обозначается

$$\color{#348FEA}{\sigma(z) = \frac1{1 + e^{-z}}}
$$

Таким образом, $p = \sigma(\langle w, x_i\rangle)$

Как теперь научиться оптимизировать $w$ так, чтобы модель как можно лучше предсказывала логиты? Нужно применить метод максимума правдоподобия для распределения Бернулли. Это самое простое распределение, которое возникает, к примеру, при бросках монетки, которая орлом выпадает с вероятностью $p$. У нас только событием будет не орёл, а то, что пользователь кликнул на объект с такой вероятностью. Если хотите больше подробностей, почитайте про распределение Бернулли в теоретическом минимуме.

Правдоподобие позволяет понять, насколько вероятно получить данные значения таргета $y$ при данных $X$ и весах $w$. Оно имеет вид

$$p(y\mid X, w) =\prod_i p(y_i\mid x_i, w)
$$

и для распределения Бернулли его можно выписать следующим образом:

$$p(y\mid X, w) =\prod_i p_i^{y_i} (1-p_i)^{1-y_i}
$$

где $p_i$ – это вероятность, посчитанная из ответов модели. Оптимизировать произведение неудобно, хочется иметь дело с суммой, так что мы перейдём к логарифмическому правдоподобию и подставим формулу для вероятности, которую мы получили выше:

$$\ell(w, X, y) = \sum_i \big( y_i \log(p_i) + (1-y_i)\log(1-p_i) \big) =
$$

$$=\sum_i \big( y_i \log(\sigma(\langle w, x_i \rangle)) + (1-y_i)\log(1 - \sigma(\langle w, x_i \rangle)) \big)
$$

Если заметить, что

$$\sigma(-z) = \frac{1}{1 + e^z} = \frac{e^{-z}}{e^{-z} + 1} = 1 - \sigma(z),
$$

то выражение можно переписать проще:

$$\ell(w, X, y)=\sum_i \big( y_i \log(\sigma(\langle w, x_i \rangle)) + (1 - y_i) \log(\sigma(-\langle w, x_i \rangle)) \big)
$$

Нас интересует $w$, для которого правдоподобие максимально. Чтобы получить функцию потерь, которую мы будем *минимизировать*, умножим его на минус один:

$$\color{#348FEA}{L(w, X, y) = -\sum_i \big( y_i \log(\sigma(\langle w, x_i \rangle)) + (1 - y_i) \log(\sigma(-\langle w, x_i \rangle)) \big)}
$$

В отличие от линейной регрессии, для логистической нет явной формулы решения. Деваться некуда, будем использовать градиентный спуск. К счастью, градиент устроен очень просто:

$$\nabla_w L(y, X, w) = -\sum_i x_i \big( y_i - \sigma(\langle w, x_i \rangle) \big)
$$

{% cut "Вывод формулы градиента" %}

Нам окажется полезным ещё одно свойство сигмоиды:

$$\frac{d \log \sigma(z)}{d z} = \left( \log \left( \frac{1}{1 + e^{-z}} \right)  \right)' = \frac{e^{-z}}{1 + e^{-z}} = \sigma(-z) \\
$$

$$\frac{d \log \sigma(-z)}{d z} =  -\sigma(z)
$$

Отсюда:

$$\nabla_w \log \sigma(\langle w, x_i \rangle) =  \sigma(-\langle w, x_i \rangle) x_i \\
$$

$$\nabla_w \log \sigma(-\langle w, x_i \rangle) =  -\sigma(\langle w, x_i \rangle) x_i
$$

и градиент оказывается равным

$$\nabla_w L(y, X, w) = -\sum_i \big( y_i x_i \sigma(-\langle w, x_i \rangle) - (1 - y_i) x_i \sigma(\langle w, x_i \rangle) \big) = \\
$$

$$= -\sum_i \big( y_i x_i (1 - \sigma(\langle w, x_i \rangle)) - (1 - y_i) x_i \sigma(\langle w, x_i \rangle)\big) = \\
$$

$$= -\sum_i \big( y_i x_i - y_i x_i \sigma(\langle w, x_i \rangle) - x_i \sigma(\langle w, x_i \rangle) + y_i x_i \sigma(\langle w, x_i \rangle) \big) = \\
$$

$$= -\sum_i \big( y_i x_i - x_i \sigma(\langle w, x_i \rangle) \big)
$$

{% endcut %}

Предсказание модели будет вычисляться, как мы договаривались, следующим образом:

$$p=\sigma(\langle w, x_i\rangle)
$$

Это вероятность положительного класса, а как от неё перейти к предсказанию самого класса? В других методах нам достаточно было посчитать знак предсказания, но теперь все наши предсказания положительные и находятся в диапазоне от 0 до 1. Что же делать? Интуитивным и не совсем (и даже совсем не) правильным является ответ «взять порог 0.5». Более корректным будет подобрать этот порог отдельно, для уже построенной регрессии минимизируя нужную вам метрику на отложенной тестовой выборке. Например, сделать так, чтобы доля положительных и отрицательных классов примерно совпадала с реальной.

Отдельно заметим, что метод называется логистической *регрессией*, а не логистической *классификацией* именно потому, что предсказываем мы не классы, а вещественные числа – логиты.

**Вопрос на подумать**. Проверьте, что, если метки классов – это $\pm1$, а не $0$ и $1$, то функцию потерь для логистической регрессии можно записать в более компактном виде:

$$\mathcal{L}(w, X, y) = \sum_{i=1}^N\log(1 + e^{-y_i\langle w, x_i\rangle})
$$

**Вопрос на подумать**. Правда ли разделяющая поверхность модели логистической регрессии является гиперплоскостью?

{% cut "Ответ (не открывайте сразу; сначала подумайте сами!)" %}

Разделяющая поверхность отделяет множество точек, которым мы присваиваем класс $0$ (или $-1$), и множество точек, которым мы присваиваем класс $1$. Представляется логичным провести отсечку по какому-либо значению предсказанной вероятности. Однако, выбор этого значения — дело не очевидное. Как мы увидим в параграфе про калибровку классификаторов, это может быть не настоящая вероятность. Допустим, мы решили провести границу по значению $\frac12$. Тогда разделяющая поверхность как раз задаётся равенством $p = \frac12$, что равносильно $\langle w, x\rangle = 0$. А это гиперплоскость.

{% endcut %}

**Вопрос на подумать**. Допустим, что матрица объекты-признаки $X$ имеет полный ранг по столбцам (то есть все её столбцы линейно независимы). Верно ли, что решение задачи восстановления логистической регрессии единственно?

{% cut "Ответ (не открывайте сразу; сначала подумайте сами!)" %}

В этот раз хорошего геометрического доказательства, как было для линейной регрессии, пожалуй, нет; нам придётся честно посчитать вторую производную и доказать, что она является положительно определённой. Сделаем это для случая, когда метки классов – это $\pm1$. Формулы так получатся немного попроще. Напомним, что в этом случае

$$L(w, X, y) = \sum_{i=1}^N\log(1 + e^{-y_i\langle w, x_i\rangle})
$$

Следовательно,

$$\frac{\partial}{\partial w_{j}}L(w, X, y) = \sum_{i=1}^N\frac{y_ix_{ij}e^{-y_i\langle w, x_i\rangle}}{1 + e^{-y_i\langle w, x_i\rangle}} = \sum_{i=1}^Ny_ix_{ij}\left(1 - \frac1{1 + e^{-y_i\langle w, x_i\rangle}}\right)
$$

$$\frac{\partial^2L}{\partial w_j\partial w_k}(w, X, y) = \sum_{i=1}^Ny^2_ix_{ij}x_{ik}\frac{e^{-y_i\langle w, x_i\rangle}}{(1 + e^{-y_i\langle w, x_i\rangle})^2} =
$$

$$= \sum_{i=1}^Ny^2_ix_{ij}x_{ik}\sigma(y_i\langle w, x_i\rangle)(1 - \sigma(y_i\langle w, x_i\rangle))
$$

Теперь заметим, что $y_i^2 = 1$ и что, если обозначить через $D$ диагональную матрицу с элементами $\sigma(y_i\langle w, x_i\rangle)(1 - \sigma(y_i\langle w, x_i\rangle))$ на диагонали, матрицу вторых производных можно представить в виде:

$$\nabla^2L = \left(\frac{\partial^2\mathcal{L}}{\partial w_j\partial w_k}\right) = X^TDX
$$

Так как $0 < \sigma(y_i\langle w, x_i\rangle) < 1$, у матрицы $D$ на диагонали стоят положительные числа, из которых можно извлечь квадратные корни, представив $D$ в виде $D = D^{1/2}D^{1/2}$. В свою очередь, матрица $X$ имеет полный ранг по столбцам. Стало быть, для любого вектора приращения $u\ne 0$ имеем

$$u^TX^TDXu = u^TX^T(D^{1/2})^TD^{1/2}Xu = \vert D^{1/2}Xu \vert^2 > 0
$$

Таким образом, функция $L$ выпукла вниз как функция от $w$, и, соответственно, точка её экстремума непременно будет точкой минимума.

А теперь – **почему это не совсем правда**. Дело в том, что, говоря «*точка её экстремума непременно будет точкой минимума*», мы уже подразумеваем существование этой самой точки экстремума. Только вот существует этот экстремум не всегда. Можно показать, что для линейно разделимой выборки функция потерь логистической регрессии не ограничена снизу, и, соответственно, никакого экстремума нет. Доказательство мы оставляем читателю.

{% endcut %}

**Вопрос на подумать**. На картинке ниже представлены результаты работы на одном и том же датасете трёх моделей логистической регрессии с разными коэффициентами $L^2$-регуляризации:

![1](https://yastatic.net/s3/education-portal/media/1_16_f62fbdcf08_ef212cc69a.webp)

Наверху показаны предсказанные вероятности положительного класса, внизу – вид разделяющей поверхности.

Как вам кажется, какие картинки соответствуют самому большому коэффициенту регуляризации, а какие – самому маленькому? Почему?

{% cut "Ответ (не открывайте сразу; сначала подумайте сами!)" %}

Коэффициент регуляризации максимален у левой модели. На это нас могут натолкнуть два соображения. Во-первых, разделяющая прямая проведена достаточно странно, то есть можно заподозрить, что регуляризационный член в лосс-функции перевесил функцию потерь исходной задачи. Во-вторых, модель предсказывает довольно близкие к $\frac12$ вероятности – это значит, что значения $\langle w, x\rangle$ близки к нулю, то есть сам вектор $w$ близок к нулевому. Это также свидетельствует о том, что регуляризационный член играет слишком важную роль при оптимизации.

Наименьший коэффициент регуляризации у правой модели. Её предсказания достаточно «уверенные» (цвета на верхнем графике сочные, то есть вероятности быстро приближаются к $0$ или $1$). Это может свидетельствовать о том, что числа $\langle w, x\rangle$ достаточно велики по модулю, то есть $\vert\vert w \vert\vert$ достаточно велик.

{% endcut %}

## Многоклассовая классификация

В этом разделе мы будем следовать изложению [из лекций Евгения Соколова](https://github.com/esokolov/ml-course-hse/blob/master/2020-fall/lecture-notes/lecture06-linclass.pdf).

Пусть каждый объект нашей выборки относится к одному из $K$ классов: $\mathbb{Y} = \{1, \ldots, K\}$. Чтобы предсказывать эти классы с помощью линейных моделей, нам придётся свести задачу многоклассовой классификации к набору бинарных, которые мы уже хорошо умеем решать. Мы разберём два самых популярных способа это сделать – one-vs-all и all-vs-all, а проиллюстрировать их нам поможет вот такой игрушечный датасет

![1](https://yastatic.net/s3/education-portal/media/1_17_ac366ff85b_9b4b97df40.webp)

### Один против всех (one-versus-all)

Обучим $K$ линейных классификаторов $b_1(x), \ldots, b_K(x)$, выдающих оценки принадлежности классам $1, \ldots, K$ соответственно. В случае с линейными моделями эти классификаторы будут иметь вид

$$b_k(x) = \text{sgn}\left(\langle w_k, x \rangle + w_{0k}\right)
$$

Классификатор с номером $k$ будем обучать по выборке $\left(x_i, 2\mathbb{I}[y_i = k] - 1\right)_{i = 1}^{N}$; иными словами, мы учим классификатор отличать $k$-й класс от всех остальных.

Логично, чтобы итоговый классификатор выдавал класс, соответствующий самому уверенному из бинарных алгоритмов. Уверенность можно в каком-то смысле измерить с помощью значений линейных функций:

$$a(x) = \text{argmax}_k \left(\langle w_k, x \rangle + w_{0k}\right)
$$

Давайте посмотрим, что даст этот подход применительно к нашему датасету. Обучим три линейных модели, отличающих один класс от остальных:

![1](https://yastatic.net/s3/education-portal/media/1_18_4d5acf1fb3_8679340170.webp)

Теперь сравним значения линейных функций

![1](https://yastatic.net/s3/education-portal/media/1_19_324b7eedcf_aad79d81c8.webp)

и для каждой точки выберем тот класс, которому соответствует большее значение, то есть самый «уверенный» классификатор:

![1](https://yastatic.net/s3/education-portal/media/1_20_8164bb85e4_1e60894484.webp)

Хочется сказать, что самый маленький класс «обидели».

Проблема данного подхода заключается в том, что каждый из классификаторов $b_1(x), \dots, b_K(x)$ обучается на своей выборке, и значения линейных функций $\langle w_k, x \rangle + w_{0k}$ или, проще говоря, "выходы" классификаторов могут иметь разные масштабы. Из-за этого сравнивать их будет неправильно. Нормировать вектора весов, чтобы они выдавали ответы в одной и той же шкале, не всегда может быть разумным решением: так, в случае с SVM веса перестанут являться решением задачи, поскольку нормировка изменит норму весов.

### Все против всех (all-versus-all)

Обучим $C_K^2$ классификаторов $a_{ij}(x)$, $i, j = 1, \dots, K$, $i \neq j$. Например, в случае с линейными моделями эти модели будут иметь вид

$$b_{ij}(x) = \text{sgn}\left( \langle w_{ij}, x \rangle + w_{0,ij} \right)
$$

Классификатор $a_{ij}(x)$ будем настраивать по подвыборке $X_{ij} \subset X$, содержащей только объекты классов $i$ и $j$. Соответственно, классификатор $a_{ij}(x)$ будет выдавать для любого объекта либо класс $i$, либо класс $j$. Проиллюстрируем это для нашей выборки:

![1](https://yastatic.net/s3/education-portal/media/1_21_6cebfcb75d_cba1fb93c0.webp)

Чтобы классифицировать новый объект, подадим его на вход каждого из построенных бинарных классификаторов. Каждый из них проголосует за свой класс; в качестве ответа выберем тот класс, за который наберется больше всего голосов:

$$a(x) = \text{argmax}_k\sum_{i = 1}^{K} \sum_{j \neq i}\mathbb{I}[a_{ij}(x) = k]
$$

Для нашего датасета получается следующая картинка:

![1](https://yastatic.net/s3/education-portal/media/1_22_3d30a2271f_9c28114951.webp)

Обратите внимание на серый треугольник на стыке областей. Это точки, для которых голоса разделились (в данном случае каждый классификатор выдал какой-то свой класс, то есть у каждого класса было по одному голосу). Для этих точек нет явного способа выдать обоснованное предсказание.

### Многоклассовая логистическая регрессия

Некоторые методы бинарной классификации можно напрямую обобщить на случай многих классов. Выясним, как это можно проделать с логистической регрессией.

В логистической регрессии для двух классов мы строили линейную модель

$$b(x) = \langle w, x \rangle + w_0,
$$

а затем переводили её прогноз в вероятность с помощью сигмоидной функции $\sigma(z) = \frac{1}{1 + \exp(-z)}$. Допустим, что мы теперь решаем многоклассовую задачу и построили $K$ линейных моделей

$$b_k(x) = \langle w_k, x \rangle + w_{0k},
$$

каждая из которых даёт оценку принадлежности объекта одному из классов. Как преобразовать вектор оценок $(b_1(x), \ldots, b_K(x))$ в вероятности? Для этого можно воспользоваться оператором $\text{softmax}(z_1, \ldots, z_K)$, который производит «нормировку» вектора:

$$\text{softmax}(z_1, \ldots, z_K) = \left(\frac{\exp(z_1)}{\sum_{k = 1}^{K} \exp(z_k)},
\dots, \frac{\exp(z_K)}{\sum_{k = 1}^{K} \exp(z_k)}\right).
$$

В этом случае вероятность $k$-го класса будет выражаться как

$$P(y = k \vert x, w) = \frac{
\exp{(\langle w_k, x \rangle + w_{0k})}}{ \sum_{j = 1}^{K} \exp{(\langle w_j, x \rangle + w_{0j})}}.
$$

Обучать эти веса предлагается с помощью метода максимального правдоподобия: так же, как и в случае с двухклассовой логистической регрессией:

$$\sum_{i = 1}^{N} \log P(y = y_i \vert x_i, w) \to \max_{w_1, \dots, w_K}
$$

## Масштабируемость линейных моделей

Мы уже обсуждали, что SGD позволяет обучению хорошо масштабироваться по числу объектов, так как мы можем не загружать их целиком в оперативную память. А что делать, если признаков очень много, или мы не знаем заранее, сколько их будет? Такое может быть актуально, например, в следующих ситуациях:

- Классификация текстов: мы можем представить текст в формате «мешка слов», то есть неупорядоченного набора слов, встретившихся в данном тексте, и обучить на нём, например, определение тональности отзыва в интернете. Наличие каждого слова из языка в тексте у нас будет кодироваться отдельной фичой. Тогда размерность каждого элемента обучающей выборки будет порядка нескольких сотен тысяч.
- В задаче предсказания кликов по рекламе можно получить выборку любой размерности, например, так: в качестве фичи закодируем индикатор того, что пользователь X побывал на веб-странице Y. Суммарная размерность тогда будет порядка $10^9 \cdot 10^7 = 10^{16}$. Кроме того, всё время появляются новые пользователи и веб-страницы, так что на этапе применения нас ждут сюрпризы.

Есть несколько хаков, которые позволяют бороться с такими проблемами:

- Несмотря на то, что полная размерность объекта в выборке огромна, количество ненулевых элементов в нём невелико. Значит, можно использовать разреженное кодирование, то есть вместо плотного вектора хранить словарь, в котором будут перечислены индексы и значения ненулевых элементов вектора.
- Даже хранить все веса не обязательно! Можно хранить их в хэш-таблице и вычислять индекс по формуле `hash(feature) %  tablesize`. Хэш может вычисляться прямо от слова или id пользователя. Таким образом, несколько фичей будут иметь общий вес, который тем не менее обучится оптимальным образом. Такой подход называется **hashing trick**. Ясно, что сжатие вектора весов приводит к потерям в качестве, но, как правило, ценой совсем небольших потерь можно сжать этот вектор на много порядков.

Примером открытой библиотеки, в которой реализованы эти возможности, является [vowpal wabbit](https://vowpalwabbit.org/).

### Parameter server

Если при решении задачи ставки столь высоки, что мы не можем разменивать качество на сжатие вектора весов, а признаков всё-таки очень много, то задачу можно решать распределённо, храня все признаки в шардированной хеш-таблице

![1](https://yastatic.net/s3/education-portal/media/1_23_2a619a4168_e576bac717.webp)

Кружки здесь означают отдельные сервера. Жёлтые загружают данные, а серые хранят части модели. Для обучения жёлтый кружок запрашивает у серого нужные ему для предсказания веса, считает градиент и отправляет его обратно, где тот потом применяется. Схема обладает бесконечной масштабируемостью, но задач, где это оправдано, не очень много.

## Подытожим

На линейную модель можно смотреть как на однослойную нейросеть, поэтому многие методы, которые были изначально разработаны для них, сейчас переиспользуются в задачах глубокого обучения, а базовые подходы к регрессии, классификации и оптимизации вообще выглядят абсолютно так же. Так что несмотря на то, что в целом линейные модели на сегодня применяются редко, то, из чего они состоят и как строятся, знать очень и очень полезно.

Надеемся также, что главным итогом прочтения этого параграфа для вас будет осознание того, что решение любой ML-задачи состоит из выбора функции потерь, параметризованного класса моделей и способа оптимизации. В следующих параграфах мы познакомимся с другими моделями и оптимизаторами, но эти базовые принципы не изменятся.

---

Теперь предлагаем вам потренировать изученный материал на практике. Скачайте [ноутбук](https://yastatic.net/s3/ml-handbook/admin/autohw_linear_models_dabd6b0378.ipynb?updated_at=2024-03-07T13:21:15.517Z) с лабораторной работой. В нём вы найдете описания заданий и дополнительные материалы. Задания из лабораторной прикреплены к этому параграфу в виде задач в системе Яндекс Контест. Чтобы проверить себя, отправляйте решения по соответствующим задачам в систему. Успехов в практике!

  ## handbook

  Учебник по машинному обучению

  ## title

  Линейные модели

  ## description

  Линейные модели от линейной до логистической регрессии. Регуляризация, работа с категориальными признаками, многоклассовая классификация

- 
  ## path

  /handbook/ml/article/metricheskiye-metody

  ## content

  Смысл метрических методов очень хорошо раскрывает фраза «Скажи мне, кто твой друг, и я скажу, кто ты». Алгоритмы этого класса почти не имеют фазы обучения. Вместо этого они просто запоминают всю обучающую выборку, а на этапе предсказания просто ищут объекты, похожие на целевой. 

Такой процесс называют **lazy learning**, потому что никакого обучения, по сути, не происходит. Также метрические модели являются непараметрическими, потому что они не делают явных допущений о глобальных законах, которым подчиняются данные. Так, линейная регрессия основывается на предположении о том, что изучаемая закономерность линейная (с неизвестными коэффициентами, которые восстанавливаются по выборке), а линейная бинарная классификация — что существует гиперплоскость, неплохо разделяющая классы. Метрические методы же локальны: они исходят из допущения, что свойства объекта можно узнать, имея представление о его соседях.
 
Указанные выше свойства могут быть полезными, особенно в случае сложно устроенных данных, для которых мы не можем придумать глобальную модель. Однако с другой стороны, из-за lazy learning алгоритм становится абсолютно неприменимым при большом количестве данных. Несмотря на то, что эти алгоритмы очень просты для понимания, они довольно точны и хорошо интерпретируемы — и часто используются как минимум в качестве бейзлайнов в разных задачах.
 
В первой части параграфа мы расскажем об одном из самых известных метрических алгоритмов — методе **k-ближайших соседей** (**k-nearest neighbors, KNN**). Этот подход в основном чисто инженерный из-за отсутствия фазы обучения — в настоящее время уже почти нигде не применяется. Однако многие техники, на которых основан алгоритм, используются и в других методах.

Например, у алгоритмов поиска ближайших соседей, — неотъемлемой часть метода, — намного более широкая область применения. Плюс ко всему KNN — очень простой и легко интерпретируемый алгоритм, поэтому изучить его всё равно полезно. Мы обсудим подробнее его преимущества, недостатки, область его применения, а также возможные обобщения.

Для метрических методов очень важно уметь эффективно находить ближайшие объекты, поэтому задача их поиска неизбежно возникает при применении любого такого алгоритма. Поэтому во второй части параграфа мы рассмотрим возможные подходы к быстрому поиску ближайших соседей.
 
 
## Метод k-ближайших соседей (KNN)
 
Представим, что мы проводим классификацию объектов на два класса — красный или жёлтый. Нам дана некоторая обучающая выборка и целевой объект (серый):
 
![2_1_b7525e2dd2.webp](https://yastatic.net/s3/education-portal/media/2_1_b7525e2dd2_5d0a4ba929.webp)
 
Мы хотим определить, к какому классу относится серый объект. Интуитивно очевидно, что он должен быть жёлтым, потому что все его соседи жёлтые. Эта интуиция и отражает суть метода KNN — классифицировать целевой объект, исходя из того, какие классы у объектов, которые максимально похожи на него.
 
Перейдём теперь к более формальному описанию алгоритма. Рассмотрим сначала задачу многоклассовой классификации, а регрессией займёмся позже.
 
Пусть дана обучающая выборка $X = (x_i, y_i)_{i=1}^N$, где $x_i \in \mathbb{X}, \ y_i \in \mathbb{Y}=\\{1,\ldots,C\\}$. Пусть также задана некоторая симметричная по своим аргументам функция расстояния $\color{#E06A27}{\rho : \mathbb{X} \times \mathbb{X} \to [0, +\infty)}$. Предположим, что требуется классифицировать новый объект $\color{#97C804}{u}$. Для этого найдём $k$ наиболее близких к $\color{#97C804}{u}$ в смысле расстояния $\color{#E06A27}{\rho}$ объектов обучающей выборки $X_k(\color{#97C804}{u}) = \\{\color{#FFC100}{x^{(1)}_u},\ldots,\color{#FFC100}{x^{(k)}_u}\\}$:

$$\forall \color{#FFC100}{x_{\rm in}}\in X_k(\color{#97C804}{u}) \ \forall x_{\rm out} \in X \setminus X_k(\color{#97C804}{u}) \quad \color{#E06A27}{\rho}(\color{#97C804}{u}, \color{#FFC100}{x_{\rm in}}) \leqslant \color{#E06A27}{\rho}(\color{#97C804}{u}, x_{\rm out}). \tag{1}$$
 
Метку класса объекта $\color{#FFC100}{x^{(i)}_u}$ будем обозначать $\color{#FFC100}{y_u^{(i)}}$. Класс нового объекта тогда естественным образом определим как наиболее часто встречающийся класс среди объектов из $X_k(\color{#97C804}{u})$:
 
$$a(\color{#97C804}{u}) = \underset{y\in \mathbb{Y}}{\operatorname{arg max}} \sum_{i=1}^k \mathbb{I}[\color{#FFC100}{y_u^{(i)}} = y] \tag{2}$$
 
Формула может показаться страшной, но на самом деле всё довольно просто: для каждой метки класса $y \in \mathbb{Y}$ количество соседей $\color{#97C804}{u}$ с такой меткой можно посчитать, просто просуммировав по всем соседям индикаторы событий, соответствующих тому, что метка соседа равна $y$.
 
Легко заметить, что этот алгоритм позволяет также оценивать вероятности классов. Для этого достаточно просто посчитать частоты классов соседей:
 
$$\mathbb{P}(\color{#97C804}{u}\sim y) = \frac{\sum_{i=1}^k \mathbb{I}[\color{#FFC100}{y_u^{(i)}} = y]}k$$
 
Стоит, однако, понимать, что, хоть такая функция и удовлетворяет свойствам вероятности (она неотрицательна, аддитивна и ограничена единицей), это не более чем эвристика.
  
Несмотря на то что формально фаза обучения отсутствует, алгоритм может легко переобучиться. Вы можете убедиться в этом сами, использовав маленькое количество соседей (например, одного или двух), — границы классов оказываются довольно сложными. Происходит это из-за того, что параметрами алгоритма можно считать всю обучающую выборку, довольно большую по размеру. Из-за этого алгоритму легко подстроиться под конкретные данные.

<p style="color:darkslategrey;font-size:14px">По <a href="https://yastatic.net/s3/academy/ml/knn_clf/knn_clf.html">ссылке</a> вы можете увидеть интерактивный пример работы алгоритма. Автор примера - <a href="mailto:Aechirikova@edu.hse.ru">Анастасия Чирикова.</a></p>
 
### Выбор метрики
 
Может возникнуть закономерный вопрос, как же правильно выбрать функцию расстояния $\color{#E06A27}{\rho}$. В подавляющем большинстве случаев обычное евклидово расстояние $\rho(x, y) = \sqrt{\sum_i (x_i - y_i)^2}$ будет хорошим выбором. Однако в некоторых случаях другие функции будут подходить лучше, поэтому давайте разберём ещё несколько функций, наиболее используемых на практике.
 
![2_2_8440e10f60.webp](https://yastatic.net/s3/education-portal/media/2_2_8440e10f60_6a35a4f1bc.webp)
 
<br/>
 
#### Манхэттенская метрика
 
$$\rho(x, y) = \sum_i \vert x_i - y_i \vert$$
 
Часто используется в высокоразмерных пространствах из-за лучшей устойчивости к выбросам. Представим, что два объекта в 1000-размерном пространстве почти идентичны, но сильно отличаются по одному из признаков. Это почти наверняка свидетельствует о выбросе в этом признаке, и объекты, скорее всего, очень близки. Однако евклидово расстояние усилит различие в единственном признаке и сделает их более далёкими друг от друга. Этого недостатка лишена манхэттенская метрика — в ней вместо квадрата используется модуль. 
 
#### Метрика Минковского
 
$$\rho(x, y) = \left(\sum_i \vert x_i - y_i\vert^p\right)^{1/p}$$
 
Является обобщением евклидовой ($p=2$) и манхэттенской ($p=1$) метрик.
 
#### Косинусное расстояние
 
$$\rho(x,y) = 1 - \cos \theta = 1 - \frac{x \cdot y}{\|x\| \|y\|}$$
 
Эта метрика хороша тем, что не зависит от норм векторов. Такое поведение бывает полезно в некоторых задачах, например при поиске похожих документов. В качестве признаков там часто используются количества слов. При этом интуитивно кажется, что если в тексте использовать каждое слово в два раза больше, то тема этого текста поменяться не должна. Поэтому как раз в этом случае нам не важна норма вектор-признака, и в задачах, связанных с текстами, часто применяется именно косинусное расстояние.
 
#### Расстояние Жаккара
 
$$\rho(A, B) = 1 - \frac{|A\cap B|}{|A\cup B|}$$
 
Его стоит использовать, если исследуемые объекты — это некоторые множества. Это полезно тем, что нет нужды придумывать векторные представления для этих множеств, чтобы использовать традиционные метрики.
 
Вообще говоря, несмотря на некоторые эвристические соображения по выбору метрики, её можно считать гиперпараметром и подбирать соответствующими способами. Часто качество модели сильно зависит от выбора метрики, а иногда выбрать правильную метрику очень тяжело. Например, в случае когда данные имеют сильно разный масштаб, выбрать подходящую метрику почти невозможно, и нужно сперва проводить нормализацию.

**Замечание**. Упомянутые в этом параграфе функции мы называем «метриками», но, конечно же, они не обязаны быть метриками в строгом математическом смысле. Они неотрицательны и симметричны, но могут не удовлетворять неравенству треугольника.
 
 
### Обобщения алгоритма
 
#### Взвешенный KNN
 
У оригинального алгоритма есть один большой недостаток: он никак не учитывает расстояния до соседних объектов, хотя эта информация может быть полезной.
 
Давайте попробуем придумать, как исправить этот недостаток. Нам нужно каким-то образом увеличивать вклад близких объектов и уменьшать вклад далёких. Можно заметить, что все индикаторы в формуле $(2)$ учитываются в сумме с одинаковыми коэффициентами. Возникает идея — назначить этим индикаторам веса, которые тем больше, чем ближе объект к целевому. Таким образом, получаем следующую формулу:
 
$$a(\color{#97C804}{u}) = \underset{y\in \mathbb{Y}}{\operatorname{arg max}} \sum_{i=1}^k w_i\mathbb{I}[\color{#FFC100}{y_u^{(i)}} = y]. \tag{3}$$
 
Такой алгоритм называется **взвешенным KNN** (**weighted KNN**).
 
Есть множество вариантов выбора весов для объектов, которые можно поделить на две большие группы. В первой группе веса зависят лишь от порядкового номера объекта в отсортированном по близости к $u$ массиве $X_k(\color{#97C804}{u})$. Чаще всего затухающие веса берутся линейно $\left( w_i = \frac{k+1-i}{k} \right)$ или экспоненциально $\left( w_i = q^i, \ 0 < q < 1\right)$ .
 
Однако здесь мы также не используем всю информацию, которая нам доступна. Зачем использовать порядок соседей, порождаемый расстояниями, если можно использовать сами расстояния? 

Во второй группе методов вес — это некоторая функция от расстояния. Давайте подумаем, какие должны быть свойства у этой функции. 
- Очевидно, она должна быть положительной на своей области определения, иначе модель будет поощрять несовпадение с некоторыми ближайшими соседями. 
- Также необходимо, чтобы функция монотонно не возрастала, чтобы вес близких соседей был больше, чем далёких. 

Таким образом вводится так называемая *ядерная функция* (*kernel function*) $K : \mathbb{R} \to \mathbb{R}$, обладающая перечисленными выше свойствами, с помощью которой и высчитывается вес каждого соседа:
 
$$a(\color{#97C804}{u}) = \underset{y\in \mathbb{Y}}{\operatorname{arg max}} \sum_{i=1}^k K\left(\frac{\color{#E06A27}{\rho}(\color{#97C804}{u}, \color{#FFC100}{x_u^{(i)}})}{h}\right)\mathbb{I}[\color{#FFC100}{y_u^{(i)}} = y], \tag{4}$$
 
где $h$ — некое положительное число, которое называется *шириной окна*.
 
От выбора ядра зависит гладкость аппроксимации, но на её качество этот выбор почти не влияет. Примеры ядерных функций в порядке увеличения их гладкости:
 
- $K(x) = \frac12 \mathbb{I} \left[ \vert x \vert \leqslant 1\right]$ — прямоугольное ядро;
- $K(x) = \left(1 - \vert x \vert \right)\mathbb{I}\left[\vert x \vert \leqslant 1\right]$ — треугольное ядро (непрерывное);
- $K(x) = \frac34\left(1 - x^2\right) \mathbb{I}\left[ \vert x \vert \leqslant 1\right]$ — ядро Епанечникова (гладкое везде, кроме –1 и 1);
- $K(x) = \frac{15}{16}\left(1 - x^2\right)^2\mathbb{I}\left[\vert x \vert \leqslant 1\right]$ — биквадратное ядро (гладкое везде);
- $K(x) = \frac{1}{\sqrt{2\pi}}e^{-2x^2}$ — гауссовское ядро (бесконечно гладкое везде).
 
На практике чаще всего используют либо прямоугольное для простоты, либо гауссовское, в случае когда важна гладкость модели (немного забегая вперёд — это особенно важно в регрессии).
 
Ширина окна, в свою очередь, сильно влияет как раз на качество модели. При слишком маленькой ширине модель сильно подстраивается под обучающую выборку и теряет свою обобщающую способность. При слишком большой ширине, напротив, модель становится слишком простой. Универсальной ширины окна не существует, поэтому для каждой задачи её приходится подбирать отдельно.

#### Kernel regression
 
Алгоритм KNN можно довольно легко обобщить и на задачу регрессии. Самые очевидные способы — брать для некоторого ядра $K$ либо обычное среднее:
 
$$a(\color{#97C804}{u}) = \frac1k \sum_{i=1}^k \color{#FFC100}{y_u^{(i)}}, \tag{5}$$
 
либо взвешенный вариант:
 
$$a(\color{#97C804}{u}) = \frac{\sum_{i=1}^k K\left(\frac{\color{#E06A27}{\rho}(\color{#97C804}{u}, \color{#FFC100}{x_u^{(i)}})}{h}\right) \color{#FFC100}{y_u^{(i)}}}{\sum_{i=1}^k K\left(\frac{\color{#E06A27}{\rho}(\color{#97C804}{u}, \color{#FFC100}{x_u^{(i)}})}{h}\right)} \tag{6}$$

Последняя формула называется *формулой Надарая — Ватсона*. Она — один из непараметрических методов восстановления регрессии, объединённых названием *ядерная регрессия* (*kernel regression*).
 
Выписать ответ, конечно, просто, но возникает интересный вопрос: можно ли использовать оптимизационные формулы из задачи классификации? Сначала давайте подумаем, что выдаст алгоритм, если формулу $(4)$ применить без изменений. 

В задаче регрессии почти наверняка все значения $y_u^{(i)}$ будут различными. Поэтому для любого $y$ сумма в формуле $(4)$ будет состоять из не более чем одного слагаемого, а значит, максимум будет достигаться на соседе с наибольшим весом, то есть на ближайшем соседе. Это означает, что метод всегда вырождается в 1-NN. Это не совсем то, чего мы добиваемся, поэтому давайте немного модифицируем алгоритм.
 
Давайте сперва подумаем, а для чего вообще в формуле $(4)$ используется индикатор. В задаче классификации индикатор — естественная мера близости двух объектов: если объекты совпадают, то значение $1$, если различаются, то $0$. Проблема в том, что в задаче регрессии объекты являются действительными числами, и для них функция, которая выдаёт отличное от нуля значение лишь в одной точке $y=y_i$, — плохая мера близости. 

В случае непрерывных значений $y$ естественно использовать более гладкие функции для выражения близости. Таким образом, для обобщения формулы $(4)$ на задачу регрессии нам необходимо всего лишь заменить индикатор на некоторую более гладкую функцию. При этом для действительных чисел чаще всего рассматривают не близость, а расстояние между ними, то есть некоторую метрику. Например, в качестве такой метрики можно взять квадрат евклидова расстояния $(y- y_u^{(i)})^2$. Отметим, что максимизация близости эквивалентна минимизации расстояния, и получим следующую формулу:
  
$$a(\color{#97C804}{u}) = \underset{y\in \mathbb{R}}{\operatorname{arg min}} \sum_{i=1}^k K\left(\frac{\color{#E06A27}{\rho}(\color{#97C804}{u}, \color{#FFC100}{x_u^{(i)}})}{h}\right)(y- \color{#FFC100}{y_u^{(i)}})^2. \tag{7}$$
 
Выбор именно этой функции хорош тем, что у этой оптимизационной задачи есть точное решение, и оно записывается как раз формулой $(6)$.
 
Для ядерной регрессии справедливы те же рассуждения про выбор ядра и ширины окна, которые были приведены в прошлом разделе про классификацию.
 
Влияние ширины окна и вида ядра на вид функции:  
![2_3_b71a476ace.webp](https://yastatic.net/s3/education-portal/media/2_3_b71a476ace_8ba7ceac1d.webp)
![2_4_1f3e8fb8d7.webp](https://yastatic.net/s3/education-portal/media/2_4_1f3e8fb8d7_4b5dfc48b1.webp)
![2_5_a94e8cc0dd.webp](https://yastatic.net/s3/education-portal/media/2_5_a94e8cc0dd_1d30dce7e4.webp)
  
### Преимущества и недостатки
 
Сперва поговорим о преимуществах алгоритма.
- Непараметрический, то есть не делает явных предположений о распределении данных.
- Очень простой в объяснении и интерпретации.
- Достаточно точный, хоть и чаще всего уступает градиентному бустингу и случайному лесу в accuracy.
- Может быть использован как для классификации, так и для регрессии.
 
Несмотря на большие преимущества, алгоритм не лишён и минусов.
- Неэффективный по памяти, поскольку нужно хранить всю обучающую выборку.
- Вычислительно дорогой по той же причине.
- Чувствителен к масштабу данных, а также к неинформативным признакам.
- Для применения алгоритма необходимо, чтобы метрическая близость объектов совпадала с их семантической близостью, чего не всегда просто добиться. Представим, например, что мы решаем задачу нахождения похожих изображений. Мы хотим, чтобы картинки с лесом находились близко друг к другу, однако, если взять любую попиксельную метрику, такие картинки могут быть очень далеки друг от друга. Зачастую для решения этой проблемы вначале обучают представления.

### Применение
 
Из-за своих недостатков алгоритм очень неэффективен в задачах с большим количеством данных. Однако у него всё равно есть много применений в реальном мире. Приведём лишь некоторые из них:
 
- Рекомендательные системы. Если посмотреть на саму формулировку задачи «предложить пользователю что-то похожее на то, что он любит», то KNN прямо напрашивается в качестве решения. Несмотря на то что сейчас часто используются более совершенные алгоритмы, метод ближайших соседей всё равно применяется в качестве хорошего бейзлайна.
- Поиск семантически похожих документов. Если векторные представления близки друг к другу, то темы документов схожи.
- Поиск аномалий и выбросов. Из-за того что алгоритм запоминает обучающую выборку полностью, ему легко посмотреть, насколько целевой объект похож на все данные, которые он видел.
- Задача кредитного скоринга. Рейтинги двух людей, у которых примерно одинаковая зарплата, схожие должности и кредитные истории, не должны сильно отличаться, поэтому KNN отлично подходит для решения такой задачи.
 
Вопрос сложности алгоритма неочевиден и требует детального анализа, который будет частично проведён в следующем разделе.

## Поиск ближайших соседей
 
Для того чтобы применять метод ближайших соседей, нужно уметь как-то находить этих самых соседей. С первого взгляда может показаться, что никакой проблемы нет: действительно, можно ведь просто перебрать все объекты из обучающей выборки $X = (x_i, y_i)_{i=1}^{N}$, посчитать для каждого из них расстояние до тестового объекта и затем найти минимум.

Однако несмотря на то что сложность такого поиска линейная по $N$, она также зависит и от размерности пространства признаков. Если $x \in \mathbb{R}^D$, то сложность такого алгоритма поиска $O(N D)$. Если вспомнить, что в типичной задаче машинного обучения количество признаков $D$ может быть порядка $100$, а размер выборки и вовсе может исчисляться десятками и сотнями тысяч объектов, то становится ясно, что такая сложность никуда не годится. Проблема осложняется ещё и тем, что данный поиск необходимо выполнять на этапе применения модели, который должен быть быстрым. Всё это означает, что возникает необходимость в более быстрых методах поиска ближайших соседей, чем простой перебор.
 
Все такие методы можно поделить на две основные группы: точные и приближённые. Последние, как следует из их названия, находят соседей лишь приближённо, то есть найденные объекты хоть и будут действительно близки, но не обязательно будут самыми близкими. В этом разделе мы подробнее рассмотрим методы из каждой группы. 
 
Перед началом обзора стоит сказать, что хоть мы и рассматриваем алгоритмы поиска соседей именно в контексте их использования в KNN, область их применения значительно шире, и она не ограничивается исключительно машинным обучением. Например, на их основе работает любая информационно-поисковая система, от поиска в «Гугле» или «Яндексе» до всем известных алгоритмов «Ютьюба». 

## Поиск ближайших соседей: точные методы
 
Точных методов существует довольно мало. Можно сказать, что их, по сути, два. 
- Первый — полный перебор с различными эвристиками. Например, можно выбрать подмножество признаков и считать расстояние только по ним. Оно будет оценкой снизу на реальное расстояние, поэтому если оно уже больше, чем до текущего ближайшего объекта, то можно сразу отбросить этот объект и переходить к следующему. Такие эвристики хоть и могут давать некоторый выигрыш по времени, но не улучшат асимптотическую сложность. 
- Второй — k-d-деревья, о которых стоит поговорить подробнее.

### K-d-деревья
 
Представим на секунду, что у нас есть всего лишь один признак, то есть объекты выражаются вещественными числами, а не векторами. В этом случае для поиска ближайшего соседа напрашивается всем вам известное бинарное дерево поиска, которое позволяет находить элементы за логарифмическое время. Оказывается, существует аналог данной структуры в многомерном пространстве, который называется **k-d-дерево** (**k-d tree**, сокращение от k-dimensional tree).
 
Как и в обычном дереве поиска, в k-d-дереве каждый узел является объектом обучающей выборки, который особым образом делит пространство на два полупространства. Таким образом, всё пространство оказывается поделено на множество малых областей, и такое деление оказывается очень полезным при поиске ближайших соседей.
 
Рассмотрим подробнее, как строится такое дерево. Трудность в применении обычного дерева поиска состоит в том, что мы не можем напрямую сравнить два вектора так же, как два вещественных числа. Чтобы эту проблему преодолеть, узлы дерева будут делить пространство лишь по одной оси. При движении вниз по дереву оси, по которым точки делят пространство, циклически сменяют друг друга. Например, в двумерном пространстве корень будет отвечать за деление по x-координате, его сыновья — за деление по y-координате, а внуки — снова за x-координату, и т. д. Посмотрим, как это работает на примере:

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/2_6_e762398494_7d10925856.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://www.cs.cmu.edu/~ckingsf/bioinfo-lectures/kdtrees.pdf">Источник</a>
  </figcaption>
</figure>
 
На картинке выше корень $(30, 40)$ делит все точки по оси х: слева оказываются точки, у которых $x < 30$, а справа — те, у которых $x\geqslant 30$. Аналогично левый сын корня $(5, 25)$ делит своё поддерево по оси y: слева оказываются точки, у которых $y < 25$, а справа — те, у которых $y\geqslant 25$.
 
Остаётся вопрос — как выбирать точки, которые будут делить пространство пополам? Чтобы дерево было сбалансированным, нужно находить точку с медианой, соответствующей уровню поддерева координаты. На практике часто ограничиваются выбором случайной точки или любой эвристикой по приближённому поиску медианы (например, медиана некоторого подмножества точек). Это позволяет ускорить построение дерева, но убирает все гарантии на его сбалансированность.
 
Добавлять новые точки можно так же, как и в одномерном дереве поиска. Спускаясь по дереву, можно однозначно определить лист, к которому нужно подвесить новую точку, чтобы не нарушить все свойства дерева. При добавлении большого количества точек, однако, дерево может перестать быть сбалансированным, и нужно проводить ребалансировку. Также существуют варианты k-d-деревьев, которые сохраняют сбалансированность при добавлении / удалении точек.
 
Поговорим теперь про то, как же находить ближайших соседей с помощью такого дерева. Будем производить обход дерева в глубину с двумя модификациями. 
- Во-первых, будем запоминать наиболее близкую точку. Это позволит не заходить в поддеревья, задающие области, которые заведомо дальше, чем текущая наиболее близкая точка, поэтому не имеет смысла искать в них ближайших соседей. 
- Во-вторых, будем прежде всего обходить те поддеревья, которые задают наиболее близкие области, а значит, с большей вероятностью содержат ближайшего соседа.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/2_7_2f24c2823e_0c7218aef4.gif" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://commons.wikimedia.org/">Источник</a>
  </figcaption>
</figure>
 
Сложность метода по размеру обучающей выборки в среднем равна $O(\log N)$ при равномерном распределении точек. При большой размерности пространства, однако, алгоритму приходится посещать больше ветвей дерева, чтобы найти ближайших соседей. Например, если $N \approx D$, то сложность становится примерно такой же, как и в случае полного перебора. В общем случае считается, что для того чтобы асимптотика действительно была логарифмической, нужно, чтобы $N \gtrsim 2^D$. Поэтому уже при количестве признаков порядка сотни алгоритм не даёт существенных преимуществ перед полным перебором.
  
Почитать по теме:  
 
- [Хорошая презентация](https://www.cs.cmu.edu/~ckingsf/bioinfo-lectures/kdtrees.pdf), объясняющая структуру и поиск соседей.
- [Балансировка деревьев](https://en.wikipedia.org/wiki/K-d_tree#Balancing).

## Поиск ближайших соседей: приближённые методы
 
Почти всегда находить именно самых близких соседей необязательно. Например, в задаче подбора рекомендаций фильмов пользователю чаще всего не нужны наиболее похожие картины, достаточно, к примеру, 10 из 15 наиболее близких. Поэтому, чтобы ускорить процесс поиска соседей, используют приближённые методы. Разберём основные идеи, которые применяются в таких методах.

### Random projection trees
 
Алгоритмы, основанные на деревьях, очень часто применяются в задачах поиска соседей. Идея всех таких методов заключается в итеративном разделении пространства случайными гиперплоскостями и построении на базе этого разделения дерева, в листах которого содержится малое число объектов.
  
Одним из наиболее ярких представителей этого семейства является **Annoy** — алгоритм, который используется Spotify для рекомендаций музыки. Задача подобных рекомендательных систем довольно простая — нужно посоветовать пользователю композиции, которые он ещё не слушал, но которые при этом с высокой вероятностью ему понравятся. Простая и рабочая идея — предлагать композиции, похожие на те, которые он уже слушает. Здесь на помощь как раз и приходят методы поиска ближайших соседей.
 
Annoy в какой-то степени похож на k-d-деревья. Сначала выбираются два случайных объекта обучающей выборки и проводится гиперплоскость, симметрично их разделяющая. Затем для каждого полученного полупространства итеративно запускается такая же процедура, которая продолжается до тех пор, пока в каждой области будет не более $M$ объектов ($M$ — гиперпараметр).

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/2_8_b3d0c30be0_effe148382.gif" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://erikbern.com/2015/10/01/nearest-neighbors-and-vector-models-part-2-how-to-search-in-high-dimensional-spaces.html">Источник</a>
  </figcaption>
</figure>
 
Таким образом задаётся бинарное дерево с глубиной порядка $O(\log N)$ в среднем.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/2_9_4597f51b7a_904febbc64.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://erikbern.com/2015/10/01/nearest-neighbors-and-vector-models-part-2-how-to-search-in-high-dimensional-spaces.html">Источник</a>
  </figcaption>
</figure>
 
Спускаясь по этому дереву, можно найти область, в которой лежит целевой объект и некоторое количество близких к нему элементов обучающей выборки. Проблема в том, что это не обязательно будут самые близкие объекты, поэтому для увеличения точности составляется лес из таких деревьев и берётся объединение соответствующих целевому объекту областей.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/2_10_f78d4d0f93_507b116847.gif" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://erikbern.com/2015/10/01/nearest-neighbors-and-vector-models-part-2-how-to-search-in-high-dimensional-spaces.html">Источник</a>
  </figcaption>
</figure>
 
Чем больше таких деревьев берётся, тем более точным будет результат, но придётся тратить большее время на его поиск.
 
Преимущество алгоритма — простота нахождения компромисса между скоростью работы и точностью с помощью тюнинга гиперпараметров. К минусам можно отнести то, что алгоритм плохо параллелится и переносится на GPU, не работает эффективно с батчами, а также то, что для добавления новой точки в обучающую выборку придётся перезапускать процедуру с самого начала.

Почитать по теме:
 
- [Отличная статья](https://erikbern.com/2015/10/01/nearest-neighbors-and-vector-models-part-2-how-to-search-in-high-dimensional-spaces.html) с иллюстрациями и подробным описанием алгоритма.

### Locality-sensitive hashing (LSH)
 
Предположим, что мы можем построить такую хеш-функцию, которая переводит близкие объекты в один бакет. Тогда близких соседей целевого объекта можно найти, посчитав его хеш и посмотрев на коллизии. Оказывается, такие хеш-функции существуют, и на этой идее основано несколько алгоритмов, которые объединяются названием **Locality-sensitive hashing** (**LSH**). К этому классу алгоритмов относится, например, *FAISS*, используемый Facebook.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/2_11_f49c854d34_1d90f2d70c.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://brc7.github.io/2019/09/19/Visual-LSH.html">Источник</a>
  </figcaption>
</figure>
 
Определим формально семейство хеш-функций, которое мы хотим использовать. Нам нужно, чтобы вероятность коллизии на близких объектах была высокая, а на далёких — низкая. Назовём семейство хеш-функций $\mathcal{H}$ $(R, cR, p_1, p_2)$-чувствительным, если для любой $h(x)\in\mathcal{H}$:
 
- для $\rho(x, y) < R$ вероятность коллизии $\Pr\left[h(x)=h(y)\right] > p_1$;
- для $\rho(x, y) > cR$ вероятность коллизии $\Pr\left[h(x)=h(y)\right] < p_2$.
 
Формулы могут выглядеть сложными, но это всего лишь формализация нашей интуиции. Картинка ниже поясняет определение: для близких красных объектов в шаре радиусом $R$ вероятность коллизии больше $p_1$, для далёких синих объектов на расстоянии больше $cR$ вероятность коллизии меньше $p_2$, а про серые объекты в слое между $R$ и $cR$ мы ничего не знаем.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/2_12_534a4903df_dc87df5f0f.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://brc7.github.io/2019/09/19/Visual-LSH.html">Источник</a>
  </figcaption>
</figure>
 
Для каждой функции расстояния, используемой в задаче, существует своё подходящее семейство хеш-функций. Например, для евклидовой и манхэттенской метрик используются *случайные проекции*, где хеш-функция имеет следующий вид:
 
$$ h_{\boldsymbol{w}, b}(\boldsymbol{x}) = \left\lfloor \frac{\boldsymbol{w}^T\boldsymbol{x} + b}{r}\right\rfloor, $$

где $\boldsymbol{w}$ и $b$ — случайные параметры, а $r$ выбирается пользователем. $b$ выбирается равномерно из отрезка $[0, r]$, а $\boldsymbol{w}$ генерируется либо из нормального распределения, что соответствует евклидовой метрике, либо из распределения Коши — для манхэттенской метрики.
 
По сути, такая функция разбивает всё пространство на слои в направлении вектора $\boldsymbol{w}$. Параметр $r$ при этом задаёт ширину слоя.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/2_13_d8675103b9_df1b8e3563.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://brc7.github.io/2019/09/19/Visual-LSH.html">Источник</a>
  </figcaption>
</figure>
 
На практике при использовании лишь одной хеш-функции разница между $p_1$ и $p_2$ оказывается очень маленькой, поэтому применяют различные методы для её увеличения. 
- Первый способ — уменьшать размер бакетов в хеш-таблице путём использования композиции разных хеш-функций из одного семейства $g(x) = (h_1(x),\ldots,h_m(x))$. Преимущество этого способа как раз хорошо видно на примере случайных проекций. При использовании лишь одной хеш-функции бакетами являются слои бесконечного объёма. Однако при использовании композиции размером, как минимум равным количеству признаков $D$, из-за случайности выбора вектора $\boldsymbol{w}$ бакеты почти наверное станут замкнутыми фигурами с конечным объёмом.
- Второй способ повышения эффективности алгоритма — использовать несколько хеш-таблиц и искать соседей среди коллизий в каждой из них. На практике используют оба метода сразу, подбирая $m$ и $L$ — количество хеш-таблиц как гиперпараметры.
 
К плюсам алгоритма можно отнести хорошие теоретические гарантии на сублинейное время и, как и в Annoy, простой поиск компромисса между точностью и скоростью работы. Минусами можно назвать высокую потребность в памяти, плохую адаптируемость под GPU, а также тот факт, что, несмотря на теоретические гарантии в среднем, на практике алгоритм может работать даже чуть дольше полного перебора из-за того, что, помимо самого поиска, требуется искать хеши объектов.

Почитать по теме:
 
- [Отличная статья](https://randorithms.com/2019/09/19/Visual-LSH.html) с объяснением в иллюстрациях и примерами хеш-функций для других метрик.
- [Ещё одна статья](https://towardsdatascience.com/understanding-locality-sensitive-hashing-49f6d1f6134), в которой шаг за шагом выводится алгоритм на примере расстояния Жаккара.
 
 
### Proximity graphs & Hierarchical navigable small world (HNSW)
 
Следующий класс алгоритмов основан на построении специального **графа близости** (**proximity graph**) на объектах выборки и дальнейшем жадном поиске по этому графу. Алгоритмы этого семейства сейчас считаются state-of-the-art (SotA) для многих задач.
 
Рассмотрим подробнее этот класс алгоритмов на примере одного из наиболее популярных из них под названием **Navigable small world (NSW)**. Идея его в следующем: на данных строится граф (он также называется NSW), который удовлетворяет двум следующим свойствам:
 
1. Между любыми двумя точками существует короткий путь, или, более формально, матожидание числа кратчайшего пути между двумя случайно выбранными вершинами растёт как $O(\log N)$.
2. Средняя степень вершины мала.
 
На первый взгляд может показаться, что тяжело выполнить одновременно оба свойства, но на самом деле большая часть графов в реальном мире являются NSW-графами. Самый простой пример — это известное правило шести рукопожатий: любые два случайных человека соединены короткой последовательностью личных контактов длиной не более шести, несмотря на то, что количество знакомых у среднего человека ($100$–$1000$) мало по сравнению с населением Земли.
 
В таких графах существует очень простой метод поиска соседей. Нужно выбрать случайную точку, среди её соседей выбрать того, который ближе всего к целевому объекту, и повторить процедуру уже для него. Показано, что такой жадный поиск имеет полилогарифмическую асимптотику.
 
<figure>
 <img src="https://yastatic.net/s3/education-portal/media/2_14_cd25cae35d_694f7dd8db.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://opendistro.github.io/for-elasticsearch/blog/odfe-updates/2020/04/Building-k-Nearest-Neighbor-%28k-NN%29-Similarity-Search-Engine-with-Elasticsearch/">Источник</a>
  </figcaption>
</figure>

 
Проблема такого подхода в том, что можно попасть в плотный кластер и очень долго оттуда выбираться. Для решения этой проблемы используется иерархия NSW, или *Hierarchical navigable small world (HNSW)*. Исходный граф является нулевым слоем. Каждый следующий слой строится в два шага:
 
1. Каждая вершина текущего слоя попадает в следующий с некоторой вероятностью $p$.
2. На всех вершинах, попавших в новый слой, строится NSW.
 
По построению количество слоёв будет $O(\log N)$.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/hnsw_layers_5571069c2e_a7a4d44708.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://arxiv.org/abs/1603.09320">Источник</a>
  </figcaption>
</figure>
 
Поиск начинается в самом верхнем слое. После нахождения ближайшей к целевому объекту вершины спускаемся на слой ниже и начинаем поиск из этой вершины. Повторяем процедуру, пока не спустимся до нулевого слоя. Таким образом, на каждом слое мы всё больше уточняем наш ответ. Стоит отметить, что для ускорения работы иногда поиск останавливают не при нахождении ближайшей вершины, а раньше, используя критерии остановки.
 
Интуитивно легко понять, почему такая иерархическая структура решает проблему плотных кластеров: в верхних слоях вершин мало, а расстояния между ними в среднем большие, а значит, таких кластеров там почти нет. Поэтому, попадая в нижний слой, мы чаще всего оказываемся уже в нужном кластере и просто уточняем результат работы алгоритма.
 
HNSW, так же как и рассмотренные ранее приближённые методы, позволяет искать трейд-офф между точностью и скоростью работы. Плюс ко всему на реальных данных он часто работает лучше других методов и сейчас считается SotA. Однако этот способ поиска не лишён и недостатков. Главный заключается в том, что нельзя добавлять точки в обучающую выборку без перестройки структуры. Помимо этого, он довольно требователен по памяти из-за того, что для каждого слоя приходится хранить как вершины, которые в него входят, так и связи между этими вершинами.
 
Подробнее — в [оригинальной статье](https://arxiv.org/abs/1603.09320).  
 
В завершение стоит сказать, что не существует универсального метода поиска соседей — каждый из описанных методов может быть лучше других в определённой задаче. К тому же, несмотря на то что приближённые методы имеют лучшую асимптотику, многие из них плохо переносятся на GPU. Из-за этого на практике полный перебор бывает быстрее любого из таких приближённых методов.

  ## handbook

  Учебник по машинному обучению

  ## title

  Метрические методы

  ## description

  Алгоритмы KNN. Быстрый поиск ближайших соседей

- 
  ## path

  /handbook/ml/article/reshayushchiye-derevya

  ## content

  В этом параграфе мы рассмотрим ещё одно семейство моделей машинного обучения — **решающие деревья (decision trees)**.

Решающее дерево предсказывает значение целевой переменной с помощью применения последовательности простых решающих правил (которые называются предикатами). Этот процесс в некотором смысле согласуется с естественным для человека процессом принятия решений.

Хотя обобщающая способность решающих деревьев невысока, их предсказания вычисляются довольно просто, из-за чего решающие деревья часто используют как кирпичики для построения [ансамблей](https://academy.yandex.ru/handbook/ml/article/ansambli-v-mashinnom-obuchenii) — моделей, делающих предсказания на основе агрегации предсказаний других моделей. О них мы поговорим в следующем параграфе.

### Пример решающего дерева

Начнём с небольшого примера. На картинке ниже изображено дерево, построенное для задачи классификации на пять классов:

![3](https://yastatic.net/s3/education-portal/media/3_1_c0e800569a_0dd86f2ea8.webp)

Объекты в этом примере имеют два признака с вещественными значениями: $X$ и $Y$. Решение о том, к какому классу будет отнесён текущий объект выборки, будет приниматься с помощью прохода от корня дерева к некоторому листу.

В каждом узле этого дерева находится предикат. Если предикат верен для текущего примера из выборки, мы переходим в правого потомка, если нет — в левого. В данном примере все предикаты — это просто взятие порога по значению какого-то признака:

$$    B(x, j, t) = [ x_j \le t ] 
$$

В листьях записаны предсказания (например, метки классов). Как только мы дошли до листа, мы присваиваем объекту ответ, записанный в вершине.

На картинке ниже визуализирован процесс построения решающих поверхностей, порождаемых деревом (правая часть картинки):

![3](https://yastatic.net/s3/education-portal/media/3_2_41c1793bef_fda82166a3.webp)

Каждый предикат порождает разделение текущего подмножества пространства признаков на две части. На первом этапе, когда происходило деление по $[ X \le X_1 ]$, вся плоскость была поделена на две соответствующие части. На следующем уровне часть плоскости, для которой выполняется $X \le X_1$, была поделена на две части по значению второго признака $Y \le Y_1$ — так образовались области 1 и 2. То же самое повторяется для правой части дерева — и так далее до листовых вершин: получится пять областей на плоскости. Теперь любому объекту выборки будет присваиваться один из пяти классов в зависимости от того, в какую из образовавшихся областей он попадает.

Этот пример хорошо демонстрирует, в частности, то, что дерево осуществляет кусочно-постоянную аппроксимацию целевой зависимости. Ниже приведён пример визуализации решающей поверхности, которая соответствует дереву глубины 4, построенному для объектов данных из [Ames Housing Dataset](https://www.openml.org/d/41211), где из всех признаков, описывающих объекты недвижимости, были выбраны ширина фасада (`Lot_Frontage`) и площадь (`Lot_Area`), а предсказать нужно стоимость.

Для более понятной визуализации перед построением дерева из датасета были выкинуты объекты с `Lot_Frontage > 150` и с `Lot_Area > 20000`. Вот что получилось — в каждой из прямоугольных областей предсказывается одна и та же стоимость:

![3](https://yastatic.net/s3/education-portal/media/3_3_64f47eb4c8_29f1b64e0b.webp)

В каждой из прямоугольных областей предсказывается одна и та же стоимость.

## Определение решающего дерева

Разобравшись с приведёнными выше примерами, мы можем дать определение решающего дерева. Пусть задано бинарное дерево, в котором:

* каждой внутренней вершине $v$ приписан предикат $B_v: \mathbb{X} \to \{ 0, 1 \}$;
* каждой листовой вершине $v$ приписан прогноз $c_v \in \mathbb{Y}$, где $\mathbb{Y}$ — область значений целевой переменной (в случае классификации листу может быть также приписан вектор вероятностей классов).

В ходе предсказания осуществляется проход по этому дереву к некоторому листу. Для каждого объекта выборки $x$ движение начинается из корня. В очередной внутренней вершине $v$ проход продолжится вправо, если $B_v(x) = 1$, и влево, если $B_v(x) = 0$. Проход продолжается до момента, пока не будет достигнут некоторый лист, и ответом алгоритма на объекте $x$ считается прогноз $c_v$, приписанный этому листу.

Вообще, предикат $B_v$ может иметь, произвольную структуру, но на практике чаще используют просто сравнение с порогом $t \in \mathbb{R}$ по какому-то $j$-му признаку:

$$    B_v(x, j, t) = [ x_j \le t ] 
$$

При проходе через узел дерева с данным предикатом объекты будут отправлены в правое поддерево, если значение $j$-го признака у них меньше либо равно $t$, и в левое — если больше. В дальнейшем рассказе мы будем по умолчанию использовать именно такие предикаты.

Из структуры дерева решений следует несколько интересных свойств:

* выученная функция — кусочно-постоянная, из-за чего производная равна нулю везде, где задана. Следовательно, о градиентных методах при поиске оптимального решения можно забыть;
* дерево решений (в отличие от, например, линейной модели) не сможет экстраполировать зависимости за границы области значений обучающей выборки;
* дерево решений способно идеально приблизить обучающую выборку и ничего не выучить (то есть такой классификатор будет обладать низкой обобщающей способностью): для этого достаточно построить такое дерево, в каждый лист которого будет попадать только один объект. Следовательно, при обучении нам надо не просто приближать обучающую выборку как можно лучше, но и стремиться оставлять дерево как можно более простым, чтобы результат обладал хорошей обобщающей способностью.

{% cut "Под катом — несколько иллюстраций для закрепления" %}

Сгенерируем для начала небольшой синтетический датасет для задачи классификации и обучим на нём решающее дерево, не ограничивая его потенциальную высоту.

![3](https://yastatic.net/s3/education-portal/media/3_5_74f1de3be9_47d76b6fce.webp)

Выученная закономерность очень сложная: иногда она выделяет прямоугольником одну точку. И действительно, как мы увидим дальше, деревья умеют идеально подстраиваться под обучающую выборку. Это чемпионы переобучения. Помешать этому может лишь ограничение на высоту дерева. Вот как будет выглядеть дерево высоты 3, построенное на том же датасете:

![3](https://yastatic.net/s3/education-portal/media/3_4_aa20f33d21_69007b9474.webp)

Не всё идеально, но классификатор уже не такой безумный. Теперь обратимся к задаче регрессии. Обучим решающее дерево, не ограничивая его высоту.

![3](https://yastatic.net/s3/education-portal/media/3_6_d1cba63f26_bcb1e57992.webp)

Фиолетовая ступенчатая пунктирная линия — это восстановленная деревом зависимость. На графике она мечется между точками, идеально следуя за обучающей выборкой. Кроме того (и это не лечится ограничением глубины дерева) за пределами обучающей выборки дерево делает константные предсказания. Это и имеют в виду, когда говорят, что древесные модели неспособны к экстраполяции.

{% endcut %}

## Почему построение оптимального решающего дерева — сложная задача?

Пусть, как обычно, у нас задан датасет $(X, y)$, где $y=\{y_i\}_{i=1}^N \subset \mathbb{R}^N$ — вектор таргетов, а $X=\{x_i\}_{i = 1}^N \in \mathbb{R}^{N \times D}, x_i \in \mathbb{R}^D$ — матрица признаков, в которой $i$-я строка — это вектор признаков $i$-го объекта выборки. Пусть у нас также задана функция потерь $L(f, X, y)$, которую мы бы хотели минимизировать.

Наша задача — построить решающее дерево, наилучшим образом предсказывающее целевую зависимость. Однако, как уже было замечено выше, оптимизировать структуру дерева с помощью градиентного спуска не представляется возможным. Как ещё можно было бы решить эту задачу? Давайте начнём с простого — научимся строить **решающие пни**, то есть решающие деревья глубины 1.

Как и раньше, мы будем рассматривать только самые простые предикаты:

$$    B_{j, t}(x_i) = [ x_{ij} \le t ] 
$$

Ясно, что задачу можно решить полным перебором: существует не более $(N - 1) D$ предикатов такого вида. Действительно, индекс $j$ (номер признака) пробегает значения от $1$ до $D$, а всего значений порога $t$, при которых меняется значение предиката, может быть не более $N - 1$:

![3](https://yastatic.net/s3/education-portal/media/3_7_f51986c1ae_056137fc2e.webp)

Решение, которое мы ищем, будет иметь вид:

$$    (j_{opt}, t_{opt}) = \arg \min_{j, t} L(B_{j, t}, X, y)
$$

Для каждого из предикатов $B_{j, t}$ нам нужно посчитать значение функции потерь на всей выборке, что, в свою очередь, тоже занимает $O(N)$.
Следовательно, полный алгоритм выглядит так:

```python
min_loss = inf
optimal_border = None

for j in range(D):
    for t in X[:, j]:     # Можно брать сами значения признаков в качестве порогов
        loss = calculate_loss(t, j, X, y)
        if loss < min_loss:
           min_loss, optimal_border = loss, (j, t)
```

Сложность алгоритма — $O(N^2 D)$. Это не заоблачная сложность, хотя, конечно, не идеальная. Но это была схема возможного алгоритма поиска оптимального дерева высоты 1.

Как обобщить алгоритм для дерева произвольной глубины? Мы можем сделать наш алгоритм поиска решающего пня рекурсивным и в теле цикла вызывать исходную функцию для всех возможных разбиений. Как мы упоминали выше, так можно построить дерево, идеально запоминающее всю выборку, однако на тестовых данных такой алгоритм вряд ли покажет высокое качество.

Можно поставить другую задачу: построить оптимальное с точки зрения качества на обучающей выборке дерево минимальной глубины (чтобы снизить переобучение). Проблема в том, что поиск такого дерева — [NP-полная](https://people.csail.mit.edu/rivest/pubs/HR76.pdf) задача, то есть человечеству пока неизвестны способы решить её за полиномиальное время. Как быть?

Идеального ответа на этот вопрос нет, но до некоторой степени ситуацию можно улучшить двумя не исключающими друг друга способами:

1. Разрешить себе искать не оптимальное решение, а просто достаточно хорошее. Начать можно с того, чтобы строить дерево с помощью *жадного алгоритма*, то есть не искать всю структуру сразу, а строить дерево этаж за этажом. Тогда в каждой внутренней вершине дерева будет решаться задача, схожая с задачей построения решающего пня. Для того чтобы этот подход хоть как-то работал, его придётся прокачать внушительным набором эвристик.
2. Заняться оптимизацией с точки зрения computer science — наивную версию алгоритма (перебор наборов возможных предикатов и порогов) можно ускорить и асимптотически, и в константу раз.

Эти две идеи мы и будем обсуждать в дальнейшем. Сначала попытаемся подробно разобраться с первой — как использовать жадный алгоритм.

## Жадный алгоритм построения решающего дерева

Пусть $X$ — исходное множество объектов обучающей выборки, а $X_m$ — множество объектов, попавших в текущий лист (в самом начале $X_m = X$). Тогда жадный алгоритм можно верхнеуровнево описать следующим образом:

1. Создаём вершину $v$.
2. Если выполнен *критерий остановки* $Stop(X_m)$, то останавливаемся, объявляем эту вершину листом и ставим ей в соответствие ответ $Ans(X_m)$, после чего возвращаем её.
3. Иначе: находим предикат (иногда ещё говорят *сплит*) $B_{j, t}$, который определит наилучшее разбиение текущего множества объектов $X_m$ на две подвыборки $X_{\ell}$ и $X_r$, максимизируя *критерий ветвления* $Branch(X_m, j, t)$.
4. Для $X_\ell$ и $X_r$ рекурсивно повторим процедуру.

Данный алгоритм содержит в себе несколько вспомогательных функций, которые надо выбрать так, чтобы итоговое дерево было способно минимизировать $L$:
$Ans(X_m)$, вычисляющая ответ для листа по попавшим в него объектам из обучающей выборки, может быть, например:

* в случае задачи классификации — меткой самого частого класса или оценкой дискретного распределения вероятностей классов для объектов, попавших в этот лист;
* в случае задачи регрессии — средним, медианой или другой статистикой;
* простой моделью. К примеру, листы в дереве, задающем регрессию, могут быть линейными функциями или синусоидами, обученными на данных, попавших в лист. Впрочем, везде ниже мы будем предполагать, что в каждом листе просто предсказывается константа.

Критерий остановки $Stop(X_m)$ — функция, которая решает, нужно ли продолжать ветвление или пора остановиться. Это может быть какое-то тривиальное правило: например, остановиться только в тот момент, когда объекты в листе получились достаточно однородными и/или их не слишком много. Более детально мы поговорим о критериях остановки в параграфе про регуляризацию деревьев.

Критерий ветвления $Branch(X_m, feature, value)$ — пожалуй, самая интересная компонента алгоритма. Это функция, измеряющая, насколько хорош предлагаемый сплит. Чаще всего эта функция оценивает, насколько улучшится некоторая финальная метрика качества дерева в случае, если получившиеся два листа будут терминальными, по сравнению с ситуацией, когда сама исходная вершина — это лист. Выбирается такой сплит, который даёт наиболее существенное улучшение.

Впрочем, есть и другие подходы. При этом строгой теории, которая бы связывала оптимальность выбора разных вариантов этих функций и разных метрик классификации и регрессии, в общем случае не существует. Однако есть набор интуитивных и хорошо себя зарекомендовавших соображений, с которыми мы вас сейчас познакомим.

### Критерии ветвления: общая идея

Давайте теперь по очереди посмотрим на популярные постановки задач ML и под каждую подберём свой критерий.

Ответы дерева будем кодировать так: $c \in \mathbb{R}$ — для ответов регрессии и меток класса. Для случаев, когда надо ответить дискретным распределением на классах, $c \in \mathbb{R}^K$ будет вектором вероятностей:

$$c = (c_1, \ldots, c_K), \quad \sum_{i = 1}^K c_i = 1 
$$

Предположим также, что задана некоторая функция потерь $L(y_i, c)$. О том, что это может быть за функция, мы поговорим ниже.

В момент, когда мы ищем оптимальный сплит $X_m = X_l\sqcup X_r$, мы можем вычислить для объектов из $X_m$ тот константный таргет $c$, которые предсказало бы дерево, будь текущая вершина терминальной, и связанное с ними значение исходного функционала качества $L$. А именно — константа $c$ должна минимизировать среднее значение функции потерь:

$$    \frac{1}{\vert X_m\vert}\sum\limits_{(x_i, y_i) \in X_m} L(y_i, c)
$$

Оптимальное значение этой величины

$$    H(X_m) = \min\limits_{c \in Y} \frac{1}{\vert X_m\vert}\sum\limits_{(x_i, y_i) \in X_m} L(y_i, c)
$$

обычно называют **информативностью**, или **impurity**. Чем она ниже, тем лучше объекты в листе можно приблизить константным значением.

Похожим образом можно определить информативность решающего пня. Пусть, как и выше, $X_l$ — множество объектов, попавших в левую вершину, а $X_r$ — в правую; пусть также $c_l$ и $c_r$ — константы, которые предсказываются в этих вершинах. Тогда функция потерь для всего пня в целом будет равна

$$    \frac1{|X_m|}\left(\sum_{x_i\in X_l}L(y_i, c_l) + \sum_{x_i\in X_r}L(y_i, c_r)\right)
$$

**Вопрос на подумать**. Как информативность решающего пня связана с информативностью его двух листьев?

{% cut "Ответ (не открывайте сразу; сначала подумайте сами!)" %}

Преобразуем выражение:

$$\frac1{|X_m|}\left(\sum_{x_i\in X_l}L(y_i, c_l) + \sum_{x_i\in X_r}L(y_i, c_r)\right) =
$$

$$= \frac1{|X_m|}\left(|X_l|\cdot\frac{1}{|X_l|}\sum_{x_i\in X_l}L(y_i, c_l) + |X_r|\cdot\frac1{|X_r|}\sum_{x_i\in X_r}L(y_i, c_r)\right)
$$

Эта сумма будет минимальна при оптимальном выборе констант $c_l$ и $c_r$, и информативность пня будет равна

$$\frac{|X_l|}{|X_m|}H(X_l) + \frac{|X_r|}{|X_m|}H(X_r)
$$

{% endcut %}

Теперь для того чтобы принять решение о разделении, мы можем сравнить значение информативности для исходного листа и для получившегося после разделения решающего пня.

Разность информативности исходной вершины и решающего пня равна

$$H(X_m) - \frac{|X_l|}{|X_m|}H(X_l) - \frac{|X_r|}{|X_m|}H(X_r)
$$

Для симметрии её принято умножить на $\vert X_m\vert$; тогда получится следующий критерий ветвления:

$$\color{#348FEA}{Branch (X_m, j, t) = |X_m| \cdot H(X_m) -  |X_l| \cdot H(X_l) -  |X_r| \cdot H(X_r)}
$$

Получившаяся величина неотрицательна: ведь, разделив объекты на две кучки и подобрав ответ для каждой, мы точно не сделаем хуже. Кроме того, она тем больше, чем лучше предлагаемый сплит.

Теперь посмотрим, какими будут критерии ветвления для типичных задач.

### Информативность в задаче регрессии: MSE

Посмотрим на простой пример — регрессию с минимизацией среднеквадратичной ошибки:

$$    L(y_i, c) = (y_i -c)^2
$$

Информативность листа будет выглядеть следующим образом:

$$    H(X_m) = \frac{1}{\vert X_m\vert} \min\limits_{c \in Y} \sum\limits_{(x_i, y_i) \in X_m} (y_i - c)^2
$$

Как мы уже знаем, оптимальным предсказанием константного классификатора для задачи минимизации MSE является среднее значение, то есть

$$    c = \frac{\sum y_i}{|X_m|}
$$

Подставив в формулу информативности сплита, получаем:

$$\color{#348FEA}{H(X_m) = \sum\limits_{(x_i, y_i) \in X_m}\frac{\left(y_i - \overline{y} \right)^2}{|X_m|}, ~ \text{где} ~ \overline{y} = \frac{1}{\vert X_m \vert} \sum_i y_i}
$$

То есть при жадной минимизации MSE информативность — это оценка дисперсии таргетов для объектов, попавших в лист. Получается очень стройная картинка: оценка значения в каждом листе — это среднее, а выбирать сплиты надо так, чтобы сумма дисперсий в листьях была как можно меньше.

### Информативность в задаче регрессии: MAE

$$    L(y_i, c) = |y_i -c|
$$

Случай средней абсолютной ошибки так же прост: в листе надо предсказывать медиану, ведь именно медиана таргетов для обучающих примеров минимизирует MAE констатного предсказателя (мы это обсуждали в параграфе про [линейные модели](https://education.yandex.ru/handbook/ml/article/linear-models)).

В качестве информативности выступает абсолютное отклонение от медианы:

$$H(X_m) = \sum\limits_{(x_i, y_i) \in X_m}\frac{|y_i - MEDIAN(Y)|}{|X_m|}
$$

### Критерий информативности в задаче классификации: misclassification error

Пусть в нашей задаче $K$ классов, а $p_k$ — доля объектов класса $k$ в текущей вершине $X_m$:

$$    p_k = \frac{1}{|X_m|} \sum_{(x_i, y_i) \in X_m} \mathbb{I}[ y_i = k ]
$$

Допустим, мы заботимся о доле верно угаданных классов, то есть функция потерь — это индикатор ошибки:

$$    L(y_i, c) = \mathbb{I}[y_i \ne c]
$$

Пусть также предсказание модели в листе — один какой-то класс. Информативность для такой функции потерь выглядит так:

$$    H(X_m) = \min_{c \in Y} \frac{1}{|X_m|} \sum_{(x_i, y_i) \in X_m} \mathbb{I}[y_i \ne c]
$$

Ясно, что оптимальным предсказанием в листе будет наиболее частотный класс $k_{\ast}$, а выражение для информативности упростится следующим образом:

$$    H(X_m) = \frac{1}{|X_m|} \sum_{(x_i, y_i) \in X_m} \mathbb{I}[y_i \ne k_{\ast}] = 1 - p_{k_{\ast}}
$$

### Информативность в задаче классификации: энтропия

Если же мы собрались предсказывать вероятностное распределение классов $(c_1, \ldots, c_K)$, то к этому вопросу можно подойти так же, как мы поступали при выводе логистической регрессии: через максимизацию логарифма правдоподобия (= минимизацию минус логарифма) распределения Бернулли. А именно, пусть в вершине дерева предсказывается фиксированное распределение $c$ (не зависящее от $x_i$), тогда правдоподобие имеет вид

$$    P(y\vert x, c) = P(y\vert c) = \prod\limits_{(x_i, y_i) \in X_m}P(y_i\vert c) = \prod\limits_{(x_i, y_i) \in X_m}
    \prod\limits_{k = 1}^K c_k^{\mathbb{I}[ y_i = k ]},
$$

откуда

$$    H(X_m) = \min\limits_{\sum\limits_{k} c_k = 1} 
    \left( 
        -\frac{1}{|X_m|} \sum\limits_{(x_i, y_i) \in X_m} \sum\limits_{k = 1}^K \mathbb{I}[ y_i = k ] \log c_k
    \right)
$$

То, что оценка вероятностей в листе $c_k$, минимизирующая $H(X_m)$, должна быть равна $p_k$, то есть доле попавших в лист объектов этого класса, до некоторой степени очевидно, но это можно вывести и строго.

{% cut "Доказательство для любопытных" %}

Из-за наличия условия на $\sum_k c_k = 1$ нам придётся минимизировать лагранжиан

$$L(c, \lambda) = \min_{c, \lambda} \left( 
\left( 
    -\frac{1}{|X_m|} \sum\limits_{(x_i, y_i) \in X_m} \sum\limits_{k = 1}^K \mathbb{I}[ y_i = k ] \log c_k
\right) + \lambda \sum\limits_{k = 1}^K c_k \right) 
$$

Как обычно, возьмём частную производную и решим уравнение:

$$    0 = \frac{\partial }{\partial c_j}L(c, \lambda) = 
$$

$$= \frac{\partial }{\partial c_j} \left( 
\left( 
    -\frac{1}{|X_m|} \sum\limits_{(x_i, y_i) \in X_m} \sum\limits_{k = 1}^K \mathbb{I}[ y_i = k ] \log c_k
\right) + \lambda \sum\limits_{k = 1}^K c_k \right) =
$$

$$= \left( 
\left( 
    -\frac{1}{|X_m|} \sum\limits_{(x_i, y_i) \in X_m} \mathbb{I}[ y_i = j ] \frac{1}{c_j}
\right) + \lambda \right) = - \frac{p_j}{c_j} + \lambda,
$$

откуда выражаем $c_j = \frac{p_j}{\lambda}$. Суммируя эти равенства, получим:

$$1 = \sum_{k = 1}^K c_k = \frac{1}{\lambda} \sum_{k = 1}^K p_k = \frac{1}{\lambda},
$$

откуда $\lambda = 1$, а значит, $c_k = p_k$.

{% endcut %}

Подставляя вектор $c = (p_1,\ldots,p_K)$ в выражение выше, мы в качестве информативности получим энтропию распределения классов:

$$\color{#348FEA}{H(X_m) = -\sum_{k = 1}^K p_k \log p_k}
$$

{% cut "Немного подробнее об энтропии" %}

Величина

$$H(X_m) = -\sum_k p_k \log p_k
$$

называется **информационной энтропией Шеннона** и измеряет непредсказуемость реализации случайной величины. В оригинальном определении, правда, речь шла не о значениях случайной величины, а о символах (первичного) алфавита, так как Шеннон придумал эту величину, занимаясь вопросами кодирования строк. Для данной задачи энтропия имеет вполне практический смысл — среднее количество битов, которое необходимо для кодирования одного символа сообщения при заданной частоте символов алфавита.

Так как $p_k\in[0,1]$, энтропия неотрицательна. Если случайная величина принимает только одно значение, то она абсолютно предсказуема и её энтропия равна 
$$- 1\log(1) = 0$$

Наибольшего значения энтропия достигает для равномерно распределённой случайной величины — и это отражает тот факт, что среди всех величин с данной областью значений она наиболее «непредсказуема». Для равномерно распределённой на множестве $\{1,\ldots,K\}$ случайной величины значение энтропии будет равно:

$-\sum_{k = 1}^K \frac{1}{K}\log\frac{1}{K} = \log K$

На следующем графике приведены три дискретных распределения на множестве $\{0, 1, \ldots, 20\}$ с их энтропиями. Как и указано выше, максимальную энтропию будет иметь равномерное распределение; у двух других проявляются пики разной степени остроты — и тем самым реализации этих величин обладают меньшей неопределённостью: мы можем с большей уверенностью говорить, что будет сгенерировано.

![3](https://yastatic.net/s3/education-portal/media/3_8_a886025a7e_b68fdb9638.webp)

Разберём на примере игрушечной задачи классификации то, как энтропия может выступать в роли impurity. Рассмотрим три разбиения синтетического датасета и посмотрим, какие значения энтропии они дают. В подписях указано, каким становится соотношение классов в половинках.

![3](https://yastatic.net/s3/education-portal/media/3_9_7bef1a9609_6d707963b8.webp)

В изначальном датасете по 25 точек каждого класса; энтропия состояния равна

$$S_0 = -\frac{25}{50}\log_2{\frac{25}{50}}-\frac{25}{50}\log_2{\frac{25}{50}} = 1
$$

Для первого разбиения, по $[X_1 \leqslant 3]$ в левую часть попадают $25$ точек класса 0 и $12$ точек класса 1, а в правую — $0$ точек класса 0 и $13$ точек класса 1. Энтропия левой группы равна

$$S_l = -\frac{25}{37}\log_2{\frac{25}{37}}-\frac{12}{37}\log_2{\frac{12}{37}}\approx 0.9
$$

Энтропия правой группы, строго говоря, не определена (под логарифмом ноль), но если заменить несуществующее значение на $\lim_{t\rightarrow 0+}t\log_2{t} = 0$, то получится

$$S_r = - 0 - 1\log_2{1}=0
$$

Что, в принципе, логично: с вероятностью 1 выпадает единица, мы всегда уверены в результате, и энтропия у такого, вырожденного распределения тоже минимальная, равная нулю.

Как можно заметить, энтропия в обеих группах уменьшилась по сравнению с изначальной. Получается, что, разделив шарики по значению координаты, равному 3, мы уменьшили общую неопределённость системы. Но какое из разбиений даёт самое радикальное уменьшение? После несложных вычислений, мы получаем для нарисованных выше разбиений:

$$Branch^{(1)} \approx 16.4
$$

$$Branch^{(2)} \approx 30.5
$$

$$Branch^{(3)} \approx 7
$$

Ожидаемо, не так ли? Второе разбиение практически идеально разделяет классы, делая из исходного, почти равномерного распределения, два почти вырожденных. При остальных разбиениях в каждой из половинок неопределённость тоже падает, но не так сильно.

Кстати, Шеннон изначально собирался назвать информационную энтропию или «информацией», или «неопределённостью», но в итоге выбрал название «энтропия», потому что концепция со схожим смыслом в статистической механике уже была названа энтропией. Употребление термина из другой научной области выглядело убедительным преимуществом при ведении научных споров.

{% endcut %}

### Информативность в задаче классификации: критерий Джини

Пусть предсказание модели — это распределение вероятностей классов $(c_1, \ldots, c_k)$. Вместо логарифма правдоподобия в качестве критерия можно выбрать, например, [метрику Бриера](https://en.wikipedia.org/wiki/Brier_score#:~:text=The%20Brier%20Score%20is%20a,as%20applied%20to%20predicted%20probabilities) (за которой стоит всего лишь идея посчитать MSE от вероятностей). Тогда информативность получится равной

$$H(X_m) = \min_{\sum_k c_k = 1} \frac{1}{|X_m|} \sum_{(x_i, y_i) \in X_m} \sum_{k = 1}^K (c_k - \mathbb{I}[ y_i = k ] )^2
$$

Можно показать, что оптимальное значение этой метрики, как и в случае энтропии, достигается на векторе $c$, состоящем из выборочных оценок частот классов $(p_1, \ldots, p_k)$, $p_i = \frac{1}{\vert X_m\vert}\sum_i \mathbb{I}[ y_i = k ]$. Если подставить $(p_1, \ldots, p_k)$ в выражение выше и упростить его, получится **критерий Джини**:

$$\color{#348FEA}{H(X_m) = \sum_{k = 1}^K p_k (1 - p_k)}
$$

Критерий Джини допускает и следующую интерпретацию: $H(X_m)$ равно математическому ожиданию числа неправильно классифицированных объектов в случае, если мы будем приписывать им случайные метки из дискретного распределения, заданного вероятностями $(p_1, \ldots, p_k)$.

### Неоптимальность полученных критериев

Казалось бы, мы вывели критерии информативности для всех популярных задач, и они довольно логично следуют из их постановок, но получилось ли у нас обмануть NP-полноту и научиться строить оптимальные деревья легко и быстро?

Конечно, нет. Простейший пример — решение задачи XOR с помощью жадного алгоритма и любого критерия, который мы построили выше:

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/3_10_08f8ee6402_84665bc716.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="http://www.machinelearning.ru/wiki/images/archive/9/97/20140227072517!Voron-ML-Logic-slides.pdf">Источник</a>
  </figcaption>
</figure>

Вне зависимости от того, что вы оптимизируете, жадный алгоритм не даст оптимального решения задачи XOR. Но этим примером проблемы не исчерпываются. Скажем, бывают ситуации, когда оптимальное с точки зрения выбранной метрики дерево вы получите с критерием ветвления, построенным по другой метрике (например, MSE-критерий для MAE-задачи или Джини для misclassification error).

## Особенности данных

### Категориальные признаки

На первый взгляд, деревья прекрасно могут работать с категориальными переменными. А именно, если признак $x^i$ принимает значения из множества $C = \\{c_1,\ldots,c_M\\}$, то при очередном разбиении мы можем рассматривать по этому признаку произвольные сплиты вида $C = C_l\sqcup C_r$ (предикат будет иметь вид $[x^i\in C_r]$). Это очень логично и естественно, но проблема в том, что при больших $M$ у нас будет $2^{M-1}-1$ сплитов, и перебирать их будет слишком долго. Было бы здорово уметь каким-то образом *упорядочивать* значения $c_m$, чтобы работать с ними так же, как с обычными числами: разделяя на значения, «не превосходящие» и «большие» определённого порога.

Оказывается, что для некоторых задач такое упорядочение можно построить вполне естественным образом.

Так, для задачи бинарной классификации значения $c_m$ можно упорядочить по неубыванию доли объектов класса 1 с $x^i = c_m$, после чего работать с ними, как со значениями вещественного признака. Показано, что в случае, если мы выбираем таким образом сплит, оптимальный с точки зрения энтропийного критерия или критерия Джини, то он будет оптимальным среди всех $2^{M-1}-1$ сплитов.

Для задачи регрессии с функцией потерь MSE значения $c_m$ можно упорядочивать по среднему значению таргета на подмножестве $\\{X\mid x^i = c_m\\}$. Полученный таким образом сплит тоже будет оптимальным.

### Работа с пропусками

Одна из приятных особенностей деревьев — это способность обрабатывать пропуски в данных. Разберёмся, что при этом происходит на этапе обучения и на этапе применения дерева.

Пусть у нас есть некоторый признак $x^i$, значение которого пропущено у некоторых объектов. Как обычно, обозначим через $X_m$ множество объектов, пришедших в рассматриваемую вершину, а через $V_m$ — подмножество $X_m$, состоящее из объектов с пропущенным значением $x^i$. В момент выбора сплитов по этому признаку мы будем просто игнорировать объекты из $V_m$, а когда сплит выбран, мы отправим их в оба поддерева. При этом логично присвоить им веса: $\frac{\vert X_l\vert}{\vert X_m\vert}$ для левого поддерева и $\frac{\vert X_r\vert}{\vert X_m\vert}$ для правого. Веса будут учитываться как коэффициенты при $L(y_i, c)$ в формуле информативности.

**Вопрос на подумать**. Во всех критериях ветвления участвуют мощности множеств $X_m$, $X_l$ и $X_r$. Нужно ли уменьшение размера выборки учитывать в формулах для информативности? Если нужно, то как?

{% cut "Ответ (не открывайте сразу; сначала подумайте сами!)" %}

Чтобы ответить на этот вопрос, вспомним, как работают критерии ветвления. Их суть в сравнении информативности $H(X_m)$ для исходной вершины с информативностью для решающего пня, порождённого рассматриваемым сплитом. Величина $H(X_m)$ никак не меняется, надо понять, что будет с пнём. Для него можно посчитать информативность так, словно объектов с пропущенными значениями нет:

$$\frac1{|X_m\setminus V_m|}\left(\sum_{x_i\in X_l\setminus V_m}L(y_i, c_l) + \sum_{x_i\in X_r\setminus V_m}L(y_i, c_r)\right)
$$

Тогда в критерии ветвления нужно поменять коэффициенты, иначе мы не будем адекватным образом сравнивать такой сплит со сплитами по другим признакам:

$$Branch = |X_m\setminus V_m|\cdot H(X_m) - |X_l\setminus V_m|\cdot H(X_l\setminus V_m) - |X_r\setminus V_m|\cdot H(X_r\setminus V_m)
$$

Если же мы предполагаем, что объекты из $V_m$ отправляются в новые листья с весами, как описано выше, то формула оказывается другой, и коэффициенты можно не менять:

$$\frac1{|X_m|}\left(\underbrace{\sum_{x_i\in X_l\setminus V_m}L(y_i, c_l) + \frac{|X_l|}{|X_m|}\sum_{x_i\in V_m}L(y_i, c_l)}_{\text{Левый лист}} + \underbrace{\sum_{x_i\in X_r\setminus V_m}L(y_i, c_r) + \frac{|X_r|}{|X_m|}\sum_{x_i\in V_m}L(y_i, c_r)}_{\text{Правый лист}}\right)
$$

{% endcut %}

Теперь рассмотрим этап применения дерева. Допустим, в вершину, где сплит идёт по $i$-му признаку, пришёл объект $x_0$ с пропущенным значением этого признака. Предлагается отправить его в каждую из дальнейших веток и получить по ним предсказания $\widehat{y}_l$ и $\widehat{y}_r$. Эти предсказания мы усредним с весами $\frac{\vert X_l\vert}{\vert X_m\vert}$ и $\frac{\vert X_r\vert}{\vert X_m\vert}$ (которые мы запомнили в ходе обучения):

$$\widehat{y} = \frac{\vert X_l\vert}{\vert X_m\vert}\widehat{y}_l + \frac{\vert X_r\vert}{\vert X_m\vert}\widehat{y}_r
$$

Для задачи регрессии это сразу даст нам таргет, а в задаче бинарной классификации — оценку вероятности класса 1.

**Замечание**. Если речь идёт о категориальном признаке, может оказаться хорошей идеей ввести дополнительное значение «пропущено» для категориального признака и дальше работать с пропусками, как с обычным значением. Особенно это актуально в ситуациях, когда пропуски имеют системный характер и их наличие несёт в себе определённую информацию.

## Методы регуляризации решающих деревьев

Мы уже упоминали выше, что деревья легко переобучаются и процесс ветвления надо в какой-то момент останавливать.

Для этого есть разные критерии, обычно используются все сразу:

* ограничение по максимальной глубине дерева;
* ограничение на минимальное количество объектов в листе;
* ограничение на максимальное количество листьев в дереве;
* требование, чтобы функционал качества $Branch$ при делении текущей подвыборки на две улучшался не менее чем на $s$ процентов.

Делать это можно на разных этапах работы алгоритма, что не меняет сути, но имеет разные устоявшиеся названия:

* можно проверять критерии прямо во время построения дерева, такой способ называется **pre-pruning** или **early stopping**;
* а можно построить дерево жадно без ограничений, а затем провести **стрижку** (**pruning**), то есть удалить некоторые вершины из дерева так, чтобы итоговое качество упало не сильно, но дерево начало подходить под условия регуляризации. При этом качество стоит измерять на отдельной, отложенной выборке.

## Алгоритмические трюки

Теперь временно снимем шапочку ML-аналитика, наденем шапочку разработчика и специалиста по computer science и посмотрим, как можно сделать полученный алгоритм более вычислительно эффективным.

В базовом алгоритме мы в каждой вершине дерева для всех возможных значений сплитов вычисляем информативность. Если в вершину пришло $q$ объектов, то мы рассматриваем $qD$ сплитов и для каждого тратим $O(q)$ операций на подсчёт информативности. Отметим, что в разных вершинах, находящихся в нашем дереве на одном уровне, оказываются разные объекты, то есть сумма этих $q$ по всем вершинам заданного уровня не превосходит $N$, а значит, выбор сплитов во всех вершинах уровня потребует $O(N^2 D)$ операций.

Таким образом, общая сложность построения дерева — $O(hN^2 D)$ (где $h$ — высота дерева), и доминирует в ней перебор всех возможных предикатов на каждом уровне построения дерева. Посмотрим, что с этим можно сделать.

### Динамическое программирование

Постараемся оптимизировать процесс выбора сплита в одной конкретной вершине.

Вместо того чтобы рассматривать все $O(ND)$ возможных сплитов, для каждого тратя $O(N)$ на вычисление информативности, можно использовать одномерную динамику. Для этого заметим, что если отсортировать объекты по какому-то признаку, то, проходя по отсортированному массиву, можно одновременно и перебирать все значения предикатов, и поддерживать все необходимые статистики для пересчёта значений информативности за $O(1)$ для каждого следующего варианта сплита (против изначальных $O(N)$).

Давайте разберём, как это работает, на примере построения дерева для MSE. Чтобы оценить информативность для листа, нам нужно знать несколько вещей:

* дисперсию и среднее значение таргета в текущем листе;
* дисперсию и среднее значение таргета в обоих потомках для каждого потенциального значения сплита.

Дисперсию и среднее текущего листа легко посчитать за $O(n)$.

С дисперсией и средним для всех значений сплитов чуть сложнее, но помогут следующие оценки математического ожидания и дисперсии:

$$\overline{Y} = \frac1N\sum y_i,
$$

$$\sigma^2 (Y) = \overline{Y^2} - (\overline{Y})^2 = \frac1N\sum y_i^2 - \frac{1}{N^2}(\sum y_i)^2 
$$

Следовательно, нам достаточно для каждого потенциального значения сплита знать количество элементов в правом и левом поддеревьях, их сумму и сумму их квадратов. Впрочем, всё это необходимо знать только для одной из половинок сплита, а для второй это можно получить, вычитая значения для первой из полных сумм. Это можно сделать за один проход по массиву, просто накапливая значения частичных сумм.

Если в вершину дерева пришло $q$ объектов, сложность построения одного сплита складывается из $D$ сортировок каждая по $O(q\log q)$ и одного линейного прохода с динамикой, всего $O(qD\log q + qD) = O(qD\log q)$, что лучше исходного $O(q^2D)$. Итоговая сложность алгоритма построения дерева — $O(hND\log N)$ (где $h$ – высота дерева) против $hN^2D$ в наивной его версии.

Какие именно статистики накапливать (средние, медианы, частоты), зависит от критерия, который вы используете.

### Гистограммный метод

Если бы мощность множества значений признаков была ограничена какой-то разумной константой $b \ll N$, то сортировку в предыдущем способе можно было бы заменить [сортировкой подсчётом](https://ru.wikipedia.org/wiki/%D0%A1%D0%BE%D1%80%D1%82%D0%B8%D1%80%D0%BE%D0%B2%D0%BA%D0%B0_%D0%BF%D0%BE%D0%B4%D1%81%D1%87%D1%91%D1%82%D0%BE%D0%BC) и за счёт этого существенно ускорить алгоритм: ведь сложность такой сортировки — $O(N)$.

Чтобы провернуть это с любой выборкой, мы можем искусственно дискретизировать значения всех признаков. Это приведёт к локально менее оптимальным значениям сплитов, но, учитывая, что наш алгоритм и без этого был весьма приблизительным, это не ухудшит ничего драматически, а вот ускорение получается очень неплохое.

Самый популярный и простой способ дискретизации основан на частотах значений признаков: отрезок между максимальным и минимальным значением признака разбивается на $b$ подотрезков, длины которых выбираются так, чтобы в каждый попадало примерно равное число обучающих примеров. После чего значения признака заменяются на номера отрезков, на которые они попали.

![3](https://yastatic.net/s3/education-portal/media/3_11_4c8cfe50d7_2cd574536f.webp)

Аналогичная процедура проводится для всех признаков выборки. Полная сложность предобработки — $O(DN\log N)$ — сортировка за $O(N\log N)$ для каждого из $D$ признаков.

Теперь в процедуре динамического алгоритма поиска оптимального сплита нам надо перебирать не все $N$ объектов выборки, а всего лишь $b$ подготовленных заранее границ подотрезков. Частичные суммы статистик тоже придётся поддерживать не для исходного массива данных, а для списка из $b$ возможных сплитов. А для того чтобы делать это эффективно, необходим объект, называемый *гистограммой*: упорядоченный словарь, сопоставляющий каждому значению дискретизированного признака сумму необходимой статистики от таргета на отрезке \[B\[i-1\], B\[i\]\].

Финальный вид алгоритма таков:

1. Дискретизируем каждый из признаков на $b$ значений. Сложность $O(DN\log N)$.
2. Создаём корневую вершину `root`.
3. Вызываем `build_tree_recursive(root, data)`.

Функция `build_tree_recursive` выглядит следующим образом:

1. Проверяем, не пора ли остановиться. Если пора — считаем значение в листе.
2. Теперь мы снова используем динамический алгоритм, но объекты будем сортировать не по исходным значениям признаков, а по их дискретизированным версиям, упорядочивая их с помощью сортировки подсчётом (для вершины, в которую попало $q$ объектов, сложность будет равна $O(qD)$ против $O(q\log q\cdot D)$ в стандартной динамике).
3. Находим оптимальный сплит за $O(qD)$.
4. Делим данные, запускаем процедуру рекурсивно для обоих поддеревьев.

**Общая сложность:** $O(DN\log N + hND)$

{% cut "Гистограммный метод на примере" %}

Давайте посчитаем информативность сплита для MSE на данных с одним признаком, по которому мы уже отсортировали данные:

#|||x | -3 | -2 | -0.05 | 1 | 1 | 2 | 6 | 8 ||
||y | 0 | 0.5 | -1 | 0 | 1 | 2 | 1 | 4 |||#

Дискретизируем на три отрезка с границами $B = [-1, 1.5]$

#|||discretized_x | 0 | 0 | 1 | 1 | 1 | 2 | 2 | 2 ||
|| y | 0 | 0.5 | -1 | 0 | 1 | 2 | 1 | 4 |||#

Строим гистограмму частичных сумм, сумм квадратов и количества объектов для двух отрезков (для третьего можно посчитать, используя общую сумму):

#||| b | 0 | 1 ||
|| cnt | 2 | 3 ||
|| sum_y | 0.5 | 0 ||
|| sum_y_sq | 0.25 | 2 |||#

Всего объектов восемь, сумма таргетов — $\text { sum\_total }=7.5$, сумма квадратов — $\text { sum\_sq\_total }=23.25$.

Информативность текущего листа:

$$D\left[X_d\right]=\frac{\text { sum\_sq\_total }}{\text { cnt\_total }}-\left(\frac{\text { sum\_total }}{\text { cnt\_total }}\right)^2=
$$

$$= \frac{23.25}{8} - \left( \frac{7.5}{8} \right)^{2} = 2.0273
$$

Посчитаем значение критерия ветвления для $b=-1$. Информативность левого листа, то есть дисперсия в нём, равна:

$$D[X_l] = E[X_l^2] - E^2[X_l] = \\ 
$$

$$=\frac{s_u \_y_{\_} s_1[0]}{\operatorname{cnt}[0]}-\left(\frac{\text { sum } \_y}{\operatorname{cnt}[0]}\right)^2=
$$

$$= 0.125 - 0.25 ^ 2 = 0.0625
$$

Информативность правого листа, то есть дисперсия в нём, равна:

$$D[X_r] = E[X_r^2] - E^2[X_r] = \\ 
$$

$$=\frac{\text { sum } \_ \text {sq\_total }- \text { sum } \_y \_s q[0]}{\text { cnt\_total }-\operatorname{cnt}[0]}-\left(\frac{\text { sum\_total }-\operatorname{sum} \_y}{\text { cnt\_total }- \text { cnt }[0]}\right)^2 \approx 2.47
$$

Полное значение критерия для разбиения по $b=-1$:

$$  Branch(b = -1) = 8 \cdot D[X_d] - 2 \cdot D[X_l] - 6 \cdot D[X_r] = \\ 
$$

$$  = 6.555
$$

{% endcut %}

### Mixed integer optimization

Если вам действительно хочется построить *оптимальное* (или хотя бы очень близкое к оптимальному) дерево, то на сегодня для решения этой проблемы не нужно придумывать кучу эвристик самостоятельно, а можно воспользоваться специальными солверами, которые решают NP-полные задачи приближённо, но всё-таки почти точно. Так что единственной (и вполне решаемой) проблемой будет представить исходную задачу в понятном для солвера виде. По ссылке — [пример](https://arxiv.org/abs/1907.02211) построения оптимального дерева с помощью решения задачи целочисленного программирования.

## Историческая справка

Как вы, может быть, уже заметили, решающие деревья — это одна большая эвристика для решения NP-полной задачи, практически лишённая какой-либо стройной теоретической подоплёки. В 1970–1990-e годы интерес к ним был весьма велик как в индустрии, где был полезен хорошо интерпретируемый классификатор, так и в науке, где учёные интересовались способами приближённого решения NP-полных задач.

В связи с этим сложилось много хорошо работающих наборов эвристик, у которых даже были имена: например, [ID3](https://en.wikipedia.org/wiki/ID3_algorithm) был первой реализацией дерева, минимизирующего энтропию, а [CART](https://en.wikipedia.org/wiki/Predictive_analytics#Classification_and_regression_trees_.28CART.29) — первым деревом для регрессии. Некоторые из них были запатентованы и распространялись коммерчески.

На сегодня это всё потеряло актуальность в связи с тем, что существуют хорошо написанные библиотеки (например, [sklearn](https://scikit-learn.org/stable/modules/tree.html), в которой реализована оптимизированная версия CART).

  ## handbook

  Учебник по машинному обучению

  ## title

  Решающие деревья

  ## description

  Обучение древесных моделей для классификации и регрессии. Эффективное построение решающих деревьев

- 
  ## path

  /handbook/ml/article/ansambli-v-mashinnom-obuchenii

  ## content

  Представим, что у вас есть несколько моделей, обученных на ваших данных. Можно ли придумать процедуру, которая позволит использовать все имеющиеся модели и при этом получить на тестовых данных качество выше, чем могла показать каждая из этих моделей в отдельности?

Да. И в этом параграфе мы расскажем, как именно.

## Смещение и разброс

Предположим, мы решаем задачу регрессии с квадратичной функцией потерь. При использовании квадратичной функции потерь для оценки качества работы алгоритма $a$ можно воспользоваться следующим функционалом:

$$Q(a) = \mathbb{E}_x \mathbb{E}_{X, \epsilon} [y(x, \epsilon) - a(x, X)]^2,
$$

где

* $X$ — обучающая выборка
* $x$ — точка из тестового множества
* $y = f(x) + \epsilon$ — целевая зависимость, которую мы можем измерить с точностью до случайного шума $\epsilon$
* $a(x, X)$ — значение алгоритма, обученного на выборке $X$, в точке $x$
* $\mathbb{E}_x$ — среднее по всем тестовым точкам и $\mathbb{E}_{X, \epsilon}$ — среднее по всем обучающим выборкам $X$ и случайному шуму $\epsilon$

Для $Q(a)$ существует разложение на три компоненты — шум, смещение и разброс. Это разложение называется **bias-variance decomposition**, оно — одно из мощных средств для анализа работы ансамблей. О том, как его вывести, вы узнаете в соответствующем [параграфе](https://education.yandex.ru/handbook/ml/article/bias-variance-decomposition), а здесь мы приведём его формулировку.

Существует представление $Q(a)$ в виде трёх компонент:

$$    Q(a) = \mathbb{E}_x \text{bias}_X^2 a(x, X) + \mathbb{E}_x \mathbb{V}_X[a(x, X)] + \sigma^2,
$$

где

$$    \text{bias}_X a(x, X) = f(x) - \mathbb{E}_X[a(x, X)]
$$

* это **смещение** предсказания алгоритма в точке $x$, усреднённого по всем возможным обучающим выборкам, относительно истинной зависимости $f$,

$$    \mathbb{V}_X[a(x, X)] = \mathbb{E}_X \left[ a(x, X) - \mathbb{E}_X[a(x, X)] \right]^2
$$

* это **дисперсия (разброс)** предсказаний алгоритма в зависимости от обучающей выборки $X$,

$$    \sigma^2 = \mathbb{E}_x \mathbb{E}_\epsilon[y(x, \epsilon) - f(x)]^2
$$

* это неустранимый **шум** в данных.

Раз нам известно, что ошибка алгоритма раскладывается на шум, смещение и разброс, можно подумать над способом сократить ошибку. Будет разумно попытаться сначала уменьшить одну из составляющих. Понятно, что с шумом уже ничего не сделать — это минимально возможная ошибка. Какую можно придумать процедуру, чтобы, например, сократить разброс, не увеличивая смещение?

Пример приходит из жизни древних греков: если много человек проголосуют независимо друг от друга, то вместе они придут к разумному решению несмотря на то, что опыт каждого из них субъективен. Аналогом голосования в мире машинного обучения является бэггинг.

## Бэггинг

Идея **бэггинга** (**bagging**, **bootstrap aggregation**) заключается в следующем. Пусть обучающая выборка состояла из $n$ объектов. Выберем из неё $n$ примеров равновероятно, с возвращением. Получим новую выборку $X^1$, в которой некоторых элементов исходной выборки не будет, а какие-то могут войти несколько раз. С помощью некоторого алгоритма $b$ обучим на этой выборке модель $b_1(x) = b(x, X^1)$. Повторим процедуру: сформируем вторую выборку $X^2$ из $n$ элементов с возвращением и с помощью того же алгоритма обучим на ней модель $b_2(x) = b(x, X^2)$. Повторив процедуру $k$ раз, получим $k$ моделей, обученных на $k$ выборках. Чтобы получить одно предсказание, усредним предсказания всех моделей:

$$a(x) = \frac{1}{k}(b_1(x) + \dots + b_k(x)).
$$

Процесс генерации подвыборок с помощью семплирования с возвращением называется **бутстрепом** (**bootstrap**), а модели $b_1(x), \ldots, b_k(x)$ часто называют **базовыми алгоритмами** (хотя, наверное, лучше было бы назвать их базовыми моделями). Модель $a(x)$ называется ансамблем этих моделей.

Посмотрим, что происходит с качеством предсказания при переходе от одной модели к ансамблю. Сначала убедимся, что смещение ансамбля не изменилось по сравнению со средним смещением отдельных моделей. Будем считать, что когда мы берём матожидание по всем обучающим выборкам $X$, то в эти выборки включены также все подвыборки, полученные бутстрепом.

$$    \color{#348FEA}{\text{bias}_X a(x, X) =} f(x) - \mathbb{E}_X[a(x, X)] = f(x) - \mathbb{E}_X \left[ \frac{1}{k} \sum_{i = 1}^k b(x, X^i) \right] =  
$$

$$    = f(x) - \frac{1}{k} \sum_{i = 1}^k \mathbb{E}_X \left[ b(x, X^i) \right] = f(x) - \frac{1}{k} \sum_{i = 1}^k \mathbb{E}_X \left[ b(x, X) \right] = f(x) - \mathbb{E}_X b(x, X)
$$

$$    = f(x) - \mathbb{E}_X b(x, X) \color{#348FEA}{= \text{bias}_X b(x, X)}
$$

Получили, что смещение композиции равно смещению одного алгоритма. Теперь посмотрим, что происходит с разбросом.

$$\mathbb{V}_X[a(x, X)] = \mathbb{E}_X \left[ a(x, X) - \mathbb{E}_X[a(x, X)] \right]^2 = 
$$

$$= \mathbb{E}_X \left[ \frac{1}{k} \sum_{i = 1}^k b(x, X^i)- \mathbb{E}_X \left[ \frac{1}{k} \sum_{i = 1}^k b(x, X^i) \right] \right]^2 = 
$$

$$= \frac{1}{k^2} \mathbb{E}_X \left[ \sum_{i = 1}^k \left( b(x, X^i) - \mathbb{E}_X b(x, X^i) \right) \right]^2 = 
$$

$$= \frac{1}{k^2} \sum_{i = 1}^k \mathbb{E}_X (b(x, X^i) - \mathbb{E}_X b(x, X^i))^2 + 
$$

$$+ \frac{1}{k^2} \sum_{k_1 \ne k_2} \mathbb{E}_X \left[ \left( b(x, X^{k_1}) - \mathbb{E}_X b(x, X^{k_1}) \right) \left( b(x, X^{k_2}) - \mathbb{E}_X b(x, X^{k_2}) \right) \right] = 
$$

$$= \frac{1}{k^2} \sum_{i = 1}^k \mathbb{V}_X b(x, X^i) + \frac{1}{k^2} \sum_{k_1 \ne k_2} \text{cov} \left( b(x, X^{k_1}), b(x, X^{k_2}) \right)
$$

Если предположить, что базовые алгоритмы некоррелированы, то:

$$\color{#348FEA}{\mathbb{V}_X[a(x, X)] =} \frac{1}{k^2} \sum_{i = 1}^k \mathbb{V}_X b(x, X^i) = 
$$

$$= \frac{1}{k^2} \sum_{i = 1}^k \mathbb{V}_X b(x, X) \color{#348FEA}{= \frac{1}{k} \mathbb{V}_X b(x, X)}
$$

Получилось, что в этом случае дисперсия композиции в $k$ раз меньше дисперсии отдельного алгоритма.

### Пример: бэггинг над решающими деревьями

Пусть наша целевая зависимость $f(x)$ задаётся как

$$    f(x) = x \sin x,
$$

и к ней добавляется нормальный шум $\epsilon \sim \mathcal{N}(0, 9)$.  Пример семпла из таких данных:

![4](https://yastatic.net/s3/education-portal/media/4_1_13d3070da7_1e530dd10e.webp)

Попробуем посмотреть, как выглядят предсказания решающих деревьев глубины 7 и бэггинга над такими деревьями в зависимости от обучающей выборки. Обучим решающие деревья 100 раз на различных случайных семплах размера 20. Возьмём также бэггинг над 10 решающими деревьями глубины 7 в качестве базовых классификаторов и тоже 100 раз обучим его на случайных выборках размера 20. Если изобразить предсказания обученных моделей на каждой из 100 итераций, то можно увидеть примерно такую картину:

![4](https://yastatic.net/s3/education-portal/media/4_2_72c446c0e9_3e3d1f70cc.webp)

По этому рисунку видно, что общая дисперсия предсказаний в зависимости от обучающего множества у бэггинга значительно ниже, чем у отдельных деревьев, а в среднем предсказания деревьев и бэггинга не отличаются.

Чтобы подтвердить это наблюдение, мы можем изобразить смещение и разброс случайных деревьев и бэггинга в зависимости от максимальной глубины:

![4](https://yastatic.net/s3/education-portal/media/4_3_32fef6a2cc_5995be8be9.webp)

На графике видно, как значительно бэггинг сократил дисперсию. На самом деле, дисперсия уменьшилась практически в 10 раз, что равняется числу базовых алгоритмов ($k$), которые бэггинг использовал для предсказания:

![4](https://yastatic.net/s3/education-portal/media/4_4_d48dd95df3_cc2bc21e01.webp)

Код для отрисовки картинок и подсчёта смещения и разброса можно найти [тут](https://github.com/yandexdataschool/ML-Handbook-materials/blob/main/chapters/ensembles/bias_variance.ipynb).

## Random Forest

В предыдущем разделе мы сделали предположение, что базовые алгоритмы некоррелированы, и за счёт этого получили очень сильное уменьшение дисперсии у ансамбля относительно входящих в него базовых алгоритмов. Однако в реальной жизни добиться этого сложно: ведь базовые алгоритмы учили одну и ту же зависимость на пересекающихся выборках. Поэтому будет странно, если корреляция на самом деле нулевая. Но на практике оказывается, что **строгое выполнение этого предположения не обязательно**. Достаточно, чтобы алгоритмы были в некоторой степени не похожи друг на друга. На этом строится развитие идеи бэггинга для решающих деревьев — случайный лес.

Построим ансамбль алгоритмов, где базовый алгоритм — это решающее дерево. Будем строить по следующей схеме:

1. Для построения $i$-го дерева:

   * Сначала, как в обычном бэггинге, из обучающей выборки $X$ выбирается с возвращением случайная подвыборка $X^i$ того же размера, что и $X$.

   * В процессе обучения каждого дерева **в каждой вершине** случайно выбираются $n < N$ признаков, где $N$ — полное число признаков (метод случайных подпространств), и среди них ищется оптимальный сплит. Такой приём как раз позволяет управлять степенью скоррелированности базовых алгоритмов.

2. Чтобы получить предсказание ансамбля на тестовом объекте, усредняем отдельные ответы деревьев (для регрессии) или берём самый популярный класс (для классификации).

3. Profit. Мы построили **Random Forest (случайный лес)** — комбинацию бэггинга и метода случайных подпространств над решающими деревьями.

Внимательный читатель мог заметить, что при построении случайного леса у специалиста по машинному обучению есть несколько степеней свободы. Давайте обсудим их подробнее.

### Какая должна быть глубина деревьев в случайном лесу?

Ошибка модели (на которую мы можем повлиять) состоит из смещения и разброса. Разброс мы уменьшаем с помощью процедуры бэггинга. На смещение бэггинг не влияет, а хочется, чтобы у леса оно было небольшим. Поэтому смещение должно быть небольшим у самих деревьев, из которых строится ансамбль.

У неглубоких деревьев малое число параметров, то есть дерево способно запомнить только верхнеуровневые статистики обучающей подвыборки. Они во всех подвыборках будут похожи, но будут не очень подробно описывать целевую зависимость. Поэтому при изменении обучающей подвыборки предсказание на тестовом объекте будет стабильным, но не точным (низкая дисперсия, высокое смещение).

Наоборот, у глубоких деревьев нет проблем запомнить подвыборку подробно. Поэтому предсказание на тестовом объекте будет сильнее меняться в зависимости от обучающей подвыборки, зато в среднем будет близко к истине (высокая дисперсия, низкое смещение).

Вывод: используем глубокие деревья.

### Сколько признаков надо подавать дереву для обучения?

Ограничивая число признаков, которые используются в обучении одного дерева, мы также управляем качеством случайного леса. Чем больше признаков, тем больше корреляция между деревьями и тем меньше чувствуется эффект от ансамблирования. Чем меньше признаков, тем слабее сами деревья.

Практическая рекомендация — брать корень из числа всех признаков для классификации и треть признаков для регрессии.

### Сколько должно быть деревьев в случайном лесе?

Выше было показано, что увеличение числа элементарных алгоритмов в ансамбле не меняет смещения и уменьшает разброс. Так как число признаков и варианты подвыборок, на которых строятся деревья в случайном лесе, ограничены, уменьшать разброс до бесконечности не получится. Поэтому имеет смысл построить график ошибки от числа деревьев и ограничить размер леса в тот момент, когда ошибка перестанет значимо уменьшаться.

Вторым практическим ограничением на количество деревьев может быть время работы ансамбля. Однако есть положительное свойство случайного леса: случайный лес можно строить и применять параллельно, что сокращает время работы, если у нас есть несколько процессоров. Но процессоров, скорее всего, всё же сильно меньше числа деревьев, а сами деревья обычно глубокие. Поэтому на большом числе деревьев Random Forest может работать дольше желаемого и количество деревьев можно сократить, немного пожертвовав качеством.

## Бустинг

**Бустинг (boosting)** — это ансамблевый метод, в котором так же, как и в методах выше, строится множество базовых алгоритмов из одного семейства, объединяющихся затем в более сильную модель. Отличие состоит в том, что в бэггинге и случайном лесе базовые алгоритмы учатся независимо и параллельно, а в бустинге — последовательно.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/4_5_98b5004456_b5c396fb5a.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    Автор изображения –
    <a href="https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205">Joseph Rocca</a>.
  </figcaption>
</figure>

Каждый следующий базовый алгоритм в бустинге обучается так, чтобы уменьшить общую ошибку  всех своих предшественников. Как следствие, итоговая композиция будет иметь меньшее смещение, чем каждый отдельный базовый алгоритм (хотя уменьшение разброса также может происходить).

Поскольку основная цель бустинга — уменьшение смещения, в качестве базовых алгоритмов часто выбирают алгоритмы с высоким смещением и небольшим разбросом. Например, если в качестве базовых классификаторов выступают деревья, то их глубина должна быть небольшой — обычно не больше 2-3 уровней.

Ещё одной важной причиной для выбора моделей с высоким смещением в качестве базовых является то, что такие модели, как правило, быстрее учатся.  Это важно для их последовательного обучения, которое может стать очень дорогим по времени, если на каждой итерации будет учиться сложная модель. На текущий момент основным видом бустинга с точки зрения применения на практике является **градиентный бустинг**, о котором подробно рассказывается в соответствующем [параграфе](https://academy.yandex.ru/handbook/ml/article/gradientnyj-busting).

Хотя случайный лес — мощный и достаточно простой для понимания и реализации алгоритм, на практике он чаще всего уступает градиентному бустингу. Поэтому градиентный бустинг сейчас — основное продакшн-решение, если работа происходит с табличными данными (в работе с однородными данными — картинками, текстами — доминируют нейросети).

## Стекинг

**Стекинг (stacking)** — алгоритм ансамблирования, основные отличия которого от предыдущих состоят в следующем:

1. он может использовать алгоритмы разного типа, а не только из какого-то фиксированного семейства. Например, в качестве базовых алгоритмов могут выступать метод ближайших соседей и линейная регрессия
2. результаты базовых алгоритмов объединяются в один с помощью обучаемой мета-модели, а не с помощью какого-либо обычного способа агрегации (суммирования или усреднения)

Обучение стекинга проходит в несколько этапов:

1. общая выборка разделяется на тренировочную и тестовую
2. тренировочная выборка делится на $n$ фолдов. Затем эти фолды перебираются тем же способом, что используется при кросс-валидации: на каждом шаге фиксируются $(n - 1)$ фолдов для обучения базовых алгоритмов и один — для их предсказаний (вычисления мета-факторов). Такой подход нужен для того, чтобы можно было использовать всё тренировочное множество, и при этом базовые алгоритмы не переобучались

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/4_6_a01e6d6da8_476bb77afa.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    Автор изображения — 
    <a href="https://medium.com/@stevenyu530_73989/stacking-and-blending-intuitive-explanation-of-advanced-ensemble-methods-46b295da413c">Steven Yu</a>.  
  </figcaption>
</figure>

3. на полученных мета-факторах обучается мета-модель. Кроме мета-факторов, она может принимать на вход и фичи из исходного датасета. Выбор зависит от решаемой задачи

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/4_7_ff33dd2f94_5e074dfd14.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    Автор изображения — 
    <a href="https://medium.com/@stevenyu530_73989/stacking-and-blending-intuitive-explanation-of-advanced-ensemble-methods-46b295da413c">Steven Yu</a>. 
  </figcaption>
</figure>

Для получения мета-факторов на тестовом множестве базовые алгоритмы можно обучить на всём тренировочном множестве — переобучения в данном случае возникнуть не должно.

Если данных достаточно много, то можно просто разделить обучающие данные на две непересекающиеся части: ту, на которой учатся базовые алгоритмы, и ту, на которой они делают свои предсказания и обучается мета-модель. Использование такого простого разбиения вместо кросс-валидации на тренировочных данных иногда называют **блендингом (blending)**. Если данных совсем много, то тестовое множество тоже можно разделить на две части: тестовую и валидационную, и использовать последнюю для подбора гиперпараметров моделей-участников.

С точки зрения смещения и разброса стекинг не имеет прямой интерпретации, так как не минимизирует напрямую ни ту, ни другую компоненту ошибки. Удачно работающий стекинг просто уменьшает ошибку, и, как следствие, её компоненты тоже будут убывать.

## Почитать по теме

* [Лекция](https://github.com/esokolov/ml-course-hse/blob/master/2020-fall/lecture-notes/lecture08-ensembles.pdf) Евгения Соколова про bias-variance decomposition и бэггинг
* [Блог-пост](https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205) про ансамбли от Joseph Rocca
* [Блог-пост](https://medium.com/@stevenyu530_73989/stacking-and-blending-intuitive-explanation-of-advanced-ensemble-methods-46b295da413c) про стекинг и блендинг от Steven Yu

  ## handbook

  Учебник по машинному обучению

  ## title

  Ансамбли в машинном обучении

  ## description

  Как смешать несколько моделей в одну. Стэкинг, бэггинг, случайные леса

- 
  ## path

  /handbook/ml/article/gradientnyj-busting

  ## content

  В прошлых разделах мы научились соединять базовые алгоритмы в ансамбль с помощью бэггинга (и, в частности, строить из решающих деревьев случайные леса). Теперь мы рассмотрим другой способ объединять базовые алгоритмы в композицию — градиентный бустинг.

В ходе обучения случайного леса каждый базовый алгоритм строится независимо от остальных. Бустинг, в свою очередь, воплощает идею последовательного построения линейной комбинации алгоритмов. Каждый следующий алгоритм старается уменьшить ошибку текущего ансамбля.

Бустинг, использующий деревья решений в качестве базовых алгоритмов, называется **градиентным бустингом над решающими деревьями**, (**Gradient Boosting on Decision Trees**, **GBDT**).

Он отлично работает на выборках с «табличными», неоднородными данными. Пример таких данных — описание пользователя Яндекса через его возраст, пол, среднее число поисковых запросов в день, число заказов такси и так далее. Такой бустинг способен эффективно находить нелинейные зависимости в данных различной природы.

Этим свойством обладают все алгоритмы, которые используют деревья решений, однако именно GBDT обычно выигрывает в подавляющем большинстве задач. Благодаря этому он широко применяется во многих конкурсах по машинному обучению и задачах из индустрии:

* поисковом ранжировании;
* рекомендательных системах;
* таргетировании рекламы;
* предсказании погоды;
* выбора пункта назначения такси и многих других.

Не так хорошо бустинг проявляет себя на однородных данных: текстах, изображениях, звуке, видео. В таких задачах нейросетевые подходы почти всегда демонстрируют лучшее качество.

И хотя деревья решений — традиционный выбор для объединения в ансамбли, никто не запрещает использовать и другие алгоритмы (например, линейные модели) в качестве базовых. Эта возможность реализована в пакете XGBoost.

Стоит только понимать, что построенная композиция окажется линейной комбинацией линейных моделей, то есть опять-таки линейной моделью - или нейросетью с одним полносвязным слоем. Это уменьшает возможности ансамбля эффективно определять нелинейные зависимости в данных. Поэтому в этом параграфе мы рассмотрим только бустинг над решающими деревьями.

## Интуиция

Рассмотрим задачу регрессии с квадратичной функцией потерь:

$$\mathcal{L}(y, x) = \frac{1}{2}\sum^{N}_{i=1}\left(y_i -  a(x_i)\right)^{2} \rightarrow \min
$$

Для решения будем строить композицию из $K$ базовых алгоритмов:

$$a(x) = a_K(x) = b_1(x) + b_2(x) + \dots +b_K(x)
$$

Если мы обучим единственное решающее дерево, то качество такой модели, скорее всего, будет низким. Однако мы знаем, на каких объектах построенное дерево давало точные предсказания, а на каких ошибалось.

Попробуем использовать эту информацию и обучим ещё одну модель. Допустим, что предсказание первой модели на объекте $x_l$ на 10 больше, чем необходимо (т.е. $b_1(x_l) = y_l + 10$). Если бы мы могли обучить новую модель, которая на $x_l$ будет выдавать ответ $-10$, то сумма ответов этих двух моделей на объекте $x_l$ в точности совпала бы с истинным значением:

$$b_1(x_l) + b_2(x_l) = (y_l + 10) + (-10) = y_l 
$$

Другими словами, если вторая модель научится предсказывать разницу между реальным значением и ответом первой, то это позволит уменьшить ошибку композиции.

В реальности вторая модель тоже не сможет обучиться идеально, поэтому обучим третью, которая будет «компенсировать» неточности первых двух. Будем продолжать так, пока не построим композицию из $K$ алгоритмов.

Для объяснения метода градиентного бустинга полезно воспользоваться следующей аналогией. Бустинг можно представить как гольфиста, цель которого — загнать мяч в лунку с координатой $y_{\text{ball}}$. Положение мяча здесь – ответ композиции $a(x_{\text{ball}})$. Гольфист мог бы один раз ударить по мячу, не попасть в лунку и пойти домой, но настырность заставляет его продолжить.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/5_1_848ce5004c_60a9bfa9fd.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://explained.ai/gradient-boosting/L2-loss.html">Источник</a>
  </figcaption>
</figure>

По счастью, ему не нужно начинать каждый раз с начальной позиции. Следующий удар гольфиста переводит мяч из текущего положения $a_k(x_{\text{ball}})$ в положение $a_{k+1}(x_{\text{ball}})$. Каждый следующий удар — это та поправка, которую вносит очередной базовый алгоритм в композицию. Если гольфист все делает правильно, то функция потерь будет уменьшаться:

$$\mathcal{L}(y, a_{k+1}(x)) < \mathcal{L}(y, a_{k}(x)),
$$

то есть мяч постепенно будет приближаться к лунке.

Удары при этом делаются не хаотично. Гольфист оценивает текущее положение мяча относительно лунки и следующим ударом старается нивелировать те проблемы, которые он создал себе всеми предыдущими.  Подбираясь к лунке, он будет бить всё аккуратнее и, возможно, даже возьмет другую клюшку, но точно не будет лупить так же, как из первоначальной позиции. В итоге комбинация всех ударов рано или поздно перенесет мяч в лунку.

Подобно тому, как гольфист постепенно подводит мяч к цели, бустинг с каждым новым базовым алгоритмом всё больше приближает предсказание к истинному значению метки объекта.

Рассмотрим теперь другую аналогию — разложение функции в ряд Тейлора. Из курса математического анализа известно, что (достаточно хорошую) бесконечно дифференцируемую функцию $f(x)$ на интервале $x \in \left(a - R, a + R\right)$ можно представить в виде бесконечной суммы степенных функций:

$$f(x) = \sum\limits_{n = 0}^{\infty}\frac{f^{(n)}\left(a\right)}{n!}\left(x - a\right)^{n}.
$$

Одна, самая первая степенная функция в разложении, очень грубо приближает $f(x)$. Прибавляя к ней следующую, мы получим более точное приближение. Каждая следующая элементарная функция увеличивает точность приближения, но менее заметна в общей сумме. Если нам не требуется абсолютно точное разложение, вместо бесконечного ряда Тейлора мы можем ограничиться суммой его первых $k$ элементов. Таким образом, интересующую нас функцию мы с некоторой точностью представили в виде суммы «простых» функций.

Перенесём эту идею на задачи машинного обучения. В машинном обучении мы пытаемся по выборке $(x_i, y_i)$ восстановить неизвестную истинную зависимость. Прежде всего, мы выбираем подходящий алгоритм. Мы можем выбрать «сложный» алгоритм, который сразу хорошо выучит истинную зависимость.

А можем обучить «простой», который выучит истинную зависимость посредственно. Затем мы добавим к нему ещё один такой простой алгоритм, чтобы уточнить предсказание первого алгоритма. Продолжая этот процесс, мы получим сумму простых алгоритмов, где первый алгоритм грубо приближает истинную зависимость, а каждый следующий делает приближение всё точнее.

## Пример с задачей регрессии: формальное описание

Рассмотрим тот же пример с задачей регрессии и квадратичной функцией потерь:

$$\mathcal{L}(y, x) = \frac{1}{2}\sum\limits^{N}_{i=1}\left(y_i -  a(x_i)\right)^{2} \rightarrow \min
$$

Для решения также будем строить композицию из $K$ базовых алгоритмов семейства $\mathcal{B}$:

$$a(x) = a_K(x) = b_1(x) + b_2(x) + \dots + b_K(x)
$$

В качестве базовых алгоритмов выберем, как и условились в начале параграфа, семейство $\mathcal{B}$ решающих деревьев некоторой фиксированной глубины.

Используя известные нам методы построения решающих деревьев, обучим алгоритм $b_1(x) \in \mathcal{B}$, который наилучшим образом приближает целевую переменную:

$$b_1(x) = \underset{b\in \mathcal{B}}{\mathrm{argmin}} \, \mathcal{L}(y, b(x))
$$

Построенный алгоритм $b_1(x)$, скорее всего, работает не идеально. Более того, если базовый алгоритм работает слишком хорошо на обучающей выборке, то высока вероятность переобучения: низкий уровень смещения, но высокий уровень разброса. Далее вычислим, насколько сильно отличаются предсказания этого дерева от истинных значений:

$$s_i^{1} = y_i - b_1(x_i)
$$

Теперь мы хотим скорректировать $b_1(x)$ с помощью $b_2(x)$. В идеале так, чтобы $b_2(x)$ идеально предсказывал величины $s_i^{1}$, ведь в этом случае

$$   a_2(x_i) = b_1(x_i) + b_2(x_i) = 
$$

$$   = b_1(x_i) + s_i^1 = b_1(x_i) + (y_i - b_1(x_i)) = y_i 
$$

Найти совершенный алгоритм, скорее всего, не получится, но по крайней мере мы можем выбрать из семейства наилучшего представителя для такой задачи. Итак, второе решающее дерево будет обучаться предсказывать разности $s_i^1$:

$$b_2(x) = \underset{b\in \mathcal{B}}{\mathrm{argmin}} \, \mathcal{L}(s^1, b(x))
$$

Ожидается, что композиция из двух таких моделей $a_2(x) = b_1(x) + b_2(x)$ станет более качественно предсказывать целевую переменную $y$.

Далее рассуждения повторяются до построения всей композиции. На $k$-ом шаге вычисляется разность между правильным ответом и текущим предсказанием композиции из $k - 1$ алгоритмов:

$$
s_i^{k - 1} = y_i - \sum_{j=1}^{k - 1} b_{j}(x_i) = y_i - a_{k - 1}(x_i)
$$


Затем $k$-й алгоритм учится предсказывать эту разность:

$$
b_k(x) = \underset{b\in \mathcal{B}}{\mathrm{argmin}} \, \mathcal{L}(s^{k - 1}, b(x)), 
$$

а композиция в целом обновляется по формуле

$$
a_k(x) = a_{k - 1}(x) + b_k(x)
$$

Обучение $K$ базовых алгоритмов завершает построение композиции.

## Обобщение на другие функции потерь

### Интуиция

Отметим теперь важное свойство функции потерь в рассмотренном выше примере с регрессией. Для этого посчитаем производную функции потерь по предсказанию $z = a_k(x_i)$ модели для $i$-го объекта:

$$
\frac{\partial{\mathcal{L}(y_i,z)}}{\partial{z}}\bigg|_{z=a_k(x_i)} = \frac{\partial}{\partial{z}}\frac{1}{2}\left(y_i -  z\right)^{2}\bigg|_{z=a_k(x_i)} = a_k(x_i) - y_i 
$$

Видим, что разность, на которую обучается $k$-й алгоритм, выражается через производную:

$$
s_i^{k} = y_i -a_k(x_i) = -\frac{\partial{\mathcal{L}(y_i,z)}}{\partial{z}}\bigg|_{z=a_k(x_i)}
$$

Таким образом, для каждого объекта $x_i$ очередной алгоритм в бустинге обучается предсказывать антиградиент функции потерь по предсказанию модели $-\frac{\partial{\mathcal{L}(y_i,z)}}{\partial{z}}$ в точке $a_k(x_i)$ предсказания текущей части композиции на объекте $x_i$.

**Почему же это важно?** Дело в том, что это наблюдение позволяет обобщить подход построения бустинга на произвольную дифференцируемую функцию потерь. Для этого мы заменяем обучение на разность $s_i^k$ обучением на антиградиент функции потерь $(-g_i^k)$, где

$$g_i^k =\frac{\partial{\mathcal{L}(y_i,z)}}{\partial{z}}\bigg|_{z=a_k(x_i)}
$$

Вспомните аналогию с гольфистом: обучение композиции можно представить как перемещение предсказания из точки $(a_k(x_1), a_k(x_2), \dots, a_k(x_N))$ в точку $(a_{k+1}(x_1), a_{k+1}(x_2), \dots, a_{k+1}(x_N))$. В конечном итоге мы ожидаем, что точка $(a_K(x_1), a_K(x_2), \dots, a_K(x_N))$ будет располагаться как можно ближе к точке с истинными значениями $(y_1, y_2, \dots, y_N)$.

![5](https://yastatic.net/s3/education-portal/media/5_2_ddc4f74a07_a6da8f6e38.webp)

В случае квадратичной функции потерь интуиция вполне подкрепляется математикой. Изменится ли что-либо в наших действиях, если мы поменяем квадратичную функцию потерь на любую другую? С одной стороны, мы, как и прежде, можем двигаться в направлении уменьшения разности предсказания и истинного значения: любая функция потерь поощряет такие шаги для каждого отдельного объекта, ведь для любой адекватной функции потерь выполнено $\mathcal{L}(y, y) = 0$.

Но мы можем посмотреть на задачу и с другой стороны: не с точки зрения уменьшения расстояния между вектором предсказаний и вектором истинных значений, а с точки зрения уменьшения значения функции потерь. Для наискорейшего уменьшения функции потерь нам необходимо шагнуть в сторону её антиградиента по вектору предсказаний текущей композиции, то есть как раз таки в сторону вектора $(-g_1^k,\dots,-g_N^k)$. Это направление не обязано совпадать с шагом по направлению уменьшения разности предсказания и истинного значения. Например, может возникнуть гипотетическая ситуация, как на рисунке ниже:

![5](https://yastatic.net/s3/education-portal/media/5_3_affa81aaa3_39366ae5f4.webp)

В изображённом примере рассматриваются два объекта $x_1$ и $x_2$. Текущее предсказание для них — $(a_k(x_1), a_k(x_2))$, а окружность определяет варианты следующего шага: первый вариант — пойти в направлении $(s_1^k, s_2^k)$, как делалось ранее; второй — пойти в направлении антиградиента. Также показаны линии уровня значений функции потерь. Функция потерь в этом примере устроена таким образом, что $L_2 < L_1$, из-за чего шаг по антиградиенту оказывается более выгодным.

Движение в сторону антиградиента более выгодно с точки зрения минимизации функции потерь — плюс оно также позволяет справляться с ситуациями, когда явно посчитать остаток (разницу между целевым значением и предсказанием) не представляется возможным.

Один из таких примеров — задача ранжирования. В задаче ранжирования объекты в датасете разбиты на группы и требуется построить модель, по предсказаниям которой можно было бы «правильно» упорядочить документы в каждой группе (обычно по убыванию предсказания модели).

**Что значит упорядочить «правильно»?** Это значит, что полученная по предсказаниям модели перестановка объектов в группе должна быть близка к идеальной по некоторой метрике.

**Как задается идеальная перестановка?** Есть два способа:

* Первый способ — проставить каждому объекту число $y$, по которому можно отсортировать объекты для получения идеальной перестановки. Это число можно рассматривать как таргет и обучать модель регрессии — в некоторых случаях это даже будет работать хорошо.
* Второй способ — задать набор пар объектов, которые обозначают их порядок относительно друг друга в идеальной перестановке. То есть пара $(i, j)$ означает, что объект с номером $i$ должен стоять раньше в перестановке, чем объект с номером $j$.

Во втором способе таргетов у объектов нет, но дифференцируемая функция потерь есть — в библиотеке CatBoost она называется PairLogit и вычисляется по формуле:

$$   PairLogit = \frac{-\sum\limits_{p, n \in Pairs} \left(log(\displaystyle\frac{1}{1 + e^{- (a_{p} - a_{n})}})\right)}{|Pairs|},
$$

где $a_p$ и $a_n$ — это предсказания модели на объектах $p$ и $n$ соответственно. Градиент такой функции потерь посчитать можно, а разницу между предсказанием и истинным значением — нет.

### Математическое обоснование

Попробуем записать наши интуитивные соображения более формально. Пусть $\mathcal{L}$ – дифференцируемая функция потерь, а наш алгоритм $a(x)$ представляет собой композицию базовых алгоритмов:

$$   a(x) = a_k(x) = b_1(x) + \ldots + b_k(x)
$$

Мы строим нашу композицию «жадно»:

$$   a_k(x) = a_{k - 1}(x) + b_k(x),
$$

где вновь добавляемый базовый алгоритм $b_k$ обучается так, чтобы улучшить предсказания текущей композиции:

$$   b_k = \underset{b\in \mathcal{B}}{\mathrm{argmin}} \sum_{i = 1}^N \mathcal{L}(y_i, a_{k - 1}(x_i) + b(x_i))
$$

Модель $b_0$ выбирается так, чтобы минимизировать потери на обучающей выборке:

$$   b_0 = \underset{b\in \mathcal{B}}{\mathrm{argmin}} \sum_{i = 1}^N \mathcal{L}(y_i, b(x_i))
$$

Для построения базовых алгоритмов на следующих шагах рассмотрим разложение Тейлора функции потерь $\mathcal L$ до первого члена в окрестности точки $(y_i, a_{k - 1}(x_i))$:

$$   \mathcal{L}(y_i, a_{k - 1}(x_i) + b(x_i)) \approx 
   \mathcal{L}(y_i, a_{k - 1}(x_i)) + b(x_i) \frac{\partial \mathcal{L}(y_i, z)}{\partial z} \bigg|_{z = a_{k - 1}(x_i)} 
   = \mathcal{L}(y_i, a_{k - 1}(x_i)) + b(x_i) g_i^{k - 1}
$$

Избавившись от постоянных членов, мы получим следующую оптимизационную задачу:

$$   b_k \approx \underset{b\in \mathcal{B}}{\mathrm{argmin}} \sum_{i = 1}^N b(x_i) g_i^{k - 1}
$$

Поскольку суммируемое выражение — это скалярное произведение двух векторов, его значение минимизируют $b(x_i)$, пропорциональные значениям $-g_i^{k - 1}$. Поэтому на каждой итерации базовые алгоритмы $b_k$ обучаются предсказывать значения антиградиента функции потерь по текущим предсказаниям композиции.

Итак, использованная нами интуиция шага в сторону «уменьшения остатка» удивительным образом привела к оптимальным смещениям в случае квадратичной функции потерь, но для других функций потерь это не так: для них смещение происходит в сторону антиградиента.

Получается, что в общем случае на каждой итерации базовые алгоритмы должны приближать значения антиградиента функции потерь. Однако есть частный случай, в котором в качестве таргета для базового алгоритма выгоднее использовать именно «остатки» — это касается функции потерь MAE. Её производная равна -1, 0 или \+1.

Приближая базовым алгоритмом антиградиент MAE, количество итераций до сходимости будет расти пропорционально масштабу таргета. То есть, если домножить целевое значение на 10, то потребуется в 10 раз больше итераций градиентного бустинга. Использование остатков в качестве таргета для базового алгоритма не имеет такой проблемы. Аналогичные рассуждения верны также для функции MAPE, в которой проблема с масштабом таргета может проявляться еще сильнее.

### Обучение базового алгоритма

При построении очередного базового алгоритма $b_{k+1}$ мы решаем задачу регрессии с таргетом, равным антиградиенту функции потерь исходной задачи на предсказании $a_k = b_1 + \ldots + b_k$.

Теоретически можно воспользоваться любым методом построения регрессионного дерева. Важно выбрать оценочную функцию $S$, которая будет показывать, насколько текущая структура дерева хорошо приближает антиградиент. Её нужно будет использовать для построения критерия ветвления:

$$|R| \cdot S(R) - |R_{right}| \cdot S(R_{right}) - |R_{left}| \cdot S(R_{left}) \rightarrow \max,
$$

где $S(R)$ — значение функции $S$ в вершине $R$, $S(R_{left}), S(R_{right})$ — значения в левом и правом сыновьях $R$ после добавления предиката, $\mid \, \cdot \, \mid$ — количество элементов, пришедших в вершину.

Например, можно использовать следующие оценочные функции:

$$ L_2(g, p) = \sum\limits_{i=1}^N\left(p_i - g_i\right)^2,\\
Cosine(g, p) = -\frac{\sum\limits_{i=1}^N(p_i \cdot g_i)}{\sqrt{\sum\limits_{i=1}^Np_i^2} \cdot \sqrt{\sum\limits_{i=1}^Ng_i^2}},
$$

где $p_i$ — предсказание дерева на объекте $x_i$, $g_i$ — антиградиент, на который учится дерево, $p = {p_i}{i=1}^N$, $g = { g_i }^N$. Функция $L_2$ представляет собой среднеквадратичную ошибку, а функция $Cosine$ определяет близость через косинусное расстояние между векторами предсказаний и антиградиентов.

В итоге обучение базового алгоритма проходит в два шага:

* по **функции потерь** вычисляется целевая переменная для обучения следующего базового алгоритма:

$$g_i^k =\frac{\partial{\mathcal{L}(y_i,z)}}{\partial{z}}\bigg|_{z=a_k(x_i)}
$$

* строится регрессионное дерево на обучающей выборке $(x_i, -g_i^k)$, минимизирующее выбранную **оценочную функцию**.

### На практике

Поскольку для построения градиентного бустинга достаточно уметь считать градиент функции потерь по предсказаниям, с его помощью можно решать широкий спектр задач. В библиотеках градиентного бустинга даже реализована возможность создавать свои функции потерь: для этого достаточно уметь вычислять ее градиент, зная истинные значения и текущие предсказания для элементов обучающей выборки.

Типичный градиентный бустинг имеет в составе несколько тысяч деревьев решений, которые необходимо строить последовательно. Построение решающего дерева на выборках типичного размера и современном железе, даже с учетом всех оптимизаций, требует небольшого, но всё-таки заметного времени (0.1-1c), которое для всего ансамбля превратится в десятки минут. Это не так быстро, как обучение линейных моделей, но всё-таки значительно быстрее, чем обучение типичных нейросетей.

### Темп обучения (learning rate)

Обучение композиции с помощью градиентного бустинга может привести к переобучению, если базовые алгоритмы слишком сложные. Например, если сделать решающие деревья слишком глубокими (более 10 уровней), то при обучении бустинга ошибка на обучающей выборке даже при довольно скромном $K$ может приблизиться к нулю, то есть предсказание будет почти идеальным, но на тестовой выборке всё будет плохо.

Существует два решения этой проблемы.

* Во-первых, необходимо упростить базовую модель, уменьшив глубину дерева (либо примерив какие-либо ещё техники регуляризации).

* Во-вторых, мы можем ввести параметр, называемый **темпом обучения** (**learning rate**) $\eta \in (0, 1]$:

$$a_{k+1}(x) = a_{k}(x) + \eta b_{k+1}(x) 
$$

Присутствие этого параметра означает, что каждый базовый алгоритм вносит относительно небольшой вклад во всю композицию: если расписать сумму целиком, она будет иметь вид

$$a_{k+1}(x) = b_1(x) + \eta b_2(x) + \eta b_3(x) + \ldots + \eta b_{k+1}(x)
$$

Значение параметра обычно определяется эмпирически по входным данным. В библиотеке CatBoost темп обучения может быть выбран автоматически по набору данных. Для этого используется заранее обученная линейная модель, предсказывающая темп обучения по мета-параметрам выборки данных: числу объектов, числу признаков и другим.

Темп обучения связан с количеством итераций градиентного бустинга. Чем меньше learning rate, тем больше итераций потребуется сделать для достижения того же качества на обучающей выборке.

### Feature importance

Отдельные деревья решений можно легко интерпретировать, просто визуализируя их структуру. Но в модели градиентного бустинга содержатся сотни деревьев, и поэтому её нелегко интерпретировать с помощью визуализации входящих в неё деревьев. При этом хотелось бы, как минимум, понимать, какие именно признаки в данных оказывают наибольшее влияние на предсказание композиции.

Можно сделать следующее наблюдение: признаки из верхней части дерева влияют на окончательное предсказание для большей доли обучающих объектов, чем признаки, попавшие на более глубокие уровни.

Таким образом, ожидаемая доля обучающих объектов, для которых происходило ветвление по данному признаку, может быть использована в качестве оценки его относительной важности для итогового предсказания. Усредняя полученные оценки важности признаков по всем решающим деревьям из ансамбля, можно уменьшить дисперсию такой оценки и использовать ее для отбора признаков. Этот метод известен как **MDI** (**mean decrease in impurity**).

Существуют и другие методы оценки важности признаков для ансамблей: например, Permutation feature importance (см. [описание](https://scikit-learn.org/stable/modules/permutation_importance.html#permutation-importance) в sklearn) и множество разных подходов, [предлагаемых](https://catboost.ai/en/docs/concepts/fstr) в библиотеке CatBoost. Все эти техники отбора признаков применимы также и для случайных лесов.

### Реализации

Для общего развития имеет смысл посмотреть реализацию в [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html), но на практике она весьма медленная и не такая уж умная.

Хороших реализаций GBDT есть, как минимум, три: [LightGBM](https://lightgbm.readthedocs.io/en/latest/), [XGBoost](https://xgboost.readthedocs.io/en/latest/) и [CatBoost](https://catboost.ai/). Исторически они отличались довольно сильно, но за последние годы успели скопировать друг у друга все хорошие идеи.

#### Форма деревьев

Одно из основных отличий LightGBM, XGBoost и CatBoost — форма решающих деревьев.

LightGBM строит деревья по принципу: «На каждом шаге делим вершину с наилучшим скором», а основным критерием остановки выступает максимально допустимое количество вершин в дереве. Это приводит к тому, что деревья получаются несимметричными, то есть поддеревья могут иметь разную глубину — например, левое поддерево может иметь глубину $2$, а правое может разрастись до глубины $15$.

С одной стороны, это позволяет быстро подогнаться под обучающие данные. С другой — бесконтрольный рост дерева в глубину неизбежно ведет к переобучению, поэтому LightGBM позволяет помимо количества вершин ограничивать и максимальную глубину. Впрочем, это ограничение обычно все равно выше, чем для XGBoost и CatBoost.

![tree](https://yastatic.net/s3/education-portal/media/tree_lightgbm_b27000abd5_dcdf18005a.webp)

XGBoost строит деревья по принципу: «Строим дерево последовательно по уровням до достижения максимальной глубины». Отдельного ограничения на количество вершин нет, так как оно естественным образом получается из ограничения на глубину дерева. В XGBoost деревья «стремятся» быть симметричными по глубине, и в идеале получается полное бинарное дерево, если это не противоречит другим ограничениям (например, ограничению на минимальное количество объектов в листе). Такие деревья обычно являются более устойчивыми к переобучению.

![tree](https://yastatic.net/s3/education-portal/media/tree_xgboost_e0caf19935_5029e855a6.webp)

CatBoost строит деревья по принципу: «Все вершины одного уровня имеют одинаковый предикат». Одинаковые сплиты во всех вершинах одного уровня позволяют избавиться от ветвлений (конструкций if-else) в коде инференса модели с помощью битовых операций и получить более эффективный код, который в разы ускоряет применение модели, в особенности в случае применения на батчах.

Кроме этого, такое ограничение на форму дерева выступает в качестве сильной регуляризации, что делает модель более устойчивой к переобучению. Основной критерий остановки, как и в случае XGBoost, — ограничение на глубину дерева. Однако, в отличие от XGBoost, в CatBoost всегда создаются полные бинарные деревья, несмотря на то, что в некоторые поддеревья может не попасть ни одного объекта из обучающей выборки.

![tree](https://yastatic.net/s3/education-portal/media/tree_catboost_d501cbfe52_d44e880b4e.webp)

### Где используется градиентный бустинг

Если коротко — везде.

Сегодня это один из двух главных подходов, которые используются на практике (второй — это нейронные сети, конечно). Формально градиентный бустинг слабее и менее гибок, чем сети, но выигрывает в простоте настройки темпа обучения и применения, размере и интерпретируемости модели.

Во многих компаниях, так или иначе связанных с ML, он используется для всех задач, которые не связаны с однородными данными (картинками, текстами, и так далее). Типичный поисковый запрос в Яндексе, выбор отеля на Booking.com или сериала на вечер в Netflix задействует несколько десятков моделей GBDT.

Впрочем, в будущем можно ожидать плавного исчезновения этого подхода, так как улучшение архитектур глубинного обучения и дальнейшее развитие железа нивелирует его преимущество по сравнению с нейросетями.

## Почитать по теме

* [Серия блог-постов](https://explained.ai/gradient-boosting/) о градиентном бустинге от Terence Parr and Jeremy Howard
* [Раздел документации](https://scikit-learn.org/stable/modules/ensemble.html#mathematical-formulation) sklearn с теоретическими выкладками для градиентного бустинга

  ## handbook

  Учебник по машинному обучению

  ## title

  Градиентный бустинг

  ## description

  Как устроено самое мощное семейство не-нейросетевых моделей: градиентный бустинг над решающими деревьями

- 
  ## path

  /handbook/ml/article/metriki-klassifikacii-i-regressii

  ## content

  <blockquote>
    <p>Гораздо легче что-то измерить, чем понять, что именно вы измеряете</p>
    <cite>
      <b>Джон Уильям Салливан</b>
    </cite>
</blockquote>

Задачи машинного обучения с учителем, как правило, состоят в восстановлении зависимости между парами (признаковое описание, целевая переменная) по данным, доступным нам для анализа. Алгоритмы машинного обучения (learning algorithm), со многими из которых вы уже успели познакомиться, позволяют построить модель, аппроксимирующую эту зависимость. Но как понять, насколько качественной получилась аппроксимация?

Почти наверняка наша модель будет ошибаться на некоторых объектах: будь она даже идеальной, шум или выбросы в тестовых данных всё испортят. При этом разные модели будут ошибаться на разных объектах и в разной степени. Задача специалиста по машинному обучению — подобрать подходящий критерий, который позволит сравнивать различные модели.

**Важно**: качество модели нельзя оценивать на обучающей выборке. Как минимум, это стоит делать на отложенной (тестовой) выборке, но если вам это позволяют время и вычислительные ресурсы, стоит прибегнуть и к более надёжным способам проверки — например, *кросс-валидации* (о ней мы поговорим в следующем параграфе).

## Выбор метрик в реальных задачах

Возможно, вы уже участвовали в соревнованиях по анализу данных. На таких соревнованиях **метрику** (критерий качества модели) организатор выбирает за вас, и она, как правило, довольно понятным образом связана с результатами предсказаний. Но на практике всё бывает намного сложнее.

Например, мы хотим:

* решить, сколько коробок с бананами нужно завтра привезти в конкретный магазин — чтобы предложение соответствовало спросу, и не пришлось выбрасывать излишки;
* увеличить счастье пользователя от работы с сервисом, чтобы он стал лояльным и приносил стабильный прогнозируемый доход;
* решить, нужно ли направить человека на дополнительное обследование.

В каждом конкретном случае может возникать целая иерархия метрик. Представим, например, что речь идёт о стриминговом музыкальном сервисе, пользователей которого мы решили порадовать сгенерированными самодельной нейросетью треками — не защищёнными авторским правом, а потому совершенно бесплатными.

Иерархия метрик могла бы иметь такой вид:

1. Самый верхний уровень: будущий доход сервиса — невозможно измерить в моменте, сложным образом зависит от совокупности всех наших усилий;
2. Медианная длина сессии, возможно, служащая оценкой радости пользователей, которая, как мы надеемся, повлияет на их желание продолжать платить за подписку — её нам придётся измерять в продакшене, ведь нас интересует реакция настоящих пользователей на новшество;
3. Доля удовлетворённых качеством сгенерированной музыки асессоров, на которых мы потестируем её до того, как выставить на суд пользователей;
4. Функция потерь, на которую мы будем обучать генеративную сеть.

На этом примере мы можем заметить сразу несколько общих закономерностей. Во-первых, метрики бывают **offline** и **online** (**оффлайновыми** и **онлайновыми**). Online-метрики вычисляются по данным, собираемым с работающей системы (например, медианная длина сессии). Offline-метрики могут быть измерены до введения модели в эксплуатацию, например, по историческим данным или с привлечением специальных людей, асессоров.

Последнее часто применяется, когда метрика — это реакция живого человека: скажем, так поступают поисковые компании, которые предлагают людям оценить качество ранжирования экспериментальной системы ещё до того, как рядовые пользователи увидят эти результаты в обычном порядке. На самом же нижнем этаже иерархии лежат оптимизируемые в ходе обучения функции потерь.

В данном разделе нас будут интересовать offline-метрики, которые могут быть измерены без привлечения людей.

## Функция потерь $\neq$ метрика качества

Как мы узнали ранее, методы обучения реализуют разные подходы к обучению:

* обучение на основе прироста информации (как в деревьях решений);
* обучение на основе сходства (как в методах ближайших соседей);
* обучение на основе вероятностной модели данных (например, максимизацией правдоподобия);
* обучение на основе ошибок (минимизация эмпирического риска).

И в рамках обучения на основе минимизации ошибок мы уже отвечали на вопрос: как можно штрафовать модель за предсказание на обучающем объекте.

Во время сведения задачи о построении решающего правила к задаче численной оптимизации, мы вводили понятие функции потерь и, обычно, объявляли целевой функцией сумму потерь от предсказаний на всех объектах обучающей выборки.

Важно понимать разницу между функцией потерь и метрикой качества. Её можно сформулировать следующим образом:

* Функция потерь возникает в тот момент, когда мы сводим задачу построения модели к задаче оптимизации. Обычно требуется, чтобы она обладала хорошими свойствами (например, дифференцируемостью).

* Метрика — внешний, объективный критерий качества, обычно зависящий не от параметров модели, а только от предсказанных меток.

В некоторых случаях метрика может совпадать с функцией потерь. Например, в задаче регрессии MSE играют роль как функции потерь, так и метрики. Но, скажем, в задаче бинарной классификации они почти всегда различаются: в качестве функции потерь может выступать кросс-энтропия, а в качестве метрики — **число верно угаданных меток** (**accuracy**). Отметим, что в последнем примере у них различные аргументы: на вход кросс-энтропии нужно подавать логиты, а на вход accuracy — предсказанные метки (то есть по сути argmax логитов).

## Бинарная классификация: метки классов

Перейдём к обзору метрик и начнём с самой простой разновидности классификации — бинарной, а затем постепенно будем наращивать сложность.

Напомним постановку задачи бинарной классификации: нам нужно по обучающей выборке $\\{(x_i, y_i)\\}_{i=1}^N$, где $y_i\in\\{0, 1\\}$ построить модель, которая по объекту $x$ предсказывает метку класса $f(x)\in\\{0, 1\\}$.

Первый критерий качества, который приходит в голову, — **accuracy**, то есть доля объектов, для которых мы правильно предсказали класс:

$$\color{#348FEA}{\text{Accuracy}(y, y^{pred}) = \frac{1}{N} \sum_{i=1}^N \mathbb{I}[y_i = f(x_i)]} 
$$

Или же сопряженная ей метрика — **доля ошибочных классификаций** (**error rate**):

$$\text{Error rate} = 1 - \text{Accuracy}
$$

Познакомившись чуть внимательнее с этой метрикой, можно заметить, что у неё есть несколько недостатков:

* она не учитывает дисбаланс классов. Например, в задаче диагностики редких заболеваний классификатор, предсказывающий всем пациентам отсутствие болезни будет иметь достаточно высокую accuracy просто потому, что больных людей в выборке намного меньше;
* она также не учитывает цену ошибки на объектах разных классов. Для примера снова можно привести задачу медицинской диагностики: если ошибочный положительный диагноз для здорового больного обернётся лишь ещё одним обследованием, то ошибочно отрицательный вердикт может повлечь роковые последствия.

### Confusion matrix (матрица ошибок)

Исторически задача бинарной классификации — это задача об обнаружении чего-то редкого в большом потоке объектов, например, поиск человека, больного туберкулёзом, по флюорографии. Или задача признания пятна на экране приёмника радиолокационной станции бомбардировщиком, представляющем угрозу охраняемому объекту (в противовес стае гусей).

Поэтому класс, который представляет для нас интерес, называется «положительным», а оставшийся — «отрицательным».

Заметим, что для каждого объекта в выборке возможно 4 ситуации:

* мы предсказали *положительную* метку и *угадали*.  Будет относить такие объекты к **true positive** (**TP**) группе. True — потому что предсказали мы правильно, а positive — потому что предсказали положительную метку;
* мы предсказали *положительную* метку, но *ошиблись* в своём предсказании — **false positive** (**FP**). False, потому что предсказание было неправильным;
* мы предсказали *отрицательную* метку и *угадали* — **true negative** (**TN**);
* и наконец, мы предсказали *отрицательную* метку, но *ошиблись* — **false negative** (**FN**).
  Для удобства все эти 4 числа изображают в виде таблицы, которую называют **confusion matrix** (**матрицей ошибок**):

![6](https://yastatic.net/s3/education-portal/media/6_1_30cbee6683_c3501f5f67.webp)

Не волнуйтесь, если первое время эти обозначения будут сводить вас с ума (будем откровенны, даже профи со стажем в них порой путаются), однако логика за ними достаточно простая: первая часть названия группы показывает угадали ли мы с классом, а вторая — какой класс мы предсказали.

![6](https://yastatic.net/s3/education-portal/media/6_2_32713f12dd_1b17ad9844.webp)

**Пример**

Попробуем воспользоваться введёнными метриками в боевом примере: сравним работу нескольких моделей классификации на [Breast cancer wisconsin (diagnostic) dataset](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+\(Diagnostic\)).

Объекты выборки — фотографии биопсии грудных опухолей. С их помощью было сформировано признаковое описание, которое заключается в характеристиках ядер клеток (таких как радиус ядра, его текстура, симметричность). Положительным классом в такой постановке будут злокачественные опухоли, а отрицательным — доброкачественные.

**Модель 1. Константное предсказание**

Решение задачи начнём с самого простого классификатора, который выдаёт на каждом объекте константное предсказание — самый часто встречающийся класс.

{% cut "Зачем вообще замерять качество на такой модели?" %}

При разработке модели машинного обучения для проекта всегда желательно иметь некоторую baseline модель. Так нам будет легче проконтролировать, что наша более сложная модель действительно даёт нам прирост качества.

{% endcut %}

```python
from sklearn.datasets 
import load_breast_cancer 
the_data = load_breast_cancer()    

# 0 — «доброкачественный» 
# 1 — «злокачественный» 
relabeled_target = 1 - the_data["target"] 

from sklearn.model_selection import train_test_split 
X = the_data["data"] 
y = relabeled_target 
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) 

from sklearn.dummy import DummyClassifier 
dc_mf = DummyClassifier(strategy="most_frequent") 
dc_mf.fit(X_train, y_train) 

from sklearn.metrics import confusion_matrix 
y_true = y_test y_pred = dc_mf.predict(X_test) 
dc_mf_tn, dc_mf_fp, dc_mf_fn, dc_mf_tp = confusion_matrix(y_true, y_pred, labels = [0, 1]).ravel() 
```

#|
||


|

Прогнозируемый    класс \+

|

Прогнозируемый класс -

||
||

Истинный класс \+

|

TP = 0

|

FN = 53

||
||

Истинный класс -

|

FP = 0

|

TN = 90

||
|#

Обучающие данные таковы, что наш dummy-классификатор все объекты записывает в отрицательный класс, то есть признаёт все опухоли доброкачественными. Такой наивный подход позволяет нам получить минимальный штраф за FP (действительно, нельзя ошибиться в предсказании, если положительный класс вообще не предсказывается), но и максимальный штраф за FN (в эту группу попадут все злокачественные опухоли).

**Модель 2. Случайный лес.**

Настало время воспользоваться всем арсеналом моделей машинного обучения, и начнём мы со случайного леса.

```python
from sklearn.ensemble import RandomForestClassifier 
rfc = RandomForestClassifier()       
rfc.fit(X_train, y_train)       
y_true = y_test       
y_pred = rfc.predict(X_test)       
rfc_tn, rfc_fp, rfc_fn, rfc_tp = confusion_matrix(y_true, y_pred, labels = [0, 1]).ravel()
```

#|
||


|

Прогнозируемый класс \+

|

Прогнозируемый класс -

||
||

Истинный класс \+

|

TP = 52

|

FN = 1

||
||

Истинный класс -

|

FP = 4

|

TN = 86

||
|#

Можно сказать, что этот классификатор чему-то научился, так как главная диагональ матрицы стала содержать все объекты из отложенной выборки, за исключением 4 \+ 1 = 5 объектов (сравните с 0 \+ 53 объектами dummy-классификатора, все опухоли объявляющего доброкачественными).

Отметим, что вычисляя долю недиагональных элементов, мы приходим к метрике **error rate**, о которой мы говорили в самом начале:

$$\text{Error rate} = \frac{FP + FN}{ TP + TN + FP + FN}
$$

тогда как доля объектов, попавших на главную диагональ — это как раз таки accuracy:

$$\text{Accuracy} = \frac{TP + TN}{ TP + TN + FP + FN}
$$

**Модель 3. Метод опорных векторов.**

Давайте построим еще один классификатор на основе линейного метода опорных векторов.

**Важно**: Не забудьте привести признаки к единому масштабу, иначе численный алгоритм не сойдется к решению и мы получим гораздо более плохо работающее решающее правило. Попробуйте проделать это упражнение.

```python
from sklearn.svm import LinearSVC
from sklearn.preprocessing import StandardScaler 
ss = StandardScaler() ss.fit(X_train) 
scaled_linsvc = LinearSVC(C=0.01,random_state=42) 
scaled_linsvc.fit(ss.transform(X_train), y_train) 
y_true = y_test 
y_pred = scaled_linsvc.predict(ss.transform(X_test)) 
tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels = [0, 1]).ravel() 
```

#|
||


|

Прогнозируемый класс \+

|

Прогнозируемый класс -

||
||

Истинный класс \+

|

TP = 50

|

FN = 3

||
||

Истинный класс -

|

FP = 1

|

TN = 89

||
|#

**Сравним результаты**

Легко заметить, что каждая из двух моделей лучше классификатора-пустышки, однако давайте попробуем сравнить их между собой. С точки зрения *error rate* модели практически одинаковы: 5/143 для леса против 4/143 для SVM.

Посмотрим на структуру ошибок чуть более внимательно: лес — (FP = 4, FN = 1), SVM — (FP = 1, FN = 3). Какая из моделей предпочтительнее?

**Замечание**: Мы сравниваем несколько классификаторов на основании их предсказаний на отложенной выборке. Насколько ошибки данных классификаторов зависят от разбиения исходного набора данных? Иногда в процессе оценки качества мы будем получать модели, чьи показатели эффективности будут статистически неразличимыми.

Пусть мы учли предыдущее замечание и эти модели действительно статистически значимо ошибаются в разную сторону. Мы встретились с очевидной вещью: на матрицах нет отношения порядка. Когда мы сравнивали dummy-классификатор и случайный лес с помощью Accuracy, мы всю сложную структуру ошибок свели к одному числу, так как на вещественных числах отношение порядка есть. Сводить оценку модели к одному числу очень удобно, однако не стоит забывать, что у вашей модели есть много аспектов качества.

Что же всё-таки важнее уменьшить: FP или FN? Вернёмся к задаче:

* FP — доля доброкачественных опухолей, которым ошибочно присваивается метка злокачественной;
* FN — доля злокачественных опухолей, которые классификатор пропускает.

В такой постановке становится понятно, что при сравнении выиграет модель с меньшим FN (то есть лес в нашем примере), ведь каждая не обнаруженная опухоль может стоить человеческой жизни.

Рассмотрим теперь другую задачу: по данным о погоде предсказать, будет ли успешным запуск спутника. FN в такой постановке — это ошибочное предсказание неуспеха, то есть не более, чем упущенный шанс (если вас, конечно не уволят за срыв сроков). С FP всё серьёзней: если вы предскажете удачный запуск спутника, а на деле он потерпит крушение из-за погодных условий, то ваши потери будут в разы существеннее.

Итак, из примеров мы видим, что в текущем виде введенная нами **доля ошибочных классификаций** не даст нам возможности учесть неравную важность FP и FN. Поэтому введем две новые метрики: точность и полноту.

### Точность и полнота

Accuracy - это метрика, которая характеризует качество модели, агрегированное по всем классам. Это полезно, когда классы для нас имеют одинаковое значение. В случае, если это не так, accuracy может быть обманчивой.

Рассмотрим ситуацию, когда положительный класс это событие редкое. Возьмем в качестве примера поисковую систему - в нашем хранилище хранятся миллиарды документов, а релевантных к конкретному поисковому запросу на несколько порядков меньше.

Пусть мы хотим решить задачу бинарной классификации «документ d релевантен по запросу q». Благодаря большому дисбалансу, Accuracy dummy-классификатора, объявляющего все документы нерелевантными, будет близка к единице. Напомним, что $\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}$, и в нашем случае высокое значение метрики будет обеспечено членом TN, в то время для пользователей более важен высокий TP.

Поэтому в случае ассиметрии классов, можно использовать метрики, которые не учитывают TN и ориентируются на TP.

Если мы рассмотрим долю правильно предсказанных положительных объектов среди всех объектов, предсказанных положительным классом, то мы получим метрику, которая называется **точностью (precision)**

$$\color{#348FEA}{\text{Precision} = \frac{TP}{TP + FP}}
$$

Интуитивно метрика показывает долю релевантных документов среди всех найденных классификатором. Чем меньше ложноположительных срабатываний будет допускать модель, тем больше будет её Precision.

Если же мы рассмотрим долю правильно найденных положительных объектов среди всех объектов положительного класса, то мы получим метрику, которая называется **полнотой (recall)**

$$\color{#348FEA}{\text{Recall} = \frac{TP}{TP + FN}}
$$

Интуитивно метрика показывает долю найденных документов из всех релевантных. Чем меньше ложно отрицательных срабатываний, тем выше recall модели.

Например, в задаче предсказания злокачественности опухоли точность показывает, сколько из определённых нами как злокачественные опухолей действительно злокачественные, а полнота — какую долю злокачественных опухолей нам удалось выявить.

Хорошее понимание происходящего даёт следующая картинка:

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/6_3_c4ba781675_908f105eb3.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://en.wikipedia.org/wiki/Precision_and_recall">Источник</a>
  </figcaption>
</figure>

### Recall@k, Precision@k

Метрики Recall и Precision хорошо подходят для задачи поиска «документ d релевантен запросу q», когда из списка рекомендованных алгоритмом документов нас интересует только первый. Но не всегда алгоритм машинного обучения вынужден работать в таких жестких условиях. Может быть такое, что вполне достаточно, что релевантный документ попал в первые k рекомендованных.

Например, в интерфейсе выдачи первые три подсказки видны всегда одновременно и вообще не очень понятно, какой у них порядок. Тогда более честной оценкой качества алгоритма будет «в выдаче D размера k по запросу q нашлись релевантные документы». Для расчёта метрики по всей выборке объединим все выдачи и рассчитаем precision, recall как обычно подокументно.

### F1-мера

Как мы уже отмечали ранее, модели очень удобно сравнивать, когда их качество выражено одним числом. В случае пары Precision-Recall существует популярный способ скомпоновать их в одну метрику - взять их среднее гармоническое. Данный показатель эффективности исторически носит название **F1-меры (F1-measure)**.

$$\color{#348FEA}{F_1 = \frac{2}{\frac{1}{Recall} + \frac{1}{Precision}}} = 
$$

$$ = 2 \frac{Recall \cdot Precision }{Recall + Precision} = \frac
{TP} {TP + \frac{FP + FN}{2}}
$$

Стоит иметь в виду, что F1-мера предполагает одинаковую важность Precision и Recall, если одна из этих метрик для вас приоритетнее, то можно воспользоваться $F_{\beta}$ мерой:

$$F_{\beta} = (\beta^2 + 1) \frac{Recall \cdot Precision }{Recall + \beta^2Precision}
$$

## Бинарная классификация: вероятности классов

Многие модели бинарной классификации устроены так, что класс объекта получается бинаризацией выхода классификатора по некоторому фиксированному порогу:

$$f\left(x ; w, w_{0}\right)=\mathbb{I}\left[g(x, w) > w_{0}\right].
$$

Например, модель логистической регрессии возвращает оценку вероятности принадлежности примера к положительному классу. Другие модели бинарной классификации обычно возвращают произвольные вещественные значения, но существуют техники, называемые [калибровкой классификатора](https://academy.yandex.ru/handbook/ml/article/kak-ocenivat-veroyatnosti), которые позволяют преобразовать предсказания в более или менее корректную оценку вероятности принадлежности к положительному классу.

Как оценить качество предсказываемых вероятностей, если именно они являются нашей конечной целью? Общепринятой мерой является логистическая функция потерь, которую мы изучали раньше, когда говорили об устройстве некоторых методов классификации (например уже упоминавшейся логистической регрессии).

Если же нашей целью является построение прогноза в терминах метки класса, то нам нужно учесть, что в зависимости от порога мы будем получать разные предсказания и разное качество на отложенной выборке. Так, чем ниже порог отсечения, тем больше объектов модель будет относить к положительному классу. Как в этом случае оценить качество модели?

### AUC

Пусть мы хотим учитывать ошибки на объектах обоих классов. При уменьшении порога отсечения мы будем находить (правильно предсказывать) всё большее число положительных объектов, но также и неправильно предсказывать положительную метку на всё большем числе отрицательных объектов. Естественным кажется ввести две метрики **TPR** и **FPR**:

**TPR** (**true positive rate**) — это полнота, доля положительных объектов, правильно предсказанных положительными:

$$TPR = \frac{TP}{P} = \frac{TP}{TP + FN} 
$$

**FPR** (**false positive rate**) — это доля отрицательных объектов, неправильно предсказанных положительными:

$$FPR = \frac{FP}{N} = \frac{FP}{FP + TN}
$$

Обе эти величины растут при уменьшении порога. Кривая в осях TPR/FPR, которая получается при варьировании порога, исторически называется **ROC-кривой** (**receiver operating characteristics curve**, сокращённо **ROC curve**). Следующий [интерактивный график](https://yastatic.net/s3/academy/ml/roc_auc/roc.html) поможет вам понять поведение ROC-кривой.

Желтая и синяя кривые показывают распределение предсказаний классификатора на объектах положительного и отрицательного классов соответственно. То есть значения на оси X (на графике с двумя гауссианами) мы получаем из классификатора.

* Если классификатор идеальный, — две кривые разделимы по оси X, — то на правом графике мы получаем ROC-кривую (0,0)-\>(0,1)-\>(1,1), площадь под которой равна 1.

* Если классификатор случайный (предсказывает одинаковые метки положительным и отрицательным объектам), то мы получаем ROC-кривую (0,0)-\>(1,1), площадь под которой равна 0.5.

Поэкспериментируйте с разными вариантами распределения предсказаний по классам и посмотрите, как меняется ROC-кривая.

Чем лучше классификатор разделяет два класса, тем больше площадь (*area under curve*) под ROC-кривой — и мы можем использовать её в качестве метрики. Эта метрика называется **AUC** и она работает благодаря следующему свойству ROC-кривой:

**AUC** равен доле пар объектов вида (объект класса 1, объект класса 0), которые алгоритм верно упорядочил, то есть предсказание классификатора на первом объекте больше:

$$\color{#348FEA}{\operatorname{AUC} = \frac{\sum\limits_{i = 1}^{N} \sum\limits_{j = 1}^{N}\mathbb{I}[y_i < y_j] I^{\prime}[f(x_{i}) < f(x_{j})]}{\sum\limits_{i = 1}^{N} \sum\limits_{j = 1}^{N}\mathbb{I}[y_i < y_j]}}
$$

$$I^{\prime}\left[f(x_{i}) < f(x_{j})\right]=
\left\{
  \begin{array}{ll}
    0, & f(x_{i}) > f(x_{j}) \\
    0.5 & f(x_{i}) = f(x_{j}) \\
    1, & f(x_{i}) < f(x_{j})
  \end{array}
\right.
$$

$$I\left[y_{i}< y_{j}\right]=
\left\{
  \begin{array}{ll}
    0, & y_{i} \geq y_{j} \\
    1, & y_{i} < y_{j}
  \end{array}
\right.
$$

Чтобы детальнее разобраться, почему это так, советуем вам обратиться к [материалам А.Г.Дьяконова](https://dyakonov.org/2017/07/28/auc-roc-%D0%BF%D0%BB%D0%BE%D1%89%D0%B0%D0%B4%D1%8C-%D0%BF%D0%BE%D0%B4-%D0%BA%D1%80%D0%B8%D0%B2%D0%BE%D0%B9-%D0%BE%D1%88%D0%B8%D0%B1%D0%BE%D0%BA/).

В каких случаях лучше отдать предпочтение этой метрике? Рассмотрим следующую задачу: некоторый сотовый оператор хочет научиться предсказывать, будет ли клиент пользоваться его услугами через месяц. На первый взгляд кажется, что задача сводится к бинарной классификации с метками 1, если клиент останется с компанией и $0$ — иначе.

Однако если копнуть глубже в процессы компании, то окажется, что такие метки практически бесполезны. Компании скорее интересно упорядочить клиентов по вероятности прекращения обслуживания и в зависимости от этого применять разные варианты удержания: кому-то прислать скидочный купон от партнёра, кому-то предложить скидку на следующий месяц, а кому-то и новый тариф на особых условиях.

Таким образом, в любой задаче, где нам важна не метка сама по себе, а правильный порядок на объектах, имеет смысл применять AUC.

Утверждение выше может вызывать у вас желание использовать AUC в качестве метрики в задачах ранжирования, но мы призываем вас быть аккуратными.

{% cut "Подробнее" %}

Продемонстрируем это на следующем примере: пусть наша выборка состоит из $9100$ объектов класса $0$ и $10$ объектов класса $1$, и модель расположила их следующим образом:

$$\underbrace{0 \dots 0}_{9000} ~ \underbrace{1 \dots 1}_{10} ~ \underbrace{0 \dots 0}_{100}
$$

Тогда AUC будет близка к единице: количество пар правильно расположенных объектов будет порядка $90000$, в то время как общее количество пар порядка $91000$.

Однако самыми высокими по вероятности положительного класса будут совсем не те объекты, которые мы ожидаем.

{% endcut %}

### Average Precision

Будем постепенно уменьшать порог бинаризации. При этом полнота будет расти от $0$ до $1$, так как будет увеличиваться количество объектов, которым мы приписываем положительный класс (а количество объектов, на самом деле относящихся к положительному классу, очевидно, меняться не будет).

Про точность же нельзя сказать ничего определённого, но мы понимаем, что скорее всего она будет выше при более высоком пороге отсечения (мы оставим только объекты, в которых модель «уверена» больше всего). Варьируя порог и пересчитывая значения Precision и Recall на каждом пороге, мы получим некоторую кривую примерно следующего вида:

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/6_4_99f6621d79_48bf9f5797.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html">Источник</a>
  </figcaption>
</figure>

Рассмотрим среднее значение точности (оно равно площади под кривой точность-полнота):

$$\text { AP }=\int_{0}^{1} p(r) d r
$$

Получим показатель эффективности, который называется **average precision**. Как в случае матрицы ошибок мы переходили к скалярным показателям эффективности, так и в случае с кривой точность-полнота мы охарактеризовали ее в виде числа.

## Многоклассовая классификация

Если классов становится больше двух, расчёт метрик усложняется. Если задача классификации на $K$ классов ставится как $K$ задач об отделении класса $i$ от остальных ($i=1,\ldots,K$), то для каждой из них можно посчитать свою матрицу ошибок. Затем есть два варианта получения итогового значения метрики из $K$ матриц ошибок:

1. Усредняем элементы матрицы ошибок (TP, FP, TN, FN) между бинарными классификаторами, например $TP = \frac{1}{K}\sum_{i=1}^{K}TP_i$. Затем по одной усреднённой матрице ошибок считаем Precision, Recall, F-меру. Это называют **микроусреднением**.
2. Считаем Precision, Recall для каждого классификатора отдельно, а потом усредняем. Это называют **макроусреднением**.

Порядок усреднения влияет на результат в случае дисбаланса классов. Показатели TP, FP, FN — это счётчики объектов. Пусть некоторый класс обладает маленькой мощностью (обозначим её $M$). Тогда значения TP и FN при классификации этого класса против остальных будут не больше $M$, то есть тоже маленькие. Про FP мы ничего уверенно сказать не можем, но скорее всего при дисбалансе классов классификатор не будет предсказывать редкий класс слишком часто, потому что есть большая вероятность ошибиться. Так что FP тоже мало. Поэтому усреднение первым способом сделает вклад маленького класса в общую метрику незаметным. А при усреднении вторым способом среднее считается уже для нормированных величин, так что вклад каждого класса будет одинаковым.

Рассмотрим пример. Пусть есть датасет из объектов трёх цветов: желтого, зелёного и синего. Желтого и зелёного цветов почти поровну — 21 и 20 объектов соответственно, а синих объектов всего 4.

![6](https://yastatic.net/s3/education-portal/media/6_5_39d7ed0a4b_4172960c78.webp)

Модель по очереди для каждого цвета пытается отделить объекты этого цвета от объектов оставшихся двух цветов. Результаты классификации проиллюстрированы матрицей ошибок. Модель «покрасила» в жёлтый 25 объектов, 20 из которых были действительно жёлтыми (левый столбец матрицы). В синий был «покрашен» только один объект, который на самом деле жёлтый (средний столбец матрицы). В зелёный — 19 объектов, все на самом деле зелёные (правый столбец матрицы).

![6](https://yastatic.net/s3/education-portal/media/6_6_345d577f40_4611fec57e.webp)

Посчитаем Precision классификации двумя способами:

1. С помощью микроусреднения получаем

$$\text{Precision} = \frac{\dfrac{1}{3}\left(20 + 0 + 19\right)}{\dfrac{1}{3}\left(20 + 0 + 19\right) + \dfrac{1}{3}\left(5 + 1 + 0\right)} = 0.87
$$

2. С помощью макроусреднения получаем

$$\text{Precision} = \dfrac{1}{3}\left( \frac{20}{20 + 5} + \frac{0}{0 + 1} + \frac{19}{19 + 0}\right) = 0.6
$$

Видим, что макроусреднение лучше отражает тот факт, что синий цвет, которого в датасете было совсем мало, модель практически игнорирует.

## Как оптимизировать метрики классификации?

Пусть мы выбрали, что метрика качества алгоритма будет $F(a(X), Y)$. Тогда мы хотим обучить модель так, чтобы $F$ на валидационной выборке была минимальная/максимальная. Лучший способ добиться минимизации метрики $F$ — оптимизировать её напрямую, то есть выбрать в качестве функции потерь ту же $F(a(X), Y)$. К сожалению, это не всегда возможно. Рассмотрим, как оптимизировать метрики иначе.

Метрики precision и recall невозможно оптимизировать напрямую, потому что эти метрики нельзя рассчитать на одном объекте, а затем усреднить. Они зависят от того, какими были правильная метка класса и ответ алгоритма на всех объектах. Чтобы понять, как оптимизировать precision, recall, рассмотрим, как расчитать эти метрики на отложенной выборке.
Пусть модель обучена на стандартную для классификации функцию потерь (LogLoss).

Для получения меток класса специалист по машинному обучению сначала применяет на объектах модель и получает вещественные предсказания модели ($p_i \in \left(0, 1\right)$). Затем предсказания бинаризуются по порогу, выбранному специалистом: если предсказание на объекте больше порога, то метка класса 1 (или «положительная»), если меньше — 0 (или «отрицательная»). Рассмотрим, что будет с метриками precision, recall в крайних положениях порога.

**Пусть порог равен нулю**

Тогда всем объектам будет присвоена положительная метка. Следовательно, все объекты будут либо TP, либо FP, потому что отрицательных предсказаний нет, $TP + FP = N$, где $N$ — размер выборки. Также все объекты, у которых метка на самом деле 1, попадут в TP.

По формуле точность $\text{Precision} = \frac{TP}{TP + FP} = \frac1N \sum_{i = 1}^N \mathbb{I} \left[ y_i = 1 \right]$ равна среднему таргету в выборке. А полнота $\text{Recall} = \frac{TP}{TP + FN} = \frac{TP}{TP + 0} = 1$ равна единице.

**Пусть теперь порог равен единице**

Тогда ни один объект не будет назван положительным, $TP = FP = 0$. Все объекты с меткой класса 1 попадут в FN. Если есть хотя бы один такой объект, то есть $FN \ne 0$, будет верна формула $\text{Recall} = \frac{TP}{TP + FN} = \frac{0}{0+ FN} = 0$.

То есть при пороге единица, полнота равна нулю. Теперь посмотрим на точность. Формула для Precision состоит только из счётчиков положительных ответов модели (TP, FP). При единичном пороге они оба равны нулю, $\text{Precision} = \frac{TP}{TP + FP} = \frac{0}{0 + 0}$то есть при единичном пороге точность неопределена. Пусть мы отступили чуть-чуть назад по порогу, чтобы хотя бы несколько объектов были названы моделью положительными.

Скорее всего это будут самые «простые» объекты, которые модель распознает хорошо, потому что её предсказание близко к единице. В этом *предположении* $FP \approx 0$. Тогда точность $\text{Precision} = \frac{TP}{TP + FP} \approx \frac{TP}{TP + 0} \approx 1$ будет близка к единице.

Изменяя порог, между крайними положениями, получим графики Precision и Recall, которые выглядят как-то так:

![6](https://yastatic.net/s3/education-portal/media/6_7_286c6fd4d1_eb7cc4d9a2.webp)

Recall меняется от единицы до нуля, а Precision от среднего тагрета до какого-то другого значения (нет гарантий, что график монотонный).

Итого оптимизация precision и recall происходит так:

1. Модель обучается на стандартную функцию потерь (например, LogLoss).
2. Используя вещественные предсказания на валидационной выборке, перебирая разные пороги от 0 до 1, получаем графики метрик в зависимости от порога.
3. Выбираем нужное сочетание точности и полноты.

Пусть теперь мы хотим максимизировать метрику **AUC**. Стандартный метод оптимизации, градиентный спуск, предполагает, что функция потерь дифференцируема. AUC этим качеством не обладает, то есть мы не можем оптимизировать её напрямую. Поэтому для метрики AUC приходится изменять оптимизационную задачу.

Метрика AUC считает долю верно упорядоченных пар. Значит от исходной выборки можно перейти к выборке упорядоченных пар объектов. На этой выборке ставится задача классификации: метка класса 1 соответствует правильно упорядоченной паре, 0 — неправильно.

Новой метрикой становится accuracy — доля правильно классифицированных объектов, то есть доля правильно упорядоченных пар. Оптимизировать accuracy можно по той же схеме, что и precision, recall: обучаем модель на LogLoss и предсказываем вероятности положительной метки у объекта выборки, считаем accuracy для разных порогов по вероятности и выбираем понравившийся.

## Регрессия

В задачах регрессии целевая метка у нас имеет потенциально бесконечное число значений. И природа этих значений, обычно, связана с каким-то процессом измерений:

* величина температуры в определенный момент времени на метеостанции
* количество прочтений статьи на сайте
* количество проданных бананов в конкретном магазине, сети магазинов или стране
* дебит добывающей скважины на нефтегазовом месторождении за месяц и т.п.

Мы видим, что иногда метка это целое число, а иногда произвольное вещественное число. Обычно случаи целочисленных меток моделируют так, словно это просто обычное вещественное число. При таком подходе может оказаться так, что модель A лучше модели B по некоторой метрике, но при этом предсказания у модели A могут быть не целыми. Если в бизнес-задаче ожидается именно целочисленный ответ, то и оценивать нужно огрубление.

Общая рекомендация такова: оценивайте весь каскад решающих правил: и те «внутренние», которые вы получаете в результате обучения, и те «итоговые», которые вы отдаёте бизнес-заказчику.

Например, вы можете быть удовлетворены, что стали ошибаться не во втором, а только в третьем знаке после запятой при предсказании погоды. Но сами погодные данные измеряются с точностью до десятых долей градуса, а пользователь и вовсе может интересоваться лишь целым числом градусов.

Итак, напомним постановку задачи регрессии: нам нужно по обучающей выборке $\{(x_i, y_i)\}_{i=1}^N$, где $y_i \in \mathbb{R}$ построить модель f(x).

Величину $e_i = f(x_i) - y_i$ называют ошибкой на объекте i или регрессионным остатком.

Весь набор ошибок на отложенной выборке может служить аналогом матрицы ошибок из задачи классификации. А именно, когда мы рассматриваем две разные модели, то, глядя на то, как и на каких объектах они ошиблись, мы можем прийти к выводу, что для решения бизнес-задачи нам выгоднее взять ту или иную модель. И, аналогично со случаем бинарной классификации, мы можем начать строить агрегаты от вектора ошибок, получая тем самым разные метрики.

### MSE, RMSE, $R^2$

MSE — одна из самых популярных метрик в задаче регрессии. Она уже знакома вам, так как применяется в качестве функции потерь (или входит в ее состав) во многих ранее рассмотренных методах.

$$MSE(y^{true}, y^{pred}) = \frac1N\sum_{i=1}^{N} (y_i - f(x_i))^2 
$$

Иногда для того, чтобы показатель эффективности MSE имел размерность исходных данных, из него извлекают квадратный корень и получают показатель эффективности RMSE.

MSE неограничен сверху, и может быть нелегко понять, насколько «хорошим» или «плохим» является то или иное его значение. Чтобы появились какие-то ориентиры, делают следующее:

* Берут наилучшее константное предсказание с точки зрения MSE — среднее арифметическое меток $\bar{y}$. При этом чтобы не было подглядывания в test, среднее нужно вычислять по обучающей выборке

* Рассматривают в качестве показателя ошибки:

  $$R^2 = 1 - \frac{\sum_{i=1}^{N} (y_i - f(x_i))^2}{\sum_{i=1}^{N} (y_i - \bar{y})^2}.
  $$
  
  У идеального решающего правила $R^2$ равен $1$, у наилучшего константного предсказания он равен $0$ на обучающей выборке. Можно заметить, что $R^2$ показывает, какая доля дисперсии таргетов (знаменатель) объяснена моделью.

MSE квадратично штрафует за большие ошибки на объектах. Мы уже видели проявление этого при обучении моделей методом минимизации квадратичных ошибок — там это проявлялось в том, что модель старалась хорошо подстроиться под выбросы.

Пусть теперь мы хотим использовать MSE для оценки наших регрессионных моделей. Если большие ошибки для нас действительно неприемлемы, то квадратичный штраф за них — очень полезное свойство (и его даже можно усиливать, повышая степень, в которую мы возводим ошибку на объекте). Однако если в наших тестовых данных присутствуют выбросы, то нам будет сложно объективно сравнить модели между собой: ошибки на выбросах будет маскировать различия в ошибках на основном множестве объектов.

Таким образом, если мы будем сравнивать две модели при помощи MSE, у нас будет выигрывать та модель, у которой меньше ошибка на объектах-выбросах, а это, скорее всего, не то, чего требует от нас наша бизнес-задача.

{% cut "История из жизни про бананы и квадратичный штраф за ошибку" %}

Из-за неверно введенных данных метка одного из объектов оказалась в 100 раз больше реального значения. Моделировалась величина при помощи градиентного бустинга над деревьями решений. Функция потерь была MSE.

Однажды уже во время эксплуатации случилось ЧП: у нас появились предсказания, в 100 раз превышающие допустимые из соображений физического смысла значения. Представьте себе, например, что вместо обычных 4 ящиков бананов система предлагала поставить в магазин 400. Были распечатаны все деревья из ансамбля, и мы увидели, что постепенно число ящиков действительно увеличивалось до прогнозных 400.

Было решено проверить гипотезу, что был выброс в данных для обучения. Так оно и оказалось: всего одна точка давала такую потерю на объекте, что алгоритм обучения решил, что лучше переобучиться под этот выброс, чем смириться с большим штрафом на этом объекте. А в эксплуатации у нас возникли точки, которые плюс-минус попадали в такие же листья ансамбля, что и объект-выброс.

Избежать такого рода проблем можно двумя способами: внимательнее контролируя качество данных или адаптировав функцию потерь.

Аналогично, можно поступать и в случае, когда мы разрабатываем метрику качества: менее жёстко штрафовать за большие отклонения от истинного таргета.

{% endcut %}

### MAE

Использовать RMSE для сравнения моделей на выборках с большим количеством выбросов может быть неудобно. В таких случаях прибегают к также знакомой вам в качестве функции потери метрике **MAE** (**mean absolute error**):

$$MAE(y^{true}, y^{pred}) = \frac{1}{N}\sum_{i=1}^{N} \left|y_i - f(x_i)\right| 
$$

### Метрики, учитывающие относительные ошибки

И MSE и MAE считаются как сумма абсолютных ошибок на объектах.

Рассмотрим следующую задачу: мы хотим спрогнозировать спрос товаров на следующий месяц. Пусть у нас есть два продукта: продукт A продаётся в количестве 100 штук, а продукт В в количестве 10 штук. И пусть базовая модель предсказывает количество продаж продукта A как 98 штук, а продукта B как 8 штук. Ошибки на этих объектах добавляют 4 штрафных единицы в MAE.

И есть 2 модели-кандидата на улучшение. Первая предсказывает товар А 99 штук, а товар B 8 штук. Вторая предсказывает товар А 98 штук, а товар B 9 штук.

Обе модели улучшают MAE базовой модели на 1 единицу. Однако, с точки зрения бизнес-заказчика вторая модель может оказаться предпочтительнее, так как предсказание продажи редких товаров может быть приоритетнее. Один из способов учесть такое требование — рассматривать не абсолютную, а относительную ошибку на объектах.

### MAPE, SMAPE

Когда речь заходит об относительных ошибках, сразу возникает вопрос: что мы будем ставить в знаменатель?

В метрике **MAPE** (**mean absolute percentage error**) в знаменатель помещают целевое значение:

$$MAPE(y^{true}, y^{pred}) = \frac{1}{N} \sum_{i=1}^{N} \frac{ \left|y_i - f(x_i)\right|}{\left|y_i\right|} 
$$

С особым случаем, когда в знаменателе оказывается $0$, обычно поступают «инженерным» способом: или выдают за непредсказание $0$ на таком объекте большой, но фиксированный штраф, или пытаются застраховаться от подобного на уровне формулы и переходят к метрике **SMAPE** (**symmetric mean absolute percentage error**):

$$SMAPE(y^{true}, y^{pred}) = \frac{1}{N} \sum_{i=1}^{N} \frac{ 2 \left|y_i - f(x_i)\right|}{y_i + f(x_i)} 
$$

Если же предсказывается ноль, штраф считаем нулевым.

Таким переходом от абсолютных ошибок на объекте к относительным мы сделали объекты в тестовой выборке равнозначными: даже если мы делаем абсурдно большое предсказание, на фоне которого истинная метка теряется, мы получаем штраф за этот объект порядка 1 в случае MAPE и 2 в случае SMAPE.

### WAPE

Как и любая другая метрика, MAPE имеет свои границы применимости: например, она плохо справляется с прогнозом спроса на товары с прерывистыми продажами. Рассмотрим такой пример:

#|
||


|

Понедельник

|

Вторник

|

Среда

||
||

Прогноз

|

55

|

2

|

50

||
||

Продажи

|

50

|

1

|

50

||
||

MAPE

|

10%

|

100%

|

0%

||
|#

Среднее MAPE — 36.7%, что не очень отражает реальную ситуацию, ведь два дня мы предсказывали с хорошей точностью. В таких ситуациях помогает **WAPE** (**weighted average percentage error**):

$$WAPE(y^{true}, y^{pred}) = \frac{\sum_{i=1}^{N} \left|y_i - f(x_i)\right|}{\sum_{i=1}^{N} \left|y_i\right|} 
$$

Если мы предсказываем идеально, то WAPE = 0, если все предсказания отдаём нулевыми, то WAPE = 1.

В нашем примере получим WAPE = 5.9%

### RMSLE

Альтернативный способ уйти от абсолютных ошибок к относительным предлагает метрика **RMSLE** (**root mean squared logarithmic error**):

$$RMSLE(y^{true}, y^{pred}| c) = \sqrt{ \frac{1}{N} \sum_{i=1}^N \left(\vphantom{\frac12}\log{\left(y_i + c \right)} - \log{\left(f(x_i) + c \right)}\right)^2 } 
$$

где нормировочная константа $c$ вводится искусственно, чтобы не брать логарифм от нуля. Также по построению видно, что метрика пригодна лишь для неотрицательных меток.

### Веса в метриках

Все вышеописанные метрики легко допускают введение весов для объектов. Если мы из каких-то соображений можем определить стоимость ошибки на объекте, можно брать эту величину в качестве веса. Например, в задаче предсказания спроса в качестве веса можно использовать стоимость объекта.

### Доля предсказаний с абсолютными ошибками больше, чем d

Еще одним способом охарактеризовать качество модели в задаче регрессии является доля предсказаний с абсолютными ошибками больше заданного порога $d$:

$$\frac{1}{N} \sum_{i=1}^{N} \mathbb{I}\left[ \left| y_i - f(x_i) \right| > d \right] 
$$

Например, можно считать, что прогноз погоды сбылся, если ошибка предсказания составила меньше 1/2/3 градусов. Тогда рассматриваемая метрика покажет, в какой доле случаев прогноз не сбылся.

## Как оптимизировать метрики регрессии?

Пусть мы выбрали, что метрика качества алгоритма будет $F(a(X), Y)$. Тогда мы хотим обучить модель так, чтобы F на валидационной выборке была минимальная/максимальная. Аналогично задачам классификации лучший способ добиться минимизации метрики $F$ — выбрать в качестве функции потерь ту же $F(a(X), Y)$. К счастью, основные метрики для регрессии: MSE, RMSE, MAE можно оптимизировать напрямую. С формальной точки зрения MAE не дифференцируема, так как там присутствует модуль, чья производная не определена в нуле. На практике для этого выколотого случая в коде можно возвращать ноль.

Для оптимизации MAPE придётся изменять оптимизационную задачу. Оптимизацию MAPE можно представить как оптимизацию MAE, где объектам выборки присвоен вес $\frac{1}{\vert y_i\vert}$.

  ## handbook

  Учебник по машинному обучению

  ## title

  Метрики классификации и регрессии

  ## description

  Как оценить качество модели для классификации или регрессии и почему для разных задач нужны разные метрики

- 
  ## path

  /handbook/ml/article/kross-validaciya

  ## content

  Кросс-валидация — это процедура для оценки качества работы модели, которая широко применяется в машинном обучении. Она помогает сравнить между собой различные модели и выбрать наилучшую для конкретной задачи.

В этом разделе мы рассмотрим наиболее распространённые методы кросс-валидации, а также обсудим возможные проблемы, которые могут возникнуть в процессе их применения.

## Hold-out

Метод **hold-out** представляет из себя простое разделение на train и test:

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/7_1_a22d1d013b_d1b3904fa6.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://neptune.ai/blog/cross-validation-in-machine-learning-how-to-do-it-right">Источник</a>
  </figcaption>
</figure>

Такое разделение очень легко реализовать с помощью библиотеки [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html):

```python
import numpy as np
from sklearn.model_selection import train_test_split
 
X, y = np.arange(1000).reshape((500, 2)), np.arange(500)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2, 
    random_state=42
)
```

Чтобы оценить модель, вы обучаете её на тренировочном множестве, а результаты измеряете на тестовом. У sklearn по дефолту выставлен параметр `shuffle=True`, то есть перед разделением на тренировочное и тестовое множества происходит перемешивание семплов (и для воспроизводимости такого разбиения нужно фиксировать `random_state`).

А что будет, если не перемешать данные?

Если обучение модели не зависит от порядка подачи в неё примеров (что верно, например, для k-NN или решающего дерева), то перемешивание данных влияет только на то, кто в итоге окажется в train и test. Если данные шли какими-то группами, например сначала 800 картинок с кошками, а за ними 200 картинок с собаками, а `train_test_split` был совершён в пропорции 0.8, то модель просто не увидит собак в трейне.

А в случае когда модель обучается с помощью градиентного спуска или его вариации (про различные модификации SGD подробно рассказывается в [параграфе](https://academy.yandex.ru/handbook/ml/article/nejronnye-seti) о нейросетях), отсутствие перемешивания данных может влиять более интересным образом.

Вот пример из практики Yandex.Research —  как вы думаете, что не так с графиком обучения данной модели?

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/7_1b_dedbd700bb_e4f6168c56.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://yadi.sk/i/nUiHl4VPMOCz0g">Источник: курс Лены Войты по NLP</a>
  </figcaption>
</figure>

{% cut "Ответ (не открывайте сразу; сначала подумайте сами!)" %}

На графике видна периодичность по числу итераций! По большим пикам можно вычислить места, где проход по данным начался заново. Кроме того, график в конце ползёт вниз, что означает, что модель уже начала переобучаться, выучив последовательность данных на трейне и используя эту информацию больше, чем сами данные.

Если данные перемешать, то график обучения станет таким:

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/7_1c_fc36f7f76b_e7a88f91dd.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://yadi.sk/i/nUiHl4VPMOCz0g">Источник: курс Лены Войты по NLP</a>
  </figcaption>
</figure> 

Можно привести даже более простой пример, когда отсутствие перемешивания данных может вас сильно подвести. Допустим, у вас большой датасет из миллиона кошек и собак и вам нужно научить модель их различать.

Пусть изначальный порядок тренировочных данных такой: сначала подряд идёт полмиллиона картинок с кошками, а затем так же подряд идут картинки с собаками. Тогда модель на первой половине обучения выучит, что на картинке всегда кошка, а за вторую забудет, что учила на первой, и будет всегда предсказывать собак. При этом на сами данные при предсказании она опираться не будет вообще.

{% endcut %}

Продолжим. Если у вас достаточно данных, лучше всегда предусматривать также валидационное множество:

```python
import numpy as np
from sklearn.model_selection import train_test_split
 
X, y = np.arange(1000).reshape((500, 2)), np.arange(500)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2, 
    random_state=42
)
X_train, X_val, y_train, y_val = train_test_split(
    X_train, y_train, 
    test_size=0.1, 
    random_state=42
)
```

Если вы перебираете какие-то модели для вашей задачи, то оптимизировать их качества стоит на валидационном множестве, а окончательное сравнение моделей проводить на тестовом множестве.

Оптимизация качеств модели может включать в себя подбор гиперпараметров, подбор архитектуры (в случае нейросетей) или подбор оптимального трешолда для максимизации значений целевой метрики (например, вы делаете двуклассовую классификацию, а модель выдаёт непрерывные значения от 0 до 1, которые нужно бинаризовать так, чтобы получить максимальный скор по F1) и так далее.

Если же оптимизировать качества моделей и проводить их сравнение на одном и том же множестве, то можно неявно заложить в модели информацию о тестовом множестве и получить результаты хуже ожидаемых на новых данных.

Немного прервёмся на [пример](https://www.johndcook.com/blog/2015/03/17/a-subtle-way-to-over-fit/) — к чему может привести неявное использование моделью тестового множества

Представьте, что вы хотите обучить модель одномерной линейной регрессии для предсказания ваших данных:

$$  y = mx + b,
$$

где $m$ и $b$ — искомые параметры вашей модели.

Однако представьте, что параметр $b$ вам кто-то запретил обучать на тренировочном множестве и для вас у этой модели всего один параметр. Пусть на первой итерации у вас задано какое-то фиксированное $b = b_0$, вы с ним подобрали на трейне лучшее $m$ при данном $b_0$ и замерили качество получившейся модели на тестовом множестве.

На следующей итерации вы взяли новое значение $b = b_1$, повторили с ним предыдущий шаг и так далее. Теперь пришло время выбирать модель, и из всех них вы выбрали ту, которая показала лучший результат на тестовом множестве. Вам может показаться, что ваша модель с **одним параметром** обучена на трейне и всё хорошо, но на самом деле вы использовали оба множества, чтобы обучить модель с **двумя параметрами**, и теперь ваша тестовая оценка качества модели завышена.

Может показаться, что этот пример довольно искусственный, но он на самом деле легко переносится на модели любой сложности. Просто представьте себе, что часть обучаемых весов вашей сложной модели вам запретили обучать на трейне и вы начинаете так же, как и выше, оценивать их на тесте, то есть по факту **учить** на тесте.

А чем такая ситуация отличается от подбора *гиперпараметров* модели (которые вы уже действительно не можете обучить на трейне) сразу на тестовом множестве? Вообще говоря, ничем.

Продолжим. Для окончательного применения найденную лучшую модель можно обучить на всех имеющихся данных. Правда, вы не сможете оценить качество получившейся модели, так как у вас уже не будет тестового множества. Чтобы примерно оценить, как будет вести себя модель при добавлении новых данных, вы можете построить **кривые обучения**: графики качества модели на трейне и на тесте в зависимости от числа поданных семплов на вход.

Кривые обучения могут выглядеть следующим образом (код для отрисовки таких кривых можно [найти](https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html) в документации библиотеки sklearn):

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/7_2_b49c30e479_3a7cee8402.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html">Источник</a>
  </figcaption>
</figure>

Если графики подсказывают, что качество модели по валидационным метрикам продолжает расти, имеет смысл добавить новые данные.

На картинке выше приведены кривые обучения двух моделей на одном и том же датасете. Модель слева показала итоговые результаты явно хуже модели справа — плюс график качества на валидации у неё близок к плато, хотя и продолжает расти, — а качество модели справа могло бы ещё вырасти при добавлении дополнительных семплов (качество на трейне константно высокое, а на валидации возрастает).

### Стратификация (stratification)

При простом случайном разделении на тренировочное и тестовое множества (как в примерах выше) может случиться так, что их распределения окажутся не такими, как у всего исходного множества. Проиллюстрируем такую ситуацию на примере случайного разбиения датасета [Iris](https://archive.ics.uci.edu/ml/datasets/iris) на трейн и тест. Распределение классов в данном датасете равномерное:

* $33.3\%$ Setosa
* $33.3\%$ Versicolor
* $33.3\%$ Virginica

Случайное разбиение, в котором две трети цветов (100) отправились в трейн, а оставшаяся треть (50) отправилась в тест, может выглядеть, например, так:

* трейн: 38 $\times$ Setosa, 28 $\times$ Versicolor, 34 $\times$ Virginica (распределение $38\%:28\%:34\%$)
* тест: 12 $\times$ Setosa, 22 $\times$ Versicolor, 16 $\times$ Virginica (распределение $24\%:44\%:32\%$)

Если распределение цветов в исходном датасете отражает то, что в природе они встречаются одинаково часто, то мы только что получили два новых датасета, не соответствующих распределению цветов в природе. Распределения обоих датасетов вышли не только несбалансированными, но ещё и разными: самый частый класс в трейне соответствует наименее частому классу в тесте.

На помощь в такой ситуации может прийти **стратификация**: разбиение на трейн и тест, сохраняющее соотношение классов, представленное в исходном датасете. В библиотеке sklearn такое разбиение можно получить с помощью параметра `stratify`:

```python
import numpy as np
from sklearn.model_selection import train_test_split
 
X, y = np.arange(1000).reshape((500, 2)), np.random.choice(4, size=500, p=[0.1, 0.2, 0.3, 0.4])
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2, 
    random_state=42,
    stratify=y
)
```

В целом на достаточно больших датасетах (порядка хотя бы 10 тысяч семплов) со сбалансированными классами можно не очень сильно беспокоиться об описанной выше проблеме и использовать обычный random split.

Но если у вас очень несбалансированные данные, в которых один класс встречается сильно чаще другого (как, например, в задачах фильтрации спама или сегментации осадков на спутниковых снимках), стратификация может довольно сильно помочь.

## k-Fold

Метод **k-Fold** чаще всего имеют в виду, когда говорят о кросс-валидации. Он является обобщением метода hold-out и представляет из себя следующий алгоритм:

1. Фиксируется некоторое целое число $k$ (обычно от 5 до 10), меньшее числа семплов в датасете.
2. Датасет разбивается на $k$ одинаковых частей (в последней части может быть меньше семплов, чем в остальных). Эти части называются *фолдами*.
3. Далее происходит $k$ итераций, во время каждой из которых один фолд выступает в роли тестового множества, а объединение остальных — в роли тренировочного. Модель учится на $k - 1$ фолде и тестируется на оставшемся.
4. Финальный скор модели получается либо усреднением $k$ получившихся тестовых результатов, либо измеряется на отложенном тестовом множестве, не участвовавшем в кросс-валидации.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/7_3_98fef6af39_2821047743.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://neptune.ai/blog/cross-validation-in-machine-learning-how-to-do-it-right">Источник</a>
  </figcaption>
</figure>

Этот метод есть в sklearn:

```python
import numpy as np
from sklearn.model_selection import KFold
 
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])
y = np.array([1, 2, 3, 4])
kf = KFold(n_splits=2)
 
for train_index, test_index in kf.split(X):
    print("TRAIN:", train_index, "TEST:", test_index)
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
'''
result:
TRAIN: [2 3] TEST: [0 1]
TRAIN: [0 1] TEST: [2 3]
'''
```

В коде выше получилось два фолда: в первый вошли объекты с индексами 2 и 3, во второй — объекты с индексами 0 и 1. На первой итерации алгоритма фолд с индексами 2 и 3 будет тренировочным, а на второй — фолд с индексами 0 и 1. В sklearn есть также метод `cross_val_score`, принимающий на вход классификатор, данные и способ разбиения данных (либо число фолдов) и возвращающий результаты кросс-валидации:

```python
from sklearn.model_selection import cross_val_score
 
clf = svm.SVC(kernel='linear', C=1, random_state=42)
scores = cross_val_score(clf, X, y, cv=5)
print(scores)
'''
result:
array([0.96..., 1. , 0.96..., 0.96..., 1. ])
'''
```

Интересный вопрос состоит в том, какую модель брать для сравнения с остальными на отложенном тестовом множестве (если оно у вас есть) либо для окончательного применения в задаче. После применения k-Fold для одной модели у вас на руках останется $k$ экземпляров (инстансов) этой модели, обученных на разных подмножествах трейна. Возможные варианты:

* делать предсказание с помощью усреднения предсказаний этих $k$ инстансов;
* из этих $k$ инстансов выбрать тот, который набрал лучший скор на своём тестовом фолде, и применять дальше его;
* заново обучить модель уже на всех $k$ фолдах и делать предсказания уже этой моделью.

Выбирать, какой способ лучше, нужно в зависимости от конкретной задачи и имеющихся вычислительных возможностей.

Метод k-Fold даёт более надёжную оценку качества модели, чем hold-out, так как обучение и тест модели происходят на разных подмножествах исходного датасета. Однако проведение $k$ итераций обучения и теста может быть вычислительно затратным, и поэтому метод обычно применяют либо когда данных достаточно мало, либо при наличии большого количества вычислительных ресурсов, позволяющих проводить все $k$ итераций параллельно.

В реальных задачах данных зачастую достаточно много для того, чтобы hold-out давал хорошую оценку качества модели, поэтому k-Fold в больших задачах применяется не очень часто.

### Leave-one-out

Метод **leave-one-out (LOO)** — частный случай метода k-Fold: в нём каждый фолд состоит ровно из одного семпла. LOO тоже есть в библиотеке sklearn:

```python
import numpy as np
from sklearn.model_selection import LeaveOneOut
 
X = np.array([[1, 2], [3, 4], [5, 6]])
y = np.array([1, 2, 3])
loo = LeaveOneOut()
 
for train_index, test_index in loo.split(X):
    print("TRAIN:", train_index, "TEST:", test_index)
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
'''
result:
TRAIN: [1 2] TEST: [0]
TRAIN: [0 2] TEST: [1]
TRAIN: [0 1] TEST: [2]
'''
```

Этот метод может понадобиться в случае, если у вас очень мало данных, — например, в задаче сегментации клеток на изображениях с оптического микроскопа, — и вы хотите использовать максимальное их количество для обучения модели.

Для валидации на каждой итерации методу требуется всего один семпл, однако и итераций будет столько, сколько семплов в данных, поэтому метод неприменим для средних и больших задач.

### Stratified k-Fold

Метод **stratified k-Fold** — это метод k-Fold, использующий стратификацию при разбиении на фолды: каждый фолд содержит примерно такое же соотношение классов, как и всё исходное множество. Такой подход может потребоваться в случае, например, очень несбалансированного соотношения классов, когда при обычном random split некоторые фолды могут либо вообще не содержать семплов каких-то классов, либо содержать их слишком мало. Этот метод также представлен в sklearn:

```python
import numpy as np
from sklearn.model_selection import StratifiedKFold
 
X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
y = np.array([0, 0, 1, 1])
skf = StratifiedKFold(n_splits=2)
 
for train_index, test_index in skf.split(X, y):
    print("TRAIN:", train_index, "TEST:", test_index)
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
'''
result:
TRAIN: [1 3] TEST: [0 2]
TRAIN: [0 2] TEST: [1 3]
'''
```

### Кросс-валидация на временных рядах

Существует такая задача, как прогнозирование временных рядов. На практике она часто возникает в форме «Что будет с показателями нашего продукта в ближайший день / месяц / год?». При этом имеются какие-то исторические данные этих показателей за предыдущее время, которые можно визуализировать в виде некоторого графика по времени:

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/7_4_ef89402b78_919466fe3d.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://habr.com/ru/company/ods/blog/327242/">Источник</a>
  </figcaption>
</figure>

Этот график — пример графика временного ряда, и наша задача — спрогнозировать, как будет выглядеть данный график в будущие моменты времени. Кросс-валидация моделей для такой задачи осложняется тем, что данные не должны пересекаться по времени: тренировочные данные должны идти до валидационных, а валидационные — до тестовых. С учётом этих особенностей фолды в кросс-валидации для временных рядов располагаются вдоль временной оси так, как показано на следующей картинке:

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/7_5_7849d6b92e_7fc7a11936.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://habr.com/ru/company/ods/blog/327242/">Источник</a>
  </figcaption>
</figure>

В sklearn реализована такая схема кросс-валидации:

```python
import numpy as np
from sklearn.model_selection import TimeSeriesSplit
X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])
y = np.array([1, 2, 3, 4, 5, 6])
tscv = TimeSeriesSplit()
print(tscv)
 
for train_index, test_index in tscv.split(X):
    print("TRAIN:", train_index, "TEST:", test_index)
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
 
'''
result:
TRAIN: [0] TEST: [1]
TRAIN: [0 1] TEST: [2]
TRAIN: [0 1 2] TEST: [3]
TRAIN: [0 1 2 3] TEST: [4]
TRAIN: [0 1 2 3 4] TEST: [5]
'''
```

## Когда стоит заподозрить, что оценка качества модели завышена?

Ваша модель показала очень высокое качество на тестовых данных, вы радостно откидываетесь на спинку кресла и достаёте шампанское... Или пока рано? Перед тем как информировать коллег о своих высоких результатах, проверьте, что вы не допустили какую-то из следующих ошибок:

* ваши данные не были перемешаны (вспоминаем пример выше с тензорбордом курильщика);
* вы подбирали гиперпараметры на тестовом множестве и на нём же оценивали качество модели;
* у вас в данных есть фича, которая в некотором смысле является «прокси» к таргету (proxy for the target). Это такая фича, которая почти равна таргету, хотя формально им не является и так же, как и таргет, не будет доступна на момент реального применения модели;

{% cut "Пример" %}

Пусть вы хотите предсказывать, сколько будут зарабатывать выпускники разных вузов с разных факультетов через 10 лет после выпуска. Допустим, что у вас есть разнообразные исторические данные о прошлых выпускниках (какие вуз / школу оканчивали, какие факультеты, в каком городе и так далее), где много колонок, и есть искушение особенно не вглядываться в каждую отдельную колонку, а просто разбить данные на трейн и тест и отправить в модель. Но потом вдруг обнаруживается, что у вас всё это время имелась колонка «Доход через пять лет после выпуска», которая явно скоррелирована с таргетом и является важной для вашей модели, но на момент реального применения модели этой информации у вас не будет. Соответственно, наличием этой колонки во многом и объяснялся высокий скор вашей модели. Мораль: всегда внимательно изучайте свои данные перед обучением моделей.

{% endcut %}

* вы проводили feature engineering на всём датасете, а не только на трейне. Например, вы строили [tf-idf](https://ru.wikipedia.org/wiki/TF-IDF) фичи или [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model) на всех данных, а не только на трейне, тем самым заложив в свои тренировочные данные информацию о тестовых данных;
* вы применяли [стандартизацию](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing) данных на всём датасете, а не только на трейне. Например, в случае [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) тестовое множество повлияет на используемые этим методом оценки среднего и стандартного отклонения;
* вы смешали трейн с тестом.

Последний пункт может звучать очень банально, но на практике часто оказывается, что правильно разделить данные на тренировочные и тестовые не так просто даже с учётом всех описанных выше техник. Об этом в следующих разделах.

### Примеры подмешивания тестовых данных в тренировочные

* Ваши данные зависят от времени, а вы при разбиении на трейн и тест это не учли. Например, вы применили обычный random split при работе с временными рядами, передав тем самым вашей модели информацию из будущего. Или вы предсказываете погоду на несколько часов вперёд, а у вас данные из одного и того же дня находятся и в трейне, и в тесте.
* У вас есть датасет с картинками, и вы решили увеличить количество семплов в нём с помощью [аугментаций](https://www.tensorflow.org/tutorials/images/data_augmentation?hl=en) (примерами аугментаций могут служить симметричные отражения, повороты, растяжения). При этом вы взяли весь датасет, применили к нему аугментации и только после этого разделили на трейн и тест. В таком случае преобразования какой-то одной картинки могут попасть в оба множества, и вы получите пересечение трейна и теста.
* Вы решаете задачу рекомендации статей или постов пользователям на основании их комментариев и прочтений, при этом в трейне и тесте у вас одни и те же пользователи.
* Вы решаете какую-то задачу, где происходит работа с видеоданными. Например, распознаёте движение по видео или предсказываете фамилию актёра, попавшего в кадр. При этом в трейн и тест у вас попадают различные кадры из одного и того же видео.
* У вас есть спутниковые снимки, и вы хотите по ним предсказывать рельеф местности. При этом у вас в трейне и тесте есть кропы снимков над одними и теми же географическими координатами (хоть и в разное время).
* Вы обучаете голосового ассистента в звуковом потоке распознавать момент, когда к нему обращаются (например, «Слушай, Алиса», «Ok, Google»). При этом у вас в трейне и тесте одни и те же люди. Это, на первый взгляд, не очень страшная проблема, но на самом деле достаточно большая нейронка может запомнить интонации и манеру речи конкретного человека и будет использовать эти сведения для тестовых записей с этим человеком. При этом на новых людях распознавание будет работать сильно хуже.
* Вы хотите расширить тренировочный датасет какими-то дополнительными данными из другого датасета, но при этом оказывается, что другой датасет содержит в себе часть тестового множества вашего исходного датасета. Например, есть два публичных датасета: [ImageNet LSVRC 2015](https://academictorrents.com/collection/imagenet-lsvrc-2015), в котором 1000 классов и чуть больше миллиона изображений, и [ImageNet](http://image-net.org/), в котором 21 тысяча классов и чуть больше 14 миллионов изображений. При этом первый полностью содержится во втором, поэтому использование ImageNet для расширения обучающей выборки из ImageNet LSVRC 2015 может закончиться тем, что в трейне окажутся примеры из тестового множества, сформированного из ImageNet LSVRC 2015.

### Ещё один интересный пример, когда что-то пошло не так

Пример заимствован [отсюда](https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/example). Допустим, что вы должны обучить модель, предсказывающую тему новостной статьи по её тексту. Если отсортировать статьи по дате их публикации, то ваши данные могут выглядеть, например, так:

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/7_6_4a97ad2ce5_410a5118ff.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/example">Источник</a>
  </figcaption>
</figure> 

Здесь форма и цвет фигуры соответствуют новости, которой посвящена статья. Почему случайное разбиение данных на трейн и тест может привести к проблемам в этой задаче?

На самом деле новостные статьи с одной и той же тематикой появляются кластерами во времени, так как статьи о новом событии выходят, как правило, порциями в то же время, когда произошло событие. Если разбить данные случайно, то тренировочное и тестовое множества с большой вероятностью будут содержать статьи на одни и те же наборы тем:

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/7_7_4998b50a02_0091ffb9bc.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/example">Источник</a>
  </figcaption>
</figure>

Такое разбиение не соответствует тому, как потом модель будет применяться в реальной задаче: при нём модель будет ожидать равномерного распределения тем, предложенных ей в трейне, тогда как в реальности ей на вход будут приходить всё те же кластеры, и они, вообще говоря, не обязаны были быть в её тренировочном множестве. Простым решением будет при разбиении на трейн и тест учитывать время, когда была опубликована статья:

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/7_8_7a0bf8731d_497f253c52.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/example">Источник</a>
  </figcaption>
</figure>

Тут нужно, однако, учитывать, что в реальности кластеры историй по времени выражены не столь чётко и могут пересекаться. Поэтому если трейн и тест расположены слишком близко друг от друга по времени, то они могут пересечься. В принципе, это не так плохо с учётом того, что новости о каких-то событиях могут продолжать выходить в течение некоторого растянутого промежутка времени. Но если хочется избежать такой ситуации, то можно оставить между трейном и тестом некоторый временной зазор: тренироваться, например, на апрельских публикациях, а тестироваться на второй неделе мая, оставив, таким образом, недельный промежуток между двумя множествами.

## Почитать по теме

* [Оригинальный текст](https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/example) описанного выше примера.
* [Ещё один классный пример](https://developers.google.com/machine-learning/crash-course/18th-century-literature), когда случайное разбиение данных может испортить ML-модель.
* [Отличный блог-пост](https://neptune.ai/blog/cross-validation-in-machine-learning-how-to-do-it-right) от Neptune про различные методы кросс-валидации.
* [Раздел нашего учебника](https://academy.yandex.ru/handbook/ml/article/metriki-klassifikacii-i-regressii), посвящённый сравнению и оценке качества моделей.
* [Большая статья-обзор](https://arxiv.org/pdf/1811.12808.pdf) про методы сравнения моделей и оценки их качества.
* [Секция Model Selection](https://scikit-learn.org/stable/model_selection.html) от sklearn.
* [Блог-пост](https://hunch.net/?p=22) про различные «умные» способы получить завышенные оценки качества моделей.
* Отличный [гайд](https://www.ritchieng.com/applying-machine-learning/#2c-learning-curves) о том, как читать графики обучающих кривых в разных случаях.
* [Статья про временные ряды](https://habr.com/ru/company/ods/blog/327242/) из курса «Открытый курс машинного обучения» от ODS
* [Библиотека Prophet от Facebook](https://facebook.github.io/prophet/docs/quick_start.html##python-api) для прогнозирования временных рядов, у которой есть своя [имплементация кросс-валидации](https://facebook.github.io/prophet/docs/diagnostics.html) с дополнительными фичами (таблицы с результатами кросс-валидации, красивые графики).
* [Здесь](https://facebook.github.io/prophet/) можно почитать статью с теоретическим обоснованием метода Prophet.
* [Отличное видео про лики в данных](https://community.datarobot.com/t5/sessions/data-cheats-how-target-leakage-affects-models/ba-p/8220) от DataRobot.
* [Блог-пост](https://community.datarobot.com/t5/blog/what-is-target-leakage-and-how-do-i-avoid-it/ba-p/1973) на эту же тему от них же.
* [Статья](https://arxiv.org/pdf/2007.13237.pdf) про методики разбиения данных в рекомендательных системах.

  ## handbook

  Учебник по машинному обучению

  ## title

  Кросс-валидация

  ## description

  Как строить надёжные оценки качества моделей и никогда не смешивать train и test

- 
  ## path

  /handbook/ml/article/podbor-giperparametrov

  ## content

  Для начала поймём, в чём отличие параметров модели от гиперпараметров:

* **параметры** настраиваются в процессе обучения модели на данных. Например, веса в линейной регрессии, нейросетях, структура решающего дерева;
* **гиперпараметры** — это характеристики модели, которые фиксируются до начала обучения: глубина решающего дерева, значение силы регуляризации в линейной модели, learning rate для градиентного спуска.

Рассмотрим, например, модель линейной регрессии:

$$    f(X) = X w, 
$$

где

* $w = (w_0, w_1, \ldots, w_n)$ — веса модели;
* $X = (x_{ij})$ — матрица, в которой каждая строка содержит признаки одного объекта выборки (для удобства записи считаем, что первый столбец в этой матрице константный).

Эта модель может обучаться посредством минимизации следующего функционала:

$$    \mathcal{L} = \| y - X w\|^2 + C \| w \|^2, 
$$

где $y$ — целевая переменная, $C$ — коэффициент регуляризации. В процессе минимизации $\mathcal{L}$ веса $w$ настраиваются по обучающей выборке, то есть являются параметрами. В то же время величина коэффициента регуляризации задаётся до начала обучения, то есть она — гиперпараметр.

![8](https://yastatic.net/s3/education-portal/media/8_1_16b2ffe33e_47a7a2d8fb.webp)

Ещё хороший пример — решающее дерево. Его гиперпараметры: максимальная глубина дерева, критерий ветвления, минимальное число семплов в листе дерева и ещё много других. А параметр — сама структура решающего дерева: обучение состоит в том, чтобы на каждом уровне дерева выбрать, по какому признаку должно произойти ветвление и с каким пороговым значением этого признака.

Качество модели может очень сильно варьироваться в зависимости от гиперпараметров, поэтому существуют разнообразные методы и инструменты для их подбора. При этом, вне зависимости от выбранного вами метода подбора гиперпараметров, оценку и сравнение моделей нужно проводить грамотно. Пусть у нас есть несколько моделей разной природы (метод ближайших соседей, случайный лес, логистическая регрессия) или несколько нейросеток с разными архитектурами. Нужно для каждой из моделей подобрать гиперпараметры, а затем модели с наилучшими гиперпараметрами сравнить между собой.

Есть два наиболее часто используемых варианта.

### Первый вариант

Разделить выборку на тренировочную, валидационную и тестовую части, для каждой модели выбирать гиперпараметры, максимизирующие её метрики на валидации, а окончательное сравнение моделей проводить по тестовым метрикам.

Разделения только на тренировочную и тестовую выборки недостаточно, так как в модель через подобранные гиперпараметры просачивается информация о тестовой выборке. Это означает, что на новых данных модели могут не сохранить свои качества и что их сравнение не будет честным.

   <figure>
    <img src="https://yastatic.net/s3/education-portal/media/8_2_b6c8676778_ef9b0cebbe.webp" loading="lazy" decoding="async" alt="">
    <figcaption>
    <a href="https://stats.stackexchange.com/questions/410118/cross-validation-vs-train-validation-test">Источник</a>
    </figcaption>
   </figure>

### Второй вариант

Провести [кросс-валидацию](https://en.wikipedia.org/wiki/Cross-validation_\(statistics\)).

Кросс-валидация может быть нужна в случаях, если данных мало или мы не хотим зависеть от конкретного выбора валидационного множества. Примерный алгоритм:

* зафиксировать некоторое тестовое множество и отложить его;
* разделить оставшееся множество данных на $k$ фолдов (подмножеств), пройтись по ним циклом, на каждой итерации фиксируя один фолд в качестве валидационного и обучаясь на остальных;
* в качестве оценки качества модели взять среднее значение валидационной метрики по фолдам;
* финальное сравнение моделей с уже подобранными гиперпараметрами проводить на отложенном тестовом множестве.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/8_3_715a0c2b1d_7a496fcd6e.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
  Визуализация алгоритма.
    <a href="https://towardsdatascience.com/cross-validation-and-hyperparameter-tuning-how-to-optimise-your-machine-learning-model-13f005af9d7d">Источник</a>
  </figcaption>
</figure>

Подробное описание процесса сравнения моделей между собой можно найти в параграфах, посвящённых [кросс-валидации](https://academy.yandex.ru/handbook/ml/article/kross-validaciya) и [сравнению и оценке качества моделей](https://academy.yandex.ru/handbook/ml/article/metriki-klassifikacii-i-regressii).

Далее мы рассмотрим несколько методов подбора гиперпараметров для моделей, а в конце будет приведён список питоновских библиотек, в которых эти методы реализованы, и дано верхнеуровневое сравнение всех описанных методов между собой.

## Grid Search

Самый естественный способ организовать перебор наборов гиперпараметров — сделать перебор по сетке (**Grid Search**):

* для каждого гиперпараметра фиксируется несколько значений;
* перебираются все комбинации значений различных гиперпараметров, на каждой из этих комбинаций модель обучается и тестируется;
* выбирается комбинация, на которой модель показывает лучшее качество.

Примеры:

* для метода ближайших соседей можно, например, перебирать по сетке число соседей (например, от 1 до 20) и метрику, по которой будет измеряться расстояние между объектами выборки (евклидова, манхэттенская и так далее);
* для решающих деревьев можно перебирать по сетке сочетания значений максимальной глубины дерева и различные критерии ветвления (критерий Джини, энтропийный критерий).

Перебор некоторых значений гиперпараметров можно вести по логарифмической шкале, так как это позволяет быстрее определить правильный порядок параметра и в то же время значительно уменьшить время поиска. Так можно подбирать, например, значение learning rate для градиентного спуска, значение константы регуляризации для линейной регрессии или метода SVM.

Сразу же видно естественное ограничение данного метода: если комбинаций параметров слишком много либо каждое обучение / тест длится долго, алгоритм не завершится за разумное время.

## Random Search

Если у вас возникает очень большое количество комбинаций параметров, вы можете какими-то способами пытаться справляться с этой проблемой:

* можно взять меньше значений каждого гиперпараметра, но тогда есть шансы пропустить наилучшую комбинацию;
* можно уменьшить число фолдов в кросс-валидации, но оценка параметров станет более шумной;
* можно оптимизировать параметры последовательно, а не перебирать их комбинации, но снова есть шанс получить неоптимальное решение;
* можно перебирать не все комбинации гиперпараметров, а только случайное подмножество.

Последний способ называется **Random Search**. Для каждого гиперпараметра задаётся распределение, из которого выбирается его значение, и комбинация гиперпараметров составляется семплированием из этих распределений (хорошие советы по поводу выбора распределений можно найти в [документации sklearn](https://scikit-learn.org/stable/modules/grid_search.html#randomized-parameter-search)). Таким образом, благодаря случайному выбору очередной комбинации гиперпараметров вы можете найти оптимальную комбинацию за меньшее число итераций.

Вот это изображение хорошо иллюстрирует отличия поиска по сетке от случайного поиска:

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/8_4_5781994f1e_95e59d0869.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="http://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf">Источник</a>
  </figcaption>
</figure>

То есть: качество нашей модели в зависимости от гиперпараметров — это функция многих переменных с некоторой нетривиальной поверхностью. Но эта поверхность [может зависеть](https://medium.com/rants-on-machine-learning/smarter-parameter-sweeps-or-why-grid-search-is-plain-stupid-c17d97a0e881#.pkwq17od8) от одной из своих переменных сильно меньше, чем от другой. Если бы мы знали, какой гиперпараметр важнее для перформанса модели, мы бы рассмотрели больше его возможных значений, но часто у нас нет такой информации, и мы рассматриваем некоторое наперёд заданное число значений для каждого гиперпараметра.

Random Search может за то же число итераций, что и Grid Search, рассмотреть более разнообразные значения гиперпараметров. Тем самым он с большей вероятностью найдёт те значения, которые больше всего влияют на качество модели, а значит, с большей вероятностью найдёт наилучшую комбинацию значений гиперпараметров.

Есть [ещё одно](https://web.archive.org/web/20160701182750/http://blog.dato.com/how-to-evaluate-machine-learning-models-part-4-hyperparameter-tuning) довольно интересное объяснение, почему Random Search работает хорошо. Рассмотрим случай, когда у нас конечная сетка гиперпараметров (каждому гиперпараметру сопоставлено конечное число значений).

В этой сетке выделим группу размера $5\%$ от общего числа наборов гиперпараметров, на которой модель достигает лучшего качества (можно мысленно отранжировать все наборы по качеству в некоторый список и взять топ $5\%$ этого списка). Тогда некоторый набор гиперпараметров не попадает в эту группу с вероятностью $1 - 0.05$. Если мы насемплировали $n$ наборов, то каждый из них не попал в эту группу с вероятностью $(1 - 0.05)^n$, и, соответственно, вероятность того, что хотя бы один насемплированный набор попал в лучшую группу, равна $1 - (1 - 0.05)^n$. Мы можем решить неравенство

$$    1 - (1 - 0.05)^n \ge 0.95 
$$

и выяснить, что при $n \ge 60$ мы попадём в топ 5% с вероятностью, не меньшей $0.95$. Это в большинстве случаев значительно быстрее, чем перебор всех комбинаций гиперпараметров с помощью Grid Search.

Если в рассуждении выше у нас некоторым гиперпараметрам соответствует непрерывное распределение, то всегда можно предположить, что мы уже насемплировали из этих распределений некоторое конечное число значений (равное числу итераций Random Search), а дальше считать, что мы работаем с конечной сеткой.

Конечно, остаётся наша зависимость от самой сетки гиперпараметров, и не всякая сетка обязана содержать в себе глобальный максимум перформанса модели или даже гиперпараметры из интервала вокруг него.

## Exploration vs exploitation

В машинном обучении достаточно часто встречаются такие термины, как **exploration** и **exploitation**. Суть этих терминов хорошо поясняет следующий пример из реальной жизни. Допустим, перед вами стоит выбор, в какой ресторан пойти сегодня. Пусть ваш любимый ресторан находится прямо за углом.

Вы ходите туда каждый день и поэтому достаточно уверены в том, насколько вкусным будет ваш обед. Но при этом не рассматриваете никакие другие опции и, возможно, упускаете возможность поесть гораздо вкуснее в другом месте. Если же вы будете обедать каждый раз в новом месте, то очень часто будете не удовлетворены результатом.

![8](https://yastatic.net/s3/education-portal/media/8_5_d5dff4b143_7276b2c3ca.webp)

В описанных далее методах подбора гиперпараметров будет так или иначе происходить поиск баланса между exploration и exploitation. Одно из основных отличий всех методов, которые будут описаны далее, от Grid Search и Random Search — возможность учитывать результаты предыдущих вычислений.

Одна из возможных стратегий выбора точки для следующей итерации — *exploration*: исследование тех областей, в которых у нас мало семплов на текущей итерации, что даёт нам возможность с меньшей вероятностью пропустить оптимальное значение.

Другая стратегия — *exploitation*: выбирать больше семплов в областях, которые мы достаточно неплохо изучили и где, как мы считаем, с большой вероятностью находится оптимум.

## Байесовская оптимизация

Байесовская оптимизация — это итерационный метод, позволяющий оценить оптимум функции, не дифференцируя её. Кроме того, на каждой итерации метод указывает, в какой следующей точке мы с наибольшей вероятностью улучшим нашу текущую оценку оптимума. Это позволяет значительно сократить количество вычислений функции, каждое из которых может быть довольно затратным по времени.

Подбор гиперпараметров тоже можно сформулировать в виде задачи, которая может решаться с помощью байесовской оптимизации. Пусть, например, наша функция — значение валидационных метрик в зависимости от текущего сочетания гиперпараметров. Её вычисление затратно по времени (нужно натренировать и провалидировать модель), и мы не можем вычислить градиенты этой функции по её переменным (нашим гиперпараметрам).

Байесовская оптимизация имеет две основные компоненты:

* вероятностную модель, которая приближает распределение значений целевой функции в зависимости от имеющихся исторических данных (часто в качестве такой модели выбирают [гауссовские процессы](https://krasserm.github.io/2018/03/19/gaussian-processes/));
* функцию, которая позволяет по некоторым статистикам текущей вероятностной модели функции $f$ указать, в какой следующей точке нужно вычислить значение $f$. Эта функция называется *acquisition function*. Она должна балансировать между *exploration* и *exploitation* в следующем смысле:
  * *exploration* — исследовать те точки, в которых дисперсия нашей вероятностной модели велика;
  * *exploitation* — исследовать те точки, где среднее нашей модели велико (и может служить оценкой максимума $f$).

Простой пример acquisition function — сумма среднего вероятностной модели и стандартного отклонения с некоторым весом:

$$    \alpha(x) = \mu(x) + \beta \sigma(x), 
$$

где $x$ — точка из пространства, в котором мы оптимизируем целевую функцию (в нашем контексте это вектор значений гиперпараметров). На картинке ниже изображены обе компоненты, из которых складывается данная acquisition function, — среднее вероятностной модели $\mu$ (синий график) и доверительный интервал, ширина которого в каждой точке пропорциональна стандартному отклонению вероятностной модели (жёлтая область).

Среднее модели $\mu$ стремится приблизить искомую функцию $f$ и в точности равно $f$ в тех точках, где значения $f$ известны. Доверительный интервал имеет переменную ширину, так как чем дальше находится некоторая точка от тех, значения в которых известны, тем более модель не уверена в том, какое значение функции в этой точке, и тем шире доверительный интервал. Наоборот, в точках, где значения известны, доверительный интервал имеет нулевой радиус.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/8_6_07b9debf98_1a548f93c4.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://www.borealisai.com/en/blog/tutorial-8-bayesian-optimization/">Источник</a>
  </figcaption>
</figure>

Байесовская оптимизация в общем случае представляет из себя следующий алгоритм. Пусть $S_t$ — множество предыдущих наблюдений целевой функции $f$: $(f(x_1), \ldots, f(x_t))$, а $\alpha(\cdot)$ — некоторая acquisition function.

* На итерации $t + 1$ вычисляется точка $x_{t + 1}$, в которой нужно провести следующее вычисление целевой функции:

$$    x_{t + 1} = \arg \max_{x \in X} \alpha(x|S_t). 
$$

* Вычисляется значение $f(x_{t + 1})$, и обновляется множество наблюдений $S_{t + 1} = (S_t, f(x_{t + 1}))$.
* Обновляется статистическая модель.

Чтобы такой алгоритм работал эффективно, $\alpha$ должна быть легко вычислимой и дифференцируемой.

На рисунке ниже изображены три итерации этого алгоритма. Здесь пунктирная линия — это целевая функция, сплошная линия — график среднего вероятностной модели, жёлтым цветом обозначен доверительный интервал модели.

Серый график снизу — это график acquisition function. Её значения велики там, где вероятностная модель предсказывает большие значения целевой функции (exploitation), и там, где велика неуверенность вероятностной модели (exploration).

На каждой итерации находится точка максимума acquisition function (чёрный крестик), и следующая итерация произойдёт в этой точке (серый кружок на графике функции). На нижнем графике побеждает exploitation, так как acquisition function верно предсказала, что наблюдения из неизвестных областей слабо повлияют на нашу текущую оценку максимума $f$.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/8_7_6b2dcdd077_c22b9ae61a.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7352306">Источник</a>
  </figcaption>
</figure>

Байесовская оптимизация хорошо работает, когда нужно оптимизировать небольшое число гиперпараметров, так как в наивной реализации алгоритм не поддаётся распараллеливанию. При большой размерности пространства гиперпараметров скорость сходимости не лучше, чем у обычного Random Search ([как утверждается в этой статье](https://arxiv.org/pdf/1603.06560.pdf)).

Байесовская оптимизация в изначальной постановке предполагалась для работы с непрерывными гиперпараметрами, а для работы с категориальными гиперпараметрами ей нужны некоторые трюки:

1. Если нужно найти оптимальное значение только одного гиперпараметра и этот параметр категориальный, то можно, например, использовать Thompson sampling (как тут в [разделе «Bernoulli bandit»](https://www.borealisai.com/en/blog/tutorial-8-bayesian-optimization/)). Вообще, проблему выбора наилучшего значения категориального гиперпараметра можно переформулировать как [multi-armed bandit problem](https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html) и использовать любой известный способ решения этой задачи.

2. Если категориальных гиперпараметров больше одного и кроме них есть некатегориальные, то:

* можно попробовать использовать специальные виды ядер в гауссовских процессах, [как, например, сделано здесь](https://www.cs.toronto.edu/~duvenaud/thesis.pdf);
* можно заменить гауссовские процессы на Random Forest (подробнее можно посмотреть здесь в [разделе «Random Forests»](https://www.borealisai.com/en/blog/tutorial-8-bayesian-optimization/)).

## Tree-structured Parzen Estimator (TPE)

Алгоритм TPE, как и алгоритм байесовской оптимизации, итерационный: на каждой итерации принимается решение о том, какие следующие значения гиперпараметров нужно выбрать, исходя из результатов предыдущих итераций. Но идейно имеет довольно сильные отличия.

Предположим сначала, что мы хотим сделать поиск оптимального значения для **одного** гиперпараметра.

На нескольких первых итерациях алгоритму требуется «разогрев»: нужно иметь некоторую группу значений данного гиперпараметра, на которой известно качество модели. Самый простой способ собрать такие наблюдения — провести несколько итераций Random Search (количество итераций определяется пользователем).

Следующим шагом будет разделение собранных во время разогрева данных на две группы. В первой группе будут те наблюдения, для которых модель продемонстрировала лучшее качество, а во второй — все остальные. Размер доли лучших наблюдений задаётся пользователем: чаще всего это 10-25% от всех наблюдений. Картинка ниже иллюстрирует такое разбиение:

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/8_8_46b76c5fad_946af17422.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html">Источник</a>
  </figcaption>
</figure>

Далее некоторым образом строятся оценки распределения $\ell(x)$ лучших наблюдений и распределения $g(x)$ всех остальных в пространстве значений рассматриваемого гиперпараметра.

{% cut "О том, как оцениваются$\ell(x)$и $g(x)$" %}

Если гиперпараметр принимает **непрерывные значения**, то распределения $\ell(x)$ и $g(x)$ можно оценить на основе **Parzen window density estimation**. Идея данного метода в следующем. Пусть у нас имеются точки $x_1, \ldots, x_n$, которые были насемплированы из некоторого неизвестного распределения $f$. Нам нужно каким-то образом оценить $f$ по известным данным. Для этого каждое наблюдение $x_i$ помещается в центр некоторого симметричного распределения $K$ с дисперсией $h$, а оценкой для $f$ становится смесь этих распределений:

$$\hat f_h(x) = \frac{1}{nh} \sum_{i = 1}^n K \left(\frac{x - x_i}{h}\right) 
$$

Распределения $K$ обычно называют *ядрами*, примеры ядер можно найти [тут](https://en.wikipedia.org/wiki/Kernel_\(statistics\)). На картинке ниже показана зависимость вида итогового распределения от параметра $h$ (который часто называют *bandwidth*):

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/8_9_417e64fa15_8d08288b69.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://stats.stackexchange.com/questions/244012/can-you-explain-parzen-window-kernel-density-estimation-in-laymans-terms">Источник</a>
  </figcaption>
</figure>

Чем больше у нас наблюдений, тем точнее можем оценить целевое распределение:

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/8_10_8600188ec2_9de7620d70.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html">Источник</a>
  </figcaption>
</figure>

Если гиперпараметр **категориальный** и принимает значения $c_1, \ldots, c_n$, то в качестве $\ell(x)$ и $g(x)$ можно задать категориальные распределения в виде наборов из $n$ вероятностей $(p_1, \ldots, p_n)$, где $p_i$ соответствует вероятности насемплировать значение $c_i$. Значения $p_i$ для $\ell(x)$ будут пропорциональны числу раз, которое каждое из значений $c_i$ встретилось в группе лучших наблюдений (и, соответственно, худших наблюдений в случае $g(x)$). Например, пусть у гиперпараметра всего 3 значения и уже прошло 60 итераций алгоритма. Пусть среди лучших 15 испытаний 2 раза встретилось значение $c_1$, 5 раз встретилось значение $с_2$ и 8 раз встретилось значение $c_3$. Тогда $\ell(x) \sim \left( p_1 = \frac{2}{15}, p_2 = \frac{5}{15}, p_3 = \frac{8}{15} \right)$. Аналогично будет строиться $g(x)$.

{% endcut %}

На следующем шаге алгоритма мы семплируем несколько значений-кандидатов из распределения $\ell(x)$ (количество таких семплирований тоже задаётся пользователем, можно задать их число равным, например, 1000). Из насемплированных кандидатов мы хотим найти тех, кто с большей вероятностью окажется в первой группе (состоящей из лучших наблюдений), чем во второй. Для этого для каждого кандидата $x$ вычисляется **Expected Improvement**:

$$EI(x) = \frac{\ell(x)}{g(x)} 
$$

**Замечание**: На самом деле стоит отметить, что в [оригинальной статье](https://proceedings.neurips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf) величина $EI$ имеет более общее определение. Но там же доказывается, что максимизация $EI$ в исходном определении эквивалентна максимизации отношения выше.

Кандидат с наибольшим значением $EI(x)$ будет включён в множество рассматриваемых гиперпараметров на следующей итерации:

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/8_11_91fd470896_82cc1456cb.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html">Источник</a>
  </figcaption>
</figure>

После того как было выбрано значение-кандидат, максимизирующее $EI$, обучается модель с этим значением гиперпараметра. После обучения мы замеряем её качество на валидационной выборке и в соответствии с этим результатом обновляем распределения $\ell(x)$ и $g(x)$: снова ранжируем всех имеющихся кандидатов по качеству модели с учётом последнего, из топ 10-25% формируется обновлённое $\ell(x)$, из остальных — $g(x)$. Так происходит столько раз, сколько итераций алгоритма мы задали.

Теперь опишем, как алгоритм работает в общем случае, когда гиперпараметров **более одного**. Алгоритм работает с гиперпараметрами, представляя их в форме дерева (отсюда «tree» в названии). Например, в документации [Hyperopt](https://github.com/hyperopt/hyperopt/wiki/FMin#22-a-search-space-example-scikit-learn) можно увидеть такой пример:

```
from hyperopt import hp
 
space = hp.choice('classifier_type', [
    {
        'type': 'naive_bayes',
    },
    {
        'type': 'svm',
        'C': hp.lognormal('svm_C', 0, 1),
        'kernel': hp.choice('svm_kernel', [
            {'ktype': 'linear'},
            {'ktype': 'RBF', 'width': hp.lognormal('svm_rbf_width', 0, 1)},
            ]),
    },
    {
        'type': 'dtree',
        'criterion': hp.choice('dtree_criterion', ['gini', 'entropy']),
        'max_depth': hp.choice('dtree_max_depth',
            [None, hp.qlognormal('dtree_max_depth_int', 3, 1, 1)]),
        'min_samples_split': hp.qlognormal('dtree_min_samples_split', 2, 1, 1),
    },
])
```

На рисунке ниже изображено дерево, соответствующее данному примеру:

![8](https://yastatic.net/s3/education-portal/media/8_12_ec1b579327_12dd0b843a.webp)

Корень дерева $\varepsilon$ — фиктивная вершина, введённая для удобства. Здесь первый уровень дерева — выбор классификатора (наивный байес, SVM, решающее дерево). Дальнейшие уровни — гиперпараметры самих классификаторов и зависящие уже от них гиперпараметры (например, SVM $\to$ kernel $\to$ RBF $\to$ width). Движение по дереву во время итераций алгоритма происходит по некоторому пути от корня к листу и обратно вдоль пройденного пути (этот процесс подробнее описан ниже).

Под некоторыми вершинами записан набор гиперпараметров в скобках (например, `kernel` и `C` под SVM). Это означает, что при приходе в эту вершину значения всех гиперпараметров, перечисленных в скобках, должны так или иначе быть выбраны.

Каждой вершине дерева, в которой будет происходить семплирование значений, сопоставляется своя пара  $\ell(x)$ и $g(x)$ с учётом значений, насемплированных на этапе «разогрева». Каждому гиперпараметру, перечисленному в скобках, соответствует своя собственная пара. Если из названия гиперпараметра не идут стрелки (например, `C` у SVM и `min_samples_split` у Decision Tree), то это означает, что от его значения не зависят значения никаких других гиперпараметров.

Поэтому либо будет выбрано его значение, максимизирующее $EI$ для соответствующих ему $\ell$ и $g$, либо уже ничего не нужно семплировать (как, например, в вершинах `linear` или `gini`). Если же из гиперпараметра идут стрелки на следующий уровень, то с помощью максимизации $EI$ будет выбрано, в каком направлении сделать переход. Например, из корня $\varepsilon$ выбирается, какой классификатор рассмотреть на следующем этапе, а из параметра `kernel` можно перейти либо к `RBF`, либо к `linear`.

Теперь опишем сам алгоритм. Сначала так же, как и в одномерном случае, происходит «разогрев»: проводится некоторое количество итераций Random Search с теми изначальными распределениями, которые были заданы для гиперпараметров (в примере из Hyperopt эти распределения задаются как `hp.qlognormal`, `hp.lognormal` и так далее). Затем начинается итерационное обновление дерева гиперпараметров. Обновление дерева на каждой итерации происходит в два этапа:

1. Сначала алгоритм идёт из корня дерева до некоторого листа. В каждой вершине для каждого соответствующего ей гиперпараметра он находит значение, максимизирующее $EI$. Если выбор значения для некоторого гиперпараметра означает переход на следующий уровень дерева, он идёт в ту вершину, которая соответствует максимизации $EI$. Так он идёт до тех пор, пока не упрётся в какой-то лист. Пройденный путь от корня до листа задаёт полный набор значений гиперпараметров для модели, и её с этими значениями можно провалидировать.

{% cut "Пример" %}

Пусть вы находитесь в корне $\varepsilon$ и выбираете классификатор. Допустим, классификатор SVM оказался оптимальным по критерию $EI$. Вы переходите в соответствующую ему вершину, и здесь вам нужно провести семплирование значений для двух гиперпараметров: `kernel` и `C`. Для `C` вы выбираете некоторое значение, которое максимизирует $EI$. Пусть оно оказалось равно $0.1$. А для `kernel` вы с помощью максимизации $EI$ выбираете, в какую вершину на следующем уровне вы отправитесь. Пусть эта вершина — `RBF`. Для него вы семплируете конкретное значение `width` — пусть оно оказалось равным $0.9$. Получилось, что вы прошли полный путь и получили модель с заданным набором гиперпараметров: $SVM(C = 0.1, kernel = RBF(width = 0.9))$, которую теперь можно провалидировать.

{% endcut %}

2. После того как модель, полученная на предыдущем этапе, была провалидирована, распределения в вершинах дерева нужно обновить в соответствии с информацией о полученном качестве. Для этого алгоритм поднимается из листа наверх, обновляя распределения во всех вершинах дерева вдоль своего пути. В каждой вершине для каждого гиперпараметра процедура обновления та же, что была описана для одного гиперпараметра: имеющиеся значения гиперпараметров переранжируются по качеству с учётом результата последнего кандидата (этот результат общий для всех вершин вдоль пути), по топ 10-25% оценивается $\ell(x)$, по остальным — $g(x)$.

В качестве окончательного ответа алгоритм выдаёт набор гиперпараметров (или, как в примере выше, не только гиперпараметры, но даже саму модель), на котором было получено лучшее качество за все итерации. Число итераций алгоритма задаётся пользователем.

За дальнейшими деталями о процедуре обновления дерева для алгоритма TPE можно обратиться к [этой статье](https://arxiv.org/pdf/1208.3719.pdf) и к [исходному коду](https://github.com/hyperopt/hyperopt/blob/master/hyperopt/tpe.py#L662) алгоритма TPE из библиотеки Hyperopt.

Стоит заметить, что если гиперпараметры не лежат вместе ни в одном пути в дереве, то TPE считает их независимыми. Это — недостаток данного алгоритма, так как некоторые гиперпараметры, находящиеся по смыслу в разных путях в дереве, зависят от друг от друга.

Например, с регуляризацией мы можем тренировать нейросеть большее число эпох, чем без регуляризации, потому что без регуляризации сеть на большом числе эпох может начать переобучаться. В этом конкретном примере можно использовать такой трюк:

```
hp.choice('training_parameters', [
    {
        'regularization': True,
        'n_epochs': hp.quniform('n_epochs', 500, 1000, q=1),
    }, {
        'regularization': False,
        'n_epochs': hp.quniform('n_epochs', 20, 300, q=1),
    },
])
```

Но если внутренние зависимости между гиперпараметрами вам неизвестны, то алгоритм не сможет найти их сам.

Критерий $EI$ позволяет методу TPE балансировать между *exploration* и *exploitation*. Семплирование из распределения $\ell(x)$ — это, с одной стороны, exploitation, так как гиперпараметры, семплируемые из него, близки к оптимуму, но это же привносит элемент exploration, так как семплируемые гиперпараметры не равны оптимуму в точности.

## Population Based Training (PBT)

Этот метод использует идеи из теории [эволюционных стратегий](https://lilianweng.github.io/lil-log/2019/09/05/evolution-strategies.html) и с самого начала включает в себя параллельные вычисления.

Методы, описанные выше, имеют свои сильные и слабые стороны.

* Grid Search и Random Search:
  * отлично параллелизуются;
  * не используют результаты предыдущих итераций.
* БО и TPE:
  * трудно параллелизуются;
  * используют результаты предыдущих итераций, при сходимости результаты лучше, чем у Random Search и Grid Search.

В алгоритме PBT была сделана попытка объединить сильные стороны обеих групп, что проиллюстрировано на картинке ниже:

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/8_13_9ca8874e96_a5714a6d1e.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://arxiv.org/pdf/1711.09846.pdf">Источник</a>
  </figcaption>
</figure>

В процессе работы алгоритм обучает не одну модель, а целую **популяцию** $\mathcal{P}$ моделей — набор моделей одинакового типа, отличающихся только набором гиперпараметров:

$$    \mathcal{P} = \{(\theta_i, h_i) \, | \, i = 1, \ldots, N \}, 
$$

где $\theta_i$ и $h_i$ — веса и гиперпараметры модели $i$ соответственно.

Предполагается также, что модели обучаются как-то итерационно, например градиентным спуском (но могут использоваться и безградиентные методы, такие как эволюционные стратегии). Изначально каждая модель в популяции имеет случайные веса и гиперпараметры. Каждая модель из популяции тренируется параллельно с остальными, и периодически качество каждой модели замеряется независимо от остальных.

Как только какая-то модель считается «созревшей» для обновления (например, прошла достаточное число шагов градиентного спуска или преодолела некоторый порог по качеству), у неё появляется шанс быть обновлённой относительно всей остальной популяции:

* процедура **exploit()**: если у модели низкое качество относительно популяции, то её веса заменяются на веса модели с более высоким качеством;
* процедура **explore()**: если веса модели были перезаписаны, шаг *explore* добавляет случайный шум в параметры модели.

При таком подходе только лучшие пары моделей и гиперпараметров выживут и будут обновляться, что позволяет добиться более высокой утилизации ресурсов.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/8_14_1816a593dc_5deea06c15.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://arxiv.org/pdf/1711.09846.pdf">Источник</a>
  </figcaption>
</figure>

Стоит отметить, что наиболее оптимальный размер популяции, выявленный авторами в результате экспериментов, — от 20 до 40, что довольно много и не реализуется на обычном ноутбуке.

Красивая гифка с демонстрацией работы алгоритма:

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/evolution_b2d6991401_845361a5fa.gif" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://deepmind.com/blog/article/population-based-training-neural-networks">Источник</a>
  </figcaption>
</figure>

## Open-source-библиотеки

### Scikit-learn

В библиотеке [Scikit-learn](https://scikit-learn.org/stable/index.html) есть реализации Grid Search и Random Search, что очень удобно, если вы используете модели из sklearn. Примеры их использования можно найти [здесь](https://scikit-learn.org/stable/modules/grid_search.html#randomized-parameter-optimization).

### Hyperopt

В библиотеке [Hyperopt](http://hyperopt.github.io/hyperopt/) реализованы три метода оптимизации гиперпараметров:

* Random Search
* TPE
* [Adaptive TPE](https://github.com/electricbrainio/hypermax)

У них есть небольшой [туториал](https://github.com/hyperopt/hyperopt/wiki/FMin) по тому, как начать пользоваться библиотекой. Кроме того, у них есть обёртка над sklearn, позволяющая работать с моделями оттуда: [Hyperopt-sklearn](https://github.com/hyperopt/hyperopt-sklearn).

### Optuna

В библиотеке [Optuna](https://optuna.org/) реализованы те же методы оптимизации, что и в Hyperopt, но по многим параметрам она оказывается удобнее. Хорошее сравнение Optuna и Hyperopt можно найти [здесь](https://neptune.ai/blog/optuna-vs-hyperopt).

### Scikit-Optimize

В библиотеке [Scikit-Optimize](https://scikit-optimize.github.io/stable/index.html) реализованы алгоритмы байесовской оптимизации и Random Search. Кроме самих методов оптимизации библиотека предоставляет отличный инструментарий для различных [визуализаций](https://neptune.ai/blog/scikit-optimize#8). Хорошее описание возможностей библиотеки можно найти [тут](https://neptune.ai/blog/scikit-optimize).

### Keras Tuner

Библиотека [Keras Tuner](https://keras-team.github.io/keras-tuner/) позволяет подбирать гиперпараметры для нейросеток, написанных на TensorFlow 2.0, и для обычных моделей из Scikit-learn. Доступные методы оптимизации — Random Search и [Hyperband](https://arxiv.org/pdf/1603.06560.pdf). Хороший гайд по использованию данной библиотеки можно найти [тут](https://blog.tensorflow.org/2020/01/hyperparameter-tuning-with-keras-tuner.html).

## Summary

Список описанных методов не исчерпывает все существующие на данный момент методы оптимизации гиперпараметров: остались за кадром такие алгоритмы, как [ASHA](https://arxiv.org/pdf/1810.05934.pdf), [Hyperband](https://arxiv.org/pdf/1603.06560.pdf), [BOHB](https://arxiv.org/abs/1807.01774). Хороший сравнительный обзор этих трёх алгоритмов можно найти [здесь](https://neptune.ai/blog/hyperband-and-bohb-understanding-state-of-the-art-hyperparameter-optimization-algorithms).

Мы собрали все описанные выше алгоритмы в таблицу, чтобы вам было удобнее сравнивать их между собой. А к некоторым оставили дополнительные комментарии.

**Grid Search**. Хорошо работает, когда у вас совсем мало гиперпараметров либо вы смогли распараллелить его работу.

* Сильные стороны:
  * самый простой для понимания и реализации;
  * тривиально распараллеливается.
* Слабые стороны:
  * не использует результаты других итераций;
  * ограничен в выборе заданной сеткой;
  * долго работает, если делает последовательный перебор по сетке. Нет гарантий на необходимое число итераций.

В защиту этого метода хочется сказать, что часто на практике приходится делать перебор гиперпараметров вообще вручную (если один инстанс вашей модели учится недели две и использует много ресурсов) либо по очень небольшой сетке. Так что метод вполне в ходу :)

**Random Search**. Метод представляет собой небольшое усложнение над Grid Search, но при этом оказывается намного более эффективным.

* Сильные стороны:
  * случайный перебор по сетке позволяет находить оптимальные гиперпараметры более эффективно, чем Grid Search, в частности из-за того, что непрерывные параметры можно задать в виде распределения, а не перечислять значения заранее;
  * тривиально распараллеливается;
  * допускает усиление за счёт [использования квазислучайных распределений](http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#random-search) при семплировании.
* Слабые стороны:
  * не использует результаты других итераций;
  * ограничен в выборе заданной сеткой, хотя и в некоторых случаях менее жёстко, чем Grid Search.

**Байесовская оптимизация**

* Сильные стороны:
  * использует результаты предыдущих итераций;
  * может моделировать внутренние зависимости между гиперпараметрами (за счёт работы с ними в едином подмножестве $\mathbb{R}^n$, где $n$ — число гиперпараметров);
  * может расширять заданные изначально границы множества поиска гиперпараметров;
  * достигает более высокого качества, чем Random Search, если удалось провести достаточное количество итераций.
* Слабые стороны:
  * паралеллится нетривиально;
  * в нераспараллеленном случае работает долго, так как для каждой итерации ему приходится заново строить вероятностную модель. В случае если такая модель — гауссовские процессы, сложность получается порядка $n^3$, где $n$ — число гиперпараметров;
  * для работы с категориальными гиперпараметрами нужны нетривиальные хаки.

**Tree-structured Parzen Estimator**

* Сильные стороны:

  * использует результаты предыдущих итераций;
  * может работать с зависимостями между гиперпараметрами, в которых один гиперпараметр не будет рассматриваться, если другой не примет какое-то определённое значение (например, число нейронов во втором слое нейросети нужно перебирать, если параметр «число слоёв» имеет значение не менее двух);
  * имеет линейную сложность по числу гиперпараметров (в отличие от БО);
  * не требует специальных хаков для работы с категориальными признаками, так как каждый гиперпараметр в этом алгоритме имеет своё отдельное одномерное распределение, и не нужно строить сложное совместное распределение всех гиперпараметров (как в БО);
  * достигает высоких результатов по качеству, довольно часто используется в соревнованиях.

* Слабые стороны:

  * не может моделировать неявные зависимости между гиперпараметрами (те, которые юзер не задал с помощью дерева);
  * хотя сложность и меньше, чем у БО, может работать довольно медленно даже на не очень большом числе гиперпараметров.

**Population Based Training**

* Сильные стороны:
  * параллельный by design;
  * может использовать результаты предыдущих итераций.
* Слабые стороны:
  * для эффективной работы нужно много воркеров (от 20 до 40), что нетривиально для реализации.

## Почитать по теме

* [Примеры](https://scikit-learn.org/stable/modules/grid_search.html#exhaustive-grid-search) использования Grid Search от sklearn.
* [Примеры](https://scikit-learn.org/stable/modules/grid_search.html#randomized-parameter-optimization) использования Random Search от sklearn.
* [Хороший блог-пост о гиперпараметрах](http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#random-search), в первом разделе которого есть интересные рассуждения про усиление Random Search с помощью квазислучайных распределений.
* [Блог-пост](https://deepmind.com/blog/article/population-based-training-neural-networks) от DeepMind про предложенный ими алгоритм Population Based Training.
* [Оригинальная статья](https://arxiv.org/pdf/1711.09846.pdf), где был предложен алгоритм.
* [Блог-пост](https://lilianweng.github.io/lil-log/2019/09/05/evolution-strategies.html) про эволюционные стратегии.

А если вам интересно как следует разобраться в байесовской оптимизации (в частности, рассмотреть больше примеров вероятностных моделей и разных acquisition function), то вот полезный контент:

* [Отличный туториал](https://www.borealisai.com/en/blog/tutorial-8-bayesian-optimization/) по различным методам оптимизации гиперпараметров, в частности по байесовской оптимизации.
* [Статья-обзор](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7352306), подробно объясняющая математические детали методов байесовской оптимизации и содержащая примеры их применения в ресёрче и индустрии.
* [Видеолекция](https://www.youtube.com/watch?v=PgJMLpIfIc8) Евгения Бурнаева на летней школе Deep \| Bayes.
* [Оригинальная статья](https://proceedings.neurips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf), в которой были предложены методы TPE и байесовская оптимизация.
* [Пример использования skopt (Scikit-Optimize)](https://scikit-optimize.github.io/stable/auto_examples/sklearn-gridsearchcv-replacement.html) — нахождение лучших параметров для SVM с помощью байесовской оптимизации.
* [Реализация](https://colab.research.google.com/github/krasserm/bayesian-machine-learning/blob/master/bayesian_optimization.ipynb) алгоритма байесовской оптимизации и примеры использования библиотечных реализаций.
* [Про гауссовские процессы](http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html) с хорошими визуализациями.
* [Более формально про гауссовские процессы](https://krasserm.github.io/2018/03/19/gaussian-processes/), но с хорошими примерами на питоне.

Для дальнейшего изучения метода TPE можно использовать следующие источники:

* [Оригинальная статья](https://proceedings.neurips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf), в которой были предложены методы TPE и байесовская оптимизация.
* [Блог-пост](http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html) про TPE и остальные методы тюнинга гиперпараметров от NeuPy. Там же можно найти пример применения TPE из [Hyperopt](https://github.com/hyperopt/hyperopt).
* [Отличное объяснение](https://stats.stackexchange.com/questions/244012/can-you-explain-parzen-window-kernel-density-estimation-in-laymans-terms) того, что такое Parzen window density estimation.
* [Отличный туториал](https://www.borealisai.com/en/blog/tutorial-8-bayesian-optimization/) по различным методам оптимизации гиперпараметров (который уже был упомянут выше в разделе про байесовскую оптимизацию).

  ## handbook

  Учебник по машинному обучению

  ## title

  Подбор гиперпараметров

  ## description

  Как эффективно подбирать значения гиперпараметров модели и не переобучиться при этом

- 
  ## path

  /handbook/ml/article/veroyatnostnyj-podhod-v-ml

  ## content

  В этом разделе мы посмотрим на те же самые модели машинного обучения, но с другой стороны: будем интерпретировать их как вероятностные.

В первом параграфе мы расскажем, как обращаться с вероятностными моделями, и покажем, что привычный вам подбор параметров модели с помощью минимизации функции потерь соответствует подбору параметров методом максимального правдоподобия. Это даст возможность транслировать в мир ML известные результаты о свойствах оценок максимального правдоподобия, но в то же время и обнажит их недостатки. Благодаря этому мы сможем по-новому взглянуть на логистическую регрессию и с новым пониманием сформулировать её обобщение — generalized linear model (GLM).

По ходу дела мы обнаружим, что большинство классификаторов, хоть и делают вид, что предсказывают корректные вероятности, на самом деле вводят в заблуждение.

В третьем параграфе мы поговорим о том, как проверить отклонение предсказанных значений от истинных вероятностей и как поправить ситуацию.

Далее мы обсудим генеративный подход к классификации и разберём несколько примеров генеративных моделей, после чего перейдём к байесовскому подходу оценивания параметров, который, хоть зачастую и трудно осуществим вычислительно, однако обладает большей теоретической стройностью. Он позволяет оценивать распределение параметров и предсказаний (например, уверенность в нашей оценке), а кроме того — даёт нам возможность измерить качество модели, не прибегая к проверке на тестовой выборке.

Если вы готовы — давайте приступим!

## Случайность как источник несовершенства модели

Практически любая наша модель — несовершенна. Но объяснять это несовершенство можно по-разному.

Представим, что мы решаем задачу регрессии $y\simeq \langle x, w\rangle$: например, пытаемся по университетским оценкам выпускника предсказать его годовую зарплату. Ясно, что точная зависимость у нас не получится как минимум потому, что мы многого не знаем о выпускнике: куда он пошёл работать, насколько он усерден, как у него с soft skills и так далее. Как же нам быть?

Первый вариант — просто признать, что мы не получим идеальную модель, но постараться выучить оптимальную, насколько это возможно. То есть приблизить таргет предсказаниями наилучшим образом с точки зрения какой-то меры близости, которую мы подберём из экспертных соображений.

Так мы получаем простой инженерный подход к машинному обучению: есть формула, в которой присутствуют некоторые параметры ($w$), есть формализация того, что такое «приблизить» (функция потерь) — и мы бодро решаем задачу оптимизации по параметрам.

Второй вариант — свалить вину за неточности наших предсказаний на случайность. В самом деле: если мы что-то не можем измерить, то для нас это всё равно что случайный фактор. В постановке задачи мы заменяем приближённое равенство $y\simeq \langle x, w\rangle$ на точное

$$y=(\langle x, w\rangle, \text { искажённое шумом } \varepsilon)
$$

Например, это может быть аддитивный шум (чаще всего так и делают):

$$y = \langle x, w\rangle + \varepsilon
$$

где $\varepsilon$ — некоторая случайная величина, которая представляет этот самый случайный шум. Тогда получается, что для каждого конкретного объекта $x_i$ соответствующий ему истинный таргет — это сумма $\langle x_i, w\rangle$ и конкретной реализации шума $\varepsilon$.

При построении такой модели мы можем выбирать различные распределения шума, кодируя тем самым, какой может быть ошибка. Чаще всего выбирают гауссовский шум: $\varepsilon\sim\mathcal{N}(0,\sigma^2)$ с некоторой фиксированной дисперсией $\sigma^2$ — но могут быть и другие варианты.

Проиллюстрируем, как ведут себя данные, подчиняющиеся закону $y = ax + b + \varepsilon$, $\varepsilon\sim\mathcal{N}(0, \sigma^2)$:

![9](https://yastatic.net/s3/education-portal/media/9_1_01798dd1e4_623222483e.webp)

**Вопрос на подумать**. Зачем человеку может прийти в голову предположить, что в модели линейной регрессии $y\sim Xw + \varepsilon$ шум $\varepsilon$ имеет распределение Лапласа? А распределение Коши? Чем свойства таких моделей будут отличаться от свойств модели с нормальным шумом?

{% cut "Ответ (не открывайте сразу; сначала подумайте сами!)" %}

Давайте посмотрим, как выглядят плотности этих трёх распределений:

![9](https://yastatic.net/s3/education-portal/media/9_2_e57a6770be_8a354aec5b.webp)

Распределение Лапласа имеет «более тяжёлые хвосты», чем нормальное: это значит, что плотность медленнее падает с удалением от среднего. Таким образом, этому распределению могут подчиняться данные, в которых имеются выбросы. Если не гнаться за строгостью, можно сказать, что модель с нормальным шумом будет пытаться объяснить выбросы, меняя под них $w$, тогда как лапласовский шум потерпит их, не подгоняя $w$.

У распределения Коши хвосты «ещё более тяжёлые», что, в теории, даёт возможность модели с таким шумом описывать даже ещё более шумные данные.

Проиллюстрируем датасеты, сгенерированные из моделей с каждым из типов шума: нормальным, лапласовским и Коши.

![9](https://yastatic.net/s3/education-portal/media/9_3_7384d3087c_7ae3d004f9.webp)

{% endcut %}

Как вы могли заметить, в каждом из подходов после того, как мы зафиксировали признаки (то есть координаты $x_i$), остаётся своя степень свободы: в инженерном это выбор функции потерь, а в вероятностном — выбор распределения шума.

Дальше в этом параграфе мы увидим, что на самом деле эти два подхода глубинным образом связаны между собой, причём выбор функции потерь — это в некотором смысле то же самое, что выбор распределения шума.

## Условное распределение на таргет, непрерывный случай

Допустим, что мы исследуем вероятностную модель таргета с аддитивным шумом

$$y = f_w(x) + \varepsilon,
$$

где $f_w$ — некоторая функция, не обязательно линейная с (неизвестными пока) параметрами $w$, а $\varepsilon$ — случайный шум с плотностью распределения $\varepsilon\sim p_{\varepsilon}(t)$. Для каждого конкретного объекта $x_i$ значение $f_w(x_i)$ — это просто константа, но для $y_i$ оно превращается в случайную величину, зависящую от $x_i$ (и ещё от $w$, на самом деле).

Таким образом, можно говорить об условном распределении

$$p_y(y \vert x, w)
$$

Для каждого конкретного $x_i$ и $w$ распределение соответствующего $y_i$ — это просто $p_{\varepsilon}(y - f_{w}(x_i))$, ведь $y - f_w(X) = \varepsilon$.

**Пример**. Рассмотрим вероятностную модель $y = \langle x, w\rangle + \varepsilon$, где $\varepsilon\sim\mathcal{N}(0, \sigma^2)$. Тогда для фиксированного $x_i$ имеем $y_i = \langle x_i, w\rangle + \varepsilon$. Поскольку $\langle x_i, w\rangle$ — константа, мы получаем

$$y_i\sim\mathcal{N}(\langle x_i, w\rangle, \sigma^2).
$$

Это можно записать и так:

$$p(y_i\vert x_i, w)\sim\mathcal{N}(y_i\vert\langle x_i, w\rangle, \sigma^2),
$$

где выражение справа — это значение функции плотности нормального распределения с параметрами $\langle x_i, w\rangle, \sigma^2$ в точке $y_i$. В частности, $\langle x_i, w\rangle = \mathbb{E}(y_i\vert x_i)$.

## Более сложные вероятностные модели

На самом деле, мы можем для нашей задачи придумывать любую вероятностную модель $p_y(y \vert x, w)$, не обязательно вида $y = f_w(X) + \varepsilon$.

Представьте, что мы хотим предсказывать точку в плоскости штанг, в которую попадает мячом бьющий по воротам футболист. Можно предположить, что она имеет нормальное распределение со средним (цель удара), которое определяется ситуацией на поле и состянием игрока, и некоторой дисперсией (то есть скалярной ковариационной матрицей), которая тоже зависит от состояния игрока и ещё разных сложных факторов, которые мы объявим случайными.

Состояние игрока — это сложное понятие, но, вероятно, мы можем выразить его, зная пульс, давление и другие физические показатели. В свою очередь, ситуацию на поле можно описать, как функцию от позиций и движений других игроков, судьи и зрителей — но всего не перечислишь, поэтому нам снова придётся привлекать случайность. Таким образом, мы получаем то, что называется **графической моделью**:

![9](https://yastatic.net/s3/education-portal/media/9_4_8a62e69dca_e58af3573c.webp)

Здесь стрелки означают статистические зависимости, а отсутствие стрелок — допущение о статистической независимости. Конечно же, это лишь допущение, принятое нами для ограничения сложности модели: ведь пульс человека и давление взаимосвязаны, равно как и поведение различных игроков на поле. Но мы уже обсуждали, что каждая модель, в том числе и вероятностная, является лишь приблизительным отражением бесконечно сложного мира. Впрочем, если у нас много вычислительных ресурсов, то никто не мешает нам попробовать учесть и все пропущенные сейчас зависимости.

Расписав всё по определению условной вероятности, мы получаем следующую вероятностную модель:

![9](https://yastatic.net/s3/education-portal/media/9_5_6348aec2cb_5cfcd9d525.webp)

в которой, конечно же, мы должны все вероятности расписать через какие-то понятные и логически обоснованные распределения — но пока воздержимся от этого.

## Оценка максимального правдоподобия = оптимизация функции потерь

Мы хотим подобрать такие значения параметров $w$, для которых модель $p_y(y \vert x, w)$ была бы наиболее адекватна обучающим данным. Суть **метода максимального правдоподобия** (maximum likelihood estimation) состоит в том, чтобы найти такое $w$, для которого вероятность (а в данном, непрерывном, случае плотность вероятности) появления выборки $y = \{y_1, \ldots, y_N\}$ была бы максимальной, то есть

$$\widehat{w}_{MLE} = \underset{w}{\operatorname{argmax}}p(y \vert X, w)
$$

Величина $p(y \vert X, w)$ называется **функцией правдоподобия** (likelihood). Если мы считаем, что все объекты независимы, то функция правдоподобия распадается в произведение:

$$p(y \vert X, w) = p(y_1 \vert x_1, w) \cdot\ldots\cdot p(y_i \vert x_i, w)
$$

Теперь, поскольку перемножать сложно, а складывать легко (и ещё поскольку мы надеемся, что раз наши объекты всё-таки наблюдаются в природе, их правдоподобие отлично от нуля), мы переходим к логарифму функции правдоподобия:

$$l(y \vert X,w) = \log{p(y_1 \vert x_1, w)} + \ldots + \log{p(y_i \vert x_i, w)}
$$

эту функцию мы так или иначе максимизируем по $w$, находя оценку максимального правдоподобия $\hat{w}$.

Как мы уже обсуждали выше, $p(y_i \vert x_i, w) = p_{\varepsilon}(y - f_{w}(x_i))$, то есть

$$l(y \vert X,w) = \sum\limits_{i=1}^N\log{p_{\varepsilon}(y_i - f_w(x_i))}
$$

Максимизация функции правдоподобия соответствует минимизации

$$\sum\limits_{i=1}^N\left[-\log{p_{\varepsilon}(y_i - f_w(x_i))}\right]
$$

а это выражение можно интерпретировать, как функцию потерь. Вот и оказывается, что подбор параметров вероятностей модели с помощью метода максимального правдоподобия — это то же самое, что «инженерная» оптимизация функции потерь. Давайте посмотрим, как это выглядит в нескольких простых случаях.

**Пример**. Давайте предположим, что наш таргет связан с данными вот так:

$$y_i = \langle x_i, w \rangle + \varepsilon
$$

где $\varepsilon\sim\mathcal{N}(0, \sigma^2)$, то есть

$$p(\varepsilon) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{\varepsilon^2}{2\sigma^2}\right)
$$

Случайная величина $y_i$ получается из шума $\varepsilon$ сдвигом на постоянный вектор $\langle x_i, w \rangle$, так что она тоже распределена нормально с той же дисперсией $\sigma^2$ и со средним $\langle x_i, w \rangle$

$$p(y_i\vert \langle x_i, w \rangle) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{(y_i - \langle x_i, w \rangle)^2}{2\sigma^2}\right)
$$

Правдоподобие выборки имеет вид

$$p(y\vert X, w) = \prod_{i=1}^N p(y_i \vert x_i, w) = \prod_{i=1}^N \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{(y_i-\langle w,x_i\rangle)^2}{2\sigma^2}\right)
$$

Логарифм правдоподобия можно переписать в виде

$$l(y \vert X,w) = \sum_{i=1}^N \left(-\log({\sqrt{2 \pi \sigma^2}}) -\frac{(y_i-\langle w,x_i\rangle)^2}{2\sigma^2}\right)
$$

Постоянными слагаемыми можно пренебречь, и тогда оказывается, что максимизация этой величины равносильна минимизации

$$ \sum_{i=1}^N (y_i-\langle w,x_i\rangle)^2
$$

Мы получили обычную квадратичную функцию потерь. Итак, обучать вероятностную модель линейной регрессии с нормальным шумом — это то же самое, что учить «инженерную» модель с функцией потерь MSE.

**Вопрос на подумать**. Какая вероятностная модель соответствует обучению линейной регрессии с функцией потерь MAE

$$\sum_{i=1}^N \vert y_i-\langle w,x_i\rangle\vert?
$$

{% cut "Ответ (не открывайте сразу; сначала подумайте сами!)" %}

Минимизация функции потерь MAE соответствует максимизации

$$\sum_{i=1}^N\left[-\vert y_i-\langle w,x_i\rangle\vert\right]
$$

Мы хотим найти такое распределение, для которого эта штука является с точностью до константы логарифмом функции правдоподобия. Что ж, возьмём экспоненту:

$$\text{exp}\left[-\sum_{i=1}^N\vert y_i-\langle w,x_i\rangle\vert\right] =
\prod_{i=1}^N\text{exp}\left(-\vert y_i-\langle w,x_i\rangle\vert\right)$$

Если теперь это умножить на $\left(\frac12\right)^{N}$, то мы получим функцию правдоподобия для распределения Лапласа:

$$\prod_{i=1}^N\frac12\text{exp}\left(-\vert y_i-\langle w,x_i\rangle\vert\right) = \prod_{i=1}^NLaplace\left(y_i-\langle w,x_i\rangle\right)
$$

Итак, учить «инженерную» модель с функцией потерь MAE — это то же самое, что обучать вероятностную модель линейной регрессии с лапласовским шумом.

{% endcut %}

## Предсказание в вероятностных моделях

Теперь представим, что параметры подобраны, и подумаем о том, как же теперь делать предсказания.

Рассмотрим модель линейной регрессии

$$y = \langle x, w\rangle + \varepsilon,\quad\varepsilon\sim\mathcal{N}(0,\sigma^2)
$$

Если $w$ известен, то для нового объекта $x_0$ соответствующий таргет имеет вид

$$y_0 = \langle x_0, w\rangle + \varepsilon\sim\mathcal{N}(\langle x_0, w\rangle, \sigma^2)
$$

Таким образом, $y_0$ дан нам не точно, а в виде распределения (и логично: ведь мы оговорились выше, что ответы у нас искажены погрешностью, проинтерпретированной, как нормальный шум). Но что делать, если требуют назвать конкретное число? Кажется логичным выдать условное матожидание $\mathbb{E}(y_0\vert x_0) = \langle x_0, w\rangle$, тем более что оно совпадает с условной медианой и условной модой этого распределения.

Если же медиана, мода и математическое ожидание различаются, то можно выбрать что-то из них с учётом особенностей задачи. Но на практике в схеме $y\sim f(x) + \varepsilon$ чаще всего рассматривают именно симметричные распределения с нулевым матожиданием, потому что для них $f(x)$ совпадает с условным матожиданием $\mathbb{E}(y\vert x)$ и является логичным точечным предсказанием.

Приведём пример. Допустим шум $\varepsilon$ был бы из экспоненциального распределения. Тогда $f(x)$ была бы условным минимумом распределения. В принципе, можно придумать задачу, для которой такая постановка (предсказание минимума) была бы логичной. Но это всё же довольно экзотическая ситуация. Приводим для сравнения модели с нормальным, лапласовским и экспоненциальным шумом:

![9](https://yastatic.net/s3/education-portal/media/9_5b_275fad6f75_71f67fcd24.webp)

## Условное распределение на таргет, дискретный случай

Допустим, мы имеем дело с задачей классификации с $K$ классами. Как мы можем её решать? Самый наивный вариант — научиться по каждому объекту $x_i$ предсказывать некоторое число для каждого класса, и у кого число больше — тот класс и выбираем! Наверное, так можно сделать, если мы придумаем хорошую функцию потерь. Но сразу в голову приходит мысль: почему бы не начать предсказывать не просто число, а вероятность?

Таким образом, задача классификации сводится к предсказанию

$$P(y_i = k \vert x_i)
$$

и как будто бы выбору класса с наибольшей вероятностью. Впрочем, как мы увидим дальше, всё не всегда работает так просто.

Одну такую модель — правда, только для бинарной классификации — вы уже знаете. Это логистическая регрессия:

$$P(y_i = 1 \vert x_i,w) = \frac{1}{1+e^{-\langle x_i, w\rangle}},\quad P(y_i = 0 \vert x_i,w) = \frac{e^{-(x_i, w)}}{1+e^{-\langle x_i, w\rangle}} = \frac{1}{1+e^{\langle x_i, w\rangle}}
$$

которую также можно записать в виде

$$y_i \vert x_i \sim \color{red}{Bern}\left(\frac{1}{1+e^{-\langle x_i, w\rangle}}\right)
$$

где $\color{red}{Bern}(p)$ — распределение Бернулли с параметром $p$.

Нахождение вероятностей классов можно разделить на два этапа:

$$\begin{aligned}
&\text { Находим }\\
&x_i \xrightarrow{\text { логиты }}\left(-\left\langle x_i, w\right\rangle,\left\langle x_i, w\right\rangle\right) \xrightarrow{\sigma}\left(\sigma\left(-\left\langle x_i, w\right\rangle\right), \sigma\left(\left\langle x_i, w\right\rangle\right)\right)
\end{aligned}
$$

где, напомним, $\sigma$ — это сигмоида:

$$\sigma(t) = \frac{1}{1+e^{-t}}
$$

Сигмоида тут не просто так. Она обладает теми счастливыми свойствами, что

* монотонно возрастает;

* отображает всю числовую прямую на интервал $(0,1)$;

* $\sigma(-x) = 1 - \sigma(x)$.

Вот такой вид имеет её график:

![9](https://yastatic.net/s3/education-portal/media/9_6_b02f5cb3c2_300bacce84.webp)

Иными словами, с помощью сигмоиды можно делать «вероятности» из чего угодно, то есть более или менее для любого отображения $f_w$ (из признакового пространства в $\mathbb{R}$) с параметрами $w$ построить модель бинарной классификации:

$$P(y_i = 0 \vert x_i, w) = \sigma(f_w(-x_i)),\quad P(y_i = 1 \vert x_i, w) = \sigma(f_w(x_i)).
$$

Как и в случае логистической регрессии, такая модель равносильна утверждению о том, что

$$f_w(x_i) = \log{\frac{p(y = 1 \vert x_i,w)}{p(y = 0 \vert x_i, w)}}.
$$

Похожим способом можно строить и модели для многоклассовой классификации. В этом нам поможет обобщение сигмоиды, которое называется **softmax**:

$$softmax(t_1,\ldots,t_K) = \left(\frac{e^{t_1}}{\sum_{k=1}^Ke^{t_k}},\ldots,\frac{e^{t_K}}{\sum_{k=1}^Ke^{t_k}}\right)
$$

А именно, для любого отображения $f_w$ из пространства признаков в $\mathbb{R}^K$ мы можем взять модель

$$\left(P(y_i = k \vert x_i, w)\right)^K_{k=1} = softmax(f_w(x_i))
$$

Если все наши признаки — вещественные числа, а $f_w(x_i) = x_iW$ — просто линейное отображение, то мы получаем однослойную нейронную сеть

$$\left(P(y_i = k \vert x_i, w)\right)^K_{k=1} = softmax(x_iW)
$$

![9](https://yastatic.net/s3/education-portal/media/9_7_ac9ec70d39_9cf6bad9a3.webp)

**Предостережение**. Всё то, что мы описали выше, вполне работает на практике (собственно, классификационные нейросети зачастую так и устроены), но корректным не является.

В самом деле, мы говорим, что строим оценки вероятностей $P(y_i = k \vert x_i, w)$, но для подбора параметров используем не эмпирические вероятности, а только лишь значения $\underset{k}{\operatorname{argmax}} \ P(y_i = k \vert x_i, w)$, то есть метки предсказываемых классов. Таким образом, при обучении мы не будем различать следующие две ситуации:

![9](https://yastatic.net/s3/education-portal/media/9_8_33dfe17736_21e3dc05b5.webp)

Это говорит нам о некоторой неполноценности такого подхода.

Заметим ещё вот что. В случае бинарной классификации выбор предсказываемого класса как $\underset{k}{\operatorname{argmax}} P(y_i=k \vert x_i,w)$ равносилен выбору того класса, для которого $P(y_i=k \vert x_i,w) > \frac{1}{2}$. Но если наши оценки вероятностей неадекватны, то этот вариант проваливается, и мы встаём перед проблемой выбора порога: каким должно быть значение $\widehat{t}$, чтобы мы могли приписать класс 1 тем объектам $x_i$, для которых $\sigma(f_w(x_i)) > \widehat{t}$?

В одном из следующих параграфов мы обсудим, как всё-таки правильно предсказывать вероятности.

  ## handbook

  Учебник по машинному обучению

  ## title

  Вероятностный подход в ML

  ## description

  *null*

- 
  ## path

  /handbook/ml/article/eksponencialnyj-klass-raspredelenij-i-princip-maksimalnoj-entropii

  ## content

  ## Мотивация: метод моментов

Метод моментов — это ещё один способ, наряду с методом максимального правдоподобия, оценки параметров распределения по данным $x_1,\ldots,x_N$. Суть его в том, что мы выражаем через параметры распределения теоретические значения моментов $\mu_k = \mathbb{E}x^k$ нашей случайной величины, затем считаем их выборочные оценки $\widehat{\mu}_k = \frac1N\sum_ix_i^k$, приравниваем их все друг к другу и, решая полученную систему, находим оценки параметров.

Можно доказать, что полученные оценки являются состоятельными, хотя могут быть смещены.

**Пример 1**. Оценим параметры нормального распределения $\mathcal{N}(\mu, \sigma^2)$ с помощью метода моментов.

Теоретические моменты равны

$$\mu_1 = \mu,\quad\mu_2 = \sigma^2 + \mu^2
$$

Запишем систему:

$$\begin{cases}
\widehat{\mu} = \frac1N\sum_i x_i,\\
\widehat{\sigma}^2 + \widehat{\mu}^2 = \frac1N\sum_ix_i^2
\end{cases}
$$

Из неё очевидным образом находим

$$\widehat{\mu} = \frac1N\sum_ix_i 
$$

$$\widehat{\sigma}^2 = \frac1N\sum_ix_i^2 - \left(\frac1N\sum_i x_i\right)^2=
$$

$$=\frac1N\sum_i\left(x_i - \widehat{\mu}\right)^2
$$

Легко видеть, что полученные оценки совпадают с оценками максимального правдоподобия

**Пример 2**. Оценим параметр $\mu$ логнормального распределения

$$p(x) = \frac1{x\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(\log{x} - \mu)^2}{2\sigma^2}\right)
$$

при известном $\sigma^2$. Будет ли оценка совпадать с оценкой, полученной с помощью метода максимального правдоподобия?

Теоретическое математическое ожидание равно $\exp\left(\mu + \frac{\sigma^2}2\right)$, откуда мы сразу находим оценку $\widehat{\mu} = \log\left(\frac1N\sum_i x_i\right) - \frac{\sigma^2}2$.

Теперь запишем логарифм правдоподобия:

$$l(X) = -\sum_i\log{x_i} - \sum_i\frac{(\log{x_i} - \mu)^2}{2\sigma^2} + const
$$

Дифференцируя по $\mu$ и приравнивая производную к нулю, получаем

$$\widehat{\mu}_{MLE} = \frac1N\sum_i\log{x_i}
$$

что вовсе не совпадает с оценкой выше.

Несколько приукрасив ситуацию, можно сделать вывод, что первые два выборочных момента позволяют если не править миром, то уверенно восстанавливать параметры распределений. А теперь давайте представим, что мы посчитали $\frac1N\sum_ix_i$ и $\frac1N\sum_ix_i^2$, а семейство распределений пока не выбрали.

Как же совершить этот судьбоносный выбор? Давайте посмотрим на следующие три семейства и подумаем, в каком из них мы бы стали искать распределение, зная его истинные матожидание и дисперсию?

![10](https://yastatic.net/s3/education-portal/media/10_1_f204345b73_03123eb66b.webp)

Почему-то хочется сказать, что в первом. Почему? Второе не симметрично — но почему мы так думаем? Если мы выберем третье, то добавим дополнительную информацию как минимум о том, что у распределения конечный носитель. А с чего бы? У нас такой инфомации вроде бы нет.

Общая идея такова: мы будем искать распределение, которое удовлетворяет только явно заданным нами ограничениям и не отражает никакого дополнительного знания о нём. Но чтобы эти нестрогие рассуждения превратить в формулы, придётся немного обогатить наш математический аппарат и научиться измерять количество информации.

## Энтропия и дивергенция Кульбака-Лейблера

Измерять «знание» можно с помощью **энтропии Шэннона**. Она определяется как

$$\color{#348FEA}{H(P) = -\sum_xP(x)\log{P(x)}}
$$

для дискретного распределения и

$$\color{#348FEA}{H(p) = -\int p(x)\log{p(x)}dx}
$$

для непрерывного. В классическом определении логарифм двоичный, хотя, конечно, варианты с разным основанием отличаются лишь умножением на константу.

Неформально можно представлять, что энтропия показывает, насколько сложно предсказать значение случайной величины. Чуть более строго — сколько в среднем бит нужно потратить, чтобы передать информацию о её значении.

**Пример 1**. Рассмотрим схему Бернулли с вероятностью успеха $p$. Энтропия её результата равна

$$-(1 - p)\cdot\log_2(1 - p) - p\cdot\log_2{p}
$$

Давайте посмотрим на график этой функции:

![10](https://yastatic.net/s3/education-portal/media/10_2_d107fffe47_f64a04ca06.webp)

Минимальное значение (нулевое) энтропия принимает при $p\in\{0,1\}$. В самом деле, для такого эксперимента мы всегда можем наверняка сказать, каков будет его исход; обращаясь к другой интерпретации — чтобы сообщить кому-то о результате эксперимента, достаточно $0$ бит (ведь получатель сообщения и так понимает, что вышло).

Максимальное значение принимается в точке $\frac12$, что вполне соответствует тому, что при $p=\frac12$ предсказать исход эксперимента сложнее всего.

{% cut "Дополнение для ценителей математики." %}

Попробуем для этого простого примера объяснить, почему среднее число бит, необходимых для передачи информации об исходе эксперимента, выражается формулой с логарифмами.

Теперь пусть $p$ произвольно. Рассмотрим $N»1$ независимых испытаний $x_1,\ldots, x_N$; среди них будет $n_0\approx (1-p)N$ неудачных и $n_1\approx pN$ удачных. Посчитаем, сколько бит потребуется, чтобы закодировать последовательность $x_i$ для известных $n_0$ и $n_1$. Общее число таких последовательностей равно $C_N^{n_1} = \frac{N!}{n_0!n_1!}$, а чтобы закодировать каждую достаточно будет $\log_2\left(\frac{N!}{n_0!n_1!}\right)$ бит — это количество информации, содержащееся во всей последовательности. Таким образом, в среднем, чтобы закодировать результат одного испытания необходимо

$$\frac1N\log_2\left(\frac{N!}{n_0!n_1!}\right)
$$

бит информации. Перепишем это выражение, использовав формулу Стирлинга $\log{N!}\approx N\log{N} - N$:

$$\frac1N\left(\log_2{N!} - \log_2{n_0!} - \log_2{n_1!}\right) \approx 
$$

$$\approx const\cdot\frac1N\left(N\log_2{N} - N - n_0\log_2{n_0} + n_0 - n_1\log_2{n_1} + n_1\right) =
$$

$$=const\cdot\frac1N\left(N\log_2{N} - (1-p)N\log_2{(1-p)N} - pN\log_2{pN}\right) =
$$

$$=const\cdot\left(-(1-p)\cdot\log_2(1-p) - p\log_2{p}\right)
$$

Вот мы и вывели формулу энтропии!

{% endcut %}

**Пример 2**. Энтропия нормального распределения $\mathcal{N}(\mu, \sigma^2)$ равна $\frac12\log(2\pi\sigma^2) + \frac12$, и чем меньше дисперсия, тем меньше энтропия, что и логично: ведь когда дисперсия мала, значения сосредоточены возле матожидания, и они становятся менее «разнообразными».

Энтропия тесно связана с другим важным понятием из теории информации — **дивергенцией Кульбака-Лейблера**. Она определяется для $p(x)$ $q(x)$ как

$$\color{#348FEA}{KL(p\vert\vert q) = \int p(x)\log{\frac{p(x)}{q(x)}}dx}
$$

в непрерывном случае и точно так же, но только с суммой вместо интеграла в дискретном.

Дивергенцию можно представить в виде разности:

$$KL(p\vert\vert q) = (-\int p(x)\log{q(x)}dx) - (-\int p(x)\log{p(x)}dx)
$$

Вычитаемое — это энтропия, которая, как мы уже поняли, показывает, сколько в среднем бит требуется, чтобы закодировать значение случайной величины. Уменьшаемое похоже по виду, и можно показать, что оно говорит о том, сколько в среднем бит потребуется на кодирование случайной величины с плотностью $p$ алгоритмом, оптимизированным для кодирования случайной величины $q$.

Иными словами, дивергенция Кульбака-Лейблера говорит о том, насколько увеличится средняя длина кодов для значений $p$, если при настройке алгоритма кодирования вместо $p$ использовать $q$. Более подробно вы можете почитать, например, в [этом посте](https://habr.com/ru/post/484756/).

Дивергенция Кульбака-Лейблера в некотором роде играет роль расстояния между распределениями. В частности, $KL(p\vert\vert q)\geqslant0$, причём дивергенция равна нулю, только если распределения совпадают почти всюду. Но при этом она не является симметричной: вообще говоря, $KL(p\vert\vert q)\ne KL(q\vert\vert p)$.

**Вопрос на подумать**. Пусть $p(x)$ — распределение, заданное на отрезке $[a, b]$. Выразите энтропию через дивергенцию Кульбака-Лейблера $p(x)$ с равномерным на отрезке распределением $q_U(x)=\frac1{b-a}\mathbb{I}_{[a,b]}(x)$.

{% cut "Ответ (не открывайте сразу; сначала подумайте сами!)" %}

Распишем дивергенцию:

$$KL(p\vert\vert q_U) = -\left(-\int_a^b p(x)\log{p(x)}dx\right) - \int_a^b p(x)\log{\underbrace{q(x)}_{=\frac1{b-a}\text{ на }[a,b]}}dx=
$$

$$=\log(b-a) - H(p)
$$

Аналогичное соотношение можно выписать и для распределения, заданного на конечном множестве.

{% endcut %}

## Принцип максимальной энтропии

Теперь наконец мы готовы сформулировать, какие распределения мы хотим искать.

**Принцип максимальной энтропии**. Среди всех распределений на заданном носителе $\mathbb{X}$, удовлетворяющих условиям $\mathbb{E}u_1(x) = \mu_1$, ..., $\mathbb{E}u_k(x) = \mu_k$, где $u_i$ — некоторые функции, мы хотим иметь дело с тем, которое имеет наибольшую энтропию.

В самом деле, энтропия выражает нашу меру незнания о том, как ведёт себя распределение, и чем она больше — тем более «произвольное распределение», по крайней мере, в теории.

Давайте рассмотрим несколько примеров, которые помогут ещё лучше понять, почему некоторые распределения так популярны:

**Пример 1**. На конечном множестве $1,\ldots,n$ наибольшую энтропию имеет равномерное распределение (носитель — конечное множество из $n$ элементов, других ограничений нет).

**Доказательство**: Пусть $p_i$, $i=1,\ldots,n$ — некоторое распределение, $q_i=\frac1n$ — равномерное. Запишем их дивергенцию Кульбака-Лейблера:

$$KL(p\vert\vert q) = \sum_i p_i\log{p_i} - \sum_i p_i\log{q_i} =
$$

$$= -H(p) + \log{n}\underbrace{\sum_ip_i}_{=1}
$$

Так как дивергенция Кульбака-Лейблера всегда неотрицательна, получаем, что $H(p)\leqslant\log{n}$. При этом равенство возможно, только если распределения совпадают.

**Пример 2**. Среди распределений, заданных на всей вещественной прямой и имеющих заданные матожидание $\mu$ и дисперсию $\sigma^2$ наибольшую энтропию имеет нормальное распределение $\mathcal{N}(\mu,\sigma^2)$.

**Доказательство**: Пусть $p(x)$ — некоторое распределение, $q(x)\sim\mathcal{N}(\mu, \sigma^2)$. Запишем их дивергенцию Кульбака-Лейблера:

$$KL(p\vert\vert q) = \int p(x)\log{p(x)}dx - \int p(x)\log{q(x)}dx =
$$

$$= -H(p) - \int p(x)\left(-\frac12\log(2\pi\sigma^2) - \frac1{2\sigma^2}(x - \mu)^2\right)dx =
$$

$$= - H(p) +\frac12\log(2\pi\sigma^2)\cdot\underbrace{\int p(x)dx}_{=1} + \frac1{2\sigma^2}\underbrace{\int(x - \mu)^2p(x)dx}_{=\mathbb{V}p=\sigma^2} =
$$

$$= - H(p) + \underbrace{\frac12\log(2\pi\sigma^2) + \frac12}_{=H(q)}
$$

Так как дивергенция Кульбака-Лейблера всегда неотрицательна, получаем, что $H(p)\leqslant H(q)$. При этом равенство возможно, только если распределения $p$ и $q$ совпадают почти всюду, а с точки зрения теории вероятностей такие распределения различать не имеет смысла.

**Пример 3**. Среди распределений, заданных на множестве положительных вещественных чисел и имеющих заданное матожидание $\lambda$ наибольшую энтропию имеет показательное распределение с параметром $\frac1{\lambda}$ (его плотность равна $p(x) = \frac1{\lambda}\exp\left(-\frac1{\lambda}x\right)\mathbb{I}_{(0;+\infty)}(x)$).

Все хорошо знакомые нам распределения, не правда ли? Проблема в том, что они свалились на нас чудесным образом. Возникает вопрос, можно ли их было не угадать, а вывести как-нибудь? И как быть, если даны не эти конкретные, а какие-то другие ограничения?

Оказывается, что при некоторых не очень обременительных ограничениях ответ можно записать с помощью распределений экспоненциального класса. Давайте же познакомимся с ними поближе.

## Экспоненциальное семейство распределений

Говорят, что семейство распределений относится к **экспоненциальному классу**, если оно может быть представлено в следующем виде:

$$\color{#348FEA}{p(x\vert\theta) = \frac1{h(\theta)}g(x)\cdot\exp\left(\theta^Tu(x)\right)}
$$

где $\theta$ — вектор вещественнозначных параметров (различные значения которых дают те или иные распределения из семейства), $h, g > 0$, $u$ — некоторая вектор-функция, и, разумеется, сумма или интеграл по $x$ равняется единице. Последнее, в частности, означает, что

$$h(\theta) = \int g(x)\exp\left(\theta^Tu(x)\right)dx
$$

(или сумма в дискретном случае).

**Пример 1**. Покажем, что нормальное распределение принадлежит экспоненциальному классу. Для этого мы должны представить привычную нам функцию плотности

$$p(x \vert \mu, \sigma^2) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
$$

в виде

$$p(x\vert\theta) = \frac{g(x)\cdot\exp\left(\sum_i\text{(параметр)}_i\cdot\text{(функция от x)}_i\right)}{\text{что-то, не зависящее от $x$}}
$$

Распишем

$$\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right) =
\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac1{2\sigma^2}x^2 + \frac{\mu}{\sigma^2}x + \frac{\mu^2}{2\sigma^2}\right)=$$

$$\frac{\exp\left(-\frac1{2\sigma^2}x^2 + \frac{2\mu}{2\sigma^2}x\right)}{\sqrt{2\pi}\sigma\exp\left(-\frac{\mu^2}{2\sigma^2}\right)}=$$

Определим

$$u_1(x) = x,\qquad u_2(x) = x^2
$$

$$\theta_1 = \frac{\mu}{\sigma^2},\quad \theta_2 = -\frac1{2\sigma^2}
$$

$$h(\theta) = \sqrt{2\pi}\sigma\exp\left(-\frac{\mu^2}{2\sigma^2}\right)
$$

Если теперь всё-таки честно выразить $h$ через $\theta$ (это мы оставляем в качестве лёгкого упражнения), то получится

$$p(x \vert \mu, \sigma^2) = \frac1{h(\theta)}\exp\left(\theta^Tu(x)\right)
$$

В данном случае функция $g(x)$ просто равна единице.

**Пример 2**. Покажем, что распределение Бернулли принадлежит экспоненциальному классу. Для этого попробуем преобразовать функцию вероятности (ниже $x$ принимает значения $0$ или $1$):

$$P(x \vert p) = p^x(1 - p)^{1 - x} = \exp\left(x\log{p} + (1 - x)\log(1 - p)\right)
$$

Теперь мы можем положить $u(x) = \left(x, 1 - x\right)$, $\theta = \left(p, 1 - p\right)$, и всё получится. Единственное, что смущает, — это то, что компоненты вектора $u(x)$ линейно зависимы. Хотя это не является формальной проблемой, но всё же хочется с этим что-то сделать. Исправить это можно, если переписать

$$p^x(1 - p)^{1 -x} = (1 - p)\exp\left(x\log{p} + (-x)\log(1 - p)\right) =
$$

$$=(1 - p)\exp\left(x\log{\frac{p}{1 - p}}\right)
$$

и определить уже минимальное представление с $u(x) = x$, $\theta = \log{\frac{p}{1 - p}}$ — мы ведь уже сталкивались с этим выражением, когда изучали логистическу регрессию, не так ли?

**Вопрос на подумать**. Принадлежит ли к экспоненциальному классу семейство равномерных распределений на отрезках $U[a, b]$? Казалось бы, да: так как:

$$p(x) = \frac{1}{b - a}\mathbb{I}_{[a,b]}(x)\exp(0)
$$

В чём может быть подвох?

{% cut "Ответ (не открывайте сразу; сначала подумайте сами!)" %}

Нет, не принадлежит. Давайте вспомним, как звучало определение экспоненциального семейства. Возможно, вас удивило, что там было написано не «распределение относится», а «семейство распределений относится».

Это важно: ведь семейство определяется именно различными значениями $\theta$, и если нас интересует семейство равномерных распределений на отрезках, определяемое параметрами $a$ и $b$, то они не могут быть в функции $g(x)$, они должны быть под экспонентой, а экспонента ни от чего не может быть равна индикатору.

При этом странное и не очень полезное семейство с нулём параметров, состоящее из одинокого распределения $U[0,1]$ можно считать относящимся к экспоненциальному классу: ведь для него формула

$$p(x) = \mathbb{I}_{[0,1]}(x)\exp(0)
$$

будет работать.

{% endcut %}

Как мы увидели, к экспоненциальным семействам относятся как непрерывные, так и дискретные распределения. Вообще, к ним относится большая часть распределений, которыми Вам на практике может захотеться описать $Y \vert X$.

В том числе:

* нормальное;
* распределение Пуассона;
* экспоненциальное;
* биномиальное, мультиномиальное (с фиксированным числом испытаний);
* геометрическое;
* $\chi^2$-распределение;
* бета-распределение;
* гамма-распределение;
* распределение Дирихле.

К экспоненциальным семействам не относятся, к примеру:

* равномерное распределение на отрезке;
* $t$-распределение Стьюдента;
* распределение Коши;
* смесь нормальных распределений.

## MLE для семейства из экспоненциального класса

Возможно, вас удивил странный и на первый взгляд не очень естественный вид $p(x\vert\theta)$. Но всё не просто так: оказывается, что оценка максимального правдоподобия параметров распределений из экспоненциального класса устроена очень интригующе.

Запишем функцию правдоподобия выборки $X = (x_1,\ldots,x_N)$:

$$p(X\vert\theta) = h(\theta)^{-N}\cdot\left(\prod_{i=1}^Ng(x_i)\right)\cdot\exp\left(\theta^T\left[\sum_{i=1}^Nu(x_i)\right]\right)
$$

Её логарифм равен

$$l(X\vert\theta) = -N\log{h(\theta)} + \sum_{i=1}^N\log{g(x_i)} + \theta^T\left[\sum_{i=1}^Nu(x_i)\right]
$$

Дифференцируя по $\theta$, получаем

$$\nabla_{\theta}l(X\vert\theta) = -N\nabla_{\theta}\log{h(\theta)} + \left[\sum_{i=1}^Nu(x_i)\right]
$$

Тут нам потребуется следующая

**Лемма**. $\nabla_{\theta}\log{h(\theta)} = \mathbb{E}u(x)$

**Доказательство**:

Как мы уже отмечали в прошлом пункте:

$$h(\theta) = \int g(x)\exp\left(\theta^Tu(x)\right)dx
$$

Следовательно,

$$\nabla_{\theta}\log{h(\theta)} = \frac{\nabla_{\theta}\int g(x)\exp\left(\theta^Tu(x)\right)dx}{\int g(x)\exp\left(\theta^Tu(x)\right)dx} =
$$

$$= \frac{\int u(x)g(x)\exp\left(\theta^Tu(x)\right)dx}{h(\theta)} =
$$

$$=\int u(x)\cdot\frac1{h(\theta)}g(x)\exp\left(\theta^Tu(x)\right)dx = \mathbb{E}u(x)
$$

Кстати, можно ещё доказать, что

$$\frac{\partial}{\partial \theta_i\partial\theta_j}\log{h(\theta)} = \text{Cov}(u_i(x), u_j(x))
$$

Приравнивая $\nabla_{\theta}l(X\vert\theta)$ к нулю и применяя лемму, мы получаем, что

$$\color{#348FEA}{\mathbb{E}u(x) = \frac1N\left[\sum_{i=1}^Nu(x_i)\right]}
$$

Таким образом, теоретические матожидания всех компонент $u_i(x)$ должны совпадать с их эмпирическими оценками, а метод максимального правдоподобия совпадает с методом моментов для $\mathbb{E}u_i(x)$ в качестве моментов.

И в следующем пункте выяснится, что распределения из семейств, относящихся к экспоненциальному классу, это те самые распределения, которые имеют максимальную энтропию из тех, что имеют заданные моменты $\mathbb{E}u_i(x)$.

\*\*Пример.\*\*Рассмотрим вновь логнормальное распределение:

$$p(x) = \frac1{x\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(\log{x} - \mu)^2}{2\sigma^2}\right) =
$$

$$=\frac1{x\sqrt{2\pi\sigma^2}}\exp\left(-\frac1{2\sigma^2}\log^2{x} + \frac{\mu}{\sigma^2}\log{x} - \frac{\mu^2}{2\sigma^2}\right) =
$$

$$=\frac1{x\sqrt{2\pi\sigma^2}\exp\left(\frac{\mu^2}{2\sigma^2}\right)}\exp\left(\underbrace{\frac{\mu}{\sigma^2}}_{=\theta_1}\underbrace{\log{x}}_{=u_1(x)} -\underbrace{\frac1{2\sigma^2}}_{=\theta_2}\underbrace{\log^2{x}}_{=u_2(x)}  \right) =
$$

$$\frac{1}{\sqrt{-\pi\theta_2^{-1}}\cdot\exp{-\frac{\theta_1^2}{4\theta_2}}}\cdot\frac1x\exp\left(\theta_1u_1(x) + \theta_2u_2(x)\right)
$$

Как видим, логнормальное распределение тоже из экспоненциального класса. Вас может это удивить: ведь выше мы обсуждали, что для него метод моментов и метод максимального правдоподобия дают разные оценки.

Но никакого подвоха тут нет: мы просто брали не те моменты. В данном случае $u_1(x) = \log{x}$, $u_2(x) = \log^2{x}$, их матожидания и надо брать; тогда для параметров, получаемых из MLE, должно выполняться

$$\mathbb{E}\log{x} = \frac1N\sum_i\log{x_i},\quad \mathbb{E}\log^2{x} = \frac1N\sum_i\log^2{x_i}
$$

Матожидания в левых частых мы должны выразить через параметры — и нам для этого совершенно не обязательно что-то интегрировать! В самом деле:

$$\mathbb{E}\log{x} = \frac{\partial}{\partial\theta_1}\log{h(\theta)} =
$$

$$=\frac{\partial}{\partial\theta_1}\left(-\frac12\log{\pi} + \frac12\log{\theta_2} - \frac{\theta_1^2}{4\theta_2^2}\right) = -\frac{\theta_1}{2\theta_2^2}
$$

$$
\mathbb{E}\log^2{x} = \frac{\partial}{\partial\theta_2}\log{h(\theta)} = \frac1{2\theta_2} + \frac{\theta_1^2}{2\theta_2^3}
$$

## Теорема Купмана-Питмана-Дармуа

Теперь мы наконец готовы сформулировать одно из самых любопытных свойств семейств экспоненциального класса.

В следующей теореме мы опустим некоторые не очень обременительные условия регулярности. Просто считайте, что для хороших дискретных и абсолютно непрерывных распределений, с которыми вы в основном и будете сталкиваться, это так.

**Теорема**. Пусть $p(x) = \frac1{h(\theta)}\exp\left(\theta^Tu(x)\right)$ — распределение, причём $\theta$ — вектор длины $n$ и $\mathbb{E}u_i(x) = \alpha_i$ для некоторых фиксированных $\alpha_i$, $i=1,\ldots,n$. Тогда распределение $p(x)$ обладает наибольшей энтропией среди распределений с тем же носителем, для которых $\mathbb{E}u_i(x) = \alpha_i$, $i=1,\ldots,n$. При этом оно  — единственное с таким свойством: в том смысле, что любое другое распределение, обладающее этим свойством, совпадает с ним почти всюду.

{% cut "Идея обоснования через оптимизацию." %}

Мы приведём рассуждение для дискретного случая; в абсолютно непрерывном рассуждения будут по сути теми же, только там придётся дифференцировать не по переменным, а по функциям, и мы решили не ввергать вас в мир вариационного исчисления.

В дискретном случае у нас есть счётное семейство точек $x_1, x_2,\ldots$, и распределение определяется счётным набором вероятностей $p_i$ принимать значение $x_i$. Мы будем решать задачу

$$\begin{cases}
-\sum_j p_j\log{p_j}\longrightarrow\max,\\
\sum_jp_ju_i(x_j) = \alpha_i, i = 1,\ldots,n,\\
\sum_jp_j = 1,\\
p_j\geqslant0
\end{cases}
$$

Запишем лагранжиан:

$$\mathcal{L} = \sum_j p_j\log{p_j} + \sum_i\theta_i\left(\alpha_i - \sum_jp_ju_i(x_j)\right)+
$$

$$+\theta_0\left(\sum_jp_j - 1\right) - \sum_j\lambda_jp_j
$$

Продифференцируем его по $p_j$:

$$\frac{\partial\mathcal{L}}{\partial p_j} = \log{p_j} + 1 - \sum_i\theta_iu_i(x_j) + \theta_0 - \lambda_j
$$

Приравнивая это к нулю, получаем

$$p_j = \frac{\exp\left(\langle \theta, u(x_j)\rangle\right)}{\exp\left(\lambda_j - \theta_0 - 1\right)}
$$

Числитель уже ровно такой, как и должен быть у распределения из экспоненциального класса. Разберёмся со знаменателем.

Легко видеть, что условие $p_j\geqslant0$ заведомо выполнено (ведь тут сплошные экспоненты), так что его можно было выкинуть из постановки задачи оптимизации или, что то же самое, положить $\lambda_j = 0$. Параметр $\theta_0$ находится из условия $\sum_jp_j = 1$, а точнее, выражается через остальные $\theta_i$, что позволяет записать знаменатель в виде $h(\theta)$.

{% endcut %}

{% cut "Идея доказательства «в лоб»." %}

Как и следовало ожидать, оно ничем не отличается от того, как мы доказывали максимальность энтропии у равномерного или нормального распределения. Пусть $q(x)$ — ещё одно распределение, для которого

$$\int u_i(x)q(x)dx = \int u_i(x)p(x)dx
$$

для всех $i = 1,\ldots,n$. Тогда

$$0\leqslant KL(q\vert\vert p) = \int q(x)\log\left(\frac{q(x)}{p(x)}\right)dx = 
$$

$$=\underbrace{\int q(x)\log{q(x)}dx}_{-H(q)} - \int q(x)\log{p(x)}dx=
$$

$$=-H(q) - \int q(x)\left(-\log{h(\theta)} + \sum_i\theta_iu_i(x)\right)dx =
$$

$$=-H(q) - \log{h(\theta)}\underbrace{\int q(x)dx}_{=1=\int p(x)dx} - \sum_i\theta_i\underbrace{\int q(x)u_i(x)dx}_{=\int p(x)u_i(x)dx} =
$$

$$=-H(q) - \int p(x)\left(-\log{h(\theta)} + \sum_i\theta_iu_i(x)\right)dx =
$$

$$=-H(q) + \int p(x)\log{p(x)}dx = -H(q) + H(p)
$$

Таким образом, $H(p)\geqslant H(q)$, причём по уже не раз использованному нами свойству дивергенции Кульбака-Лейблера из $H(p) = H(q)$ будет следовать то, что $p$ и $q$ совпадают почти всюду.

{% endcut %}

Рассмотрим несколько примеров:

**Пример 1**. Среди распределений на множестве $\{1,2,3,\ldots\}$ неотрицательных целых чисел с заданным математическим ожиданием $\mu$ найдём распределение с максимальной энтропией.

В данном случае у нас лишь одна функция $u_1(x) = x$, которая соответствует фиксации матожидания $\mathbb{E}x$. Плотность будет вычисляться только в точках $x=k$, $k=1,2,\ldots$ и будет иметь вид

$$p_k = p(k) = \frac1{h(\theta)}\exp\left(\theta k\right)
$$

В этой формуле уже безошибочно угадывается геометрическое распределение с $p = 1 - e^{\theta}$. Параметр $p$ можно подобрать из соображений того, что математическое ожидание равно $\mu$. Матожидание геометрического распределения равно $\frac1p$, так что $p = \frac1{\mu}$. Окончательно,

$$p_k = \frac1{\mu}\left(1 - \frac1{\mu}\right)^{k-1}
$$

**Пример 2**. Среди распределений на всей вещественной прямой с заданным математическим ожиданием $\mu$ найдём распределение с максимальной энтропией.

{% cut "А сможете ли вы его найти? Решение под катом." %}

Теория говорит нам, что его плотность должна иметь вид

$$p(x) = \frac1{h(\theta)}\exp\left(\theta x\right)
$$

но интеграла экспоненты не существует, то есть применение «в лоб» теоремы провалилось. И неспроста: если даже рассмотреть все нормально распределённые случайные величины со средним $\mu$, их энтропии, равные $\frac12 + \frac12\log(2\pi\sigma^2)$ не ограничены сверху, то есть величины с наибольшей энтропией не существует.

{% endcut %}

  ## handbook

  Учебник по машинному обучению

  ## title

  Экспоненциальный класс распределений и принцип максимальной энтропии

  ## description

  *null*

- 
  ## path

  /handbook/ml/article/obobshyonnye-linejnye-modeli

  ## content

  ## Мотивация

До сих пор мы рассматривали в основном модели вида

$$y\sim f(x) + \varepsilon
$$

с шумом $\varepsilon$ из того или иного распределения. Но у этих моделей:

* шум не зависит от $x$;
* $y$ может принимать любые значения.

А что, если мы захотим предсказывать время ожидания доставки? Казалось бы, чем дольше время потенциального ожидания, тем больше его дисперсия. А как корректно предсказывать таргет, который принимает только целые значения?

Один из подходов мы обсудим в этом параграфе. Грубо говоря, вместо того, чтобы прибавлять один и тот же шум, мы зафиксируем семейство распределений $p(y\vert\mu(x))$, в котором изменяемым параметром будет зависящее от $x$ математическое ожидание $\mu(x)$.

Вот как могут выглядеть такие модели для случаев, если $p$ нормальное с фиксированной дисперсией, экспоненциальное или пуассоновское соответственно:

![11](https://yastatic.net/s3/education-portal/media/11_1_2d3c074374_99fed934e7.webp)

Как видим, такой подход позволяет получать и модели с меняющейся дисперсией шума, и модели с целочисленным таргетом.

## Определение

Мы рассмотрим достаточно широкий класс моделей — **обобщённые линейные модели** (**generalized linear models**, **GLM**). К ним относятся, в частности, линейная и логистическая регрессии. В итоге мы научимся подбирать подходящую регрессионную модель для самых разных типов данных.

Вспомним, что вероятностную модель линейной регрессии можно записать как

$$y \vert x \sim\color{red}{\mathcal N}(\langle x, w\rangle, \tau^2),
$$

а вероятностную модель логистической регрессии — как

$$y \vert x \sim \color{red}{Bern}(\color{blue}{\sigma}(\langle x, w\rangle)),
$$

где $\color{red}{Bern}(p)$ — распределение Бернулли с параметром $p$, а $\color{blue}{\sigma}(u) = \frac{1}{1+e^{-u}}$.

Итак, чем в этих терминах отличаются вероятностные модели линейной и логистической регрессии? Во первых, параметризованное семейство распределений для $y \vert x$, а именно, $\textcolor{red}{\mathcal{N}(\ast, \sigma^2)}$ в случае линейной регрессии и $\color{red}{Bern}$ в случае логистической.

Во-вторых, в обоих случаях математическое ожидание условного распределения $y\vert x$ является функцией от $\langle x, w\rangle$. На это можно посмотреть и по-другому: для каждой из задач выбрана функция $g$ такая, что $g(\mathbb E(y \vert x)) = \langle x, w\rangle$. Эта функция называется **функцией связи** (**link function**).

В случае линейной регрессии  $g(u) = u$. В самом деле, $\mathbb E(y \vert x) = \mathbb E{\mathcal N}(\langle x, w\rangle, \tau^2) = \langle x, w\rangle$. В случае логистической регрессии $g(u) = \sigma^{-1}(u) = \text{logit}(u) = \log\frac{u}{1-u}$. Давайте это тоже проверим. В модели логистической регрессии условное распределение $y \vert x$ — это распределение Бернулли с вероятностью успеха $\sigma(\langle x, w\rangle)$, и этой же вероятности равно его математическое ожидание. Следовательно, $g(\sigma(\langle x, w\rangle)) = \sigma^{-1}(\sigma(\langle x, w\rangle)) = \langle x, w\rangle$.

Обобщая, можно сказать, что, если данные таковы, что $\mathbb E(Y \vert X)$ не является линейной функцией от $x$, мы линеаризуем $\mathbb E(Y \vert X)$ с помощью функции связи $g$.

**Замечание:** Вообще говоря, нормальное распределение определяется не только своим математическим ожиданием, но и стандартным отклонением. То есть, в отличие от логистической регрессии, модель линейной регрессии не позволяет для данного $x$ оценить все параметры распределения $y \vert x$, и дисперсию приходится фиксировать изначально. К счастью, выбор её значения в нормальном распределении не влияет ни на оптимальный вектор весов $w$, ни на итоговые предсказания $\mathbb E(Y \vert X)$, которые выдаёт обученная модель.

Продолжим. Задав эти две составляющие — параметризованное семейство распределений и функцию связи — мы получим обобщённую линейную модель (GLM).

Для нового объекта $x$ она выдаст предсказание $\widehat{y} = \mathbb{E}(y\vert x) = g^{-1}(\langle x, w\rangle)$, а выбор класса распределений $y \vert x$ потребуется нам для подбора весов $w$. В принципе, можно выбрать любой класс распределений $y \vert x$ и любую монотонную функцию связи $g$, получив некоторую вероятностную модель. Однако обычно для упрощения поиска оптимальных весов $w$ в GLM предполагают, что $y \vert x$ принадлежит одному из достаточно простых семейств экспоненциального класса.

## Что даёт нам принадлежность экспоненциальному классу?

В контексте GLM обычно рассматривают подкласс экспоненциального класса, состоящий из семейств, представимых в виде

$$\color{#348FEA}{p(y \vert \theta, \phi) = \exp\left(\frac{y\theta - a(\theta)}{\phi} + b(y, \phi)\right)}
$$

где $\theta$ и $\phi$ — скалярные параметры, причём $\phi$ — нечто фиксированное, обычно дисперсия, которая чаще всего полагается равной $1$, а значения $\theta$ параметризуют распределения из семейства. Нетрудно переписать плотность в более привычном для нас виде, чтобы стало очевидно, что это семейство действительно из экспоненциального класса:

$$p(y \vert \theta, \phi) = \frac1{\exp\left(\frac{a(\theta)}{\phi}\right)}\exp(b(y,\phi))\exp\left(\frac{y\theta}{\phi}\right)
$$

Действительно, если вспомнить, что $\varphi$ — это константа, а не параметр, то получается очень похоже на

$$p(y\vert\nu) = \frac1{h(\nu)}g(y)\cdot\exp\left(\nu^Tu(y)\right)
$$

В частности, мы видим, что $u(y)$ состоит из единственной компоненты $u_1(y)$, равной $\frac{y}{\phi}$. По доказанной в предыдущем разделе лемме имеем тогда, что математическое ожидание $\mu$ такой случайной величины равно

$$\mu = \phi\mathbb{E}u_1(y) = \phi\frac{\partial}{\partial\theta}\left(\frac{a(\theta)}{\phi}\right) = a'(\theta)
$$

До сих пор мы рассуждали о распределении $p(y)$ без $x$ в условии. Что будет, если его добавить? Параметр $\phi$ мы договорились сохранять постоянным, тогда от $x$ должен зависеть единственный оставшийся параметр $\theta$.

Самый естественный в нашей ситуации вариант — это положить $\theta = \langle x, w\rangle$. В GLM мы вводили функцию $g$, для которой $g(\mathbb E(y \vert x)) = \langle x, w\rangle$, то есть $\mathbb{E}(y\vert x) = g^{-1}(\langle x, w\rangle)$. Но ведь матожидание $y$ равно $a'(\theta)$, то есть $a'(\langle x, w\rangle)$. Это позволяет нам однозначно определить функцию связи $g = (a')^{-1}$. Такая функция связи называется **канонической функцией связи** (**canonical link function**).

## Примеры

Поговорим немного о том, как на практике подбирать $\phi, a, b$, чтобы по классу распределений $y \vert x$ определить каноническую функцию связи. Чтобы разобраться, рассмотрим несколько примеров.

**Пример 1**. Пусть мы решили применить к данным линейную регрессию. Тогда

$$p(y \vert x, w, \sigma^2) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(y-\langle x, w\rangle)^2}{2\sigma^2}\right)
$$

Обозначим для краткости $\mu = \langle x, w\rangle$ и будем рассматривать $p(y\vert \mu, \sigma^2)$.

Мы уже знаем, что семейство нормальных распределений относится к экспоненциальному классу, но давайте выразим эту плотность в описанном выше более частном виде:

$$p(y \vert \mu, \sigma^2) = \exp\left(-\frac{(y-\mu)^2 }{2\sigma^2}- \log(2\pi\sigma^2)\right)
$$

В формуле экспоненциального семейства распределений единственная часть, не зависящая от $\theta$, — это функция $b$. Поскольку $\mu=a'(\theta)$, функция $b$ также не должна зависеть от $\mu$. Так что внутри экспоненты выделим в качестве функции $b$ всё, что не зависит от $\mu$:

$$p(y \vert \mu, \sigma^2) = \exp\left(\frac{\overbrace{y\mu}^{=y\theta} - \overbrace{\mu^2/2}^{=a(\theta)}}{\underbrace{\sigma^2}_{=\phi}}\underbrace{ - \left(\frac{y_i^2}{2\sigma^2} + \log(2\pi\sigma^2)\right)}_{=b(y,\phi)}\right)
$$

Эта формула уже похожа на формулу экспоненциального семейства распределений и видно, что $\phi=\sigma^2$, $\theta=g(\mu)=\mu$ (коэффициент при $y$), $a(\theta) = \mu^2/2 = \theta^2/2$, $b(y, \phi) = -\frac{y^2}{2\sigma^2} - \log(2\pi\sigma^2)$.

Каноническая функция связи является обратной к $a'(\theta) = \theta$, то есть $\langle x, w\rangle = g(\mu) = \mu$, как мы и привыкли.

**Пример 2**. Проделаем то же самое, но теперь для распределения Бернулли.

{% cut "Если вам интересно — попробуйте сперва сами, прежде чем читать решение." %}

Пусть $\mu = \mathbb{E}(y\vert x)$. Тогда функция вероятности имеет вид

$$p(y \vert \mu) = \mu^y(1-\mu)^{1-y} = \exp\left(y\log\left(\frac{\mu}{1-\mu}\right) + \log\left(1-\mu\right)\right) =
$$

$$=\exp\left(\frac{y\log\left(\frac{\mu}{1-\mu}\right) - \left[-\log\left(1-\mu\right)\right]}{1} + 0\right)
$$

То есть мы можем положить $\phi=1$, $\theta = \log\frac{\mu}{1-\mu}$, $a(\theta) = -\log\left(1-\mu\right) = \log\left(1 + e^{\theta}\right)$, $b=0$.

Найдём каноническую функцию связи. Имеем $a'(\theta) = \frac{e^{\theta}}{1 + e^{\theta}}$. Следовательно, $g(t) = \log\left(\frac{t}{1-t}\right)$. Как и ожидалось, это функция $\text{logit}=\sigma^{-1}$, используемая в логистической регрессии. Соответственно, $\langle x, w\rangle = \sigma^{-1}(\mathbb{E}(y\vert x))$.

{% endcut %}

**Пример 3**. Хорошо, про линейную и логистическую регрессию мы и так знали. Давайте попробуем решить с помощью GLM новую задачу.

Пусть мы хотим по каким-то признакам $X$ предсказать количество «лайков», которое пользователи поставят посту в социальной сети за первые 10 минут после публикации. Конечно, можно использовать для этого линейную регрессию. Однако предположение линейной регрессии, что $Y \vert X\sim\mathcal N$, в данном случае странное по нескольким причинам.

Во-первых, количество лайков заведомо не может быть отрицательным, а нормальное распределение всегда будет допускать ненулевую вероятность отрицательного значения.

Во-вторых, количество лайков — всегда целое число.

В-третьих, у распределения количества лайков, скорее всего, **положительный коэффициент асимметрии** (**skewness**). То есть, если модель предсказывает, что под постом будет 100 лайков, мы скорее можем ожидать, что под ним окажется 200 лайков, чем 0. Нормальное распределение симметрично и не может описать такие данные.

С другой стороны, если мы предположим, что в первые 10 минут после публикации есть какая-то постоянная частота (своя для каждого поста, зависящая от $x$), с которой пользователи ставят лайк, мы получим, что количество лайков имеет распределение Пуассона. Распределение Пуассона не имеет описанных выше проблем:

![11](https://yastatic.net/s3/education-portal/media/11_2_afee3ae8d2_7a01880860.webp)

Но какая будет каноническая функция связи, если мы считаем, что $Y \vert X\sim\text{Poisson}$? Аналогично первому и второму примерам:

$$p(y \vert \mu) = \frac{e^{-\mu}\mu^y}{y!} = \exp\left(y\log\mu - \mu - \log y!\right)
$$

Откуда $\phi=1$, $\theta = g(\mu) = \log\mu$, $a(\theta) = \mu = \exp(\theta)$, $b(y, \phi) = -\log y!$

Значит, эта модель (она называется **пуассоновская регрессия**), будет предсказывать с помощью формулы $\mathbb E(y \vert x) = g^{-1}(\langle x, w\rangle)= \exp(\langle x, w\rangle)$.

**Вопрос на подумать**. В каких ситуациях была бы полезной функция связи **complementary log-log link** (cloglog)

$$g(x) = \log(-log(1 - x))?
$$

{% cut "Ответ (не открывайте сразу; сначала подумайте сами!)" %}

Давайте разбираться. Предсказание с такой функцией связи будет иметь вид

$$\mathbb{E}(y\vert x) = g^{-1}(\langle x, w\rangle) = 1 - \exp(-\exp(\langle x, w\rangle))
$$

Функция $g^{-1}$ принимает значения на отрезке от $0$ до $1$, так что речь идёт о предсказании вероятностей в задаче бинарной классификации. Сравним $g^{-1}$ с привычной нам сигмоидой:

![11](https://yastatic.net/s3/education-portal/media/11_3_e80e95a22d_176bc8cb91.webp)

График $g^{-1}$ не симметричен.  Можно сказать, что модель, основанная на функции связи cloglog, похожа на логистическую регрессию в области точек, уверенно отнесённых к нулевому классу, но с уменьшением этой уверенности они приписываются близкие к единице вероятности всё «охотнее».

Эта функция связи может оказаться полезной, если объекты положительного класса редки (обнаружение редких событий).

{% endcut %}

  ## handbook

  Учебник по машинному обучению

  ## title

  Обобщённые линейные модели

  ## description

  Как прокачать линейную модель с помощью распределений из экспоненциального класса

- 
  ## path

  /handbook/ml/article/kak-ocenivat-veroyatnosti

  ## content

  Мы уже упоминали, что оценивать вероятности классов как $softmax(f_w(x_i))$ для какой-то произвольной функции $f_w$ — это дело подозрительное.

В этом разделе мы поговорим о том, как это делать хорошо и правильно.

## Что же такое вероятность класса, если объект либо принадлежит этому классу, либо нет?

Ограничимся пока случаем двуклассовой классификации — с классами 0 и 1. Если утверждается, что мы предсказываем корректную вероятность класса 1 (обозначим её $q(x_i)$), то прогноз «объект $x_i$ принадлежит классу 1 с вероятностью $\frac23$» должен сбываться в $\frac23$ случаев.

То есть, условно говоря, если мы возьмём все объекты, то среди них что-то около двух третей действительно имеет класс 1.

На математическом языке это можно сформулировать так: **Если** $\widehat{p}$ — предсказанная вероятность класса 1, то $P(y_i = 1 \vert q(x_i) = \widehat{p}) = \widehat{p}$.

К сожалению, в реальной жизни $\widehat{p}$ — это скорее всего вещественные числа, которые будут различными для различных $y_i$, и никаких вероятностей мы не посчитаем, но мы можем разбить отрезок $[0,1]$ на бины, внутри каждого из которых уже вычислить, каковая там доля объектов класса 1, и сравнить эту долю со средним значением вероятности в бине:

![12](https://yastatic.net/s3/education-portal/media/12_1_dd31ed3340_7d8ad4073b.webp)

У модели, которая идеально предсказывает вероятности (как обычно говорят, у идеально **калиброванной** модели) жёлтые точки на диаграме калибровки должны совпадать с розовыми.

А вот на картинке выше это не так: жёлтые точки всегда ниже розовых. Давайте поймём, что это значит. Получается, что наша модель систематически завышает предсказанную вероятность (розовые точки), и порог отсечения нам, выходит, тоже надо было бы сдвинуть вправо:

![12](https://yastatic.net/s3/education-portal/media/12_2_32a327a143_5d630cba01.webp)

Но такая картинка, пожалуй, говорит о какой-то серьёзной патологии классификатора. Гораздо чаще встречаются следующие две ситуации:

* Слишком уверенный (**overconfident**) классификатор:
  ![12](https://yastatic.net/s3/education-portal/media/12_3_033f331fed_bfa91b03f0.webp)
  Такое случается с сильными классификаторыми (например, нейросетями), которые учились на метки классов, а не на вероятности: тем самым процесс обучения стимулировал их всегда давать ответ, как можно более близкий к 0 или 1.

* Неуверенный (**underconfident**) классификатор:
  ![12](https://yastatic.net/s3/education-portal/media/12_4_d557d0984e_3ae1636aec.webp)

Такое может случиться, например, если мы слишком много обращаем внимания на трудные для классификации объекты на границе классов (как, скажем, в SVM), в каком-то смысле в ущерб более однозначно определяемым точкам. Этим же могут и грешить модели на основе бэггинга (например, случайный лес). Грубо говоря, среднее нескольких моделей предскажет что-то близкое к единице только если все слагаемые предскажут что-то, близкое к единице — но из-за дисперсии моделей это будет случаться реже, чем могло бы. Подробнее можно почитать в [статье](https://www.cs.cornell.edu/~alexn/papers/calibration.icml05.crc.rev3.pdf).

## Вам скажут: логистическая регрессия корректно действительно предсказывает вероятности

Вам даже будут приводить какие-то обоснования. Важно понимать, что происходит на самом деле, и не дать ввести себя в заблуждение. В качестве противоядия от иллюзий предлагаем рассмотреть два примера.

Рассмотрим датасет c двумя классами (ниже на картинке обучающая выборка)

![12](https://yastatic.net/s3/education-portal/media/12_5_3bf7088887_bf6be965c8.webp)

Обучим на нём логистическую регрессию из sklearn безо всяких параметров (то есть $L^2$-регуляризованную, но это не так важно). Классы не так-то просто разделить, вот и логистическая регрессия так себе справляется. Ниже изображена часть тестовой выборки вместе с предсказанными вероятностями классов для всех точек области

![12](https://yastatic.net/s3/education-portal/media/12_6_92240fb793_9a11d1ca90.webp)

Видим, что модель не больно-то уверена в себе, и ясно почему: признаковое описание достаточно бедное и не позволяет нам хорошо разделить классы, хотя, казалось бы, это можно довольно неплохо сделать.

Попробуем поправить дело, добавив полиномиальные фичи, то есть все $x^jy^k$ для $0\leqslant j,k\leqslant 5$ в качестве признаков, и обучив поверх этих данных логистическую регрессию. Снова нарисуем некоторые точки тестовой выборки и предсказания вероятностей для всех точек области:

![12](https://yastatic.net/s3/education-portal/media/12_7_c7891d2b70_b4811d8762.webp)

Видим, что у нас сочетание двух проблем: неуверенности посередине и очень уверенных ошибок по краям.

Нарисуем теперь калибровочные кривые для обеих моделей:

![12](https://yastatic.net/s3/education-portal/media/12_8_e2058527bc_1a80004c21.webp)

Калибровочные кривые весьма примечательны; в любом случае ясно, что с предсказанием вероятностей всё довольно плохо. Посмотрим ещё, какие вероятности наши классификаторы чаще приписывают объектам:

![12](https://yastatic.net/s3/education-portal/media/12_9_cf3ce123f2_048ddd0652.webp)

Как и следовало ожидать, предсказания слабого классификатора тяготеют к серединке (та самая неуверенность), а среди предсказаний переобученного очень много крайне уверенных — и совсем не всегда правильных.

## Но почему же все твердят, что логистическая регрессия хорошо калибрована?!

Попробуем понять и простить её.

Как мы помним, логистическая регрессия учится путём минимизации функционала

$$l(X, y) = -\sum_{i=1}^N(y_i\log(\sigma(\langle w, x_i\rangle)) + (1 - y_i)\log(1 - \sigma(\langle w, x_i\rangle)))
$$

Отметим между делом, что каждое слагаемое — это кроссэнтропия распределения $P$, заданного вероятностями $P(0) = 1 - \sigma(\langle w, x_i\rangle)$ и $P(1) = \sigma(\langle w, x_i\rangle)$, и тривиального распределения, которое равно $y_i$ с вероятностью $1$.

Допустим, что мы обучили по всему универсуму данных $\mathbb{X}$ идеальную логистическую регрессию с идеальными весами $w^{\ast}$. Пусть, далее, оказалось, что у нас есть $n$ объектов $x_1,\ldots,x_n$ с одинаковым признаковым описанием (то есть по сути представленных одинаковыми векторами $x_i$), но, возможно, разными истинными метками классов $y_1,\ldots,y_n$. Тогда соответствующий им кусок функции потерь имеет вид

$$-\left(\sum_{i=1}^ny_i\right)\log(\sigma(\langle w, x_1\rangle)) -\left(\sum_{i=1}^n (1 - y_i)\right)\log(1 - \sigma(\langle w, x_1\rangle)) =
$$

$$=-n\left(\vphantom{\frac12}p_0\log(\sigma(\langle w, x_1\rangle)) + p_1\log(1 - \sigma(\langle w, x_1\rangle))\right)
$$

где $p_j$ — частота $j$-го класса среди истинных меток. В скобках также стоит кросс-энтропия распределения, задаваемого частотой меток истинных классов, и распределения, предсказываемого логистической регрессией. Минимальное значение кросс-энтропии (и минимум функции потерь) достигается, когда

$$\sigma(\langle w, x_1\rangle) = p_0,\quad 1 - \sigma(\langle w, x_1\rangle) = p_1
$$

Результат, полученный для $n$ совпадающих точек будет приблизительно верным и для $n$ достаточно близких точек в случае, когда:

* признаковое описание данных достаточно хорошее — классы не перемешаны как попало и всё-таки близки к разделимым;
* модель не переобученная — то есть, предсказания вероятностей не скачут очень уж резко — вспомните второй пример.

На всех этих точках модель будет выдавать примерно долю положительных, то есть тоже хорошую оценку вероятности.

## Как же всё-таки предсказать вероятности: методы калибровки

Пусть наша модель (бинарной классификации) для каждого объекта $x_i$ выдаёт некоторое число $q(x_i)\in[0,1]$. Как же эти числа превратить в корректные вероятности?

* **Гистограммная калибровка**. Мы разбиваем отрезок $[0,1]$ на бины $\mathbb{B}_1,\ldots,\mathbb{B}_k$ (одинаковой ширины или равномощные) и хотим на каждом из них предсказывать всегда одну и ту же вероятность: $\theta_j$, если $q(x_i)\in \mathbb{B}_j$. Вероятности $\theta_i$ подбираются так, чтобы они как можно лучше приближали средние метки классов на соответствующих бинах. Иными словами, мы решаем задачу

$$\sum_{j=1}^k\left|\frac{\sum_{i=1}^N\mathbb{I}\{q(x_i)\in\mathbb{B}_j\}y_i}{ \vert \mathbb{B}_j \vert } - \theta_j\right|\longrightarrow\min\limits_{(\theta_1,\ldots,\theta_k)}
$$

Вместо разности модулей можно рассматривать и разность квадратов.

Метод довольно простой и понятный, но требует подбора числа бинов и предсказывает лишь дискретное множество вероятностей.

**Изотоническая регрессия**. Этот метод похож на предыдущий, только мы будем, во-первых, настраивать и границы $0=b_0,b_1,\ldots,b_k = 1$ бинов $\mathbb{B}_j = \{t \vert  b_{j-1}\leqslant b_j\}$, а кроме того, накладываем условие $\theta_1\leqslant\ldots\leqslant\theta_k$. Искать $b_j$ и $\theta_j$ мы будем, приближая $y_i$ кусочно постоянной функцией $g$ от $q(x_i)$:

$$\sum_{i=1}^N(y_i - g(q(x_i)))^2\longrightarrow\min_{g}
$$

![12](https://yastatic.net/s3/education-portal/media/12_10_cb986c5c52_bf6f8b6e33.webp)

Минимизация осуществляется при помощи pool adjacent violators algorithm, и эти страницы слишком хрупки, чтобы выдержать его формулировку.

* **Калибровка Платта** представляет собой по сути применение сигмоиды поверх другой модели (то есть самый наивный способ получения «вероятностей»). Более точно, если $q(x_i)$ — предсказанная вероятность, то мы полагаем

$$P(y_i = 1\mid x_i) = \sigma(aq(x_i) + b) = \frac1{1 + e^{-aq(x_i) - b}}
$$

где $a$ и $b$ подбираются методом максимального правдоподобия на отложенной выборке:

$$-\sum_{i=1}^N(\vphantom{\frac12}y_i\log(\sigma(q(x_i))) + (1 - y_i)\log(1 - \sigma(q(x_i))))\longrightarrow\min\limits_{a,b}
$$

Для избежания переобучения Платт предлагал также заменить метки $y_i$ и $(1 - y_i)$ на регуляризованные вероятности таргетов:

$$t_0 = \frac1{\#\{i \vert y_i = 0\} + 2},\quad t_1 = \frac{\#\{i \vert y_i = 1\} + 1}{\#\{i \vert y_i = 0\} + 2}
$$

Калибровка Платта неплохо справляется с выколачиванием вероятностей из SVM, но для более хитрых классификаторов может спасовать. В целом, можно показать, что этот метод хорошо работает, если для каждого из истинных классов предсказанные вероятности $q(x_i)$ распределы нормально с одинаковыми дисперсиями. Подробнее об этом вы можете почитать в [этой статье](https://research-information.bris.ac.uk/ws/portalfiles/portal/154625753/Full_text_PDF_final_published_version_.pdf). Там же описано обобщение данного подхода — бета-калибровка.

С большим количеством других методов калибровки вы можете познакомиться в [этой статье](https://dyakonov.org/2020/03/27/%D0%BF%D1%80%D0%BE%D0%B1%D0%BB%D0%B5%D0%BC%D0%B0-%D0%BA%D0%B0%D0%BB%D0%B8%D0%B1%D1%80%D0%BE%D0%B2%D0%BA%D0%B8-%D1%83%D0%B2%D0%B5%D1%80%D0%B5%D0%BD%D0%BD%D0%BE%D1%81%D1%82%D0%B8)

## Как измерить качество калибровки

Калибровочные кривые хорошо показывают, что есть проблемы, но как оценить наши усилия по улучшению предсказания вероятностей? Хочется иметь какую-то численную метрику. Мы упомянем две разновидности — прямое воплощение описанных выше идей.

* **Expected/Maximum calibration error**. Самый простой способ, впрочем — он наследник идеи с калибровочной кривой. А именно, разобьём отрезок $[0,1]$ на бины $\mathbb{B}_1,\ldots,\mathbb{B}_k$ по предсказанным вероятностям и вычислим

$$\sum_{j=1}^k\frac{\#\mathbb{B}_j}{N}\left|\overline{y}(\mathbb{B}_j) - \overline{q}(\mathbb{B}_j)\right|
$$

или

$$\max\limits_{j=1,\ldots,k}\left|\overline{y}(\mathbb{B}_j) - \overline{q}(\mathbb{B}_j)\right|
$$

где $\overline{y}(\mathbb{B}_j)$ — среднее значение $y_i$, а $\overline{q}(\mathbb{B}_j)$ — среднее значение $q(x_i)$ для $x_i$, таких что $q(x_i)\in\mathbb{B}_j$. Проблема этого способа в том, что мы можем очень по-разному предсказывать в каждом из бинов вероятности (в том числе константой) без ущерба для метрики.

* **Brier score**. Одна из популярных метрик, которая попросту измеряет разницу между предсказанными вероятностями и \$ y_i \$:

$$\sum_{i=1}^N(y_i - q(x_i))^2
$$

Казалось бы, в чём смысл? Немного подрастить мотивацию помогает следующий пример. Допустим, наши таргеты совершенно случайны, то есть $P(y_i = 1 \vert x_i) = P(y_i)$. Тогда хорошо калиброванный классификатор должен для каждого $x_i$ предсказывать вероятность $\frac12$; соответственно, его brier score равен $\frac14$. Если же классификатор хоть в одной точке выдаёт вероятность $p>\frac12$, то в маленькой окрестности он должен выдавать примерно такие же вероятности.

Поскольку же таргет случаен, локальный кусочек суммы из brier score будет иметь вид $\frac{N'}{2}p^2 + \frac{N'}{2}(1-p)^2 < \frac{N'}2$, что хуже, чем получил бы всегда выдающий $\frac12$ классификатор.

Не обязательно брать квадратичную ошибку; сгодится и наш любимый log-loss:

$$\sum_{i=1}^N\left(\vphantom{\frac12}y_i\log{q(x_i)} + (1 - y_i)\log(1 - q(x_i))\right)
$$

Это же и помогает высветить ограничения подхода, если вспомнить рассуждения о калиброванности логистической регрессии. Для достаточно гладких классификатора и датасета brier score и log-loss будут адекватными средствами оценки, но если нет — возможно всякое.

**Вопрос на засыпку**: а как быть, если у нас классификация не бинарная, а многоклассовая? Что такое хорошо калиброванный классификатор? Как это определить численно? Как заставить произвольный классификатор предсказывать вероятности?

Мы не будем про это рассказывать, но призываем читателя подумать над этим самостоятельно или, например, посмотреть [туториал](https://classifier-calibration.github.io/) с ECML KDD 2020.

  ## handbook

  Учебник по машинному обучению

  ## title

  Как оценивать вероятности

  ## description

  Как правильно оценить вероятности классов в задаче классификации

- 
  ## path

  /handbook/ml/article/generativnyj-podhod-k-klassifikacii

  ## content

  Классификационные модели, которые мы рассматривали в предыдущих параграфах, нацелены непосредственно на оценку $P(Y \vert X)$. Такие модели называются **дискриминативными**.

К ним относится, например, логистическая регрессия: она предлагает оценку $\hat P(y=1 \vert x) = \sigma(w\^Tx)$. В процессе обучения дискриминативные модели подбирают разделяющую поверхность (гиперплоскость в случае логистической регрессии). Новые объекты дискриминативная модель классифицирует в зависимости от того, по какую сторону от разделяющей поверхности они лежат.

Например, обучившись на изображениях домашних кошек (y=0) и рысей (y=1), дискриминативная модель будет определять, новое изображение больше похоже на кошку или на рысь. При этом, если на вход такой модели дать изображение собаки (объект класса, которого не было в обучении, выброс), дискриминативная модель заведомо не сможет обнаружить, что это и не кошка, и не рысь, и отнесёт такой объект к одному из «знакомых» ей классов.

В этом параграфе мы поговорим о другой группе моделей, которые нацелены на оценку $P(X, Y) = P(X \vert Y)P(Y)$. Такая модель описала бы, как обычно выглядят кошки, как они могут выглядеть, а каких кошек точно не бывает. Так же она описала бы и рысей. Она также определила бы по обучающим данным, насколько изображения кошек встречаются чаще, чем изображения рысей, т.е. оценила бы $P(Y)$.

## Генеративный и дискриминативный подходы к обучению

Если модель позволила точно оценить распределение $P(X \vert Y)$, с её помощью можно генерировать объекты из этого условного распределения, в нашем примере — изображения кошек и рысей соответственно.

А вместе распределение $P(X, Y)$ дало бы нам возможность генерировать изображения и кошек, и рысей, причём именно в той пропорции, в которой они встречаются в реальном мире.  Поэтому модели, оценивающие $P(X, Y)$, называют **генеративными**. Ещё одно достоинство генеративных моделей — их способность находить выбросы в данных: объект $x$ можно считать выбросом, если $P(x \vert y)$ мало для каждого класса $y$.

Заметим, что находить выбросы с помощью генеративной модели можно и когда класс всего один — то есть никакие метки классов не доступны. Такая задача называется одноклассовой классификацией. Например, если у нас есть не размеченный датасет с аудиозаписями речи людей, то, обучив на нём генеративную модель, оценивающую в данном случае $P(X \vert Y)=P(X)$, мы сможем для нового аудио $x$ определить, похоже ли оно на аудиозапись человеческой речи (значение $P(x)$ велико), или это что-то другое: синтезированная речь, посторонний шум и т.п. (значение $P(x)$ мало).

Если мы знаем, что «выбросы», с которыми модели предстоит сталкиваться, — это, как правило, синтезированная речь, то, мы можем дополнить датасет вторым классом, состоящим из синтезированной речи, и смоделировать также распределение этого класса. Это позволит существенно увеличить качество детектирования таких выбросов.

Чтобы использовать генеративную модель для классификации, необходимо выразить $P(Y \vert X)$ через $P(X \vert Y)$ и $P(Y)$. Сделать это позволяет формула Байеса:

$$P(y \vert x) = \frac{P(x, y)}{\sum\limits_{y'\in Y} P(y')P(x \vert y')} = \frac{P(y)P(x \vert y)}{\sum\limits_{y'\in Y} P(y')P(x \vert y')}
$$

Классификация в генеративных моделях осуществляется с помощью байесовского классификатора:

$$a(x) = \arg\max\limits_{y\in Y} P(y \vert x) = \arg\max\limits_{y\in Y} \frac{P(y)P(x \vert y)}{\sum\limits_{y'\in Y} P(y')P(x \vert y')} = \arg\max\limits_{y\in Y} P(y)P(x \vert y)
$$

Оценить $P(Y)$, как правило, несложно. Для этого используют частотные оценки, полученные в обучающей выборке:

<a name="eq:class_proba_estimation"><b>Выражение (1)</b></a>

$$\hat P(Y=y) = \frac{\#(Y=y)}{N}
$$

Отметим ещё раз, что использование генеративного подхода позволяет внедрять в модель априорные знания о $P(y)$. Это не очень впечатляет, когда речь идёт о бинарной классификации, но всё меняется, если рассмотреть задачу ASR (автоматического распознавания речи), в которой по записи голоса восстанавливается произносимый текст.

Таргетами здесь могут быть любые предложения или даже более развёрнутые тексты. При этом размеченных данных (запись, текст) обычно намного меньше, чем доступных текстов, и обученная на большом чисто текстовом корпусе языковая модель, которая будет оценивать вероятность того или иного предложения, может стать большим подспорьем, позволив из нескольких фонетически корректных наборов слов выбрать тот, который в большей степени похож на настоящее предложение.

Но как смоделировать распределение $P(X, Y)$? Пространство всех возможных функций распределения $P(X, Y)$ бесконечномерно, из-за чего оценить произвольное распределение с помощью конечной выборки невозможно. Поэтому перед оценкой $P(X, Y)$ на это распределение накладывают дополнительные ограничения. Некоторые простые примеры таких ограничений мы рассмотрим в следующих разделах.

### Gaussian discriminant analysis

Модель гауссовского (или квадратичного) дискриминантного анализа (GDA) строится в предположении, что распределение объектов каждого класса $y$ подчиняется многомерному нормальному закону со средним $\mu_y$ и ковариационной матрицей $\Sigma_y$:

$$p(x \mid y)=\frac{1}{(2 \pi)^{n / 2}\left|\Sigma_y\right|^{1 / 2}} \exp \left(-\frac{1}{2}\left(x-\mu_y\right)^T \Sigma_y^{-1}\left(x-\mu_y\right)\right)
$$

Тогда функция правдоподобия

$$\mathcal L(P(Y), \mu, \Sigma) = \prod_{i=1}^N p(x_i \vert y_i; \mu_{y_i}, \Sigma_{y_i})P(y_i)
$$

достигает максимума при

$$\hat\mu_y = \frac{\sum\limits_{i=1}^N{x_i\mathbb{1}_{y_i=y}}}{\sum\limits_{i=1}^N\mathbb{1}_{y_i=y}},\hspace{5mm} 
\hat\Sigma_y = \frac{\sum\limits_{i=1}^N{(x_i - \hat\mu_{y})(x_i - \hat\mu_{y})^T \mathbb{1}_{y_i=y}}}{\sum\limits_{i=1}^N\mathbb{1}_{y_i=y}}
$$

И $\hat P(Y)$, представленной выше [см. выражение ](#eq:class_proba_estimation)$(1)$.

Рассмотрим, как выглядит разделяющая поверхность в модели GDA. На поверхности, разделяющей классы $y_i$ и $y_j$ выполняется

$$P(y_i \vert x)=P(y_j \vert x) \Leftrightarrow
$$

$$p(x \vert y_i)P(y_i) = p(x \vert y_j)P(y_j)\Leftrightarrow
$$

$$\log p(x \vert y_i) + \log P(y_i) - \log p(x \vert y_j) - \log P(y_j) = 0\Leftrightarrow
$$

<a name="eq:GDA_boundary"><b>Выражение</b> (2)</a>

$$ \begin{equation}
 -\frac{1}{2}(x-\mu_{y_i})^T\Sigma_{y_i}^{-1} (x-\mu_{y_i})-\log (2\pi)^{n/2}|\Sigma_{y_i}|^{1/2} + \log P(y_i) +
\frac{1}{2}(x-\mu_{y_j})^T\Sigma_{y_j}^{-1} (x-\mu_{y_j}) + \log (2\pi)^{n/2}|\Sigma_{y_j}|^{1/2} - \log P(y_j) = 0
 \end{equation}
$$

Поскольку левая часть [уравнения (2)](#eq:GDA_boundary) квадратична по $x$, разделяющая поверхность между двумя классами будет представлять из себя гиперповерхность порядка 2. Пример разделяющей поверхности многоклассовой модели GDA приведён [на рис.](#fig:GDA_boundary)

<a name="#fig:GDA_boundary"></a>

![13](https://yastatic.net/s3/education-portal/media/13_1_2660eb2f8c_71298899ca.webp)

Плотность классов и разделяющая поверхность в многоклассовой модели LDA [см. рисунок](#fig:LDA_boundary).

<a name="ss:GDA"></a>

![13](https://yastatic.net/s3/education-portal/media/13_2_c3eb5fda88_7f6c93e95f.webp)

### Linear Discriminant Analysis

В [выражении (2)](#eq:GDA_boundary) член второго порядка $x^T (\Sigma_{y_j}^{-1} - \Sigma_{y_i}^{-1})x$ зануляется при $\Sigma_{y_i}=\Sigma_{y_j}$. Таким образом, если дополнительно предположить, что все классы имеют общую ковариационную матрицу $\Sigma$, разделяющая поверхность между любыми двумя классами будет  линейной ([см. рисунок](#fig:LDA_boundary)). Поэтому такая модель называется линейным дискриминантным анализом (LDA).

На этапе обучения единственное отличие модели LDA от GDA состоит в оценке ковариационной матрицы:

$$\hat \Sigma = \frac{1}{N}\sum\limits_{i=1}^N{(x_i - \hat\mu_{y_i})(x_i - \hat\mu_{y_i})^T}
$$

Заметим, что в модели GDA для каждого класса требовалось оценить порядка $d^2$ параметров. Это может привести к переобучению в случае, если размерность пространства признаков велика, а некоторые классы представлены в обучающей выборке малым количеством объектов. В LDA для каждого класса требуется оценить лишь порядка $d$ параметров (значение $P(y)$ и элементы вектора $\mu_y$), и ещё $d^2$ общих для всех классов параметров (элементы матрицы $\Sigma$).

Таким образом, основное преимущество модели LDA перед GDA — её меньшая склонность к переобучению, недостаток — линейная разделяющая поверхность.

## Метод наивного байеса

Предположим, что признаки $X$ объектов каждого класса $y$ — независимые случайные величины:

$$\forall y\in Y \hspace{2mm} \forall U, V: U\sqcup V = \{1, ... d\}, \hspace{2mm} \forall x^u\subset \mathbb R^{|U|}, x^v\subset \mathbb R^{|V|}
$$

$$P(X^U\in x^u, X^V\in x^v|Y=y) = P(X^U\in x^u|Y=y)P(X^V\in x^u|Y=y).
$$

В таком случае говорят, что величины $X$ условно независимы относительно $Y$. Тогда справедливо

<a name="eq:cond_independent"><b>Выражение (3)</b></a>

$$\begin{equation}
P(X \vert Y) = P(X^1, X^2, ..., X^d \vert Y) = P(X^1 \vert Y)P(X^2, ..., X^d \vert Y) = ... = P(X^1 \vert Y)P(X^2 \vert Y)...P(X^d \vert Y)
\end{equation}
$$

То есть для того, чтобы оценить плотность многомерного распределения $P(X \vert Y)$ достаточно оценить плотности одномерных распределений $P(X^i \vert Y)$, [см. рисунок](#fig:blobs_density).

<a name="#fig:blobs_density"></a>

![13](https://yastatic.net/s3/education-portal/media/13_3_c4a7d7b883_d120c7e624.webp)

На рисунке приведён пример условно независимых относительно $Y$ случайных величин $X^1, X^2$. Для оценки плотности двумерных распределений объектов классов достаточно оценить плотности маргинальных распределений, изображённые графиками вдоль осей.

Рассмотрим пример. Пусть решается задача классификации отзывов об интернет-магазине на 2 категории: $Y=0$ — отрицательный отзыв, клиент остался не доволен, и $Y=1$ — положительный отзыв. Пусть признак $X^w$ равен 1, если слово $w$ присутствует в отзыве, и 0 иначе. Тогда условие [выражения ](#eq:cond_independent)$(3)$ означает, что, в частности, наличие или отсутствие слова «дозвониться» в отрицательном отзыве не влияет на вероятность наличия в этом отзыве слова «телефон».

На практике в процессе feature engineering почти всегда создаётся много похожих признаков, и условно независимые признаки можно встретить очень редко. Поэтому генеративную модель, построенную в предположении условия [выражения ](#eq:cond_independent)$(3)$, называют наивным байесовским классификатором (Naive Bayes classifier, NB).

Обучение модели NB заключается в оценке распределений $P(Y)$ и $P(X^i \vert Y)$. Для $P(Y)$ можно использовать частотную оценку [выражения ](#eq:class_proba_estimation)$(1)$. $P(X^i \vert y)$ — одномерное распределение. Рассмотрим несколько способов оценки одномерного распределения.

## Оценка одномерного распределения

Пусть мы хотим оценить одномерное распределение $P(X)$.

Если распределение $P(X)$ дискретное, требуется оценить его функцию массы, то есть вероятность того, что величина $X$ примет значение $x_j$. Метод максимума правдоподобия приводит к частотной оценке:

<a name="eq:freq_estimation"><b>Выражение (4)</b></a>

$$\hat P(X = x_j) = \frac{\#(X = x_j)}{N}
$$

Где $N$ — размер выборки, по которой оценивается распределение $X$ (количество объектов класса $y$ в случае оценки плотности класса $y$).

При этом может оказаться, что некоторое значение $x_j$ ни разу не встречается в обучающей выборке. Например, в случае классификации отзывов методом Наивного Байеса, слово «амбивалентно» не встретилось ни в одном положительном отзыве, но встретилось в отрицательных. Тогда использование [оценки выражения ](#eq:freq_estimation)$(4)$ приведёт к тому, что все отзывы с этим словом будут определяться NB как отрицательные с вероятностью 1. Чтобы избежать принятия таких радикальных решений при недостатке статистики, используют сглаживание Лапласа:

$$\hat P(X = x_j) = \frac{\#(X = x_j) + \alpha}{N + m\alpha},
$$

где $m$ — количество различных значений, принимаемых случайной величиной $X$, $\alpha$ — гиперпараметр.

Для оценки плотности $p$ абсолютно непрерывного распределения в точке $a$ можно разделить количество объектов обучающей выборки в окрестности точки $a$ на размер этой окрестности:

$$\hat p(a) = \frac{\sum\limits_{j}\mathbb{1}_{a - h < X_j < a + h}}{2h} = \frac{\sum\limits_{j}\mathbb{1}_{-h < X_j - a < h}}{2h}.
$$

Обычно объекты, лежащие дальше от точки $a$, учитывают с меньшим весом. Таким образом, оценка плотности приобретает вид

$$\hat p(a) = \frac{\sum\limits_{j}K_h(X_j - a)}{2h},
$$

где функция $K_h$, называемая ядром, обычно имеет носитель $(-h, h)$ ([см. рисунок ниже](#fig:kernels.png)). Такой способ оценки плотности называют непараметрическим.

<a name="#fig:kernels.png"></a>

![13](https://yastatic.net/s3/education-portal/media/13_4_afa4131520_58616d333d.webp)

Результат оценки плотности с разными ядрами. Использованы [изображения из:](https://scikit-learn.org/stable/auto_examples/neighbors/plot_kde_1d.html)

![13](https://yastatic.net/s3/education-portal/media/13_5_8f654203b4_7ec3cb808d.webp)

При параметрической оценке плотности предполагают, что искомое распределение лежит в параметризованном классе, и подбирают значения параметров при помощи метода максимума правдоподобия. Например, предположим, что искомое распределение нормальное. Тогда функция его плотности имеет вид

$$p(x) = \frac{1}{\sigma\sqrt{2\pi}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
$$

Таким образом, чтобы оценить плотность $p(x)$, достаточно оценить параметры $\mu, \sigma$. Метод максимума правдоподобия в этом случае даст такие оценки:

$\hat\mu = \overline X$ — выборочное среднее, $\hat\sigma = \sqrt{\frac{1}{N}\sum_{j=1}^N (X_j - \overline X)}$ — выборочное стандартное отклонение.

Если в модели NB распределения всех признаков объектов каждого класса нормальные, оценив параметры этих распределений, мы сможем каждый класс $y$ описать нормальным распределением со средним $\mu_p$ и диагональной ковариационной матрицей, значения на диагонали которой обозначим $\sigma_p$.

Таким образом, полученная модель (Gaussian Naive Bayes, GNB) эквивалентна модели [GDA](#ss:GDA) с дополнительным ограничением на диагональность ковариационных матриц.

## Наивный байесовский подход и логистическая регрессия

Предположим теперь, что в модели GNB класса всего 2, причём соответствующие им ковариационные матрицы совпадают, как это было в модели LDA. Таким образом $\sigma_0 = \sigma_1 = \sigma$.

Посмотрим, как будет выглядеть $P(Y \vert X)$ в этом случае. По теореме Байеса имеем

$$P(Y=1 \vert X) = \frac{P(Y = 1)P(X \vert Y = 1)}{P(Y = 1)P(X \vert Y = 1) + P(Y = 0)P(X \vert Y = 0)}
$$

Разделим числитель и знаменатель полученного выражения на числитель:

$$P(Y=1 \vert X) = \frac{1}{1 + \frac{P(Y = 0)P(X \vert Y = 0)}{P(Y = 1)P(X \vert Y = 1)}} = \frac{1}{1 + \exp\left(\ln\frac{P(Y=0)P(X \vert Y=0)}{P(Y=1)P(X \vert Y=1)}\right)}
$$

Из условной независимости $X^i$ относительно $Y$ получаем

<a name="eq:posterior"><b>Формула (5)</b></a>

$$\begin{equation}
P(Y=1 \vert X) = \frac{1}{1 + \exp\left(\ln\frac{P(Y=0)}{P(Y=1)} + \sum\limits_{i=1}^d \ln\frac{P(X^i \vert Y=0)}{P(X^i \vert Y=1)}\right)}
\end{equation}
$$

Перепишем сумму в знаменателе, воспользовавшись формулой плотности нормального распределения

$$\sum\limits_{i=1}^d \ln\frac{P(X^i \vert Y=0)}{P(X^i \vert Y=1)} = \sum\limits_{i=1}^d \ln\frac{\frac{1}{\sqrt{2\pi\sigma_i^2}}\exp \left(\frac{-(X^i - \mu_{0,i})^2}{2\sigma_i^2}\right)}{\frac{1}{\sqrt{2\pi\sigma_i^2}}\exp \left(\frac{-(X^i - \mu_{1,i})^2}{2\sigma_i^2}\right)} 
$$

$$= \sum\limits_{i=1}^d \frac{\left(X^i - \mu_{1, i}\right)^2 - \left(X^i - \mu_{0, i}\right)^2}{2\sigma_i^2}= 
\sum\limits_{i=1}^d \left(\frac{\mu_{0, i} - \mu_{1, i}}{\sigma_i^2}X^i + \frac{\mu_{1, i} ^ 2 - \mu_{0, i} ^ 2}{2\sigma_i^2}\right)$$

Подставляя это выражение в [формулу  (5) ](#eq:posterior), получаем

$$P(Y=1 \vert X) = \frac{1}{1 + \exp\left(\ln\frac{P(Y=0)}{P(Y=1)} + \sum\limits_{i=1}^d \left(\frac{\mu_{0, i} - \mu_{1, i}}{\sigma_i^2}X^i + \frac{\mu_{1, i} ^ 2 - \mu_{0, i} ^ 2}{2\sigma_i^2}\right)\right)}
$$

Таким образом, $P(Y=1 \vert X)$ представляется в GNB с общей ковариационной матрицей в таком же виде, как в модели логистической регрессии:

<a name="eq:logreg"><b>Формула (6)</b></a>

$$\begin{equation}
P(Y=1 \vert X) = \frac{1}{1 + \exp\left(w_0 + \sum\limits_{i=1}^d w_i X^i\right)}
\end{equation}
$$

где в случае GNB

$$w_0 = \ln\frac{P(Y=1)}{P(Y=0)} +\sum\limits_{i=1}^d\frac{\mu_{1, i} ^ 2 - \mu_{0, i} ^ 2}{2\sigma_i^2}, \quad
w_i = \frac{\mu_{0, i} - \mu_{1, i}}{\sigma_i^2} \hspace{1cm} i=1, \dots, l$$

Однако это не значит, что модели эквивалентны: модель логистической регрессии накладывает менее строгие ограничения на распределение $P(X, Y)$, чем GNB.

Так, $X^i$ могут не являться условно независимыми относительно $Y$, а распределения $P(X \vert Y=y)$ могут не удовлетворять нормальному закону, но $P(y \vert X)$ может при этом всё равно представляться в виде [формулы  (6) ](#eq:logreg).

В этом случае использование метода логистической регрессии предпочтительнее. С другой стороны, если есть основания полагать, что требования GNB выполняются, то от GNB можно ожидать более высокого качества классификации по сравнению с логистической регрессией.

  ## handbook

  Учебник по машинному обучению

  ## title

  Генеративный подход к классификации

  ## description

  Как использовать распределение меток классов в задаче классификации. LDA, QDA и наивный байес

- 
  ## path

  /handbook/ml/article/bajesovskij-podhod-k-ocenivaniyu

  ## content

  ## Априорное знание

Начнём с простого вопроса: как нам внести в модель априорные знания.

Представьте, что мы обучаем модель линейной регрессии $y\sim \langle x, w\rangle + \varepsilon$, $\varepsilon\sim\mathcal{N}(0,\sigma^2)$. С помощью MLE мы получили некоторую оценку $\widehat{w}$ на веса $w$ — всякие ли их значения мы встретим с покорностью и смирением? Наверное, мы удивимся, если какие-то компоненты вектора $\widehat{w}$ будут очень большими по сравнению с элементами $X$: пожалуй, наша физическая интуиция будет бунтовать против этого, мы задумаемся о том, что из-за потенциальных ошибок сокращения вычисление предсказаний $(x_i, \widehat{w})$ окажутся неточным — в общем, хотелось бы по возможности избежать этого. Но как?

Будь мы приверженцами чисто инженерного подхода, мы бы сделали просто: прибавили бы к функции потерь слагаемое $\alpha \left\|\omega  \right\|_{2}^{2}$, или $\alpha \left\|\omega  \right\|_1$, или ещё что-то такое — тогда процедура обучения стала бы компромиссом между минимизацией исходного лосса и этой добавки, что попортило бы слегка близость $y\sim \langle x, w \rangle$, но зато позволило бы лучше контролировать масштаб $\widehat{w}$. Надо думать, вы узнали в этой конструкции старую добрую регуляризацию.

Но наша цель — зашить наше априорное знание о том, что компоненты $w$ не слишком велики по модулю, в вероятностную модель. Введение в модель априорного знания соответствует введению априорного распределения на $w$. Какое распределение выбрать? Ну, наверное, компоненты $w$ будут независимыми (ещё нам не хватало задавать взаимосвязи между ними!), а каждая из них будет иметь какое-то непрерывное распределение, в котором небольшие по модулю значения более правдоподобны, а совсем большие очень неправдоподобны.

Мы знаем такие распределения? Да, и сразу несколько. Например, нормальное. Логично было бы определить

$$p(w) = \prod_{i=1}^D\mathcal{N}(w_i \vert 0,\tau^2)
$$

где $\tau^2$ — какая-то дисперсия, которую мы возьмём с потолка или подберём по валидационной выборке. Отметим, что выбор нормального распределение следует и из принципа максимальной энтропии: ведь у него наибольшая энтропия среди распределений на всей числовой оси с нулевым матожиданием и дисперсией $\tau^2$.

Контроль масштаба весов — это, вообще говоря, не единственное, что мы можем потребовать. Например, мы можем из каких-то физических соображений знать, что тот или иной вес в линейной модели непременно должен быть неотрицательным. Тогда в качестве априорного на этот вес мы можем взять, например, показательное распределение (которое, напомним, обладает максимальной энтропией среди распределений на положительных числах с данным матожиданием).

## Оцениваем не значение параметра, а его распределение

Раз уж мы начали говорить о распределении на веса $w$, то почему бы не пойти дальше. Решая задачу классификации, мы уже столкнулись с тем, что может быть важна не только предсказанная метка класса, но и вероятности. Аналогичное верно и для задачи регрессии. Давайте рассмотрим две следующих ситуации, в каждой из которых мы пытаемся построить регрессию $y\sim ax + b$:

![14](https://yastatic.net/s3/education-portal/media/14_1_697c390275_382aa2e9ab.webp)

Несмотря на то, что в каждом из случаев «точная формула» или градиентный спуск выдадут нам что-то, степень нашей уверенности в ответе совершенно различная. Один из способов выразить (не)уверенность — оценить распределение параметров. Так, для примеров выше распределения на параметр $a$ могли бы иметь какой-то такой вид:

![14](https://yastatic.net/s3/education-portal/media/14_2_74dbd0c395_3d885b645a.webp)

Дальше мы постараемся формализовать процесс получения таких оценок.

## Построение апостериорного распределения

Давайте ненадолго забудем про линейную регрессию и представим, что мы подобрали с пола монету, которая выпадает орлом с некоторой неизвестной пока вероятностью $\theta$. До тех пор, пока мы не начали её подкидывать, мы совершенно ничего не знаем о $\theta$, эта вероятность может быть совершенно любой — то есть априорное распределение на $\theta$ является равномерным (на отрезке $[0,1]$):

$$p(\theta) = \mathbb{I}_{[0;1]}(\theta)
$$

Теперь представим, что мы подкинули её $n$ раз, получив результаты $Y = (y_1,\ldots,y_n)$ ($0$ — решка, $1$ — орёл), среди которых $n_0 = n - \sum_{i=1}^ny_i$ решек и $n_1=\sum_{i=1}^ny_i$ орлов. Определённо наши познания о числе $p$ стали точнее: так, если $n_1$ мало, то можно заподозрить, что и $p$ невелико (уже чувствуете, запахло распределением!).

Распределение мы посчитаем с помощью формулы Байеса:

$$\color{#348FEA}{p(\theta \vert Y) = \frac{p(\theta , Y)}{p(Y)} = \frac{p(Y \vert \theta)p(\theta)}{
\int p(Y \vert \psi)p(\psi)d\psi}}
$$

в нашем случае:

$$p(\theta \vert Y) = \frac{\prod_{i=1}^n\theta^{y_i}(1 - \theta)^{1 - y_i}\mathbb{I}_{[0,1]}(\theta)}{
\int_0^1\prod_{i=1}^n\psi^{y_i}(1 - \psi)^{1 - y_i}d\psi} =
$$

$$=\frac{\theta^{n_1}(1 - \theta)^{n_0}\mathbb{I}_{[0,1]}(\theta)}{
\int_0^1\psi^{n_1}(1 - \psi)^{n_0}d\psi}
$$

В этом выражении нетрудно узнать бета-распределение: $\text{Beta}(n_1 + 1, n_0 + 1)$. Давайте нарисует графики его плотности для нескольких конкретных значений $n_0$ и $n_1$:

![14](https://yastatic.net/s3/education-portal/media/14_3_f944ffcbc1_b36f7d62ea.webp)

Как можно заметить, с ростом $n$ мы всё лучше понимаем, каким может быть $\theta$, при этом если орёл выпадал редко, то пик оказывается ближе к нулю, и наоборот. Ширина пика в каком-то смысле отражает нашу уверенность в том, какими могут быть значения параметра, и не случайно чем больше у нас данных — тем уже будет пик, то есть тем больше уверенности.

Распределение $p(\theta\vert Y)$ параметра, полученное с учётом данных, называется **апостериорным**. Переход от априорного распределения к апостериорному отражает обновление нашего представления о параметрах распределения с учётом полученной информации, и этот процесс является сердцем байесовского подхода. Отметим, что если нам придут новые данные $Y' = (y_1',\ldots,y_m')$, в которых $m_0$ решек и $m_1$ орлов, мы сможем ещё раз обновить распределение по той же формуле Байеса:

$$p(\theta \vert Y\cup Y') = p([\theta\vert Y]\vert Y') = \frac{p(Y'\vert\theta)p(\theta\vert Y)}{p(Y')}=
$$

$$=\frac{\theta^{m_1}(1 - \theta)^{m_0}\frac{\theta^{n_1}(1 - \theta)^{n_0}}{B(n_1 + 1, n_0 + 1)}\mathbb{I}_{[0,1]}(\theta)}{
\text{злой интеграл}} =
$$

$$=\frac{\theta^{n_1 + m_1}(1 - \theta)^{n_0 + m_0}}{\text{константа}}\sim\text{Beta}(n_1 + m_1 + 1, n_0 + m_0 + 1)
$$

**Вопрос на подумать**. Пусть $p(y\vert\mu) = \mathcal{N}(y\vert\mu,\sigma^2)$ — нормальное распределение с фиксированной дисперсией $\sigma^2$, а для параметра $\mu$ в качестве априорного выбрано также нормальное распределение $\mathcal{N}(y\vert \lambda,\theta^2)$. Каким будет апостериорное распределение при условии данных $Y = (y_1,\ldots,y_n)$?

{% cut "Ответ (не открывайте сразу; сначала подумайте сами!)" %}

$$p(\theta \vert Y) = \frac{\left[\prod_{i=1}^n\frac1{\sqrt{2\pi\sigma^2}}e^{-\frac{(y_i - \mu)^2}{2\sigma^2}}\right]\cdot\frac1{\sqrt{2\pi\theta^2}}e^{-\frac{(\mu - \lambda)^2}{2\theta^2}}}{
\text{злой интеграл}} =
$$

$$=\frac{\frac1{(2\pi\sigma^2)^{\frac{n}2}}\cdot\frac1{\sqrt{2\pi\theta^2}}e^{-\frac1{2\sigma^2}\sum_{i=1}^n(y_i - \mu)^2-\frac{1}{2\theta^2}(\mu - \lambda)^2}}{
\text{злой интеграл}}
$$

С точностью до множителей, не зависящих от $\mu$, это экспонента квадратичной формы, то есть распределение будет нормальным. Давайте найдём его параметры. Для этого нам нужно выделить в показателе полный квадрат:

$$=\frac{e^{-\frac12\left(\frac{n}{\sigma^2} + \frac{1}{\theta^2}\right)\mu^2 + \left(\frac1{\sigma^2}\sum_{i=1}^ny_i + \frac1{\theta^2}\lambda\right)\mu + \text{свободный член}}}{
\text{что-то, где нет }\mu} =
$$

Обозначим $\rho^2 = \left(\frac{n}{\sigma^2} + \frac{1}{\theta^2}\right)^{-1}$

$$=frac{e^{-\frac1{2\rho^2}\left[\mu^2 - \rho^2\left(\frac1{\sigma^2}\sum_{i=1}^ny_i + \frac1{\theta^2}\lambda\right)\right]^2 + \text{остальное}}}{
\text{что-то, где нет }\mu}
$$

Теперь уже хорошо видно, что получилось нормальное распределение с параметрами

$$\lambda_{new} = \left(\frac{n}{\sigma^2} + \frac{1}{\theta^2}\right)^{-1}\left(\frac1{\sigma^2}\sum_{i=1}^ny_i + \frac1{\theta^2}\lambda\right)
$$

$$\theta_{new}^2 = \left(\frac{n}{\sigma^2} + \frac{1}{\theta^2}\right)^{-1}
$$

Формулы жутковатые, но проинтерпретировать их можно. Например, мы видим, что появление больших $y_i$ будет сдвигать среднее, а дисперсия уменьшается с ростом $n$.

{% endcut %}

## Сопряжённые распределения

В двух предыдущих примерах нам очень сильно повезло, что апостериорные распределения оказались нашими добрыми знакомыми. Если же взять случайную пару распределений $p(y\vert\theta)$ и $p(\theta)$, результат может оказаться совсем не таким приятным.

В самом деле, нет никакой проблемы в том, чтобы посчитать числитель формулы Байеса, но вот интеграл в знаменателе может и не найтись. Поэтому выбирать распределения нужно с умом. Более того, поскольку апостериорное распределение само станет априорным, когда придут новые данные, хочется, чтобы априорное и апостериорное распределения были из одного семейства; пары (семейств) распределений $p(y\vert\theta)$ и $p(\theta)$, для которых это выполняется, называются **сопряжёнными** $p(\theta)$ называется **сопряжённым** к $p(y\vert\theta)$. Полезно помнить несколько наиболее распространённых пар сопряжённых распределений:

* $p(y\vert\theta)$ — распределение Бернулли с вероятностью успеха $\theta$, $p(\theta)$ — бета распределение;
* $p(y\vert\mu)$ — нормальное с матожиданием $\mu$ и фиксированной дисперсией $\sigma^2$, $p(\theta)$ также нормальное;
* $p(y\vert\lambda)$ — показательное с параметром $\lambda$, $p(\lambda)$ — гамма распределение;
* $p(y\vert\lambda)$ — пуассоновское с параметром $\lambda$, $p(\lambda)$ — гамма распределение;
* $p(y\vert\theta)$ — равномерное на отрезке $[0,\theta]$, $p(\theta)$ — Парето;

Возможно, вы заметили, что почти все указанные выше семейства распределений (кроме равномерного и Парето) относятся к экспоненциальному классу. И это не случайность! Экспоненциальный класс и тут лучше всех: оказывается, что для $p(y\vert\theta)$ из экспоненциального класса можно легко подобрать сопряжённое $p(\theta)$. Давайте же это сделаем.

Пусть $p(y\vert\theta)$ имеет вид

$$p(y\vert\theta) = \frac1{h(\theta)}g(y)\exp(\theta^Tu(y))
$$

Положим

$$p(\theta) = \frac1{h^{\nu}(\theta)}\exp(\eta^T\theta)\cdot f(\eta, \nu)
$$

где $f(\eta, \nu)$ — множитель, обеспечивающий равенство единице интеграла от этой функции. Найдём апостериорное распределение:

$$p(\theta\vert Y) = \frac{\left[\prod_{i=1}^n\frac1{h(\theta)}g(y_i)\exp(\theta^Tu(y_i))\right]\frac1{h^{\nu}(\theta)}\exp(\eta^T\theta)\cdot f(\eta, \nu)}{\text{злой интеграл}} =
$$

$$= \frac{\frac1{h^{\nu + n}(\theta)}\exp\left(\theta^T\left[\eta + \sum_{i=1}^nu(y_i)\right]\right)}{\text{что-то, где нет }\theta}
$$

Это распределение действительно из того же семейства, что и $p(\theta)$, только с новыми параметрами:

$$\eta_{new} = \eta + \sum_{i=1}^nu(y_i),\quad\nu_{new} = \nu + n
$$

**Пример**. Пусть $p(y\vert q) = q^y(1 - q)^{1 - y}$ подчиняется распределению Бернулли. Напомним, что оно следующим образом представляется в привычном для экспоненциального класса виде:

$$p(y\vert q) = \underbrace{(1 - q)}_{=\frac1{h(q)}}\exp\left(\underbrace{y}_{=u_1(y)}\underbrace{\log{\frac{q}{1 - q}}}_{=\theta}\right)
$$

Предлагается брать априорное распределение вида

$$p(q) = \frac{(1 - q)^{\nu}\exp\left(\eta\log{\frac{q}{1-q}}\right)}{\text{что-то, где нет}q}
$$

Тогда апостериорное распределение будет иметь вид (проверьте, посчитав по формуле Байеса!)

$$p(q\vert Y) = \frac{(1 - q)^{\nu + n}\exp\left(\left[\eta + \sum_{i=1}^ny_i\right]\log{\frac{q}{1-q}}\right)}{\text{что-то, где нет}q}
$$

Превратив логарифм частного в сумму, а экспоненту суммы в произведение, легко убедиться, что получается то самое бета распределение, которое мы уже получали выше.

## Оценка апостериорного максимума (MAP)

Апостериорное распределение — это очень тонкий инструмент анализа данных, но иногда надо просто сказать число (или же интеграл в знаменателе не берётся и мы не можем толком посчитать распределение). В качестве точечной оценки логично выдать самое вероятное значение $\theta\vert Y$ (интеграл в знаменателе от $\theta$ не зависит, поэтому на максимизацию не влияет):

$$\color{blue}{\widehat{\theta}_{MAP} = \underset{\theta}{\operatorname{argmax}}{p(\theta \vert Y)} = \underset{\theta}{\operatorname{argmax}}{p(Y \vert \theta)p(\theta)}}
$$

Это число называется **оценкой апостериорного максимума (MAP)**.

Если же в формуле выше перейти к логарифмам, то мы получим кое-что, до боли напоминающее старую добрую регуляризацию (и не просто так, как мы вскоре убедимся!):

$$\underset{\theta}{\operatorname{argmax}}{p(Y \vert \theta)p(\theta)} = \underset{\theta}{\operatorname{argmax}}\log(p(Y \vert \theta)p(\theta)) = 
$$

$$=\underset{\theta}{\operatorname{argmax}}\left(\vphantom{\frac12}\log{p(Y \vert \theta)} + \log{p(\theta)}\right)
$$

**Пример**. Рассмотрим снова распределение Бернулли $p(y\vert q)$ и априорное распределение $p(q)\sim\text{Beta}(q\vert a, b)$. Тогда MAP-оценка будет равна

$$\underset{q}{\operatorname{argmax}}{p(Y \vert q)p(q)} = \underset{q}{\operatorname{argmax}}{q^{\sum_{i=1}^ny_i}(1 - q)^{n - \sum_{i=1}^ny_i}\cdot q^{a - 1}(1 - q)^{b - 1}} = 
$$

$$\underset{q}{\operatorname{argmax}}\left((a - 1 + \sum_{i=1}^ny_i)\log{q} + (b - 1 + n - \sum_{i=1}^ny_i)\log(1 - q)\right)
$$

Дифференцируя по $q$ и приравнивая производную к нулю, мы получаем

$$q = \frac{a + \sum_{i=1}^ny_i - 1}{a + b + n - 2}
$$

В отличие от оценки максимального правдоподобия $\frac{\sum_{i=1}^ny_i}{n}$ мы здесь используем априорное знание: параметры $(a - 1)$ и $(b - 1)$ работают как «память о воображаемых испытаниях», как будто бы до того, как получить данные $y_i$, мы уже имели $(a - 1)$ успехов и $(b - 1)$ неудач.

## Связь MAP- и MLE-оценок

Оценка максимального правдоподобия является частным случаем апостериорной оценки.

В самом деле, если априорное распределение является равномерным, то есть $p(\theta)$ не зависит $\theta$ (если веса $\theta$ вещественные, могут потребоваться дополнительные усилия, чтобы понять, как такое вообще получается), и тогда

$$\widehat{\theta}_{MAP} = \underset{\theta}{\operatorname{argmax}}\log{p(Y \vert \theta)p(\theta)} = \underset{\theta}{\operatorname{argmax}}\left(\log{p(Y \vert \theta)} + \underbrace{\log{p(\theta)}}_{=const}\right) =
$$

$$= \underset{\theta}{\operatorname{argmax}}\log{p(y \vert \theta)} = \widehat{\theta}_{MLE}
$$

## Байесовские оценки для условных распределений

В предыдущих разделах мы разобрали, как байесовский подход работает для обычных, не условных распределений. Теперь вернёмся к чему-то более близкому к машинному обучению, а именно к распределениям вида $y\vert x,w$, и убедимся, что для них байесовских подход работает точно так же, как и для обычных распределений.

Имея некоторое распределение $p(y\vert x, w)$, мы подбираем для него априорное распределение на веса $p(w)$ (и да, оно не зависит от $x$: ведь априорное распределение существует ещё до появления данных) и вычисляем апостериорное распределение на веса:

$$p(w \vert X, y)
$$

Вычислять его мы будем по уже привычной формуле Байеса:

$$\color{blue}{p(w \vert X, y) = \frac{p(y, w \vert X)}{p(y)} = \frac{p(y \vert X, w)p(w)}{p(y)}}
$$

Повторим ещё разок, в чём суть байесовского подхода: у нас было некоторое априорное представление $\color{blue}{p(w)}$ о распределении весов $\color{blue}{w}$, а теперь, посмотрев на данные $(x_i, y_i)_{i=1}^n$, мы уточняем своё понимание, формулируя апостериорное представление $p(w \vert X, y)$.

Если же нам нужна только точечная оценка, мы можем ограничиться оценкой апостериорного максимума (MAP):

$$\color{blue}{\widehat{w}_{MAP} = \underset{w}{\operatorname{argmax}}{p(w \vert X,y)} = \underset{w}{\operatorname{argmax}}{p(y \vert X, w)p(w)}} = 
$$

$$=\underset{w}{\operatorname{argmax}}\left(\vphantom{\frac12}\log{p(y \vert X, w)} + \log{p(w)}\right)
$$

что уже до неприличия напоминает регуляризованную модель

## Пример: линейная регрессия с $L^2$-регуляризацией как модель с гауссовским априорным распределением на веса

В модели линейной регрессии $y = \langle x, w\rangle + \varepsilon$, $\varepsilon\sim\mathcal{N}(0, \sigma^2)$ введём априорное распределение на веса вида

$$\color{blue}{p(w) = \mathcal{N}(w  \vert  0, \tau^2I) = \prod_{j=1}^D \mathcal{N}(w_j \vert  0, \tau^2) = \prod_{j=1}^D p(w_j)}
$$

Тогда $\widehat{w}_{MAP}$ — точка минимума следующего выражения:

$$-\log{p(y \vert X, w)} - \log{p(w)} =-\sum_{i=1}^Np(y_i \vert x_i, w) - \sum_{j=1}^Dp(w_j) =
$$

$$=-\sum_{i=1}^N\left(-\frac12\log(2\pi\sigma^2) - \frac{(y_i - (w, x_i))^2}{2\sigma^2}\right)
-\sum_{j=1}^D\left(-\frac12\log(2\pi\tau^2) - \frac{w_j^2}{2\tau^2}\right)=$$

$$
= \frac1{2\sigma^2}\sum_{i=1}^N(y_i - (w, x_i))^2 + \frac1{2\tau^2}\sum_{j=1}^D w_j^2+\text{ не зависящие от }w\text{ члены}
$$

Получается, что

$$\color{blue}{\widehat{w}_{MAP} = \underset{w}{\operatorname{argmin}}\left(\vphantom{\frac12}\sum_{i=1}^N(y_i - (w, x_i))^2 + \frac{\sigma^2}{\tau^2}\|w\|^2\right)}
$$

а это же функция потерь для линейной регрессии с $L^2$-регуляризацией! Напомним на всякий случай, что у этой задачи есть «точное» решение

$$\color{blue}{\widehat{w}_{MAP} = \left(X^TX + \frac{\sigma^2}{\tau^2}I\right)^{-1}X^Ty}
$$

Для этого примера мы можем вычислить и апостериорное распределение $p(w \vert X, y)$. В самом деле, из написанного выше мы можем заключить, что

$$\log{p(w \vert X, y)} = \log(p(y \vert X, w)p(w)) - \log{p(y)} = 
$$

$$=\frac1{2\sigma^2}(y - Xw)^T(y - Xw) + \frac1{2\tau^2}w^Tw+\text{ не зависящие от }w\text{ члены}
$$

Таким образом, $\log{p(w \vert X, y)}$ — это квадратичная функция от $w$, откуда следует, что апостериорное распределение является нормальным. Чтобы найти его параметры, нужно немного преобразовать полученное выражение:

$$\ldots=\frac1{2\sigma^2}(y^Ty - w^TX^Ty - y^TWx + w^TX^TXw) + \frac1{2\tau^2}w^Tw+\mathrm{const}(w) =
$$

$$=w^T\left(\frac1{2\sigma^2}X^TX + \frac1{2\tau^2}I\right)w - \frac{1}{2\sigma^2}w^TX^Ty - \frac1{2\sigma^2}y^TWx + \mathrm{const}(w) =
$$

$$=\frac12\left(w - \widehat{w}_{MAP}\right)^T\left(\frac1{\sigma^2}X^TX + \frac1{\tau^2}I\right)\left(w - \widehat{w}_{MAP}\right) + \mathrm{const}(w)=
$$

Таким образом,

$$\color{blue}{p(w \vert X,y) = \mathcal{N}\left(\widehat{w}_{MAP}, \left(\frac1{\sigma^2}X^TX + \frac1{\tau^2}I\right)^{-1}  \right)}
$$

Как видим, от априорного распределения оно отличается корректировкой как матожидания $0\mapsto\widehat{w}_{MAP}$, так и ковариационной матрицы $\left(\frac1{\tau^2}I\right)^{-1}\mapsto\left(\frac1{\sigma^2}X^TX + \frac1{\tau^2}I\right)^{-1}$. Отметим, что $X^TX$ — это, с точностью до численного множителя, оценка ковариационной матрицы признаков нашего датасета (элементы матрицы $X^TX$ — это скалярные произведения столбцов $X$, то есть столбцов значений признаков).

**Иллюстрация**. Давайте на простом примере (датасет с двумя признаками) посмотрим, как меняется апостериорное распределение $w$ с ростом размера обучающей выборки:

![14](https://yastatic.net/s3/education-portal/media/14_4_80113d832e_8eae2745c6.webp)

Как видим, не только мода распределения, то есть $\widehat{w}_{MAP}$ приближается к своему истинному значению, но и дисперсия распределения постепенно уменьшается.

**Ещё иллюстрация**. Теперь рассмотрим задачу аппроксимации неизвестной функции одной переменной (чьи значения в обучающей выборке искажены нормальным шумом) многочленом третьей степени. Её, разумеется, тоже можно решать, как задачу линейной регрессии на коэффициенты многочлена. Давайте нарисуем, как будут выглядеть функции, сгенерированные из распределения ${p(w \vert X,y)}$ для разного объёма обучающей выборки:

![14](https://yastatic.net/s3/education-portal/media/14_5_eee8390d4e_a5da4d0d9c.webp)

Тут тоже видим, что функции не только становятся ближе к истинной, но и разброс их уменьшается.

## Пример: линейная регрессия с $L^1$-регуляризацией как модель с лапласовским априорным распределением на веса

Другое распределение, которое тоже может кодировать наше желание, чтобы небольшие по модулю значения $w_j$ были правдоподобными, а большие не очень, — распределение Лапласа. Посмотрим, что будет, если его взять в качестве априорного распределения на веса.

$$\color{blue}{p(w) = \prod_{j=1}^D p(w_j) = \prod_{j=1}^D\frac{\lambda}{2}\exp(-\lambda|w_m|)}
$$

Проводя такое же вычисление, получаем, что

$$\color{blue}{\widehat{w}_{MAP} = \underset{w}{\operatorname{argmin}}\left(\vphantom{\frac12}\sum_{i=1}^N(y_i - (w, x_i))^2 + \lambda\sum_{j=1}^D|w_j|\right)}
$$

а это же функция потерь для линейной регрессии с $L^1$-регуляризацией!

## Как делать предсказания

Все изложенные выше рассуждения проводились в ситуации, когда $X = X_{train}$ — обучающая выборка. Для неё мы можем посчитать

$$p(w \vert X_{train}, y_{train}) = \frac{(y \vert X,w)p(w)}{p(y)}
$$

и точечную апостериорную оценку $\widehat{w}_{MAP} = \underset{w}{\operatorname{argmax}}{p(y \vert X,w)p(y)}$. А теперь пусть нам дан новый объект $x_0\in\mathbb{X}$. Какой таргет $y_0$ мы для него предскажем?

Было бы естественным, раз уж мы предсказываем распределение для $w$, и для $y_0$ тоже предсказывать распределение. Делается это следующим образом:

$$p(y_0 \vert x_0, X_{train}, y_{train}) = \int{p(y_0 \vert x_0,w)p(w \vert X_{train}, y_{train})}dw
$$

Надо признать, что вычисление этого интеграла не всегда посильная задача, поэтому зачастую приходится «просто подставлять $\widehat{w}_{MAP}$». В вероятностных терминах это можно описать так: вместо сложного апостериорного распределения $p(w \vert X_{train}, y_{train})$ мы берём самое грубое на свете приближение

$$p(w \vert X_{train}, y_{train})\approx\delta(w - \widehat{w}_{MAP}),
$$

где $\delta(t)$ — дельта-функция, которая не является честной функцией (а является тем, что математики называют обобщёнными функциями), которая определяется тем свойством, что $\int f(t)\delta(t)dt = f(0)$ для достаточно разумных функций $f$. Если не мудрствовать лукаво, то это всё значит, что

$$p(y_0 \vert x_0,X_{train}, y_{train})\approx p(y_0 \vert x_0,\widehat{w}_{MAP})
$$

**Пример**. Пусть $y\sim Xw + \varepsilon$, $\varepsilon\sim\mathcal{N}(0,\sigma)^2$ — модель линейной регрессии с априорным распределением $p(w) = \mathcal{N}(0,\tau^2)$ на параметры. Тогда, как мы уже видели раньше,

$$p(w \vert X,y) = \mathcal{N}\left(w \vert \widehat{w}_{MAP}, \left(\frac1{\sigma^2}X^TX + \frac1{\tau^2}I\right)^{-1}  \right)
$$

Попробуем для новой точки $x_0$ посчитать распределение на $y_0$. Рекомендуем читателю попробовать самостоятельно посчитать интеграл или же обратиться к пункту 7.6.2 книжки «Machine Learning A Probabilistic Perspective» автора Kevin P. Murphy, убедившись, что

$$p(y_0 \vert x_0, X_{train}, y_{train}) = \mathcal{N}\left(y_0 \vert x_0\widehat{w}_{MAP},
\sigma^2 + \sigma^2x_0^T\left(X^TX + \frac{\sigma^2}{\tau^2}I\right)^{-1}x_0\right)$$

что, очевидно, более содержательно, чем оценка, полученная с помощью приближения $p(w \vert X_{train}, y_{train})\approx\delta(w - \widehat{w}_{MAP})$:

$$p(y_0 \vert x_0, \widehat{w}_{MAP}) = \mathcal{N}\left(y_0\left \vert x_0\widehat{w}_{MAP},
\sigma^2\right.\right)$$

Собственно, видно, что в этом случае

**Пример в примере**. Рассмотрим полюбившуюся уже нам задачу приближения функции многочленом степени не выше $3$ (в которой мы строим модели с $\sigma^2 = \tau^2 = 1$). Для $N = 8$ мы получали такую картинку:

![14](https://yastatic.net/s3/education-portal/media/14_6_c96cd72460_919321cff2.webp)

Если оценить по приведённым выше формулам $p(y_0 \vert x_0, X_{train}, y_{train})$ для разных $x_0$, то можно убедиться, что модель в большей степени уверена в предсказаниях для точек из областей, где было больше точек из обучающей выборки:

![14](https://yastatic.net/s3/education-portal/media/14_7_f8bdef58c8_d73727a08c.webp)

## Байесовский подход и дообучение моделей

До сих пор мы в основном рассуждали о моделях машинного обучения как о чём-то, что один раз обучается и дальше навсегда застывает в таком виде, но в жизни такое скорее редкость. Мы пока не будем обсуждать изменчивость истинных зависимостей во времени, но даже если истина неизменна, к нам могут поступать новые данные, которые очень хотелось бы использовать для дообучения модели.

Обычные, не байесовские вероятностные модели не предоставляют таких инструментов. Оценку максимального правдоподобия придётся пересчитывать заново (хотя, конечно, можно схитрить, использовав старое значение в качестве начального приближения при итеративной оптимизации). Байесовский же подход позволяет оформить дообучения в виде простой и элегантной формулы: при добавлении новых данных $(x_{N+1}, y_{N+1}),\ldots,(x_M, y_M)$ имеем

$$p\left(w\vert (x_i, y_i)_{i=1}^M\right) = \frac{p\left((y_i)_{i=N+1}^M\vert (x_i)_{i=N+1}^M\right) p\left(w\vert (x_i, y_i)_{i=1}^N\right)}{p\left( (y_i)_{i=N+1}^M \right)}
$$

## Байесовский подход к выбору модели: мотивация

Нам часто приходится выбирать: дерево или случайный лес, линейная модель или метод ближайших соседей; да, собственно, и внутри наших вероятностных моделей есть параметры (скажем, дисперсия шума $\sigma^2$ и $\tau^2$), которые надо бы подбирать. Но как?

В обычной ситуации мы выбираем модель, обученную на выборке $(X_{train}, y_{train})$ в зависимости от того, как она себя ведёт на валидационной выборке $(X_{val}, y_{val})$ (сравниваем правдоподобие или более сложные метрики) — или же делаем кросс-валидацию. Но как сравнивать модели, выдающие распределение?

Ответим вопросом на вопрос: а как вообще сравнивать модели? Назначение любой модели — объяснять мир вокруг нас, и её качество определяется именно тем, насколько хорошо она справляется с этой задачей. Тестовая выборка — это хороший способ оценки, потому что она показывает, насколько вписываются в модель новые данные. Но могут быть и другие соображения, помогающие оценить качество модели.

### Пример №1

Аналитик Василий опоздал на работу. Своему руководителю он может предложить самые разные объяснения — и это будет выработанная на одном обучающем примере модель, описывающая причины опоздания и потенциально позволяющая руководителю принять решение о том, карать ли Василия.

Конечно, руководитель мог бы принять изложенную Василием модель к сведению, подождать, пока появятся другие опоздавшие, и оценить её, так скажем, на тестовой выборке, но стоит ли? Давайте рассмотрим несколько конкретных примеров:

* Модель «Василий опоздал, потому что так получилось», то есть факт опоздания — это просто ни от чего не зависящая случайная величина. Такая модель плоха тем, что (а) не предлагает, на самом деле, никакого объяснения тому факту, что Василий опоздал, а его коллега Надежда не опоздала и (б) совершенно не помогает решить, наказывать ли за опоздание. Наверное, такое не удовлетворит руководителя.

* Модель «Василий опоздал, потому что рядом с его домом открылся портал в другой мир, где шла великая битва орков с эльфами, и он почувствовал, что просто обязан принять в ней участие на стороне орков, которых привёл к победе, завоевав руку и сердце орочьей принцессы, после чего был перенесён обратно в наш скучный мир завистливым шаманом». Чем же она плоха? Битва с эльфами — это, безусловно, важное и нужное дело, и на месте руководителя мы бы дружно согласились, что причина уважительная. Но заметим, что в рамках этой модели можно объяснить множество потенциальных исходов, среди которых довольно маловероятным представляется наблюдаемый: тот, в котором Василий не погиб в бою, не остался со своей принцессой и не был порабощён каким-нибудь завистливым шаманом. Отметим и другой недостаток этой модели: её невозможно провалидировать. Если в совершенно случайной модели можно оценить вероятность опоздания и впоследствии, когда накопятся ещё примеры, проверить, правильно ли мы её посчитали, то в мире, где открываются порталы и любой аналитик может завоевать сердце орочьей принцессы, возможно всё, и даже если больше никто не попадёт в такую ситуацию, Василий всё равно сможет бить себя в грудь кулаком и говорить, что он избранный. Так что, наверное, это тоже не очень хорошая модель.

* Модель «Василий опоздал, потому что проспал» достаточно проста, чтобы в неё поверить, и в то же время даёт руководителю возможность принять решение, что делать с Василием.

### Пример №2

Обратимся к примеру из машинного обучения. Сравним три модели линейной регрессии:

![14](https://yastatic.net/s3/education-portal/media/14_8_625e0200be_9761c8224e.webp)

Даже и не запрашивая тестовую выборку, мы можем сделать определённые выводы о качестве этих моделей. Средняя (квадратичная) явно лучше левой (линейной), потому что она лучше объясняет то, что мы видим: тот факт, что облако точек обучающей выборки выглядит вогнутым вниз.

А что с правым, почему мы можем утверждать, что он хуже? Есть много причин критиковать его. Остановимся вот на какой. На средней картинке у нас приближение квадратичной функцией, а на правой — многочленом довольно большой степени (на самом деле, десятой). А ради интереса: как выглядит график квадратичной функции и как — многочлена десятой степени со случайно сгенерированными коэффициентами? Давайте сгенерируем несколько и отметим их значения в точках обучающей выборки:

![14](https://yastatic.net/s3/education-portal/media/14_9_cc0a3b2fb0_5a712ffb7f.webp)

Обратите внимание на масштаб на графиках справа. И какова вероятность, что нам достался именно тот многочлен десятой степени, у которого значения в обучающих точках по модулю в пределах сотни? Очевидно, она очень мала. Поэтому мы можем сказать, что выбор в качестве модели многочлена десятой степени не очень обоснован.

### Попробуем резюмировать

Слишком простая модель плохо объясняет наблюдаемые нами данные, тогда как слишком сложная делает это хорошо, но при этом описывает слишком многообразный мир, в котором имеющиеся у нас данные оказываются уже слишком частным случаем. В каком-то смысле наш способ выбора модели оказывается переформулировкой **бритвы Оккама**: из моделей, пристойно описывающих наблюдаемые явления, следует выбирать наиболее минималистичную.

## Байесовский подход к выбору модели: формализация

Пусть у нас есть некоторое семейство моделей $\mathcal{J}$ и для каждого $j\in\mathcal{J}$ задана какая-то своя вероятностная модель. В духе байесовского подхода было бы оценить условное распределение моделей

$$p(j \vert y, X) = \frac{p(y \vert X,j)p(j)}{\sum\limits_{j\in\mathcal{J}}p(j, y \vert X)}
$$

и в качестве наилучшей модели взять её моду. Если же считать все модели равновероятными, то мы сводим всё к максимизации только лишь $p(y \vert X,j) = p_j(y \vert X)$:

$$\color{blue}{\widehat{\jmath} = \underset{j}{\operatorname{argmax}}\int{p_j(y \vert X,w)p_j(w)}dw =\underset{j}{\operatorname{argmax}}p_j(y \vert X)}
$$

Величина $p_j(y \vert X)$ называется **обоснованностью** (**evidence, marginal likelihood**) модели.

Отметим, что такое определение вполне согласуется с мотивацией из предыдущего подраздела. Слишком простая модель плохо описывает наблюдаемые данные, и потому будет отвергнута. В свою очередь, слишком сложная модель способна описывать гораздо большее многообразие явлений, чем нам было бы достаточно. Таким образом, компромисс между качеством описания и сложностью и даёт нам оптимальную модель.

### Пример

Вернёмся к нашей любимой задаче аппроксимации функции одной переменной многочленом небольшой степени по нескольким точкам, значение в которых было искажено нормальным шумом. Построим несколько моделей, приближающих многочленом степени не выше некоторого $\mathrm{deg}$ (будет принимать значения 1, 3 и 6), положив в вероятностной модели $\sigma^2 = \tau^2 = 1$.

Мы не будем приводить полный вывод обоснованности для задачи регрессии $p(y \vert X,w) = \mathcal{N}(y \vert Xw,\sigma^2I)p(w \vert \tau^2I)$, а сразу выпишем ответ:

$$p(y \vert X) = \mathcal{N}\left(0, \sigma^2I + \tau^2XX^T\right)
$$

Посмотрим, какой будет обоснованность для разного числа обучающих точек:

![14](https://yastatic.net/s3/education-portal/media/14_10_9007290755_b7cce910b1.webp)

Можно убедиться, что для регрессии по двум точкам наиболее обоснованная — линейная модель (и неудивительно), тогда как с ростом числа точек более обоснованной становится модель с многочленом третьей степени; слишком сложная же модель шестой степени всегда плетётся в хвосте.

### Аппроксимация обоснованности и байесовский информационный критерий

Точно вычислить обоснованность может быть трудной задачей (попробуйте проделать это сами хотя бы для линейной регрессии!). Есть разные способы посчитать её приближённо; мы рассмотрим самый простой.

Напомним, что

$$p(y \vert X) = \int{p(y \vert X,w)p(w)}dw 
$$

Воспользуемся **приближением Лапласа**, то есть разложим $p(y \vert X,w)$ (как функцию от $w$) вблизи своего максимума, то есть вблизи $\widehat{w} := \widehat{w}_{MLE}$ в ряд Тейлора:

$$\log{p(y \vert X,w)} \approx \log{p(y \vert X,\widehat{w})} - \frac12(w - \widehat{w})^TI_N(\widehat{w})(w - \widehat{w}),
$$

где линейный член отсутствует, поскольку разложение делается в точке локального экстремума, а $I(\widehat{w})$ — знакомая нам матрица Фишера $I_N(\widehat{w}) = -\mathbb{E}\nabla^2_w\log{p(y \vert X,w)}\vert_{\widehat{w}} = NI_1(\widehat{w})$.

Далее, $p(w)$ мы можем с точностью до второго порядка приблизить $p(\widehat{w}_{MAP})$. Получается, что

$$p(y \vert X)\approx\int e^{\log{p(y \vert X,\widehat{w})} - \frac{N}2(w - \widehat{w})^TI_1(\widehat{w})(w - \widehat{w})}p(\widehat{w}_{MAP})dw =
$$

$$= e^{\log{p(y \vert X,\widehat{w})}}p(\widehat{w}_{MAP})\int e^{ - \frac{N}{2}(w - \widehat{w})^TI_1(\widehat{w})(w - \widehat{w})}dw = 
$$

$$= e^{\log{p(y \vert X,\widehat{w})}}p(\widehat{w}_{MAP})\cdot (2\pi)^{D/2}\frac{|I_1(\widehat{w})|^{-\frac12}}{N^{D/2}} =
$$

$$=\exp\left(\log{p(y \vert X,\widehat{w})} - \frac{D}2\log{N} + \text{всякие штуки}\right)
$$

Несмотря на то, что $p(\widehat{w}_{MAP})$ и $\vert I_1(\widehat{w})\vert^{-\frac12}$, сгруппированные нами во «всякие штуки», существенным образом зависят от модели, при больших $N$ они вносят в показатель гораздо меньше вклада, чем первые два слагаемых. Таким образом, мы можем себе позволить вместо трудновычисляемых $p(y \vert X)$ использовать для сравнения модели \$ \\color\{blue\} \{ \\mbox\{байесовский информационный критерий (BIC)\}:\} \$

$$\color{blue}{BIC = D\log{N} - 2\log{p(y \vert X,\widehat{w})}}
$$

## Фреквентисты против байесиан: кто кого?

Мы с вами познакомились с двумя парадигмами оценивания:

* **фреквентистской** (frequentist, от слова "frequency", частота) — в которой считается, что данные являются случайным (настоящая случайность!) семплом из некоторого фиксированного распределения, которое мы стараемся оценить по этому семплу, и
* **байесовской** — в которой данные считаются данностью и в которой мы используем данные для обновления наших априорных представлений о распределении параметров (здесь случайности нет, а есть лишь нехватка знания).

У обеих есть свои достоинства и недостатки, поборники и гонители. К недостаткам байесовской относится, безусловно, её вычислительная сложность: возможно, вы помните, в пучину вычислений сколь мрачных нас низвергла банальная задача линейной регрессии, и дальше становится только ещё трудней. Если мы захотим байесовский подход применять к более сложным моделям, например, нейросетям, нам придётся прибегать к упрощениям, огрублениям, приближениям, что, разумеется, ухудшает наши оценки. Но, если простить ему эту вынужденную неточность, он логичнее и честней, и мы продемонстрируем это на следующем примере.

Одно известное свойство оценки максимального правдоподобия — **асимптотическая нормальность**. Если оценивать наши веса $w$ по различным наборам из $N$ обучающих примеров, причём считать, что наборы выбираются случайно (не будем уточнять, как именно), то оценка $\widehat{w}_{MLE}$ тоже превращается в случайную величину, которая как-то распределена. Теория утверждает, что при $N\rightarrow\infty$

$$\quad \widehat{w}_{MLE}\sim\mathcal{N}\left(w^{\ast}, I_N({w}^{\ast})^{-1}\right)
$$

где $w^{\ast}$ — истинное значение весов, а $I_N({w}^{\ast})$ — матрица информации Фишера, которая определяется как

$$I_N({w}^{\ast}) = \mathbb{E}\left[\left(\left.\frac{\partial}{\partial w_i}\log{p(y \vert X,w)}\right|_{w^{\ast}}\right)\left(\left.\frac{\partial}{\partial w_j}\log{p(y \vert X,w)}\right|_{w^{\ast}}\right)\right]
$$

что при некоторых не слишком обременительных ограничениях равно

$$I_N({w}^{\ast}) = -\mathbb{E}\left[\left.\frac{\partial^2}{\partial w_i\partial w_j}\log{p(y \vert X,w)}\right|_{w^{\ast}}\right]
$$

При этом поскольку $\log{p(y \vert X,w)} = \sum_{i=1}^N\log{p(y \vert X,w)}$, матрица тоже распадается в сумму, и получается, что $I_N({w}^{\ast}) = NI_1(w^{\ast})$, то есть с ростом $N$ ковариация $(NI_1(w^{\ast}))^{-1}$ оценки максимального правдоподобия стремится к нулю.

На интуитивном уровне можно сказать, что матрица информации Фишера показывает, сколько информации о весах $w$ содержится в $X$.

Поговорим о проблемах. В реальной ситуации мы не знаем $w^{\ast}$ и тем более не можем посчитать матрицу Фишера, то есть мы с самого начала вынуждены лукавить. Ясно, что вместо $w^{\ast}$ можно взять просто $\widehat{w}$, а вместо $I_N(w^{\ast})$ — матрицу $I_N(\widehat{w})$, которую можно даже при желании определить как

$$-\left(\left.\frac{\partial^2}{\partial w_i\partial w_j}\log(p(y \vert X, w))\right|_{w^{\ast}}\right)
$$

безо всякого математического ожидания. Итак, хотя мы можем теперь построить доверительный интервал для оцениваемых параметров, по ходу нами было сделано много упрощений: мы предположили, что асимптотическая оценка распределения уже достигнута, от $w^{\ast}$ перешли к $\widehat{w}$, а для полноты чувств ещё и избавились от математического ожидания. В байесовском подходе мы такого себе не позволяем.

{% cut "Если вам интересно, посмотрите, как это будет выглядеть для линейной регрессии." %}

Рассмотрим модель линейной регрессии $y\sim Xw + \varepsilon$, $\varepsilon\sim\mathcal{N}(0, \sigma^2)$. Для неё

$$\log{p(y \vert X,w)} = -\frac{N}{2\log(2\pi\sigma^2)} - \frac{1}{2\sigma^2}(y - Xw)^T(y - Xw)
$$

Нетрудно убедиться, что

$$\nabla_w\log{p(y \vert X,w)} = \frac{1}{\sigma^2}X^T(y - Xw)
$$

$$\nabla^2_w\log{p(y \vert X,w)} = -\frac{1}{\sigma^2}X^TX
$$

Соответственно,

$$I_N(\widehat{w}) = \frac{1}{\sigma^2}X^TX
$$

где $\widehat{w}$ — это полученная по датасету $X$ оценка весов. Заметим, что $W^TX$ — это с точностью до коэффициента $\frac{1}{N}$ оценка ковариационной матрицы признаков нашего датасета (элементы $X^TX$ — это скалярные произведения столбцов $X$, то есть столбцов признаков). Можно легко убедиться, что

$$\frac{1}{\sigma^2}X^TX = \frac{1}{\sigma^2}\sum_{i=1}^Nx_i^Tx_i
$$

По-хорошему, нам надо было бы ещё взять математическое ожидание. Найти его мы не можем, но можем очень наивно оценить как $C = \frac1N\sum_{i=1}^Nx_i^Tx_i$. Тогда получаем, что $I_N(\widehat{w}) = \frac{N}{\sigma^2}C$. Таким образом, имея один датасет $X$ и одну посчитанную по нему оценку $\widehat{w}$, мы можем довольно грубо оценить распределение оценок максимального правдоподобия для заданного $N$ как

$$\mathcal{N}\left(\widehat{w}, \frac{N}{\sigma^2}C\right)
$$

**Пример в примере**. Давайте рассмотрим задачу аппроксимации функции одной переменной (чьи значения в обучающей выборке искажены нормальным шумом) многочленом степени не выше $3$. Положим в вероятностной модели $\sigma^2 = 1$. Тогда различный выбор обучающих датасетов будет приводить к различным результатам:

![14](https://yastatic.net/s3/education-portal/media/14_11_8334b6c580_39771752b9.webp)

Но разброс результатов падает с ростом $N$.

**Ещё пример в примере**. Рассмотрим ещё одну задачу регрессии с двумя признаками (в которой всё так же будем полагать $\sigma^2 = 1$), для которой оценим распределение $w_0$ через первую компоненту $\mathcal{N}(\widehat{w}, I_N(\widehat{w})^{-1})$ для одного конкретного $\widehat{w}$ и нарисуем несколько различных $\widehat{w}$, полученных из других датасетов той же мощности:

![14](https://yastatic.net/s3/education-portal/media/14_12_dd76b88160_5fdf85100a.webp)

Видим, что средние оцененного распределения сходятся к истинному значению $-1$; при этом дисперсия падает. Красные крестики не вполне подчиняются синему распределению, но мы от них ждём лишь приближённой согласованности, которая имеет место.

{% endcut %}

  ## handbook

  Учебник по машинному обучению

  ## title

  Байесовский подход к оцениванию

  ## description

  Байесовская статистика. Априорные и апостериорные распределения на параметры моделей. MAP-оценки. Байесовский подход к выбору моделей. Байесовский подход для задачи линейной регресии

- 
  ## path

  /handbook/ml/article/modeli-s-latentnymi-peremennymi

  ## content

  ## Зачем нужны модели с латентными переменными

Предположим, что мы делаем анализ данных для банка, и нам предоставили данные о годовых зарплатах клиентов.

![ml](https://yastatic.net/s3/education-portal/media/ml_4_7_1_97ddd4916a_bedbb832da.svg)

В этом графике заметны три моды, которым, наверное, соответствуют три кластера клиентов. Неопытный аналитик мог бы проигнорировать это и попытаться описать график в отчете для руководства с помощью двух чисел — средней зарплаты и стандартного отклонения зарплат.

Однако данные с гистограммы ниже имеют точно такое же среднее и стандартное отклонение, как и мультимодальные данные выше. Распределение совсем другое, правда?

Очевидно, что графики выглядят совершенно по-разному, и правильная интерпретация первого графика может принести бизнесу дополнительные деньги (скажем, если банк научится предлагать клиентам из каждого кластера более кастомизированные предложения). Так что не надо пытаться описывать мультимодальные данные с помощью унимодальных распределений.

![ml](https://yastatic.net/s3/education-portal/media/ml_4_7_2_98eb1026a6_af084afd19.svg)

Проблема заключается в том, что нам неизвестно, к какому кластеру относится каждый клиент, и неизвестны характеристики кластеров — как же их тогда описать?

Для каждого кластера можно попытаться задать свои параметры (среднее и дисперсию). Но как определить, из какого кластера конкретный клиент в выборке? Более того, один клиент может, например, «на 0.7» относиться к одному кластеру и «на 0.3» к другому.

![ml](https://yastatic.net/s3/education-portal/media/ml_4_7_3_8b8fb6435b_bd310510a2.svg)

Как решать такую задачу «мягкой» кластеризации («мягкой», потому что один объект может относиться к нескольким кластерам)? Мы могли бы действовать итерационно. Сначала зададим начальное приближение на параметры распределений.

Например, в нашем случае с клиентами банка из графика можно предположить, что среднее для первой кластера 30, для второго — 40, для третьей — 50, а стандартные отклонения у всех равняются, скажем, 10. Зная эти начальные параметры, мы можем для каждого клиента посчитать степень принадлежности к каждому из трёх кластеров (важно не забыть отнормировать эти числа, чтобы их сумма действительно равнялась единице).

Дальше мы бы могли пересчитать наши средние и дисперсии, «взвешивая» вклад объектов пропорционально степени их принадлежности к каждому кластеру, и таким образом уточнить средние и дисперсии для всех трёх кластеров. Повторяя эти два шага последовательно, мы получили бы средние и дисперсии кластеров, а для каждого объекта — степени принадлежности к кластерам. Это — EM-алгоритм, подробнее о нём мы поговорим ниже.

Кстати, оказывается, что и метод кластеризации K-средних во многом сродни EM-алгоритму (и на самом деле представляет собой его предельный случай). Действительно, мы сначала случайно расставляем центры кластеров.

Затем мы для каждого объекта пересчитываем расстояние до центра каждого кластера, после чего получаем «вес» объекта в каждом кластере («вес» в том смысле, что чем ближе объект к центру кластера, тем больше этот объект учитывается при пересчете центра этого кластера) через нормировку расстояний.

Теперь, если мы применяем настоящий метод K-средних, то приписываем объект к кластеру с самым большим «весом», после чего опять пересчитываем центры кластеров и потом опять измеряем вес для каждого объекта кластера. Строгое же применение EM-алгоритма даёт «мягкую» версию метода K-средних.

## Смеси распределений

Говорят, что распределение $p(x)$ является **смесью распределений**, если его плотность имеет вид

$$p(x)
=
\sum_{k = 1}^{K} \pi_k p_k(x),
\qquad
\sum_{k = 1}^{K} \pi_k = 1,
\qquad
\pi_k \geq 0,
$$

где:

- $K$ — число компонент;
- $\pi_k$ — вероятности компонент;
- $p_k(x)$ — функции правдоподобия, то есть функции вероятности компонент (в дискретном случае) или их плотности (в абсолютно непрерывном случае).

Проиллюстрируем это понятие на примере с банком. Будем считать, что распределения компонент смеси принадлежат некоторому параметрическому семейству: $p_k(x) = \phi ( x \vert \theta_k )$ (например, гауссовскому с параметром $\theta_k = (\mu_k, \sigma_k)$).

Мы можем говорить, что каждое из распределений $p_k(x)$ задаёт свой кластер, причём каждый кластер имеет некоторую априорную вероятность $\pi_k$. Если у нас нет дополнительных данных, разумно положить $\pi_k = \frac{1}{3}$.

Если же нам, к примеру, известно, что какой-нибудь кластер описывает сравнительно малочисленную группу людей, эти вероятности окажутся различными. Таким образом, мы проинтерпретировали нашу **мягкую кластеризацию** в терминах смеси распределений.

## Как генерировать из смеси распределений

Рассмотрим следующий эксперимент: сначала из дискретного распределения  $\{\pi_1, \dots, \pi_K\}$ выбирается номер $k$, а затем из распределения $\phi(x \vert \theta_k)$ выбирается значение $x$. Покажем, что распределение переменной $x$ будет представлять собой смесь.

Введем **скрытую переменную** $z$, отвечающую за то, к какой компоненте смеси будет относиться очередной $x$. Пусть она представляет собой $K$-мерный бинарный случайный вектор, ровно одна компонента которого равна единице:

$$z \in \{0, 1\}^K,
\qquad
\sum_{k = 1}^{K} z_k = 1.
$$

Вероятность того, что единице будет равна $k$-я компонента, положим равной $\pi_k$:

$$p(z_k = 1) = \pi_k.
$$

Запишем распределение сразу всего вектора:

$$p(z) = \prod_{k = 1}^{K} \pi_k^{z_k}.
$$

Теперь, когда номер компоненты смеси известен, сгенерируем $x$ из распределения $\phi(x \vert \theta_k)$:

$$p(x \vert z_k = 1)
=
\phi(x \vert \theta_k),
$$

или, что то же самое,

$$p(x \vert z)
=
\prod_{k = 1}^{K}
\Bigl[
    \phi(x \vert \theta_k)
\Bigr]^{z_k}.
$$

Проверим, что $x$ имеет нужное нам распределение. Запишем совместное распределение переменных $x$ и $z$:

$$p(x, z)
=
p(z) p(x \vert z)
=
\prod_{k = 1}^{K}
\Bigl[
    \pi_k \phi(x \vert \theta_k)
\Bigr]^{z_k}.
$$

Чтобы найти распределение переменной $x$, нужно избавиться от скрытой переменной:

$$p(x)
=
\sum_{z} p(x, z).
$$

Суммирование здесь ведется по всем возможным значениям $z$, то есть по всем $K$-мерным бинарным векторам с одной единицей:

$$p(x)
=
\sum_{z} p(x, z)
=
\sum_{k = 1}^{K}
\pi_k \phi(x \vert \theta_k).
$$

Мы получили, что распределение сгенерированной переменной $x$ в описанном эксперименте представляет собой смесь $K$ компонент.

## Модели со скрытыми переменными

Рассмотрим вероятностную модель с наблюдаемыми переменными $X$ и параметрами $\Theta$, для которой задано правдоподобие $\log p(X \vert \Theta)$.

Предположим, что в модели также существуют **скрытые переменные** $Z$, описывающие её внутреннее состояние и, возможно, недоступные для непосредственного наблюдения (как то, к какому из кластеров относится клиент). Тогда правдоподобие $\log p(X \vert \Theta)$ называется **неполным**, а правдоподобие $\log p(X, Z \vert \Theta)$ — **полным**. Они связаны соотношением

$$\log p(X \vert \Theta) = \log \Biggl\{ \sum_{Z} p(X, Z \vert \Theta) \Biggr\}.
$$

Нашей основной целью будет создать хорошую модель $X$, то есть оценить параметры $\Theta$. И оказывается, что с помощью введения скрытых переменных нередко удаётся существенно упростить правдоподобие и эффективно решить задачу.

Рассмотрим пример со смесями распределений. В качестве наблюдаемых переменных здесь выступает выборка $X = \{x_1, \dots, x_\ell\}$, в качестве скрытых переменных $Z = \{z_1, \dots, z_\ell\}$ — номера компонент, из которых сгенерированы объекты (здесь каждый из $z_i$ — $K$-мерный вектор), в качестве параметров — априорные вероятности и параметры компонент $\Theta = (\pi_1, \dots, \pi_K, \theta_1, \dots, \theta_K)$.

Неполное правдоподобие выглядит так:

$$\log p(X \vert \Theta) = \sum_{i = 1}^{l} \log \Biggl\{\sum_{k = 1}^{K} \pi_k p(x_i \vert \theta_k) \Biggr\}.
$$

Правдоподобие здесь имеет вид **логарифма суммы**. Если приравнять нулю его градиент, то получатся сложные уравнения, не имеющие аналитического решения. Данное правдоподобие сложно вычислять: оно не является выпуклым (а точнее, вогнутым) и может иметь много локальных экстремумов, поэтому применение обычных итерационных методов для его непосредственной максимизации приводит к медленной сходимости.

Рассмотрим теперь полное правдоподобие:

$$\log p(X, Z \vert \Theta) = \sum_{i = 1}^{l}p(x_i, z_i\vert\Theta) = \sum_{i = 1}^{l} \sum_{k = 1}^{K} z_{ik} \Bigl\{ \log \pi_k + \log \phi(x_i \vert \theta_k)
\Bigr\}.
$$

Оно имеет вид **суммы логарифмов**, и это позволяет аналитически найти оценки максимального правдоподобия на параметры $\Theta$ при известных переменных $X$ и $Z$. В общем случае $Z$ также стараются выбирать таким способом, чтобы распределение $p(X, Z \vert \Theta)$ оказалось «лучше» исходного. В каком именно смысле, мы увидим дальше.

Проблема, впрочем, заключается в том, что нам не известны скрытые переменные $Z$, поэтому их необходимо оценивать одновременно с параметрами, что никак не легче максимизации неполного правдоподобия. Осуществить это позволяет **EM-алгоритм**.

## EM-алгоритм

EM-алгоритм решает задачу максимизации полного правдоподобия путём попеременной оптимизации по параметрам и по скрытым переменным.

Опишем сначала **наивный** способ оптимизации. Зафиксируем некоторое начальное приближение для параметров $\Theta^{\text{old}}$. При известных наблюдаемых переменных $X$ и параметрах $\Theta^{\text{old}}$ мы можем оценить скрытые переменные, найдя их наиболее правдоподобные значения:

$$Z^* = \underset{Z}{\operatorname{arg max}} p(Z \vert X, \Theta^\text{old}) = \underset{Z}{\operatorname{arg max}} p(X, Z \vert \Theta^\text{old}).
$$

Зная скрытые переменные, мы можем теперь найти следующее приближение для параметров:

$$\Theta^\text{new} = \underset{\Theta}{\operatorname{arg max}} p(X, Z^* \vert \Theta).
$$

Повторяя итерации до сходимости, мы получим некоторый итоговый вектор параметров $\Theta^\text{*}$.

Данная процедура, однако, далека от идеальной — и ниже мы предложим решение, которое приводит к более качественным результатам.

Воспользуемся байесовским подходом. Точечные оценки параметров несут меньше информации, чем их распределение; учтём это и будем оптимизировать не $Z$, а условное распределение $Z$.

Как и прежде, зафиксируем вектор параметров $\Theta^\text{old}$, но вместо точечной оценки вычислим апостериорное распределение на скрытых переменных $p(Z \vert X, \Theta^\text{old})$, которое будет в некотором смысле оптимальным образом описывать распределение $Z$ при известных $X$ и $\Theta$. В этом заключается **E-шаг** EM-алгоритма.

Отметим, что вычислить $p(Z \vert X \Theta)$ аналитически возможно не для всех распределений, и скрытые переменные стоит подбирать так, чтобы это всё-таки получилось.

Теперь мы должны произвести оптимизацию по $\Theta$. Для этого возьмём логарифм полного правдоподобия $\log p(X, Z \vert \Theta)$ и усредним его по всем возможным значениям скрытых переменных $Z$:

$$Q(\Theta, \Theta^\text{old}) = \mathbb{E}_{Z \sim p(Z \vert X, \Theta^\text{old})} \log p(X, Z \vert \Theta) = \sum_{Z} p(Z \vert X, \Theta^\text{old}) \log p(X, Z \vert \Theta)
$$

Формально говоря, мы нашли матожидание логарифма полного правдоподобия по апостериорному распределению на скрытых переменных.

На **M-шаге** новый вектор параметров находится как максимизатор данного матожидания:

$$\Theta^\text{new} = \underset{\Theta}{\operatorname{arg max}} Q(\Theta, \Theta^\text{old}) = \underset{\Theta}{\operatorname{arg max}} \sum_{Z} p(Z \vert X, \Theta^\text{old}) \log p(X, Z \vert \Theta).
$$

EM-алгоритм состоит в чередовании E-шага и M-шага.

Можно показать, что такой итерационной процесс всегда не уменьшает правдоподобие и сходится.

{% cut "В двух словах: почему это работает" %}

Напомним, что наша цель — найти параметры $\Theta$, максимизирующие $\log p(X \vert \Theta)$. Преобразуем этот логарифм правдоподобия, введя некоторое (пока произвольное) распределение $q(Z)$ на латентных переменных:

$$\log p(X \vert \Theta) = \left(\sum_Zq(Z)dZ\right)\cdot \log p(X \vert \Theta) = 
$$

$$= \sum_Zq(Z)\log p(X \vert \Theta)dZ = \sum_Zq(Z)\log{\frac{p(X, Z \vert \Theta)}{p(Z \vert X, \Theta)}}dZ
$$

Здесь мы применили то, что $p(b) = \frac{p(a, b)}{p(a\vert b)}$ в силу равенства $p(a\vert b) = \frac{p(a, b)}{p(b)}$. Далее, домножим числитель и знаменатель под логарифмом на $q(Z)$:

$$= \sum_Zq(Z)\log{\frac{p(X, Z \vert \Theta)}{p(Z \vert X, \Theta)}}dZ = 
\sum_Zq(Z)\log{\frac{p(X, Z \vert \Theta)q(Z)}{p(Z \vert X, \Theta)q(Z)}}dZ =$$

$$= \underbrace{\sum_Zq(Z)\log{\frac{p(X, Z \vert \Theta)}{q(Z)}}dZ}_{=:\mathcal{L}(q, \Theta)} +
\underbrace{\sum_Zq(Z)\log{\frac{q(Z)}{p(Z \vert X, \Theta)}}dZ}_{=KL(q(Z)\parallel p(Z \vert X, \Theta))}$$

Второе слагаемое — это расстояние Кульбака-Лейблера между распределениями $q(Z)$ и $p(Z \vert X, \Theta)$. Так как расстояние Кульбака-Лейблера всегда неотрицательно, мы получаем, что $\log p(X \vert \Theta) \geqslant \mathcal{L}(q, \Theta)$. Первое слагаемое, $\mathcal{L}(q, \Theta)$, называется **вариационной нижней оценкой** на $\log p(X \vert \Theta)$, и, максимизируя его, мы будем также увеличивать и всю сумму — поэтому в дальнейшем мы позволим себе ограничиться работой только с ним.

Вариационную нижнюю оценку мы будем попеременно оптимизировать по $q$ (да-да, по распределению) и по $\Theta$.

**E-шаг** состоит в максимизации по $q$ при фиксированных $\Theta$. Так как

$$\mathcal{L}(q, \Theta) = \log p(X \vert \Theta) - KL(q(Z)\parallel p(Z \vert X, \Theta)),
$$

а $\log p(X \vert \Theta)$ от $q$ не зависит, максимизировать $\mathcal{L}(q, \Theta)$ по $q$ — это то же самое, что минимизировать $KL(q(Z)\parallel p(Z \vert X, \Theta))$. Расстояние Кульбака-Лейблера минимально, когда распределения совпадают, то есть мы должны обновить $q$ по правилу

$$q(Z) = p(Z \vert X, \Theta)
$$

На **M-шаге** мы максимизируем по $\Theta$ при фиксированном $q$. Преобразуем вариационную нижнюю оценку:

$$\mathcal{L}(q, \Theta) = \sum_Zq(Z)\log{\frac{p(X, Z \vert \Theta)}{q(Z)}}dZ =
$$

$$= \sum_Zq(Z)\log{p(X, Z \vert \Theta)}dZ - \sum_Zq(Z)\log{q(Z)}dZ
$$

Заметим, что $\sum_Zq(Z)\log{q(Z)}dZ$ — это энтропия распределения $q$, и она не зависит от $\Theta$. Таким образом, нам достаточно максимизировать по $\Theta$ математическое ожидание

$$\sum_Zq(Z)\log{p(X, Z \vert \Theta)}dZ = \mathbb{E}_{Z\sim q(Z)}\log{p(X, Z \vert \Theta)}
$$

{% endcut %}

### Жёсткий EM-алгоритм

Не всегда получается подобрать латентные переменные $Z$ так, чтобы $p(Z \vert X, \Theta)$ можно было выразить аналитически, то есть на E-шаге не удаётся минимизировать $\mathcal{L}(q, \Theta)$ по $q$.

В такой ситуации иногда приходится брать оптимум не по всему пространству распределений, а только по некоторому семейству — например, параметрическому, в котором оптимизацию можно проводить градиентными методами. В максимально упрощённой ситуации мы возьмём семейство дельта-функций, то есть вместо распределения на $Z$ будем брать просто точечную оценку. Такая модификация EM-алгоритма называется **жёстким EM-алгоритмом**.

В начале параграфа мы упоминали кластеризацию методом K-средних и отмечали, что EM-алгоритм даёт «мягкую» версию алгоритма: на E-шаге мы не приписываем однозначно точку к какому-то из кластеров (то есть не берём точечную оценку скрытой переменной «номер кластера, к которому принадлежит точка»), а сопоставляем ей вероятности принадлежности каждому из кластеров (то есть распределение на скрытые переменные). Настоящий метод K-средних как раз таки соответствует жёсткому EM-алгоритму.

## Разделение смеси гауссиан

Пусть теперь нам известно, что $N$ точек были семплированы из K **разных** гауссовских распределений и нам неизвестно, какая точка из какого распределения пришла в выборку. Нам нужно оценить параметры ($\mu_1$, $\sigma_1$) для первого распределения, ($\mu_2$, $\sigma_2$) для второго и, соответственно, ($\mu_k$, $\sigma_k$) для $k$-го распределения.

Если мы знаем, что точка $x_i$ пришла из распределения $z_i$, то её правдоподобие в равно:

$$p(x_i \mid z_i, \theta) = \frac1{\sqrt{ 2 \pi }\sigma_{z_i}} \exp\left( -\frac{ (x_i - \mu_{z_i})^2 }{2 \sigma_{z_i}^2} \right)   
$$

Напомним, что $z_i$ — это латентная переменная, обозначающая номер гауссианы (от 1 до $K$), из которой была просемплирована точка $x_i$. Например, если точка $x_i$ просемплирована из распределения с номером 3, то в формулу вместо ($\mu_{z_i}$, $\sigma_{z_i}$) нужно подставлять $(\mu_3, \sigma_3)$.

Воспользуемся EM-алгоритмом для нахождения параметров ($\mu_{z_i}$, $\sigma_{z_i}$). Сначала инициализируем их:

$$\Theta_{\text{old}}={ \mu_{1\_{\text{old}}}, \sigma_{1\_{\text{old}}}, \mu_{2\_{\text{old}}}, \sigma_{2\_{\text{old}}}, \ldots, \mu_{K\_{\text{old}}}, \sigma_{K\_{\text{old}}}\\}
$$

Зная $\Theta\_{\text{old}}$, выполним E-шаг: нужно найти $p(Z \mid X, \Theta)$ или что то же самое, для каждого объекта $x_i$ найти распределение на вероятности $p(z_i = k \mid x_i, \Theta\_{\text{old}})$.

Как найти апостериорную вероятность $p(z_i = k \mid x_i, \Theta\_{\text{old}})$, если мы знаем $x_i$ и у нас есть приближение $\Theta\_{\text{old}}$?

Ответ — по формуле Байеса:

$$p(z_i = k \mid x_i, \Theta_{\text{old}}) = \frac{p(x_i \mid z_i = k, \Theta_{\text{old}}) \cdot p(z_i = k) } {p(x_i | \Theta_{\text{old}} )} = \frac {p(x_i \mid z_i = k, \Theta_{\text{old}}) \cdot p(z_i = k) } { \sum_{k=1}^K p(x_i \mid z_i = k, \Theta_{\text{old}}) \cdot p(z_i = k) }
$$

где $p(z_i = k)$ — априорная вероятность того, что объекта $x_i$ получен из распределения с номером $k$. На первом шаге априорную вероятность можно положить равной $p(z_i = k) = \frac{1}{K}$ для всех гауссиан.

Введём обозначение

$$u_{ik} := p(x_i \mid z_i = k, \Theta_{\text{old}}) =  \frac1{\sqrt{(2 \pi)} \sigma_{k_{\text{old}}} } \exp\left( -\frac{(x_i - \mu_{k_{\text{old}}})^2 }{2 \sigma_{k_{\text{old}}}^2} \right)
$$

— правдоподобие того, что объект $x_i$ пришел из нормального распределения с параметрами $(\mu_{k_{\text{old}}}, \sigma_{k_{\text{old}}}^2)$.

Тогда по формуле Байеса:

$$p(z_i = k| x_i, \theta_{\text{old}}) = \frac{ u_{ik} \cdot \frac{1}{K}} { \sum_{k=1}^K u_{ik} \cdot \frac{1}{K} } 
$$

Вот так для каждого объекта $x_i$ по начальному приближению $\theta_{\text{old}}$ мы посчитаем распределение $p(z_i )$ — с какими вероятностями объект $x_i$ приходит из той или иной компоненты смеси.

Теперь выведем формулы для М-шага.

$$ \theta = \underset{\Theta}{\operatorname{arg max}} \mathbb{E}_{q(Z)} \log p(X | Z, \Theta) = \underset{\Theta}{\operatorname{arg max}} \sum_{i=1}^N \sum_{k=1}^K p(z_i = k | x_i, \Theta) \cdot \log p(x_i | z_i = k, \Theta)  
$$

$$= \sum_{i=1}^N \sum_{k=1}^K p(z_i = k | x_i, \Theta) \cdot \left ( \log \frac{1}{K} - \frac{1}{2 \sigma_k^2} (x_i - \mu_k)^2 - \frac{1}{2} \log \sigma_k^2 \right) + const 
$$

Запишем производную и приравняем к $0$, чтобы найти экстремум:

$$ 0 = \frac{\partial \mathbb{E}_{q(Z)} \log p(X | Z, \Theta) }{\partial \mu_k} = - \sum_{i=1}^N p(z_i = k | x_i, \Theta) \cdot \frac{x_i - \mu_k}{\sigma_k^2} 
$$

Отсюда

$$\mu_k = \frac{ \sum_{i=1}^N p(z_i = k | x_i, \theta) \cdot x_i } {\sum_{i=1}^N p(z_i = k | x_i, \theta) } 
$$

Мы получили конечную формулу для пересчета $\mu_k$ по $z_i$ и предыдущему значению $\theta$. Причем у этой формулы есть простая интерпретация — каждый объект мы взвешиваем с его вероятностью принадлежности к этому классу $p(z_i = k \mid x, \theta)$.

Теперь посчитаем производную по $\sigma_k^2$ (обратите внимание, что именно по квадрату $\sigma_k$):

$$\frac{\partial \mathbb{E}_{q(Z)} \log p(X | Z, \Theta) }{\partial \sigma_k^2} = \sum_{i=1}^N   p(z_i = k | x_i, \theta) \cdot \left( \frac{(x_i - \mu_k)^2}{2 \sigma_k^4} - \frac{1}{2 \sigma_k^2} \right) = 0
$$

Стало быть,

$$\sigma_k^2 = \frac{ \sum_{i=1}^N  p(z_i = k | x_i, \Theta) (x_i - \mu_k)^2 }{ \sum_{i=1}^N p(z_i = k | x_i, \Theta) }
$$

Мы снова получили интерпретируемый результат: подсчитывая дисперсию для $k$-ой гауссианы, мы учитываем вес каждого объекта при подсчете среднеквадратичноого отклонения. То есть веса — вероятности происхождения из той или иной компоненты смеси. Сравните эту формулу с формулой для подсчета выборочной дисперсии, где каждый из $N$ объектов вносит одинаковый вклад в дисперсию с весом $\frac{1}{N}$:

$$\sigma^2 = \frac{\sum_i^N (x_i - \mu)^2}{N}
$$

Вы можете «пощупать» EM-алгоритм в задаче разделения вероятностной смеси с помощью интерактивной визуализации — попробуйте сделать E и M шаги и последить за изменениями параметров: после одной итераций алгоритма можно выбрать точку на графике и наблюдать за вероятностью её принадлежности к разным кластерам.

## Вероятностный PCA

Теперь давайте рассмотрим простой пример того, как введение латентных переменных может помочь выделять новые информативные признаки в данных.

Предположим, что мы имеем выборку данных $x_i$ (вектор-**строку**), где каждый объект имеет $D$ признаков (предположим, что число $D$ очень большое). Это достаточно типичная ситуация, например, при работе с текстами или изображениями.

Теперь введём следующую вероятностную модель $x_i = z_i W^T + \varepsilon$, где $z_i$ — латентный вектор-строка меньшей размерности $T$, а $\varepsilon \sim \mathcal{N}(0, \sigma^2 E)$, где $E$ — единичная матрица размером $D \times D$, $\sigma$ — скаляр больший 0.

Что означает эта модель? Она означает, что наши сложные многоразмерные данные $x_i$ могут иметь более простое малоразмерное представление $z_i$, а отображение $z_i \rightarrow x_i$ линейно с точностью до нормально распределенного шума.

Заметим, что так как $\varepsilon \sim  \mathcal{N}(0, \sigma^2 E)$, отсюда следует, что $x_i \sim \mathcal{N} (z_i W^T,  \sigma^2 E)$. Зададим априорное распределение на $z_i$ как стандартное нормальное $z_i \sim \mathcal{N}(0, E)$ и распишем совместное распределение $(x_i, z_i)$ через условное и априорное:

$$p(x_i, z_i \vert W, \sigma) = p(x_i \vert z_i, W, \sigma ) p(z_i) 
$$

Чтобы восстановить параметры $W$, $\sigma$ и латентные переменные $z_i$, снова воспользуемся EM-алгоритмом.

**На E-шаге** мы оцениваем распределение на $z_i$ при фиксированных $W$ и $\sigma$:

По формуле Байеса распределение на $z_i$ при условии $x_i$:

$$p(z_i \vert x_i, W, \sigma) = \frac{p(x_i \vert z_i, W, \sigma) \cdot p(z_i)}{p(x_i \vert W, \sigma)}
$$

С точностью до констант и слагаемых, которые не зависят от $z_i$, логарифм правдоподобия равен:

$$\log p(z_i \vert x_i, W, \sigma) \sim - \frac{1}{2 \sigma^2} (x_i - z_iW^T) (x_i - z_i W^T)^T - \frac{1}{2} z_i z_i^T  
$$

$$\log p(z_i \vert x_i, W, \sigma) \sim -\frac{1}{2 \sigma^2} z_i W^T W z_i^T - \frac{1}{2} z_i z_i^T + \frac{1}{2\sigma^2} (x_i W z_i^T + z_i W^T x_i^T) 
$$

$$\log p(z_i \vert x_i, W, \sigma) \sim - \frac{1}{2 \sigma^2} z_i (W^T W + \sigma^2 E) z_i^T + \frac{1}{2\sigma^2} (x_i W z_i^T + z_i W^T x_i^T) 
$$

Обозначим $M:= W^T W + \sigma^2 E$, тогда

$$\log p(z_i \vert x_i, W, \sigma) \sim -\frac{1}{2 \sigma^2} z_i (\sigma^2 M^{-1})^{-1} z_i^T + \frac{1}{2} \left( z_i (\sigma^2 M^{-1})^{-1} \cdot M^{-1} W^T x_i^T + x_i W M^{-1} \cdot (\sigma^2 M^{-1})^{-1} z_i  \right) 
$$

$$\log p(z_i \vert x_i, W, \sigma) \sim -\frac{1}{2}  (z_i  - x_i W M^{-1}) \cdot (\sigma^2 M)^{-1} \cdot (z_i - x_i W M^{-1})^T
$$

Если теперь взять от этого экспоненту, увидим, что $p(z_i \vert x_i, W, \sigma) \sim \mathcal{N} (x_i W M^{-1}, \sigma^2 M)$.

**M-шаг**.

Теперь мы оптимизировать по $W$ и $\sigma^2$:

$$\mathbb{E}_{p(Z \vert X, W, \sigma)} \log p(X, Z \vert W, \sigma) = \sum_i^n \mathbb{E}_{p(z_i \vert x_i, W, \sigma)} \log p(x_i, z_i \vert W, \sigma)   \rightarrow \min_{W,\sigma}
$$

Приравняв производные к $0$, можно найти:

$$W_{\text{new}} = \left( \sum_i^n ( \mathbb{E} z_i ) x_i^T\right) \cdot \left( \sum_i^n \mathbb{E} (z_i^T z_i) \right)^{-1}
$$

$$\sigma_{\text{new}} = \frac{1}{ND} \sum_i^n \left[  \vert \vert x_i \vert \vert^2 - 2x_i W_{\text{new}} \mathbb{E} z_i^T + \text{tr} \left( W_{\text{new}}^T W_{\text{new}} \mathbb{E} (z_i^T z_i)   \right) \right]
$$

Вероятностный PCA хорош тем, что:

1. как любая байесовская модель, может служить промежуточным участком в более сложной вероятностной модели;

2. если в данных есть пропуски, то вероятностный PCA легко обобщается и на этот случай с добавлением дополнительных скрытых переменных;

3. так как параметры $W, \sigma$ и оценки на $z_i$ получаются через итерационный EM-алгоритм, то вероятностный PCA может быть вычислительно эффективнее. Так, в вычислениях и промежуточных формулах нигде не используется матрица $X^T X \in \mathcal{R}^{D \times D}$, и все рассматриваемые матрицы имеют меньший размер.

**Связь с обычным PCA**

Как вероятностный PCA связан с обычным, который мы изучили в теме про разложение матриц?

Напомним, что в обычном SVD-разложении мы полагали, что $x_i \sim z_i \hat{\Sigma} \hat{V}^T$. Давайте опять положим, что разница между $x_i$ и $z_i \hat{\Sigma} \hat{V}^T$ есть гауссовский шум с нулевым средним $\varepsilon \sim \mathcal{N}(0, \sigma^2 E)$:

$$x_i = z_i \hat{\Sigma} \hat{V}^T + \varepsilon
$$

или

$$x_i = \mathcal{N} (z_i \hat{\Sigma} \hat{V}^T , \sigma^2 E)
$$

Если зададим априорное распределение на $z_i$ как стандартное нормальное $p(z_i) \sim \mathcal{N}(0, E)$, тогда $z_i V^T \sim \mathcal{N}(0, \hat{V}^T \hat{\Sigma}^2 \hat{V})$ и соответственно $x_i \sim \mathcal{N}(0, \hat{V}^T \hat{\Sigma}^2 \hat{V} + \sigma^2 E)$.

Теперь сделаем обратную замену $W^T = \hat{\Sigma} \hat{V}^T$ и убедимся, что оценка максимального правдоподобия в точности равна $\hat{\Sigma} \hat{V}^T$.

$$\log p(x_i \vert W, \sigma) = - \frac{N}{2} \log \det (W^T W + \sigma^2 E) - \frac{1}{2} \sum_i x_i (W^T W + \sigma^2 E)^{-1} x_i^T + const
$$

(напомним, что $x_i$ и $z_i$ это вектор-строки). Заметим, что число есть след матрицы, состоящей из этого числа, поэтому можно преобразовать вторую часть, как

$$-\frac{1}{2} \sum_i x_i (W^T W + \sigma^2 E)^{-1} x_i^T = -\frac{1}{2} \textit{tr} \left( \sum_i x_i ( W^T W + \sigma^2 E)^{-1} x_i^T  \right) =
$$

$$= -\frac{1}{2} \textit{tr} \left( (W^T W + \sigma^2 E)^{-1} \cdot \left( \sum_i x_i^T x_i \right) \right) = 
$$

$$= -\frac{1}{2} \textit{tr} \left( (W^T W + \sigma^2 E)^{-1} \cdot XX^T \right) 
$$

Отсюда следует, что

$$\log p(x_i \vert W, \sigma) = -\frac{N}{2} \log \det (W^T W + \sigma^2 E) - \frac{1}{2}  \textit{tr} \left( (W^T W + \sigma^2 E)^{-1} \cdot XX^T \right) 
$$

Приравняв производную по $W$ к нулю, найдем:

$$W^T_{\textit{ML}} = (\hat{\Sigma}^2 - \sigma^2 E)^{\frac{1}{2}} \hat{V}^T
$$

Оценка максимума правдоподобия на $\sigma^2$:

$$\sigma^2_{\textit{ML}} = \frac{1}{D - T} \sum_{j=T+1}^{D} \sigma_j^2,
$$

Эту оценку можно интерпретировать как среднюю потерю дисперсии по всем проигнорированным сингулярным направлениям. Если же $\sigma^2$ — константа, то при $\sigma \rightarrow 0$ получаем обычный PCA.

Другой способ получить обычный PCA — это вместо обычного EM-алгоритма воспользоваться его жёсткой модификацией.

--------

Теперь предлагаем вам потренировать изученный материал на практике. Предлагаем вам выполнить лабораторную работу, которая покрывает большинство тем главы “Вероятностные модели”. Скачайте [ноутбук](https://yastatic.net/s3/ml-handbook/admin/autohw_lm_probability_7de017bf61.ipynb?updated_at=2024-03-07T13:21:16.239Z) с лабораторной работой. В нём вы найдете описания заданий и дополнительные материалы. Задания из лабораторной прикреплены к этому параграфу в виде задач в системе Яндекс Контест. Чтобы проверить себя, отправляйте решения по соответствующим задачам в систему. Успехов в практике!

  ## handbook

  Учебник по машинному обучению

  ## title

  Модели с латентными переменными

  ## description

  *null*

- 
  ## path

  /handbook/ml/article/nejronnye-seti

  ## content

  В этом параграфе вы познакомитесь с **нейронными сетями** — семейством моделей, которое начиная с 2012-го постепенно добивается превосходства во всё новых и новых приложениях, во многих став де-факто стандартом.  

Они были придуманы ещё в 70-х, но техническая возможность и понимание того, как обучать нейросети большого размера, появились лишь примерно к 2011 году, и это дало мощный толчок к их развитию. Совокупность нейросетевых подходов и сама наука о нейросетях носит название **глубинного обучения** или **deep learning**.  

Глубинное обучение основано на двух идеях.  

- Во-первых, это стремление к переходу от построения сложных пайплайнов, каждая компонента которых тренируется сама по себе решать кусочек задачи, к end-to-end обучению всей системы, как одного целого. Так, мы можем обучать не каждый слой отдельно, а все вместе, и не учить какие-нибудь линейные модели поверх случайных лесов, а работать с одной цельной моделью
- Во-вторых, это обучение представлений объектов — информативных признаковых описаний, учитывающих структуру данных, построенных лишь на основе самих данных, зачастую неразмеченных. Это позволяет автоматизировать процесс отбора признаков: теперь вместо того, чтобы руками экспертов искать «более информативное» или «более простое» признаковое описание наших объектов, мы можем получить их автоматически, притом используя не только данные, доступные нам для задачи, но и, к примеру, все тексты мира.

Обучению представлений будет посвящён отдельный параграф, а в этом мы постараемся убедить вас, что нейросети — это гибкий инструмент для решения самых разных задач и для работы с самыми разными типами данных.

Надо признать, впрочем, что современные модели весьма сложны и мало напоминают своих элегантных предшественников из 2012 года. Развитие нейросетей во многом мотивируется нуждами и возможностями индустрии, ростом производительности компьютеров и объёмов доступных данных.  

При этом теория отчаянно не поспевает за практикой. В глубинном обучении весьма распространён чисто инженерный подход к построению алгоритмов, изобилие деталей, основанных на интуиции и обосновывающихся фразой «просто потому, что так работает, а иначе — нет», поэтому ряд учёных продолжает относиться к нейросетям скептически.  

Однако результаты, достигнутые с их помощью за последние 10 лет, столь впечатляющие, что их нельзя игнорировать. Особенно существенный прогресс был достигнут в области анализа данных, обладающих некоторой внутренней _структурой_: текстов, изображений, видео, облаков точек, графов и так далее.  

Тем не менее, есть и подходы к теоретическому осмыслению того, почему нейросети работают так хорошо, и мы познакомим вас с ними в отдельном параграфе, посвящённой теории машинного обучения.  

Но довольно предисловий! Давайте набросаем план нашего вторжения в мир глубинного обучения:  

1. [Первое знакомство с полносвязными нейросетями](https://academy.yandex.ru/handbook/ml/article/pervoe-znakomstvo-s-polnosvyaznymi-nejrosetyami). Вы впервые встретитесь с самой простой нейросетевой архитектурой, узнаете, как строятся и как применяются нейронные сети.  

2. [Обратное распространение ошибки](https://academy.yandex.ru/handbook/ml/article/metod-obratnogo-rasprostraneniya-oshibki). Вы узнаете, как легко и быстро дифференцировать по параметрам сложного вычислительного графа, после чего будете (теоретически) понимать, как обучить несложную нейросеть.  

3. [Тонкости обучения](https://academy.yandex.ru/handbook/ml/article/tonkosti-obucheniya). Нейросети — могущественный, но капризный инструмент, и чем сложнее глубинная модель, тем труднее обучить её. В этом разделе мы начнём знакомить вас с разными приёмами, которые позволяют повысить вероятность успеха, а также с регуляризационными техниками для нейросетей.  

4. [Свёрточные нейросети](https://academy.yandex.ru/handbook/ml/article/svyortochnye-nejroseti). Классический пример структурированных данных — это изображения. В этом параграфе вы познакомитесь с базовым инструментарием для работы с ними — свёртками, пулингом и со свёрточными сетями в целом.

В первых четырёх главах вы познали основы глубинного обучения, но в основном имели дело с ситуацией, когда и данные, и выходы представляют из себя непритязательные векторы. А что делать, если мы должны работать с чем-то более сложно устроенным? Оказывается, за счёт своей гибкости и возможности сочетать в себе самые разные компоненты нейросети отлично справляются с данными, имеющими однородную структуру (изображениями, текстами, облаками точек $\ldots$). Для работы с каждым из этих типов данных требуются специфические инженерные решения, и мы постараемся познакомить вас с ними, разверзнув перед вами всю бездну способностей нейросетей!  


  ## handbook

  Учебник по машинному обучению

  ## title

  Нейронные сети

  ## description

  Краткий путеводитель по разделу

- 
  ## path

  /handbook/ml/article/pervoe-znakomstvo-s-polnosvyaznymi-nejrosetyami

  ## content

  ### Основные определения

**Искусственная нейронная сеть (далее — нейронная сеть)** — это сложная дифференцируемая функция, задающая отображение из исходного признакового пространства в пространство ответов, все параметры которой могут настраиваться одновременно и взаимосвязанно (то есть сеть может обучаться end-to-end).  

В частном (и наиболее частом) случае представляет собой последовательность дифференцируемых параметрических преобразований.  

Внимательный читатель может заметить, что под указанное выше определение нейронной сети подходят и логистическая, и линейная регрессия. Это верное замечание: и линейная, и логистическая регрессии могут рассматриваться как нейронные сети, задающие отображения в пространство ответов и логитов соответственно.

Сложную функцию удобно представлять в виде суперпозиции простых, и нейронные сети обычно предстают перед программистом в виде конструктора, состоящего из более-менее простых блоков (**слоёв**, **layers**). Вот две простейшие их разновидности:

- **Линейный слой** (**linear layer**, **dense layer**) — линейное преобразование над входящими данными. Его обучаемые параметры — это матрица $W$ и вектор $b$: $x \mapsto xW + b$ ($W \in \mathbb{R}^{d \times k}, x \in \mathbb{R}^{d}, b \in \mathbb{R}^{k}$). Такой слой преобразует $d$-мерные векторы в $k$-мерные.

- **Функция активации** (**activation function**) — нелинейное преобразование, поэлементно применяющееся к пришедшим на вход данным. Благодаря функциям активации нейронные сети способны порождать более информативные признаковые описания, преобразуя данные *нелинейным* образом. Может использоваться, например, ReLU (rectified linear unit) $\text{ReLU}(x) = \text{max}(0, x)$ или уже знакомая вам из логистической регрессии сигмоида $\sigma(x) = \frac1{1 + e^{-x}}$. К более глубокому рассмотрению разновидностей и свойств различных функций активации вернёмся позднее.

Даже самые сложные нейронные сети обычно собираются из относительно простых блоков, подобных этим. Таким образом, их можно представить в виде **вычислительного графа** (**computational graph**), где промежуточным вершинам соответствуют преобразования. На иллюстрации ниже приведён вычислительный граф для логистической регрессии.
![16_1_89d47c2aca.webp](https://yastatic.net/s3/education-portal/media/16_1_89d47c2aca_9e25689e44.webp)

Не правда ли, похоже на слоёный пирог из преобразований? Отсюда и *слои*.

Графы могут быть и более сложными, в том числе нелинейными:
![16_2_a93003bf17.webp](https://yastatic.net/s3/education-portal/media/16_2_a93003bf17_fc60402048.webp)

Давайте разберёмся, что тут происходит.

Input — это **вход** нейросети, который получает исходные данные. Обычно требуется, чтобы они имели вид матрицы («объекты-признаки») или тензора (многомерной матрицы). Вообще говоря, входов может быть несколько: например, мы можем подавать в нейросеть картинку и какие-нибудь ещё сведения о ней — преобразовывать их мы будем по-разному, поэтому логично предусмотреть два входа в графе.  

Дальше к исходным данным $X^0$ применяются два линейных слоя, которые превращают их в **промежуточные (внутренние, скрытые) представления** $X^1$ и $X^2$. В литературе они также называются **активациями** (не путайте с функциями активации).  

Каждое из представлений $X^1$ и $X^2$ подвергается нелинейному преобразованию, превращаясь в новые промежуточные представления $X^3$ и $X^4$ соответственно. Переход от $X^0$ к двум новым матрицам (или тензорам) $X^3$ и $X^4$ можно рассматривать как построение двух новых (возможно, более информативных) признаковых описаний исходных данных.  

Затем представления $X^3$ и $X^4$ конкатенируются (то есть признаковые описания всех объектов объединяются).  

Дальше следует ещё один линейный слой и ещё одна активация, и полученный результат попадает на **выход** сети, то есть отдаётся обратно пользователю.

Нейросеть, в которой есть только линейные слои и различные функции активации, называют **полносвязной** (**fully connected**) нейронной сетью или **многослойным перцептроном** (**multilayer perceptron**, MLP).

Посмотрим, что происходит с размерностями, если на вход подаётся матрица $N\times d$:

![16_3_a5bc57bada.gif](https://yastatic.net/s3/education-portal/media/16_3_a5bc57bada_99b56bb226.gif)

{% cut "Примечание о терминологии" %}

В литературе, увы, нет единства терминологии.

Так, например, никто не мешает нам объявить «единым и неделимым слоем» композицию линейного слоя и активации (в ознаменование того, что мы почти никогда не используем просто линейный слой без нелинейности). Например, в фреймворке [keras](https://keras.io/) активацию можно указать в [линейном слое](https://keras.io/api/layers/core_layers/dense/) в качестве параметра.

Также в ряде источников слоями называется то, что мы называем промежуточными представлениями. Нам, впрочем, кажется, что промежуточные результаты $X^i$ правильнее называть именно представлениями: ведь это новые признаковые описания, представляющие исходные объекты. Кроме того, во всех нейросетевых фреймворках слои — это именно преобразования, поэтому и нам кажется правильным объявлять слоями именно преобразования, связывающие промежуточные представления.

{% endcut %}


А вот и настоящий пример из реальной жизни. GoogLeNet (она же Inception-v1), показавшая SotA-результат на ILSVRC 2014 (ImageNet challenge), выглядит так: 

![16_4_268cfc9ef3.webp](https://yastatic.net/s3/education-portal/media/16_4_268cfc9ef3_32d682d766.webp)
Здесь каждый кирпичик — это некоторое относительно простое преобразование, а белым помечены входы и выходы вычислительного графа.

Современные же сети часто выглядят и ещё сложней, но всё равно они собираются из достаточно простых кирпичиков-слоёв.

**Примечание**. Впрочем, в общем случае нейронная сеть — это просто некоторая сложная функция (или, что эквивалентно, граф вычислений). Поэтому в некоторых (очень нетривиальных) случаях нет смысла разбивать её на слои. 

В качестве иллюстрации ниже приведены структуры агностических нейронных сетей WANN, представленных в работе [Weight Agnostic Neural Networks, NeurIPS 2019](https://weightagnostic.github.io).

![16_5_023febedc9.webp](https://yastatic.net/s3/education-portal/media/16_5_023febedc9_b912763670.webp)

## Forward & backward propagation
 
Информация может течь по графу в двух направлениях.
 
Применение нейронной сети к данным (вычисление выхода по заданному входу) часто называют **прямым проходом**, или же **forward propagation** (**forward pass**). На этом этапе происходит преобразование исходного представления данных в целевое и последовательно строятся промежуточные (внутренние) представления данных — результаты применения слоёв к предыдущим представлениям. Именно поэтому проход называют прямым. 

![16_6_fea341f9c3.webp](https://yastatic.net/s3/education-portal/media/16_6_fea341f9c3_60d08130d5.webp)

При **обратном проходе**, или же **backward propagation** (**backward pass**), информация (обычно об ошибке предсказания целевого представления) движется от финального представления (а чаще даже от функции потерь) к исходному через все преобразования.  

Механизм обратного распространения ошибки, играющий важнейшую роль в обучении нейронных сетей, как раз предполагает _обратное_ движение по вычислительному графу сети. С ним вы познакомитесь в следующем [параграфе](https://education.yandex.ru/handbook/ml/article/metod-obratnogo-rasprostraneniya-oshibki).

![16_7_baa8e48487.webp](https://yastatic.net/s3/education-portal/media/16_7_baa8e48487_0545f55a15.webp)

## Архитектуры для простейших задач

Как мы уже упоминали выше, нейросети — это универсальный конструктор, который из простых блоков позволяет собрать орудия для решения самых разных задач. Давайте посмотрим на конкретные примеры. Безусловно, мир намного разнообразнее того, что мы покажем вам в этом параграфе, но с чего-то ведь надо начинать, не так ли?

В тех несложных ситуациях, которые мы сейчас рассмотрим, архитектура будет отличаться лишь самыми последними этапами вычисления (у сетей будут разные «головы»). Для иллюстрации приведём примеры нескольких игрушечных архитектур для решения игрушечных задач классификации и регрессии на двумерных данных:

![16_8_6ecb784579.webp](https://yastatic.net/s3/education-portal/media/16_8_6ecb784579_4fb31a95f3.webp)

### Бинарная классификация

Для решения задачи бинарной классификации подойдёт любая архитектура, на выходе у которой одно число от $0$ до $1$, интерпретируемое как «вероятность класса 1». Обычно этого добиваются, взяв 

$$\widehat{y} = \sigma(f(X^m)),$$

где $f$ — некоторая функция, превращающая представление $X^m$ в число (если $X^m$ — матрица, то подойдёт $f(X^m) = X^mw + b$, где $w$ — вектор-столбец), а $\sigma$ — наша любимая сигмоида. При этом $X^m$ может получаться как угодно, лишь бы хватало оперативной памяти и не было переобучения.

В качестве функции потерь удобно брать уже знакомый нам log loss.

### Многоклассовая классификация

Работая с другими моделями, мы порой вынуждены были выдумывать сложные стратегии многоклассовой классификации; нейросети позволяют это делать легко и элегантно.  

Достаточно построить сеть, которая будет выдавать $K$ неотрицательных чисел, суммирующихся в 1 (где $K$ — число классов); тогда им можно придать смысл вероятностей классов и предсказывать тот класс, «вероятность» которого максимальна.  

Превратить произвольный набор из $K$ чисел в набор из неотрицательных чисел, суммирующихся в 1, позволяет, к примеру, функция

$$\text{softmax}(x_1,\ldots,x_K) = \left( \frac{e^{x_1}}{\sum_ie^{x_i}},\ldots,\frac{e^{x_K}}{\sum_ie^{x_i}} \right)$$

Наиболее популярные архитектуры для многоклассовой классификации имеют вид

$$\widehat{y} = \text{softmax}(f(X^m)),$$

где $f$ — функция, превращающая $X^m$ в матрицу $B\times K$ (где $B$ — размер батча), а $X^m$ может быть получен любым приятным вам образом.

Но какой будет функция потерь для такой сети? Мы должны научиться сравнивать «распределение вероятностей классов» с истинным (в котором на месте истинного класса стоит 1, а в остальных местах 0). Сделать это позволяет **кросс-энтропия**, она же **negative log-likelihood** — некоторый аналог расстояния между распределениями:

$$\mathcal{L}(\widehat{y}, y) = -\frac1B\sum_{i=1}^B\sum_{k=1}^Ky_{ik}\log{\widehat{y}_{ik}},$$

где снова $B$ — размер батча, а $K$ — число классов. Легко видеть, что при $k = 2$ получается та самая функция потерь, которую мы использовали для обучения бинарной классификации.

### (Множественная) регрессия

С помощью нейросетей легко создать модель, которая предсказывает не одно число, а сразу несколько. Например, координаты ключевых точек лица — кончика носа, уголков рта и так далее.  

Достаточно сделать, чтобы последнее представление было матрицей $B\times M$, где $B$ — размер батча, а $M$ — количество предсказываемых чисел. Особенностью большинства моделей регрессии является то, что после последнего слоя (часто линейного) не ставят функций активации. Вы тоже этого не делайте, если только чётко не понимаете, зачем вам это. В качестве функции потерь можно брать, например, $MSE$ по всей матрице $B\times M$.

### Всё вместе

Если вы используете нейросети, то ваши таргеты могут иметь и различную природу. Например, можно соорудить одну-единственную сеть, которая по фотографии нескольких котиков определяет их количество (регрессия) и породу каждого из них (многоклассовая классификация).  

Лосс для такой модели может быть равен (взвешенной) сумме лоссов для каждой из задач (правда, не факт, что это хорошая идея). Так что, по крайней мере в теории, сетям подвластны любые задачи. На практике, конечно, всё гораздо хитрей: для обучения слишком сложной сети у вас может не хватить данных или вычислительных мощностей.

## Популярные функции активации

Для начала поговорим о том, зачем они нужны. 

Казалось бы, можно последовательно выстраивать лишь линейные слои, но так не делают: после каждого линейного слоя обязательно вставляют функцию активации. Но зачем? Попробуем разобраться. 

Рассмотрим нейронную сеть из двух линейных слоёв. Что произойдёт, если между ними будет отсутствовать нелинейная функция активации?

![16_11_f51987e443.webp](https://yastatic.net/s3/education-portal/media/16_11_f51987e443_18d80b4075.webp)


$$\widehat{y} = X^{out} = X^{1}W^{2} + b^2 = (X^0W^1 + b^1)W^2 + b^2 = $$

$$=  X^0\color{blue}{W^1W^2} + \color{green}{b^1W^2 + b^2} = X^0\color{blue}{\widetilde{W}} + \color{green}{\widetilde{b}}$$

Линейная комбинация линейных отображений есть линейное отображение, то есть два последовательных линейных слоя эквивалентны одному линейному слою. 

Добавление функций активации после линейного слоя позволяет получить нелинейное преобразование, и подобной проблемы уже не возникает. Вдобавок правильный выбор функции активации позволяет получить преобразование, обладающее подходящими свойствами.

В качестве функции активации может использоваться, например, уже знакомая вам из логистической регрессии сигмоида $\sigma(x) = \frac1{1 + \exp(-x)}$ или ReLU (Rectified linear unit) $\text{ReLU}(x) = \text{max}(0, x)$. К более глубокому рассмотрению разновидностей и свойств различных функций активации вернёмся позднее.

**Примечание**. На самом деле бывают ситуации, когда два линейных слоя подряд — это полезно. Например, если вы понимаете, что у вас очень много параметров, а информации в данных не так много, вы можете заменить линейный слой, превращающий $m$-мерные векторы в $n$-мерные, на два, вставив посередине $k$-мерное представление, где $k \ll m, n$:

![16_12_cd50e0d27f.webp](https://yastatic.net/s3/education-portal/media/16_12_cd50e0d27f_c921999cdf.webp)

С точки зрения линейной алгебры это примерно то же самое, что потребовать, чтобы матрица исходного линейного слоя имела ранг не выше $k$. И с точки зрения сужения «информационного канала» это иногда может сработать. Но в любом случае вы должны понимать, что два линейных слоя подряд стоит ставить, только если вы хорошо понимаете, чего хотите добиться.

Вернёмся к функциям активации. Вот наиболее популярные:

![16_9_9688e57c43.webp](https://yastatic.net/s3/education-portal/media/16_9_9688e57c43_44ff70e160.webp)

Рассмотрим их подробнее.

### **ReLU**, Rectified linear unit

Формула:
	
$$
\text{ReLU}(x) = \max(0, x),
$$
	
$$
\text{ReLU}: \mathbb{R} \to [0, +\infty).
$$

ReLU это простая кусочно-линейная функция. Одна из наиболее популярных функций активации. В нуле производная доопределяется нулевым значением. 

Плюсы:  
* простота вычисления активации и производной.  

Минусы:  
* область значений является смещённой относительно нуля;  
* для отрицательных значений производная равна нулю, что может привести к затуханию градиента.   

ReLU и её производная очень просты для вычисления: достаточно лишь сравнить значение с нулём. Благодаря этому использование ReLU позволяет достигать прироста в скорости до четырёх-шести раз относительно сигмоиды. 

{% cut "Более подробно о затухании градиента" %}

Рассмотрим нейронную сеть из нескольких линейных слоёв с сигмоидой в качестве функции активации. Пусть

$$
X^1 = \sigma(Z^1) = \sigma(W^1 X^0),
$$

где в качестве $Z^1$ обозначен результат применения линейного слоя. Свободный член, как и ранее, опущен для упрощения выкладок.

Рассмотрим график сигмоиды и её производной:
![16_10_24315262a6.webp](https://yastatic.net/s3/education-portal/media/16_10_24315262a6_85a99cae9e.webp)

На «хвостах» её производная практически равна нулю (ведь сигмоида представляет собой почти константу). То есть если какое-то значение $Z^1$ было достаточно велико по абсолютной величине (например, $\vert z^1_k \vert = 7.4$), то градиент функции потерь для этой компоненты будет домножен на очень малое число и фактически станет равным нулю:

$$
\frac{\partial l}{\partial W^0_{k, :}} = \frac{\partial l}{\partial X^1_k}\frac{\partial X^1_k}{\partial Z^1_k} \frac{\partial Z^1_k}{\partial W^0_{k,:}} \approx \frac{\partial l}{\partial X^1_k} \cdot 0 \cdot \frac{\partial Z^1_k}{\partial W^0_{k,:}} \approx 0.
$$

Получается, что $k$-ая строка матрицы $W^0$ не будет обновлена: ведь градиент равен нулю. Это может привести к «отмиранию» части весов: неудачное значение параметров приведёт к невозможности их обновления.

**Примечание**. Это одна из причин необходимости нормировки данных и выбора правильной инициализации для начальных значений параметров нейронной сети.

Но помимо явного обнуления градиента есть и вторая проблема: максимальное значение производной сигмоиды составляет $0.25$. То есть она всегда уменьшает значение градиента не менее чем в четыре раза. Если же представить себе глубокую сеть с 20+ слоями, то для каждого следующего (если считать с конца) слоя градиент будет домножаться на число, не превосходящее $0.25$. Так, для переменных из первого слоя коэффициент составит $4^{-20}$.

{% endcut %}

### **Leaky ReLU**

Формула:
	
$$
\text{Leaky ReLU}(x) = \max(\alpha x, x), \quad \alpha = \text{const}, 0 < \alpha \ll 1
$$

$$
\text{Leaky ReLU}: \mathbb{R} \to (-\infty, +\infty).
$$
	
Гиперпараметр $\alpha$ обеспечивает небольшой уклон слева от нуля, что позволяет получить более симметричную относительно нуля область значений. Также меньше провоцирует затухание градиента благодаря наличию ненулевого градиента и слева, и справа от нуля.

### **PReLU**, Parametric ReLU

Формула:
	
$$
\text{PReLU}(x) = \max(\alpha x, x), \quad 0 < \alpha \ll 1
$$

$$
\text{PReLU}: \mathbb{R} \to (-\infty, +\infty).
$$
	
Аналогична Leaky ReLU, но параметр $\alpha$ настраивается градиентными методами.

### **ELU**

ELU – это гладкая аппроксимация ReLU. Обладает более высокой вычислительной сложностью, достаточно редко используется на практике.

### **Sigmoid**, сигмоида

Формула:
	
$$
\sigma(x) = \frac{1}{1 + \exp(-x)},
$$

$$
\sigma: \mathbb{R} \to (0, 1).
$$

Исторически одна из первых функций активации. Рассматривалась в том числе и как гладкая аппроксимация порогового правила, эмулирующая активацию естественного нейрона. 

Плюсы: 

Минусы:
* область значений смещена относительно нуля;  
* сигмоида (как и её производная) требует вычисления экспоненты, что является достаточно сложной вычислительной операцией. Её приближённое значение вычисляется на основе ряда Тейлора или с помощью полиномов, Stack Overflow [question 1](https://stackoverflow.com/questions/53419270/how-does-numpy-compute-an-exponential), [question 2](https://stackoverflow.com/questions/9799041/efficient-implementation-of-natural-logarithm-ln-and-exponentiation);  
* на «хвостах» обладает практически нулевой производной, что может привести к затуханию градиента;  
* максимальное значение производной составляет $0.25$, что также приводит к затуханию градиента.  
	
На практике сигмоида редко используется внутри сетей, чаще всего в случаях, когда внутри модели решается задача бинарной классификации (например, вероятность забывания информации в LSTM).

### **Tanh**, гиперболический тангенс

Формула:
	
$$
\tanh(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(-x)},
$$

$$
\tanh: \mathbb{R} \to (-1, 1).
$$
	
Плюсы:  
* как и сигмоида, имеет ограниченную область значений;  
* в отличие от сигмоиды, область значений симметрична.  

Минусы:  
* требует вычисления экспоненты, что является достаточно сложной вычислительной операцией;  
* на «хвостах» обладает практически нулевой производной, что может привести к затуханию градиента.  
    
**Вопрос на подумать**. А почему симметричность области значений может быть ценным свойством?

{% cut "Ответ (не открывайте сразу; сначала подумайте сами!)" %}

Разберём на примере сигмоиды. Пусть оказалось так, что все веса линейных слоёв инициализированы положительными числами. 

Сигмоида от положительного числа даёт нечто, большее $\frac12$, и с каждым дальнейшим слоем ситуация может усугубляться, приводя к тому, что сигмоиды будут выдавать всё более близкие к единице значения. Это покарает нас, когда мы начнём считать градиенты: они начнут «затухать».

{% endcut %}

## Немного о мощи нейросетей

Рассмотрим для начала задачу регрессии. Ясно, что линейная модель (то есть однослойная нейросеть) может приблизить только линейную функцию, но уже двухслойная нейросеть может приблизить почти что угодно. Есть ряд теорем на эту тему, мы упомянем одну из них. Обратите внимание на год: как мы уже упоминали, нейросети начали серьёзно изучать задолго до того, как они начали превращаться в state of the art.

**Теорема Цыбенко (1989).** Для любой непрерывной функции $f(x):\mathbb{R}^m\rightarrow\mathbb{R}$ и для любого $\varepsilon > 0$ найдётся число $N$, а также числа $w_1\ldots,w_N$, $b_1,\ldots,b_N$ $\alpha_1,\ldots,\alpha_N$, для которых

$$\left|f(x) - \sum_{i=1}^N\alpha_i\sigma(\langle x, w_i\rangle + b_i)\right| < \varepsilon$$

для любых $x$ из единичного куба $[0,1]^m$ в $\mathbb{R}^m$.

В сумме из теоремы Цыбенко легко опознать двуслойную нейросеть с сигмоидной функцией активации. В самом деле, сперва мы превращаем $x$ в $\langle x, w_i\rangle + b_i$ — это можно представить в виде одной матричной операции (линейный слой!):

$$x\mapsto x^{(1)} = x\cdot\begin{pmatrix}
\phantom{\frac12}w_1 & \ldots &
w_N
\end{pmatrix} + \begin{pmatrix}
\phantom{\frac12}b_1 & \ldots &
b_N
\end{pmatrix},$$

где $w_i$ — вектор-столбцы, а каждое из $b_i$ прибавляется к $i$-му столбцу, после чего поэлементно берём от $x^{(1)}$ сигмоиду (активация) $x^{(2)} = \sigma(x^{(1)})$, после чего вычисляем

$$\sum_{i=1}^N\alpha_ix^{(2)}_i = \left(\alpha_1,\ldots,\alpha_N\right)\cdot x,$$

и это второй линейный слой (без свободного члена).

Правда, теорема не очень помогает находить такие функции, но это уже другое дело. В любом случае — если дать нейросети достаточно данных, она действительно может выучить почти что угодно.

**Упражнение.** Мы не будем приводить результатов, касающихся классификации, но рекомендуем воспользоваться замечательной [песочницей](https://playground.tensorflow.org). Убедитесь сами, что при использовании одного скрытого слоя из двух нейронов и сигмоиды в качестве функции активации, можно неплохо классифицировать данные со сложной, совсем даже не линейной границей между классами. Вы также можете поиграть с разными функциями активации.

А для получения решения нам необходим метод автоматической настройки всех параметров нейронной сети — **метод обратного распространения ошибки**, или же **error backpropagation**. Рассмотрим его в деталях в следующем параграфе.

  ## handbook

  Учебник по машинному обучению

  ## title

  Первое знакомство с полносвязными нейросетями

  ## description

  Основные понятия глубинного обучения. Базовые слои и функции активации

- 
  ## path

  /handbook/ml/article/metod-obratnogo-rasprostraneniya-oshibki

  ## content

  Нейронные сети обучаются с помощью тех или иных модификаций градиентного спуска, а чтобы применять его, нужно уметь эффективно вычислять градиенты функции потерь по всем обучающим параметрам. Казалось бы, для какого-нибудь запутанного вычислительного графа это может быть очень сложной задачей, но на помощь спешит метод обратного распространения ошибки.

## Метод обратного распространения ошибки (backward propagation)

Открытие метода обратного распространения ошибки стало одним из наиболее значимых событий в области искусственного интеллекта. В актуальном виде он был [предложен](https://web.stanford.edu/class/psych209a/ReadingsByDate/02_06/PDPVolIChapter8.pdf) в 1986 году Дэвидом Э. Румельхартом, Джеффри Э. Хинтоном и Рональдом Дж. Вильямсом, а также независимо и одновременно красноярскими математиками С. И. Барцевым и В. А. Охониным.

С тех пор для нахождения градиентов параметров нейронной сети используется метод вычисления производной сложной функции, и оценка градиентов параметров сети стала хоть и сложной инженерной задачей, но уже не искусством. Несмотря на простоту используемого математического аппарата, появление этого метода привело к значительному скачку в развитии искусственных нейронных сетей.

Суть метода можно записать одной формулой, тривиально следующей из формулы производной сложной функции: если $f(x) = g_m(g_{m-1}(\ldots (g_1(x)) \ldots))$, то $\frac{\partial f}{\partial x} = \frac{\partial g_m}{\partial g_{m-1}}\frac{\partial g_{m-1}}{\partial g_{m-2}}\ldots \frac{\partial g_2}{\partial g_1}\frac{\partial g_1}{\partial x}$. Уже сейчас мы видим, что градиенты можно вычислять последовательно, в ходе одного обратного прохода, начиная с $\frac{\partial g_m}{\partial g_{m-1}}$ и умножая каждый раз на частные производные предыдущего слоя.

## Backward propagation в одномерном случае

В одномерном случае всё выглядит особенно просто. Пусть $w_0$ — переменная, по которой мы хотим продифференцировать , причём сложная функция имеет вид

$$f(w_0) = g_m(g_{m-1}(\ldots g_1(w_0)\ldots)),
$$

где все $g_i$ скалярные. Тогда

$$f'(w_0) = g_m'(g_{m-1}(\ldots g_1(w_0)\ldots))\cdot g'_{m-1}(g_{m-2}(\ldots g_1(w_0)\ldots))\cdot\ldots \cdot g'_1(w_0)
$$

Суть этой формулы такова. Если мы уже совершили прямой проход (forward propagation), значит мы уже знаем

$$g_1(w_0), g_2(g_1(w_0)),\ldots,g_{m-1}(\ldots g_1(w_0)\ldots),
$$

Поэтому мы можем действовать следующим образом:

* берём производную $g_m$ в точке $g_{m-1}(\ldots g_1(w_0)\ldots)$;

* умножаем на производную $g_{m-1}$ в точке $g_{m-2}(\ldots g_1(w_0)\ldots)$;

* и так далее, пока не дойдём до производной $g_1$ в точке $w_0$.

Проиллюстрируем это на картинке, расписав по шагам дифференцирование по весам $w_i$ функции потерь логистической регрессии на одном объекте (то есть для батча размера 1):

![17](https://yastatic.net/s3/education-portal/media/17_1_7a336adac8_e7010805ee.webp)

Собирая все множители вместе, получаем:

$$\frac{\partial f}{\partial w_0} = (-y)\cdot e^{-y(w_0 + w_1x_1 + w_2x_2)}\cdot\frac{-1}{1 + e^{-y(w_0 + w_1x_1 + w_2x_2)}}
$$

$$\frac{\partial f}{\partial w_1} = x_1\cdot(-y)\cdot e^{-y(w_0 + w_1x_1 + w_2x_2)}\cdot\frac{-1}{1 + e^{-y(w_0 + w_1x_1 + w_2x_2)}}
$$

$$\frac{\partial f}{\partial w_2} = x_2\cdot(-y)\cdot e^{-y(w_0 + w_1x_1 + w_2x_2)}\cdot\frac{-1}{1 + e^{-y(w_0 + w_1x_1 + w_2x_2)}}
$$

Таким образом, сперва совершается forward propagation для вычисления всех промежуточных значений (да, все промежуточные представления нужно будет хранить в памяти), а потом запускается backward propagation, на котором в один проход вычисляются все градиенты.

## Почему же нельзя просто пойти и начать везде вычислять производные?

В параграфе, посвящённом [матричным дифференцированиям](https://academy.yandex.ru/handbook/ml/article/matrichnoe-differencirovanie), мы поднимаем вопрос о том, что вычислять частные производные по отдельности — это зло, лучше пользоваться матричными вычислениями. Но есть и ещё одна причина: даже и с матричной производной в принципе не всегда хочется иметь дело.

Рассмотрим простой пример. Допустим, что $X^r$ и $X^{r+1}$ — два последовательных промежуточных представления $N\times M$ и $N\times K$, связанных функцией $X^{r+1} = f^{r+1}(X^r)$. Предположим, что мы как-то посчитали производную $\frac{\partial\mathcal{L}}{\partial X^{r+1}_{ij}}$ функции потерь $\mathcal{L}$, тогда

$$\frac{\partial\mathcal{L}}{\partial X^{r}_{st}} = \sum_{i,j}\frac{\partial f^{r+1}_{ij}}{\partial X^{r}_{st}}\frac{\partial\mathcal{L}}{\partial X^{r+1}_{ij}}
$$

И мы видим, что, хотя оба градиента $\frac{\partial\mathcal{L}}{\partial X_{ij}^{r+1}}$ и $\frac{\partial\mathcal{L}}{\partial X_{st}^{r}}$ — это просто матрицы, в ходе вычислений возникает «четырёхмерный кубик» $\frac{\partial f_{ij}^{r+1}}{\partial X_{st}^{r}}$. Его болезненно даже хранить: уж больно много памяти он требует —м$N^2MK$ по сравнению с безобидными $NM + NK$, требуемыми для хранения градиентов.

Поэтому хочется промежуточные производные $\frac{\partial f^{r+1}}{\partial X^{r}}$ рассматривать не как вычисляемые объекты $\frac{\partial f_{ij}^{r+1}}{\partial X_{st}^{r}}$, а как преобразования, которые превращают $\frac{\partial\mathcal{L}}{\partial X_{ij}^{r+1}}$ в $\frac{\partial\mathcal{L}}{\partial X_{st}^{r}}$.

Целью следующих параграфов будет именно это: понять, как преобразуется градиент в ходе error backward propagation при переходе через тот или иной слой.

**Вы спросите себя**: надо ли мне сейчас пойти и прочитать параграф учебника про матричное дифференцирование?

**Короткий ответ**: Зависит от ваших знаний.

**Длинный ответ**: Найдите производную функции по вектору $x$:

$$f(x) = x^TAx,\ A\in Mat_{n}{\mathbb{R}}\text{ — матрица размера }n\times n
$$

А как всё поменяется, если $A$ тоже зависит от $x$? Чему равен градиент функции, если $A$ является скаляром?

Если вы готовы прямо сейчас взять ручку и бумагу и посчитать всё, то вам, вероятно, не надо читать про матричные дифференцирования. Но мы советуем всё-таки заглянуть в этот параграф, если обозначения, которые мы будем дальше использовать, покажутся вам непонятными: единой нотации для матричных дифференцирований человечество пока, увы, не изобрело, и переводить с одной на другую не всегда легко.

А мы же сразу перейдём к интересующей нас вещи: к вычислению градиентов сложных функций.

## Градиент сложной функции

Напомним, что формула производной сложной функции выглядит следующим образом:

$$\left[D_{x_0} (\color{#5002A7}{u} \circ \color{#4CB9C0}{v}) \right](h) = \color{#5002A7}{\left[D_{v(x_0)} u \right]} \left( \color{#4CB9C0}{\left[D_{x_0} v\right]} (h)\right)
$$

Теперь разберёмся с градиентами. Пусть $f(x) = g(h(x))$ – скалярная функция. Тогда

$$\left[D_{x_0} f \right] (x-x_0) = \langle\nabla_{x_0} f, x-x_0\rangle.
$$

С другой стороны,

$$\left[D_{h(x_0)} g \right] \left(\left[D_{x_0}h \right] (x-x_0)\right) = \langle\nabla_{h_{x_0}} g, \left[D_{x_0} h\right] (x-x_0)\rangle = \langle\left[D_{x_0} h\right]^* \nabla_{h(x_0)} g, x-x_0\rangle.
$$

То есть $\color{#FFC100}{\nabla_{x_0} f} = \color{#348FEA}{\left[D_{x_0} h \right]}^* \color{#FFC100}{\nabla_{h(x_0)}}g$ — применение сопряжённого к $D_{x_0} h$ линейного отображения к вектору $\nabla_{h(x_0)} g$.

Эта формула — сердце механизма обратного распространения ошибки. Она говорит следующее: если мы каким-то образом получили градиент функции потерь по переменным из некоторого промежуточного представления $X^k$ нейронной сети и при этом знаем, как преобразуется градиент при проходе через слой $f^k$ между $X^{k-1}$ и $X^k$ (то есть как выглядит сопряжённое к дифференциалу слоя между ними отображение), то мы сразу же находим градиент и по переменным из $X^{k-1}$:

![17](https://yastatic.net/s3/education-portal/media/17_2_734d32744c_04fe5332ad.webp)

Таким образом слой за слоем мы посчитаем градиенты по всем $X^i$ вплоть до самых первых слоёв.

Далее мы разберёмся, как именно преобразуются градиенты при переходе через некоторые распространённые слои.

## Градиенты для типичных слоёв

Рассмотрим несколько важных примеров.

### Пример №1

$f(x) = u(v(x))$, где $x$ — вектор, а $v(x)$ – поэлементное применение $v$:

$$v\begin{pmatrix}
x_1 \\
\vdots\\
x_N
\end{pmatrix}
= \begin{pmatrix}
v(x_1)\\
\vdots\\
v(x_N)
\end{pmatrix}$$

Тогда, как мы знаем,

$$\left[D_{x_0} f\right] (h) = \langle\nabla_{x_0} f, h\rangle = \left[\nabla_{x_0} f\right]^T h.
$$

Следовательно,

$$\begin{aligned}\left[D_{v(x_{0})}u\right]([D_{x_{0}}v](h))&=\left[\nabla_{v(x_{0})}u\right]^{T}\left(v^{\prime}(x_{0})\odot h\right)=\\&=\sum_{i}\left[\nabla_{v(x_{0})}u\right]_{i}v^{\prime}(x_{0i})h_{i}=\langle\left[\nabla_{v(x_{0})}u\right]\odot v^{\prime}(x_{0}),h\rangle.\end{aligned}
$$

где $\odot$ означает поэлементное перемножение. Окончательно получаем

$$\color{#348FEA}{\nabla_{x_0} f = \left[\nabla_{v(x_0)}u\right] \odot v'(x_0) = v'(x_0) \odot \left[\nabla_{v(x_0)} u\right]}
$$

Отметим, что если $x$ и $h(x)$ — это просто векторы, то мы могли бы вычислять всё и по формуле $\frac{\partial f}{\partial x_i} = \sum_j\big(\frac{\partial z_j}{\partial x_i}\big)\cdot\big(\frac{\partial h}{\partial z_j}\big)$.

В этом случае матрица $\big(\frac{\partial z_j}{\partial x_i}\big)$ была бы диагональной (так как $z_j$ зависит только от $x_j$: ведь $h$ берётся поэлементно), и матричное умножение приводило бы к тому же результату. Однако если $x$ и $h(x)$ — матрицы, то $\big(\frac{\partial z_j}{\partial x_i}\big)$ представлялась бы уже «четырёхмерным кубиком», и работать с ним было бы ужасно неудобно.

### Пример №2

$f(X) = g(XW)$, где $X$ и $W$ — матрицы. Как мы знаем,

$$\left[D_{X_0} f \right] (X-X_0) = \text{tr}\, \left(\left[\nabla_{X_0} f\right]^T (X-X_0)\right).
$$

Тогда

$$\begin{gathered}
[D_{X_{0}W}g]\left([D_{X_{0}}\left(*W\right)](H)\right)=[D_{X_{0}W}g]\left(HW\right)= \\
=\mathrm{tr} \left(\left[\nabla_{X_{0}W}g\right]^{T}\cdot(H)W\right)= \\
=\mathrm{tr} \left(W[\nabla_{X_{0}W}(g)]^{T}\cdot(H)\right)=\mathrm{tr} \left(\left[[\nabla_{X_{0}W}g]W^{T}\right]^{T}(H)\right) 
\end{gathered}
$$

Здесь через $\ast W$ мы обозначили отображение $Y \hookrightarrow  YW$, а в предпоследнем переходе использовалось следующее свойство следа:

$$	\text{tr} \, (A B C) = \text{tr} \, (C A B),
$$

где $A, B, C$ — произвольные матрицы подходящих размеров (то есть допускающие перемножение в обоих приведённых порядках). Следовательно, получаем

$$\color{#348FEA}{\nabla_{X_0} f = \left[\nabla_{X_0W} (g) \right] \cdot W^T}
$$

### Пример №3

$f(W) = g(XW)$, где $W$ и $X$ — матрицы. Для приращения $H = W - W_0$ имеем

$$\left[D_{W_0} f \right] (H) = \text{tr} \, \left( \left[\nabla_{W_0} f \right]^T (H)\right)
$$

Тогда

$$\left[D_{XW_0} g \right] \left( \left[D_{W_0} \left(X \ast\right) \right] (H)\right) = \left[D_{XW_0} g \right] \left( XH \right) = \\
= \text{tr} \, \left( \left[\nabla_{XW_0} g \right]^T \cdot X (H)\right) =
	\text{tr}\, \left(\left[X^T \left[\nabla_{XW_0} g \right] \right]^T (H)\right)
$$

Здесь через $X \ast$ обозначено отображение $Y \hookrightarrow XY$. Значит,

$$\color{#348FEA}{\nabla_{X_0} f = X^T \cdot \left[\nabla_{XW_0} (g)\right]}
$$

### Пример №4

$f(X) = g(softmax(X))$, где $X$ — матрица $N\times K$, а $softmax$ — функция, которая вычисляется построчно, причём для каждой строки $x$:

$$softmax(x) = \left(\frac{e^{x_1}}{\sum_te^{x_t}},\ldots,\frac{e^{x_K}}{\sum_te^{x_t}}\right)
$$

В этом примере нам будет удобно воспользоваться формализмом с частными производными. Сначала вычислим $\frac{\partial s_l}{\partial x_j}$ для одной строки $x$, где через $s_l$ мы для краткости обозначим $softmax(x)_l = \frac{e^{x_l}}	{\sum_te^{x_t}}$. Нетрудно проверить, что

$$\frac{\partial s_l}{\partial x_j} = \begin{cases}
s_j(1 - s_j),\ & j = l,\\
-s_ls_j,\ & j\ne l
\end{cases}$$

Так как softmax вычисляется независимо от каждой строчки, то

$$\frac{\partial s_{rl}}{\partial x_{ij}} = \begin{cases}
s_{ij}(1 - s_{ij}),\ & r=i, j = l,\\
-s_{il}s_{ij},\ & r = i, j\ne l,\\
0,\ & r\ne i
\end{cases},$$

где через $s_{rl}$ мы обозначили для краткости $softmax(X)_{rl}$.

Теперь пусть $\nabla_{rl} = \nabla g = \frac{\partial\mathcal{L}}{\partial s_{rl}}$ (пришедший со следующего слоя, уже известный градиент). Тогда

$$\frac{\partial\mathcal{L}}{\partial x_{ij}} = \sum_{r,l}\frac{\partial s_{rl}}{\partial x_{ij}} \nabla_{rl}
$$

Так как $\frac{\partial s_{rl}}{\partial x_{ij}} = 0$ при $r\ne i$, мы можем убрать суммирование по $r$:

$$\ldots = \sum_{l}\frac{\partial s_{il}}{\partial x_{ij}} \nabla_{il} = -s_{i1}s_{ij}\nabla_{i1} - \ldots + s_{ij}(1 - s_{ij})\nabla_{ij}-\ldots - s_{iK}s_{ij}\nabla_{iK} =
$$

$$= -s_{ij}\sum_t s_{it}\nabla_{it} + s_{ij}\nabla_{ij}
$$

Таким образом, если мы хотим продифференцировать $f$ в какой-то конкретной точке $X_0$, то, смешивая математические обозначения с нотацией Python, мы можем записать:

$$\begin{aligned}&\nabla_{X_0}f=\\&=-softmax(X_0)\odot\mathrm{sum~}(softmax(X_0)\odot\nabla_{softmax(X_0)}g,\mathrm{~axis}=1)+\\&softmax(X_0)\odot\nabla_{softmax(X_0)}g\end{aligned}
$$

## Backward propagation в общем виде

Подытожим предыдущее обсуждение, описав алгоритм **error backward propagation** (**алгоритм обратного распространения ошибки**). Допустим, у нас есть текущие значения весов $W^i_0$ и мы хотим совершить шаг SGD по мини-батчу $X$. Мы должны сделать следующее:

1. Совершить forward propagation, вычислив и запомнив все промежуточные представления $X = X^0, X^1, \ldots, X^m = \widehat{y}$.
2. Вычислить все градиенты с помощью backward propagation.
3. С помощью полученных градиентов совершить шаг SGD.

Проиллюстрируем алгоритм на примере двухслойной нейронной сети со скалярным output. Для простоты опустим свободные члены в линейных слоях.

![17](https://yastatic.net/s3/education-portal/media/17_3_51099c217b_9aa1983711.webp)
Обучаемые параметры – матрицы $U$ и $W$. Как найти градиенты по ним в точке $U_0, W_0$?

$$\nabla_{W_0}\mathcal{L} = \nabla_{W_0}{\left({\vphantom{\frac12}\mathcal{L}\circ h\circ\left[W\mapsto g(XU_0)W\right]}\right)}=
$$

$$=g(XU_0)^T\nabla_{g(XU_0)W_0}(\mathcal{L}\circ h) = \underbrace{g(XU_0)^T}_{k\times N}\cdot
\left[\vphantom{\frac12}\underbrace{h'\left(\vphantom{\int_0^1}g(XU_0)W_0\right)}_{N\times 1}\odot
\underbrace{\nabla_{h\left(\vphantom{\int_0^1}g(XU_0)W_0\right)}\mathcal{L}}_{N\times 1}\right]$$

Итого матрица $k\times 1$, как и $W_0$

$$\nabla_{U_0}\mathcal{L} = \nabla_{U_0}\left(\vphantom{\frac12}
\mathcal{L}\circ h\circ\left[Y\mapsto YW_0\right]\circ g\circ\left[ U\mapsto XU\right]
\right)=$$

$$=X^T\cdot\nabla_{XU^0}\left(\vphantom{\frac12}\mathcal{L}\circ h\circ [Y\mapsto YW_0]\circ g\right) =
$$

$$=X^T\cdot\left(\vphantom{\frac12}g'(XU_0)\odot
\nabla_{g(XU_0)}\left[\vphantom{\in_0^1}\mathcal{L}\circ h\circ[Y\mapsto YW_0\right]
\right)$$

$$=\ldots = \underset{D\times N}{X^T}\cdot\left(\vphantom{\frac12}
\underbrace{g'(XU_0)}_{N\times K}\odot
\underbrace{\left[\vphantom{\int_0^1}\left(
\underbrace{h'\left(\vphantom{\int_0^1}g(XU_0)W_0\right)}_{N\times1}\odot\underbrace{\nabla_{h(\vphantom{\int_0^1}g\left(XU_0\right)W_0)}\mathcal{L}}_{N\times 1}
\right)\cdot \underbrace{W^T}_{1\times K}\right]}_{N\times K}
\right)$$

Итого $D\times K$, как и $U_0$

Схематически это можно представить следующим образом:

![17](https://yastatic.net/s3/education-portal/media/17_4_b1b2356957_f57438f722.gif)

### Backward propagation для двухслойной нейронной сети

Если вы не уследили за вычислениями в предыдущем примере, давайте более подробно разберём его чуть более конкретную версию (для $g = h = \sigma$)

Рассмотрим двуслойную нейронную сеть для классификации. Мы уже встречали её ранее при рассмотрении линейно неразделимой выборки. Предсказания получаются следующим образом:

$$\widehat{y} = \sigma(X^1 W^2) = \sigma\Big(\big(\sigma(X^0 W^1 )\big) W^2 \Big).
$$

Пусть $W^1_0$ и $W^2_0$ — текущее приближение матриц весов. Мы хотим совершить шаг по градиенту функции потерь, и для этого мы должны вычислить её градиенты по $W^1$ и $W^2$ в точке $(W^1_0, W^2_0)$.

Прежде всего мы совершаем forward propagation, в ходе которого мы должны запомнить все промежуточные представления: $X^1 = X^0 W^1_0$, $X^2 = \sigma(X^0 W^1_0)$, $X^3 = \sigma(X^0 W^1_0) W^2_0$, $X^4 = \sigma(\sigma(X^0 W^1_0) W^2_0) = \widehat{y}$. Они понадобятся нам дальше.

Для полученных предсказаний вычисляется значение функции потерь:

$$l = \mathcal{L}(y, \widehat{y}) = y \log(\widehat{y}) + (1-y) \log(1-\widehat{y}).
$$

Дальше мы шаг за шагом будем находить производные по переменным из всё более глубоких слоёв.

1. Градиент $\mathcal{L}$ по предсказаниям имеет вид

   $$    \nabla_{\widehat{y}}l = \frac{y}{\widehat{y}} - \frac{1 - y}{1 - \widehat{y}} = \frac{y - \widehat{y}}{\widehat{y} (1 - \widehat{y})},
   $$
   
   где, напомним, $\widehat{y} = \sigma(X^3) = \sigma\Big(\big(\sigma(X^0 W^1_0 )\big) W^2_0 \Big)$ (обратите внимание на то, что $W^1_0$ и $W^2_0$ тут именно те, из которых мы делаем градиентный шаг).

2. Следующий слой — поэлементное взятие $\sigma$. Как мы помним, при переходе через него градиент поэлементно умножается на производную $\sigma$, в которую подставлено предыдущее промежуточное представление:

   $$    \nabla_{X^3}l = \sigma'(X^3)\odot\nabla_{\widehat{y}}l = \sigma(X^3)\left( 1 - \sigma(X^3) \right) \odot \frac{y - \widehat{y}}{\widehat{y} (1 - \widehat{y})} = 
   $$
   
   $$    = \sigma(X^3)\left( 1 - \sigma(X^3) \right) \odot \frac{y - \sigma(X^3)}{\sigma(X^3) (1 - \sigma(X^3))} = 
   $$
   
   $$$$
   
   $$    \color{blue}{\nabla_{W^2_0}l} = (X^2)^T\cdot \nabla_{X^3}l = (X^2)^T\cdot(y - \sigma(X^3)) = 
   $$
   
   $$    = \color{blue}{\left( \sigma(X^0W^1_0) \right)^T \cdot (y - \sigma(\sigma(X^0W^1_0)W^2_0))}
   $$
   
   Аналогичным образом

   $$    \nabla_{X^2}l = \nabla_{X^3}l\cdot (W^2_0)^T = (y - \sigma(X^3))\cdot (W^2_0)^T = 
   $$
   
   $$    = (y - \sigma(X^2W_0^2))\cdot (W^2_0)^T
   $$
   
   

3. Следующий слой — снова взятие $\sigma$.

   $$    \nabla_{X^1}l = \sigma'(X^1)\odot\nabla_{X^2}l = \sigma(X^1)\left( 1 - \sigma(X^1) \right) \odot \left( (y - \sigma(X^2W_0^2))\cdot (W^2_0)^T \right) = 
   $$
   
   $$    = \sigma(X^1)\left( 1 - \sigma(X^1) \right) \odot\left(  (y - \sigma(\sigma(X^1)W_0^2))\cdot (W^2_0)^T \right)
   $$
   
   

4. Наконец, последний слой — это умножение $X^0$ на $W^1_0$. Тут мы дифференцируем только по $W^1$:

   $$    \color{blue}{\nabla_{W^1_0}l} = (X^0)^T\cdot \nabla_{X^1}l = (X^0)^T\cdot \big( \sigma(X^1) \left( 1 - \sigma(X^1) \right) \odot (y - \sigma(\sigma(X^1)W_0^2))\cdot (W^2_0)^T\big) =
   $$
   
   $$    = \color{blue}{(X^0)^T\cdot\big(\sigma(X^0W^1_0)\left( 1 - \sigma(X^0W^1_0) \right) \odot (y - \sigma(\sigma(X^0W^1_0)W_0^2))\cdot (W^2_0)^T\big) }
   $$
   
   

Итоговые формулы для градиентов получились страшноватыми, но они были получены друг из друга итеративно с помощью очень простых операций: матричного и поэлементного умножения, в которые порой подставлялись значения заранее вычисленных промежуточных представлений.

## Автоматизация и autograd

Итак, чтобы нейросеть обучалась, достаточно для любого слоя $f^k: X^{k-1}\mapsto X^k$ с параметрами $W^k$ уметь:

* превращать $\nabla_{X^k_0}\mathcal{L}$ в $\nabla_{X^{k-1}_0}\mathcal{L}$ (градиент по выходу в градиент по входу);
* считать градиент по его параметрам $\nabla_{W^k_0}\mathcal{L}$.

При этом слою совершенно не надо знать, что происходит вокруг. То есть слой действительно может быть запрограммирован как отдельная сущность, умеющая внутри себя делать forward propagation и backward propagation, после чего слои механически, как кубики в конструкторе, собираются в большую сеть, которая сможет работать как одно целое.

Более того, во многих случаях авторы библиотек для глубинного обучения уже о вас позаботились и создали средства для **автоматического дифференцирования выражений** (**autograd**). Поэтому, программируя нейросеть, вы почти всегда можете думать только о forward-проходе, прямом преобразовании данных, предоставив библиотеке дифференцировать всё самостоятельно.

Это делает код нейросетей весьма понятным и выразительным (да, в реальности он тоже бывает большим и страшным, но сравните на досуге код какой-нибудь разухабистой нейросети и код градиентного бустинга на решающих деревьях и почувствуйте разницу).

## Но это лишь начало

Метод обратного распространения ошибки позволяет удобно посчитать градиенты, но дальше с ними что-то надо делать, и старый добрый SGD едва ли справится с обучением современной сетки. Так что же делать? О некоторых приёмах мы расскажем в следующем параграфе.

  ## handbook

  Учебник по машинному обучению

  ## title

  Метод обратного распространения ошибки

  ## description

  Как эффективно посчитать градиенты по весам нейронной сети

- 
  ## path

  /handbook/ml/article/tonkosti-obucheniya

  ## content

  Если открыть случайную научную статью по глубинному обучению и попробовать воспроизвести её результаты, можно запросто потерпеть крах, и даже код на github, если он есть, может не помочь.  

А дело в том, что обучение сложной модели — это сложная инженерная задача, в которой успеху сопутствует огромное число разных хаков, и изменение какого-нибудь безобидного параметра может очень сильно повлиять на результат.  

В этом параграфе мы познакомим вас с некоторыми из таких приёмов.

## Инициализируем правильно

Как вы уже успели заметить, нейронные сети — достаточно сложные модели, чувствительные к изменениям архитектуры, гиперпараметров, распределения данных и другим вещам.  

Поэтому значительную роль играет начальная инициализация весов вашей сети. Стоит отметить, что здесь речь идет именно о начальной инициализации параметров сети, вопрос дообучения (и использования предобученных сетей в качестве backbone) в данном параграфе рассматриваться не будет.

Нейронные сети включают в себя различные преобразования, и инициализация по-хорошему также должна зависеть от типа используемого преобразования. На практике вопрос часто остается без внимания, так как в большинстве современных фреймворков уже реализованы методы инициализации, зависящие от используемой функции активации и гиперпараметров слоя, и пользователь может не задумываться об этом. Но всё же важно понимать, какие соображения привели к появлению тех или иных стратегий инициализации.

Давайте разберём несколько методов инициализации и обсудим их свойства.

### Наивный подход №0: инициализация нулем/константой

Казалось бы, инициализация параметров слоя нулями — это достаточно просто и лаконично. Но инициализация нулём (как и любой другой константой) ведёт к катастрофе! Вот пример того, что может получиться:

![18_1_2b5d95c793.webp](https://yastatic.net/s3/education-portal/media/18_1_2b5d95c793_04c574719d.webp)

Стоит, впрочем, отметить, что из-за численных ошибок значения параметров могут всё-таки сдвинуться с мёртвой точки, и тогда нейросеть что-нибудь выучит:

![18_2_c83ea1ee87.webp](https://yastatic.net/s3/education-portal/media/18_2_c83ea1ee87_24cfa5267c.webp)

{% cut "Математическая иллюстрация того, почему плохо инициализировать нулями" %}

Нам понадобится сеть с хотя бы двумя слоями (иначе ничего не получится).

Рассмотрим несколько последовательных скрытых представлений:

* $X^1$;
* $X^2 = X^1W$, где $W = 0$ — матрица весов, которую инициализировали нулями;
* $X^3 = h(X^2)$, где $h$ — поэлементная нелинейность;
* $X^4 = X^3U$, где $U = 0$ — ещё одна инициализированная нулями матрица весов.

Проследим, что происходит во время forward pass.

$$X^2 = X^1\cdot W = 0$$

Получилась снова матрица с одинаковыми столбцами, и это сохраняется дальше:

$$X^3 = h(0)$$

$$X^4 = X^3U = 0$$

Теперь рассмотрим обратный проход. Допустим, мы вычислили градиент $\nabla_{X^4}\mathcal{L}$. Тогда

$$\nabla_{U}\mathcal{L} = (X^3)^T\nabla_{X^4}\mathcal{L} = 0,$$

то есть матрица $U$ никак не обновится. Далее,

$$\nabla_{X^3}\mathcal{L} = \nabla_{X^4}\mathcal{L}U^T = 0,$$

$$\nabla_{X^2}\mathcal{L} = \nabla_{X^3}\mathcal{L}\odot h'(X^2) = 0,$$

$$\nabla_W\mathcal{L} = (X^1)^T\nabla_{X^2}\mathcal{L} = 0,$$

то есть веса $W$ тоже не обновятся. Таким образом, обучение происходить не будет.

А что же будет с однослойной нейросетью? Вспомним, что веса логистической регрессии (нейросети с одним-единственным, последним слоем) можно инициализировать чем угодно, в том числе нулями: функция потерь выпукла, поэтому градиентный спуск сходится из любой точки (по модулю численных эффектов). 
 
{% endcut %}


Здесь также стоит привести цитату из замечательной Deep Learning book (страница 301):  

```
Perhaps the only property known with complete certainty is that the initial parameters need to “break symmetry” between different units. If two hidden units with the same activation function are connected to the same inputs, then these units must have different initial parameters. If they have the same initial parameters, then a deterministic learning algorithm applied to a deterministic cost and model will constantly update both of these units in the same way.
```

###  Эвристический подход №1: инициализация случайными числами

Если константная инициализация не подходит, можно инициализировать нейросеть случайными числами. Допустим, веса пришли из распределения с нулевым средним и дисперсией $\sigma^2$, например, из нормального распределения $\mathcal{N}(0, \sigma^2)$.

Пусть теперь на вход линейному слою с весами $\mathbf{w}$ размерности $n_{\text{in}}$ пришел вектор $\mathbf{x}$ аналогичной размерности. 

**Замечание**. Можем считать, что мы рассматриваем лишь одну компоненту следующего промежуточного представления $\mathbf{z}$.

Все компоненты $\mathbf{x}$ распределены одинаковым образом и обладают нулевым средним. Тогда дисперсия их [произведения](https://href.li/?http://en.wikipedia.org/wiki/Variance#Product_of_independent_variables) $y$ имеет вид:

$$\text{Var}(y) = \text{Var}(\mathbf{w}^T\mathbf{x}) = \sum_{i=1}^n [ \mathbb{E}[x_i]^2 \text{Var}(w_i) + \mathbb{E}[w_i]^2 \text{Var}(x_i) + \text{Var}(w_i)\text{Var}(x_i)]$$

Первое и второе слагаемые равны нулю так как математические ожидание и весов, и значений $\mathbf{x}$ равны нулю. 

**Замечание**. Стоит заметить, что это будет верно и для промежуточных слоев в случае использования симметричной относительно нуля функции активации, например, `tanh`. 

Поскольку все веса пришли из одного распределения, можно выразить дисперсию результата следующим образом:

$$\text{Var}(\mathbf{w}^T\mathbf{x}) = n_{\text{in}} \text{Var}(w)\text{Var}(x),$$

где $\text{Var}(x)$ — это дисперсия любой компоненты $\mathbf{x}$ (как было оговорено ранее, они распределены одинаково), а $\text{Var}(w) = \sigma^2$ — дисперсия компоненты $\mathbf{w}$.

Следовательно, дисперсия результата линейно зависит от дисперсии входных данных с коэффициентом $n_{\text{in}} \text{Var}(w)$.

Увеличение дисперсии промежуточных представлений с каждым новым преобразованием (слоем) может вызвать численные ошибки или насыщение функций активации (таких как `tanh` и `sigmoid`), что не лучшим образом скажется на обучении сети.

Снижение дисперсии может привести к почти нулевым промежуточным представлениям (плюс «линейному» поведению `tanh` и `sigmoid` в непосредственной близости от нуля), что тоже негативно повлияет на результаты обучения.

Поэтому для начальной инициализации весов имеет смысл использовать распределение, дисперсия которого позволила бы сохранить дисперсию результата. Например, $\forall i, w_i \sim \mathbb{N}(0, \frac{1}{n_{\text{in}}}),$ или же в общем случае 

$$
\forall i, \text{Var}(w_i) = \frac{1}{n_\text{in}}
$$

Данный подход часто упоминается как `calibrated random numbers initialization`.  

###  Подход №2: Xavier & Normalized Xavier initialization

Если обратиться к предыдущему подходу, можно обнаружить, что все выкладки верны как для «прямого» прохода (forward propagation), так и для обратного (backward propagation). Дисперсия градиента при этом меняется в $n_{\text{out}} \text{Var}(w)$ раз, где $n_{\text{out}}$ — размерность следующего за $\mathbf{x}$ промежуточного представления.

И если мы хотим, чтобы сохранялись дисперсии и промежуточных представлений, и градиентов, у нас возникают сразу два ограничения:

$$\forall i, \text{Var}(w_i) = \frac{1}{n_\text{in}}$$

и

$$\forall i, \text{Var}(w_i) = \frac{1}{n_\text{out}}.$$

Легко заметить, что оба этих ограничения могут быть выполнены только в случае, когда размерность пространства не меняется при отображении, что случается далеко не всегда.

В [работе](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) Understanding the difficulty of training deep feedforward neural networks за авторством Xavier Glorot и Yoshua Bengio в качестве компромисса предлагается использовать параметры из распределения с дисперсией

$$\forall i, \text{Var}(w_i) = \frac{2}{n_\text{in} + n_\text{out}}.$$

Подробный вывод данного результата можно найти в оригинальной статье в формулах 2-12. 

**Обратите внимание**: эта инициализация хорошо подходит именно для `tanh`, так как в выводе явно учитывается симметричность функции активации относительно нуля.

В случае использования равномерного распределения $U$ для инициализации весов с учетом описанных выше ограничений мы получим **normalized Xavier initialization**:

$$
\forall i, w_i \sim U\left[-\frac{\sqrt{6}}{\sqrt{n_\text{in} + n_\text{out}}}, \frac{\sqrt{6}}{\sqrt{n_\text{in} + n_\text{out}}}\right].
$$

**Замечание**. Здесь используется тот факт, что дисперсия непрерывного равномерного распределения $\text{Var}[U[a, b]] = \frac{1}{12} (b-a)^2$.

Сравнение подобной инициализации для поведения промежуточных представлений (сверху) и градиентов (снизу) проиллюстрированы ниже (иллюстрации из [оригинальной статьи](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)):
![18_3_57a77dc9b8.webp](https://yastatic.net/s3/education-portal/media/18_3_57a77dc9b8_31a2c41180.webp)
![18_4_d4389fdff2.webp](https://yastatic.net/s3/education-portal/media/18_4_d4389fdff2_638f7749c0.webp)


###  Подход №3: Kaiming initialization
Вы могли обратить внимание, что Xavier initialization во многом опиралась на поведение функции активации `tanh`. Данный тип инициализации и впрямь лучше подходит для нее, но само использование гиперболического тангенса приводит к некоторым сложностям (например, к затуханию градиентов). 

В 2015 году в [работе](https://arxiv.org/abs/1502.01852v1) Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification за авторством Kaiming He, Xiangyu Zhang, Shaoqing Ren и Jian Sun были рассмотрены особые свойства функции активации `ReLU`, в частности, существенно смещенная относительно нуля область значений.

Пусть представление на входе было получено после применения данной функции активации к предыдущему представлению $\mathbf{z}_\text{prev}$: 

$$\mathbf{x} = \text{ReLU}(\mathbf{z}_\text{prev}),$$

где  $\mathbf{z}_\text{prev}$, в свою очередь, — это выход предыдущего линейного слоя с нулевым средним для каждой компоненты весов, то есть, в частности, $\mathbb{E}(z_\text{prev}) = 0$ 

В таком случае дисперсия выхода следующего линейного слоя примет вид:
$$\text{Var}(\mathbf{w}^T\mathbf{x}) = \sum_{i=1}^n [ \mathbb{E}[x_i]^2 \text{Var}(w_i) + \mathbb{E}[w_i]^2 \text{Var}(x_i) + \text{Var}(w_i)\text{Var}(x_i)]=$$

$$=\big(\mathbb{E}[x_i]^2 + \mathbb{V}(x_i) \big)\mathbb{V}(w_i).$$

В данном случае первый член не может быть проигнорирован, так как `ReLU` имеет ассиметричную область значений, а значит, распределения $x_i$ будут смещёнными. 

С учетом того, что $\text{Var}(x) = \mathbb{E}[x_i^2] - \mathbb{E}[x_i]^2$, выражение выше примет итоговый вид:

$$
\text{Var}(\mathbf{w}^T\mathbf{x}) = \text{Var}(\mathbf{w}^T\mathbf{x}) = n_\text{in} \text{Var}(w_i) \mathbb{E}(x_i^2).
$$

С учётом поведения `ReLU` и того, что $\mathbb{E}(z_\text{prev}) = 0$, можно сказать, что 

$$
\mathbb{E}(x_i^2) = \frac{1}{2}\text{Var}(z_\text{prev}),
$$

то есть

$$\text{Var}(\mathbf{w}^T\mathbf{x}) = \frac{1}{2}n_\text{in} \text{Var}(w_i) \text{Var}(z_\text{prev}).
$$

Получается, что использование `ReLU` приводит к необходимости инициализировать веса из распределения, чья дисперсия удовлетворяет следующему ограничению:

$$
\forall i, \frac{1}{2}n_\text{in}\text{Var}(w_i) = 1.
$$

Например, подходит нормальное распределение $\mathbb{N}(0, \frac{2}{n_\text{in}})$.

Данный способ инициализации (и его сравнение с Xavier initialization) проиллюстрирован ниже:

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/18_5_6c35dd096e_67edc2d034.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
  <a href="https://arxiv.org/pdf/1502.01852v1.pdf">Источник</a>
  </figcaption>
</figure>

### Промежуточные выводы
Рассмотренные способы инициализации используют достаточно много предположений, но все-таки они работают и позволяют нейронным сетям в некоторых случаях значительно быстрее сходиться. 

Понимание принципов работы даже таких небольших механизмов – ключ к глубокому освоению области глубокого обучения :)

## Методы оптимизации в нейронных сетях

Так как мы договорились, что нейросети представляют собой параметризованные дифференцируемые функции и для каждого параметра мы можем посчитать градиент, то, так же как и линейные модели, их можно настраивать с помощью градиентных методов.  

В [параграфе](https://academy.yandex.ru/handbook/ml/article/linejnye-modeli) про линейные модели мы под этим подразумевали обычно стохастический градиентный спуск на батчах, и это совершенно подходящий способ и для нейросетей тоже. Но существует множество модификаций и эвристик, позволяющих ускорить его сходимость, познакомиться с которыми вы можете в специальном [параграфе](https://academy.yandex.ru/handbook/ml/article/optimizaciya-v-ml), посвящённом методам оптимизации.

## Регуляризация нейронных сетей

Смысл термина **регуляризация** (англ. *regularization*) гораздо шире привычного вам прибавления $L_1$- или $L_2$-нормы вектора весов к функции потерь. Фактически он объединяет большое количество техник для борьбы с переобучением и для получения более подходящего решения с точки зрения эксперта.  

Каждая из них позволяет навязать модели определённые свойства, пусть даже и ценой некоторого снижения качества предсказания на обучающей выборке. Например, уже знакомая читателю $L_1$- или $L_2$-регуляризация в задаче линейной регрессии (регуляризация Тихонова) позволяет исключить наименее значимые признаки (для линейной модели) или же получить устойчивое (хоть и смещённое) решение в случае мультиколлинеарных признаков.

В нейронных сетях техники регуляризации можно разделить на три обширные группы: 
* связанные с изменением функции потерь;
* связанные с изменением структуры сети;
* связанные с изменением данных.

Рассмотрим каждую из них подробнее. 

### Регуляризация через функцию потерь
Изменение функции потерь — классический способ получить решение, удовлетворяющее определённым условиям. В глубинном обучении часто используется техника Weight Decay, очень близкая к регуляризации Тихонова. Она представляет собой аналогичный штраф за высокие значения весов нейронной сети с коэффициентом регуляризации $\lambda$:

$$
L_\text{with regularization} = L_\text{original} + \lambda ||\mathbf{W}||_2
$$

Данная техника регуляризации была совмещена с методом градиентной оптимизации Adam, в результате чего был получен метод AdamW (описанный в параграфе [параграфе](https://academy.yandex.ru/handbook/ml/article/optimizaciya-v-ml) про методы оптимизации).

Также достаточно часто в качестве регуляризационного члена встречается энтропия распределения, предсказанного нейронной сетью.  

Представьте, что вы рекомендуете пользователю товары по истории его взаимодействия с сервисом, семплируя товары для показа в соответствии с распределением предсказанной релевантности. Вам может быть важно, чтобы рекомендации не были фиксированными (менялись при обновлении страницы), ведь это повысит вероятность того, что пользователь найдёт что-то интересное, а вы узнаете о нём что-нибудь новое.  

В такой ситуации при обучении модели вы можете потребовать, чтобы распределение предсказаний не сходилось к вырожденному, и в качестве дополнительной штрафной функции может выступать энтропия этого распределения. Энтропия дифференцируема, как и сами предсказанные величины, и может быть использована в качестве регуляризационного члена. Для задачи классификации он будет выглядеть следующим образом:

$$
\widehat{\mathbf{p}} = f(x; \mathbf{\theta}),
$$

$$
L_\text{with regularization} = L_\text{original} - \lambda \; \sum_k \widehat{p}_k \log \widehat{p}_k,
$$

где $\lambda$ — коэффициент регуляризации, $\widehat{\mathbf{p}}$ — предсказанные вероятности.  

Тем самым эксперт привносит своё знание непосредственно в процесс обучения модели в подходящей математической форме: «предсказания должны быть разнообразными» —> «распределение не должно быть вырожденным» —> «энтропия не должна быть слишком низкой».

### Регуляризация через ограничение структуры модели

Внесение подходящих преобразований в структуру сети также может быть хорошим способом добиться желаемых результатов. Огромное влияние на развитие нейронных сетей оказали техники [dropout (2014)](https://jmlr.org/papers/v15/srivastava14a.html) и [batch normalization (2015)](https://arxiv.org/abs/1502.03167), позволившие сделать нейронные сети более устойчивыми к переобучению и многократно ускорить их сходимость соответственно.

**Dropout**

Обратимся к простым полносвязным (FC/Dense) сетям из нескольких слоёв. Каждый из слоёв порождает новое признаковое описание $x^{k}$ объекта $x^{\text{in}}$, который пришёл на вход:

$$
x^{k} = f_k(x^{k-1}).
$$

Но как можно гарантировать, что модель будет эффективно использовать все доступные параметры, а не переобучится под использование лишь небольшого их подмножества, поделив для себя внутреннее представление на сигнал и шум?

$$
x^{k}_\text{overfitted} = [\text{signal}, \text{noise}]
$$

Для этого можно было бы случайным образом «выключать» доступ к некоторым координатам внутренних представлений на этапе обучения. Тогда при выключении «полезных» координат произойдёт резкое изменение предсказаний модели, что приведёт к увеличению ошибки, а полученные градиенты этой ошибки укажут, как её исправить с использованием (и изменением) других координат. Сравнение тока информации по исходной модели и по модели с «выключенными» координатами внутренних представлений можно проиллюстрировать с помощью классической картинки:

![18_6_854b5a5ed2.webp](https://yastatic.net/s3/education-portal/media/18_6_854b5a5ed2_4463fadefa.webp)

**Обратите внимание**: «выключать» можно как оригинальные признаки, так и признаки, возникающие на любом другом уровне представления объектов. С точки зрения $(k+1)$-го слоя нейронной сети данные приходят откуда-то извне: при $k = 0$ — из реального мира, а при $k > 1$ — с предыдущих слоёв.

Технически это осуществляется следующим образом: некоторые координаты внутреннего представления домножаются на ноль. То есть добавляется ещё одно преобразование, которое представляет собой домножение выхода предыдущего слоя на маску из нулей и единиц.

$$
x^{k+1} = \frac1{1-p} x^{k} \odot \text{mask},
$$

$$
\text{mask}_{i} \sim \text{Bernoulli}(1 - p).
$$

где $(1-p)$ (вероятность обнуления координаты) — это гиперпараметр слоя. Отметим, что во многих фреймворках для глубинного обучения в качестве параметра слоя указывается именно вероятность обнуления, а не выживания. Данная маска участвует и при подсчёте градиентов:

$$
\nabla_{x^{k}}{\mathcal{L}} = \frac1{1-p}\nabla_{x^{k+1}}{\mathcal{L}} \odot \text{mask}
$$

Как правило, маска генерируется независимо на каждом шаге градиентного спуска. Важно отметить, что на этапе предсказания dropout ничего не меняет, то есть $x^{k+1} = x^k$.

Множитель $\frac1{1-p}$ нужен для того, чтобы распределение $x^{k+1}$ на этапе предсказания совпадало с распределением на этапе обучения. В самом деле, если даже математическое ожидание $x^k$ было равно нулю, выборочная дисперсия $x^k\odot\text{mask}$ ниже, чем у $x^k$: ведь часть значений обнулилась.

На этапе предсказания dropout «выключается»: внутренние представления используются как есть, без умножения на маску. А чтобы слой знал, обучается он сейчас или предсказывает, в нейросетевых библиотеках в классе слоя обычно реализовано переключение между этими режимами (например, булев флаг `training` в pytorch-модулях).

{% cut "Примечание" %}

Полезно провести аналогию с другим алгоритмом, использующим техники ансамблирования и метод случайных подпространств: речь о случайном лесе (Random Forest). При обучении сети на каждом шаге обучается лишь некоторая подсеть (некоторый подграф вычислений из исходного графа). При переходе в режим inference (то есть применения к реальным данным с целью получения результата, а не обучения) активируются сразу все подсети, и их результаты усредняются. Таким образом, сеть с dropout можно рассматривать как ансамбль из экспоненциально большого числа сетей меньшего размера (подробнее можно прочитать [здесь](https://arxiv.org/abs/1706.06859)). Это приводит к получению более устойчивой оценки значений целевой переменной.

В этом свойстве кроется и главное коварство dropout (как и большинства других техник регуляризации): благодаря получению более устойчивой оценки целевой переменной путём усреднения множества подсетей, эффективная обобщающая способность итоговой сети снижается! В самом деле, пусть при обучении каждый раз модели была доступна лишь половина параметров. В таком случае итоговая модель представляет собой усреднение множества более слабых моделей, в которых вдвое меньше параметров. Её предсказания будут более устойчивы к шуму, но при этом она неспособна выучить столь сложные зависимости, как сеть аналогичной структуры, но без dropout. То есть за более устойчивые предсказания (и получение менее переобученной модели) приходится расплачиваться и меньшей обобщающей способностью.

{% endcut %}

Стоит отметить, что dropout может применяться и к входным данным (то есть слой dropout может стоять первым в сети), и это может приводить к получению более качественных результатов. Например, если в данных множество мультикоррелирующих признаков или присутствует шум, наличие dropout позволит избежать обусловливания модели на лишь их подмножество и позволит учитывать их все. Так, подобный подход может быть использован, если данные представляют собой сильно разреженные векторы высокой размерности (скажем, сведения об интересе пользователя к тем или иным товарам).

**Batch normalization**

Появление техники batch normalization привело к значительному ускорению обучения нейронных сетей. В данном параграфе мы рассмотрим лишь основные принципы работы batch normalization.  

Дискуссия о свойствах и причинах эффективности batch normalization всё ещё ведётся, рекомендуем обратить внимание на [статью](https://arxiv.org/abs/1805.11604) с NeurIPS 2018. Нам, впрочем, кажется, что, несмотря на активную критику в его адрес, полезно знать и предложенное авторами подхода объяснение необходимости batch normalization.

{% cut "Предложенная авторами мотивация" %}

Обратимся к механизму обратного распространения ошибки. Пусть мы находимся на этапе обновления параметров $W^{k}$ некоторого $k$-го слоя:

$$
x^k = f(x^{k-1}, W^{k}),
$$

где $f$ — некоторая функция, которая вычисляется на данном слое. В общем случае $W^{k}$ и $x^{k-1}$ не обязательно взаимодействуют линейным образом; функция $f$ может быть и нелинейной. В ходе error backward propagation мы вычисляем градиент:

$$
\nabla_{W^{k}}\mathcal{L} = g(x^{k-1}, x^k, x^{k+1},\ldots;W^k),
$$

где $g$ — некоторая функция, в которой будут участвовать представления со слоёв, начиная с $(k-1)$-го (вычисленные в ходе forward pass и запомненные).  

Новое значение параметров примет вид:

$$
W^k_{\text{new}} = W^{k} - \alpha \nabla_{W^{k}} \mathcal{L} = W^{k} - \alpha g(x^{k-1},x^k,\ldots)
$$

После обновления параметров $W^{k}$ мы перейдём к обновлению параметров предыдущего слоя $W^{k-1}$ и обновим их аналогичным образом.

**Важно**. Это приведёт к изменению представления, которое пришло на вход $k$-му слою, которое мы не учитываем:

$$
x^k_{\text{new}} = f(x^{k-1}, W^{k}_{\text{new}}) = f(x^{k-1}, W^{k} + \phi),
$$

где $\phi$ — разница между предыдущими и новыми параметрами $W^{k}$.

То есть параметры $(k-1)$-го слоя будут обновлены исходя из предположения, что данные приходят из некоторого распределения на $x^k$, которое параметризовалось $W^{k-1}$, но теперь параметры изменились и данные могут __обладать иными свойствами__. Например, может существенно измениться среднее или дисперсия, что может привести, например, к попаданию на «хвосты» функции активации и затуханию градиента.

До появления batch normalization с этой проблемой боролись достаточно просто: использовали небольшие значения шага обучения (learning rate) $\alpha$. Благодаря этому изменения были не слишком большими и можно было предположить, что и распределение внутренних представлений поменялось незначительно.

{% endcut %}

Использование batch normalization гарантирует, что каждая компонента представления на выходе будет иметь контролируемое среднее и дисперсию. Достигается это следующим образом:

1. Сперва идёт собственно слой *batch normalization*, на котором текущий батч приводится к нулевому среднему и единичной дисперсии:

  $$X^{k+1} = \frac{X^k - \mu}{\sqrt{\sigma}^2 + \varepsilon},$$

  где $\mu$ и $\sigma^2$ — среднее и дисперсия признаков по обрабатываемому батчу, а $\varepsilon$ — гиперпараметр слоя, небольшое положительное число, добавляемое для улучшения численной устойчивости. Отметим, что $\mu$ и $\sigma$, будучи функциями от $X^k$, тоже участвуют в вычислении градиентов. В ходе **предсказания** (или, как ещё говорят, **инференса**, от английского **inference**) используются фиксированные значения $\mu_{\ast}$ и $\sigma_{\ast}^2$, которые были получены в ходе обучения как скользящее среднее всех $\mu$ и $\sigma^2$. Более точно: на каждой итерации forward pass мы вычисляем

  $$
   \mu_{\ast} = \mu_{\ast} \lambda + \mu (1 - \lambda)
  $$

  $$
  \sigma^2_{\ast} = \sigma^2_{\ast} \lambda + \sigma^2 (1 - \lambda),
  $$

  где $\lambda$ также является гиперпараметром слоя.

2. Далее идёт слой *channelwise scaling*, который позволяет выучить оптимальное шкалирование для всех признаков $X^{k+2}$:

  $$X^{k+2} = \beta X^{k+1} + \gamma,$$

  где $\beta$ и $\gamma$ — обучаемые параметры, позволяющие настраивать в ходе обучения оптимальные значения матожидания и дисперсии выходного слоя $X^{k+2}$.

Ниже приведён алгоритм из оригинальноq [статьи](https://arxiv.org/abs/1502.03167)  2015 года за авторством Сергея Иоффе и Кристиана Сегеди:

![18_7_f3355fcfaf.webp](https://yastatic.net/s3/education-portal/media/18_7_f3355fcfaf_fd4a8794e1.webp)

Причина популярности batch normalization заключается в значительном ускорении обучения нейронных сетей и в улучшении их сходимости в целом. Рассмотрим график из оригинальной статьи:

![18_8_8bc635c7bc.webp](https://yastatic.net/s3/education-portal/media/18_8_8bc635c7bc_b21340d08e.webp)

Как видно на иллюстрации выше, использование batch normalization позволило ускорить обучение в несколько раз и даже добиться лучших результатов, чем SotA-подход 2014 года Inception (структура которого была приведена на одной из иллюстраций в начале этого параграфа).

Значительное ускорение достигается в том числе благодаря использованию более высокого learning rate: благодаря нормировке связь между слоями не нарушается столь сильно.

Стоит заметить, что причины столь эффективной работы batch normalization до сих пор являются поводом для дискуссий и строгого теоретического объяснения эффекта от batch normalization ещё нет. Несмотря на это, он перевернул область глубинного обучения и вошёл в стандартный инструментарий при обучении нейронных сетей.

**Примечание**: стоит заметить, что в настоящее время существуют и другие способы нормировать промежуточные представления: instance normalization, layer normalization и так далее.

В завершение рекомендуем ознакомиться со [статьёй](https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html)  о работе метода обратного распространения ошибки в слое batch normalization.

### Регуляризация через изменение данных

Внесение изменений в данные (аугментация данных) также является популярной техникой регуляризации. Рассмотрим её на примере.  

Пусть перед нами фотография самолёта. Добавим мелкодисперсный шум к изображению. Мы всё ещё сможем увидеть на фотографии самолёт, но, с точки зрения модели машинного обучения (в данном случае — нейронной сети), полученное изображение является новым объектом!  

Повернём изображение самолёта на 10 градусов по часовой стрелке. В нашем распоряжении ещё одно изображение с известной целевой меткой (например, меткой класса «самолёт»), в котором присутствует поворот. Таким образом, внесение новых данных позволяет дать модели понять, какие преобразования над данными являются допустимыми, и она уже будет более устойчивой к наличию небольшого шума в данных или к поворотам (к которым чувствительна операция свёртки).  

Вдобавок аугментации позволяют значительно увеличить объём обучающей выборки. Особую популярность аугментации приобрели в области компьютерного зрения. В качестве примера приведём отличную [библиотеку](https://github.com/albumentations-team/albumentations), позволяющую производить аугментацию изображений.

Стоит обратить внимание, что используемые аугментации должны быть адекватны решаемой задаче. Инвертирование цветов на фотографии, внесение значительного количества шумов или переворот изображения могут привести и к негативным результатам (по сути, просто сделают выборку более зашумлённой или даже заставят сеть учиться на данных, которые она никогда не встретит в реальности), так как обобщающая способность сети ограниченна. Можно сказать, что аугментированные данные должны принадлежать к той же генеральной совокупности, что и оригинальный датасет.

Итак, эксперт может привнести своё понимание задачи и на уровне аугментации данных: если данное преобразование является допустимым (то есть преобразованный объект мог бы попасть в обучающую выборку и самостоятельно — как фотография с другого устройства или запись речи другого человека с опечаткой), то модель должна быть устойчива к данным с подобными преобразованиями.

  ## handbook

  Учебник по машинному обучению

  ## title

  Тонкости обучения

  ## description

  Инициализация весов. Регуляризация нейросетей. Dropout и Batchnorm

- 
  ## path

  /handbook/ml/article/svyortochnye-nejroseti

  ## content

  В этом параграфе мы на примере задачи распознавания изображений познакомимся со свёрточными нейронными сетями, уже ставшими стандартом в области. Для начала мы разберёмся, с какого рода данными придётся работать, затем попробуем решить задачу «в лоб» при помощи знакомых вам полносвязных сетей и поймём, чем это чревато. А после чего рассмотрим свёртки и попробуем выработать нужную интуицию.

## Формат данных

Картинки в большинстве случаев представляют собой упорядоченный набор пикселей, где каждый пиксель — это вектор из трех «каналов»: интенсивность красного, интенсивность зелёного, интенсивность синего.

![2_rgb_rooster_1_9610445be0.svg](https://yastatic.net/s3/education-portal/media/2_rgb_rooster_1_9610445be0_11cb0f9215.svg)

Каждая интенсивность характеризуется числом от 0 до 1, но для привычных нам изображений этот интервал равномерно дискретизирован для экономии памяти, чтобы уместиться в 8 бит (от 0 до 255). При этом нулевая интенсивность (0, 0, 0), соответствует чёрному цвету, а максимальная интенсивность (255, 255, 255) — белому.

Когда мы наблюдаем изображение на мониторе компьютера, мы видим эти пиксели «уложенными» в строки одинаковой длины (человек не сможет воспринять картинку, вытянутую в один вектор). Длину каждой такой строки называют шириной `W` картинки, а количество строк — высотой `H`. Резюмирую, мы можем рассматривать картинку, как тензор `HxWx3`, состоящий из чисел uint8.

![1_rgb_split_rooster_1_72ea43ba3f.svg](https://yastatic.net/s3/education-portal/media/1_rgb_split_rooster_1_72ea43ba3f_b7355dedf3.svg)

Существует множество разных форматов хранения картинок: вместо трех интенсивностей мы можем использовать триплет «оттенок, насыщенность, интенсивность», а сами картинки хранить, например, как тензор `CxHxW`.

## MLP

Наверное, самый простой способ построить нейронную сеть для решения задачи классификации на наших данных — это «развернуть» нашу картинку в вектор, а затем использовать обычную многослойную сеть с кросс-энтропией в качестве лосса.

Однако, такой подход имеет несколько недостатков.

### Недостаток №1: количество параметров

В первом слое у нас получается `HxWxCxCout` параметров, где Cout — это количество нейронов в первом слое. Если поставить `Cout` слишком маленьким, мы рискуем потерять много важной информации, особенно, если рассматривать картинки размером, например, 1920x1080.  

Если же выставить Cout большим, рискуем получить слишком много параметров (а это только первый слой), а с этим и все вытекающие проблемы — переобучение, сложность оптимизации и так далее.  

### Недостаток №2: структура данных никак не учитывается. 

Что здесь имеется в виду под «структурой»? Попробуем объяснить на примере. Для этого рассмотрим картинку щеночка:

![puppy_a1d259493d.webp](https://yastatic.net/s3/education-portal/media/puppy_a1d259493d_708f608dce.webp)

Если мы сдвинем картинку на несколько пикселей, то мы все еще будем уверены в том, что это щенок:

![shifted_puppy_c27db8708f.webp](https://yastatic.net/s3/education-portal/media/shifted_puppy_c27db8708f_ca16064a3b.webp)

Точно также мы останемся неизменны в своем мнении, если картинку отмасштабировать:

![scaled_puppy_d6c7abac44.webp](https://yastatic.net/s3/education-portal/media/scaled_puppy_d6c7abac44_a80f12b60e.webp)

или повернуть/развернуть:

![rotated_puppy_523e9847b2.webp](https://yastatic.net/s3/education-portal/media/rotated_puppy_523e9847b2_31bef7bb3c.webp)
<br>
![flipped_puppy_a8a6029dce.webp](https://yastatic.net/s3/education-portal/media/flipped_puppy_a8a6029dce_c8300e6bd9.webp)
	
Получается, что нейронная сеть должна «сама» понять, что ее ответ должен быть инвариантен к описанным преобразованиям. Но, обычно, это достигается за счет увеличения количества нейронов в скрытых слоях (как мы можем помнить из _universal approximation theorem_), что и так для нас — головная боль из-за первого пункта.

С частью этих проблем нам поможет новый «строительный блок» — свёртка. О ней в следующем разделе.

## Свёртки

Строгое определение свёртки мы дадим ниже, а вначале разберёмся в мотивации.

Давайте попробуем решить хотя бы проблему инвариантности к сдвигу. Щенок может быть где угодно на картинке, и мы не можем наверняка сказать, в какой части изображения наша модель «лучше всего» научилась видеть щенков. Поэтому для надёжного предсказания будет логично посдвигать картинку на все возможные смещения (пустоты заполним нулями):

![gif_shift_template_35384897e9.svg](https://yastatic.net/s3/education-portal/media/gif_shift_template_35384897e9_16cdab70dc.svg)

Затем для каждого смещения мы предскажем вероятность наличия щенка на картинке. Получившиеся предсказания можно уже агрегировать как удобно: среднее, максимум и так далее.

Давайте взглянем на эту операцию под другим углом. Рассмотрим картинку, размером в 3 раза превышающую оригинальную, в центре которой находится наше изображение щеночка:

![padded_puppy_3bd370a19b.webp](https://yastatic.net/s3/education-portal/media/padded_puppy_3bd370a19b_83e0aba93a.webp)

Возьмём окно размером с исходную картинку, и будем его сдвигать на все возможные смещения внутри нового изображения:

![gif_conv_template0_fbf83244f0.svg](https://yastatic.net/s3/education-portal/media/gif_conv_template0_fbf83244f0_c38899448f.svg)

Легко видеть, что получается то же самое, как если бы мы картинку сдвигали относительно окна.

Представим себе самую простую модель, основанную на данном принципе — что-то вроде ансамбля линейных. Каждую из сдвинутых картинок вытянем в вектор и скалярно умножим на вектор весов (для простоты один и тот же для всех сдвигов) — получим линейный оператор, для которого есть специальное имя — **свёртка**.  

Это один из важнейших компонент в **свёрточных** нейронных сетях. Веса свёртки, упорядоченные в тензор (в нашем случае размерности `HxWx3`), составляют её **ядро**. Область картинки, которая обрабатывается в текущий момент, обычно называется **окном свёртки**.  

**Обратите внимание**, что обычно такие свёртки называются двумерными — так как окно свёртки пробегает по двум измерениям картинки (при этом все цветовые каналы участвуют в вычислениях).  

Следующая картинка поможет разобраться (внимание: на ней _нет_ изображения весов оператора):

![11_image_tensor_conv_cdaf6d395d_f54b12323d.svg](https://yastatic.net/s3/education-portal/media/11_image_tensor_conv_cdaf6d395d_f54b12323d_a3d8bf4b65.svg)

Каждый «кубик» на картинке — это число. Большой черный тензор слева — это изображение щеночка $X$. Фиолетовым на нем выделено окно, из которого мы достаем все пиксели и разворачиваем в вектор (аналогично операции flatten в numpy) $v$.  

Далее этот вектор умножается на вектор весов класса «щенок» $w_1$, и получается число $k_1$ — логит интересующего класса. Добавив остальные классы, получим матрицу весов $W$ — прямо как в мультиномиальной логистической регрессии. Эту операцию мы повторяем для каждого возможного сдвига окна свёртки.

Результаты домножения удобно бывает скомпоновать в двумерную табличку, которую при желании можно трактовать, как некоторую новую картинку (в серых тонах, потому что канал уже только один). Воспользуемся этим, чтобы получше осознать, что происходит в ходе свёртки.

**Вопрос на подумать**. Какой геометрический смысл имеет свёртка с ядром

$$B_1 = \frac19\begin{pmatrix}1 & 1 & 1\\ 1 & 1 & 1\\ 1 & 1 & 1\end{pmatrix}?$$

А с ядром

$$B_2 = \begin{pmatrix}-1 & -1 & -1\\ -1 & 8 & -1\\ -1 & -1 & -1\end{pmatrix}?$$

{% cut "Ответ (не открывайте сразу; сначала подумайте сами!)" %}

Первая свёртка усредняет каждый пиксель с соседними, и изображение размывается. Смысл второй можно грубо описать так: пиксели из однородных участков изображения слабеют, тогда как контрастные точки, напротив, усиливаются. Можно сказать, что такая свёртка выделяет границы. Проиллюстрируем работу этих ядер на примере небольшого изображения в серых тонах:

![conv_examples_7a0dba822d.webp](https://yastatic.net/s3/education-portal/media/conv_examples_7a0dba822d_10b47a420f.webp)

В донейросетевую эпоху различные свёртки играли существенную роль в обработке изображений, и сейчас мы видим, почему.

{% endcut %}

**Вопрос на подумать**. На краях картинок из ответа к предыдущему вопросу заметны тёмные рамки. Что это такое? Откуда они берутся?

{% cut "Ответ (не открывайте сразу; сначала подумайте сами!)" %}

Их появление связано с особенностями обработки свёртками краёв изображения. Вообще, есть несколько стратегий борьбы с краями. Например:

* Дополнить изображение по краям нулями. Когда мы будем рассматривать окна свёртки с центрами в крайних пикселях, они будут захватывать эти нули. Такая свёртка будет превращать изображение размером `HxWx3` в изображение размером `HxW`, без уменьшения размера. Но так как нули соответствуют чёрному цвету, это будет вносить определённые изменения в крайние пиксели результата. Именно благодаря этому у картинок из предыдущего вопроса на подумать по краям появились тёмные рамки.
* Разрешить только такие окна, которые целиком лежат внутри изображения. Это будет приводить к падению размера. Например, для окна размером `5x5` картинка размером `HxWx3` превратится в картинку размером `(H-2)x(W-2)`.

{% endcut %}

Решив проблему обеспечения устойчивости к сдвигу картинки и имея на руках наш огромный свёрточный фильтр, давайте попробуем теперь справиться с первой проблемой — количество параметров. Самое простое, что можно придумать, — это уменьшить размер окна с `HxW` до, допустим, `kxk` (обычно нечётное и $k \in [3,11]$). В этом случае получается радикальное снижение количества параметров и сложности вычислений.

![gif_small_conv_template_a2dd681565.svg](https://yastatic.net/s3/education-portal/media/gif_small_conv_template_a2dd681565_6ce335cf1b.svg)

К сожалению, с таким подходом возникает новая проблема: предсказание для какого-то окна никак не учитывает контекст вокруг него. Получается, мы можем получить разумные предсказания только в случае, если распознаваемый объект обладает признаками, которые «помещаются» в окно свёртки (например, лого автомобиля при классификации марок машин), либо объекты заметно отличаются по своей текстуре (шерсть кошки vs кирпич, например).  

На картинке ниже сделана попытка изобразить проблему:

![very_small_conv_4bdde25018.webp](https://yastatic.net/s3/education-portal/media/very_small_conv_4bdde25018_dab7fdba0b.webp)

Область картинки, на которую «смотрит» наша нейронная сеть, назвается **receptive field** — и про него приходится часто думать в задачах компьютерного зрения.  

Давайте и мы подумаем, как его можно было бы увеличить, не увеличивая размер ядра. Вспомним, что в нашей нейронке сейчас есть только один слой, сразу предсказывающий класс. Выглядит так, что мы можем применить уже знакомую технику стекинга слоев: пусть на первой стадии мы делаем $C_1$ разных свёрток с фильтрам размером `kxk`. Результаты каждой свёртки можно упорядочить в виде новой «картинки», а из этих «картинок» сложить трёхмерный тензор. Получаем так называемую карту признаков размером `HxWxC_1`.  

Применим к ней поэлементно нелинейность и воспользуемся `K` новыми свёртками для получения предсказаний для каждого пикселя. На таком шаге получается, что наш receptive field для финальных нейронов вырос от `kxk` до `(2k-1)x(2k-1)` (пояснение на [картинке](#fig:receptive_field)).  

Повторяя такую операцию, мы можем добиться, чтобы наши финальные нейроны уже могли «видеть» почти всю нужную информацию для хорошего предикта. Более того, у нас возникает меньшее количество параметров и падает сложность вычислений в сравнении с использованием одной большой полносвязной сети. 

Как это схематично выглядит:

![15_conv_composition_e686591745_b5d7657ff6.svg](https://yastatic.net/s3/education-portal/media/15_conv_composition_e686591745_b5d7657ff6_01f88a670a.svg)

Промежуточный тензор $L_1$, полученный при помощи $C_1$ свёрток, можно себе представить, как новую картинку, у которой уже $C_1$ каналов.

На следующей картинке можно отследить, как меняется receptive field в зависимости от глубины:

![16_receptive_field_fbd1c12519_af58719882.svg](https://yastatic.net/s3/education-portal/media/16_receptive_field_fbd1c12519_af58719882_dc46e45295.svg)

На картинке схематично изображен «плоский» двумерный тензор (количество каналов = 1), к которому последовательно применили три свёртки 3x3. В каждом случае рассматривается пиксель в центре. Каждый соответствующий тензор помечен, как $L_i$. Если рассматривать первую свёртку ($X\to L_1$), то размер receptive field равен размеру е окна = 3.  

Рассмотрим вторую свёртку $L_1 \to L_2$. В ее вычислении участвуют пиксели из квадрата 3х3, причём каждый из них, в свою очередь, был получен при помощи предыдущей свёртки $X \to L_1$. Получается, что receptive field композиции свёрток $X\to L_1\to L_2$ — это объединение receptive fields свёртки $X\to L_1$ по всем пикселям из окна свёртки $L_1\to L_2$, образуя новый, размером 5x5. Аналогичные рассуждения можно повторить и для всех последующих свёрток.

Ещё один способ увеличить receptive field — это использовать dilated convolution, в которых окно свёртки (то есть те пиксели картинки, на которые умножается ядро) не обязано быть цельным, а может идти с некоторым шагом (вообще говоря, даже разным по осям `H` и `W`).  

Проиллюстрируем, как будет выглядеть окно для обычной свёртки и для свёртки с шагом `dilation=2`:

![dilated_convolution_f480c5e9be.svg](https://yastatic.net/s3/education-portal/media/dilated_convolution_36bafd970d.webp)

Если установить параметр `dilation=(1,1)`, получится обычная свёртка.

Итак, свёртки помогли нам решить сразу две проблемы: устойчивости к сдвигу и минимизации числа параметров. Теперь давайте попробуем определить оператор более формально.

### Формальное определение свёртки

![18_conv_def_142fec410a_2b151b5c5f.svg](https://yastatic.net/s3/education-portal/media/18_conv_def_142fec410a_2b151b5c5f_23ec612516.svg)

**Вопрос на подумать**. Пусть у нас есть тензор размером `HxWxC_{in}`, к которому одновременно применяется $C_{out}$ свёрток, размер окна каждой равен `kxk`. Посчитайте количество обучаемых параметров. Как изменится формула, если к свёртке добавить смещение (bias)? Во сколько раз изменится количество параметров, если увеличить размер окна в 2 раза? А если увеличить количество каналов $C_{in}$ и $C_{out}$ в два раза? А если увеличить размер входного тензора в 2 раза по высоте и ширине?

**Вопрос на подумать**. Оцените количество операций сложений-умножений для предыдущего упражнения. Как оно поменяется, если увеличить в два раза размер окна? Количество каналов? Размер входного тензора?

**Вопрос на подумать**. Пусть последовательно применяется $N$ свёрток $k \times k$. Посчитайте размер receptive field для последнего оператора.

### Свёртки не только для изображений

Нетрудно видеть, что аналоги двумерной свёртки можно определить и для тензоров другой размерности, в любой ситуации, когда для нас актуально поддерживать устойчивость модели к сдвигам данных. Например, это актуально для работы с текстами. Обычно текст разбивается на последовательные токены (например, на слова или какие-то subword units), и каждому из этих токенов ставится в соответствие вектор (более подробно об этом вы можете почитать в параграфе про работу с текстами или в [разделе](https://lena-voita.github.io/nlp_course/word_embeddings.html) про вложения слов учебника по NLP Лены Войта).

![cnn_word_1_a7eae439ca.svg](https://yastatic.net/s3/education-portal/media/cnn_word_1_a7eae439ca_e7becfd688.svg)

Представим теперь, что мы хотим определить, позитивно или негативно окрашен этот текст. Мы можем предположить, что эмоциональная окраска локальна и может проявляться на любом участке текста, и тогда нам нужна модель, которая может «посмотреть» отдельно на каждый последовательный фрагмент текста некоторой длины. И здесь тоже может сработать свёртка:

![cnn_word_c294cce45a.svg](https://yastatic.net/s3/education-portal/media/cnn_word_c294cce45a_13a54daa9b.svg)

Существуют свёртки и для тензоров более высокой размерности, например, для видео (где прибавляется ещё координата «время»).

### Поворот, отражение, масштабирование

А что делать с остальными проблемами: поворотом, отражением, масштабированием? К сожалению, на момент написания параграфа (вторая половина 2021 года), автору не было известно об успешном опыте решения этих проблем в архитектуре сети. При этом оказывается, что приведенного оператора уже достаточно, чтобы нейронная сеть могла хорошо обобщать на невиданные ранее картинки (лишь бы свёрток было больше и сеть глубже).

В качестве потенциально интересного (но пока не проявившего себя на практике) направления исследований можно упомянуть капсульные нейросети. Кроме того, вам может быть интересно познакомиться с [геометрическим глубинным обучением](https://geometricdeeplearning.com/lectures/). В качестве короткого введения рекомендуем посмотреть вот этот [keynote с ICLR 2021](https://www.youtube.com/watch?v=w6Pw4MOzMuo), которое ставит своей целью исследование общих принципов, связывающих устойчивость к различным преобразованием и современные нейросетевые архитектуры (авторы сравнивают свои идеи с эрлангенской программой Феликса Кляйна — отсюда название).

## Свёрточный слой и обратное распространение ошибки

Поговорим о том, как через свёрточный слой протекают градиенты. Нам нужно будет научиться градиент по выходу превращать в градиент по входу и в градиент по весам из ядра.

Начнём с иллюстрации для одномерной свёртки с одним входным каналом, ядром длины $3$ с дополнением по бокам нулями. Заметим, что её можно представить в виде матричного умножения:

$$(x_1,\ldots,x_d) \ast (w_{-1},w_0,w_1) = $$

$$= (0,x_1,\ldots,x_d,0) \cdot\begin{pmatrix}
w_{-1} &  & & & \\
w_0 & w_{-1}  & & &  \\
w_1 & w_0  & w_{-1} & &  \\
& w_1  & w_0  & \ddots &  \\
&  & w_1  & \ddots & w_{-1}  \\
&  &   & \ddots & w_0  \\
&  &   &  & w_1 \\
\end{pmatrix} = $$

$$= (x_1,\ldots,x_d) \cdot\begin{pmatrix}
w_0 & w_{-1} &  & & & & \\
w_1 & w_0 & w_{-1}  & & & &  \\
& w_1 & w_0  & w_{-1} & &&  \\
& & w_1  & w_0  & \ddots & &  \\
& &  & w_1  & \ddots & w_{-1} &  \\
& &  &   & \ddots & w_0 & w_{-1}  \\
& &  &   &  & w_1 & w_0 \\
\end{pmatrix} = $$

Обозначим последнюю матрицу через $\widehat{W}$, а ядро свёртки через $W$. Что происходит с градиентом при переходе через матричное умножение, мы уже отлично знаем. Градиент по весам равен

$$\nabla_{X_0}\mathcal{L} = \nabla_{X_0\ast W}\mathcal{L}\cdot\widehat{W}^T$$

Разберёмся, что из себя представляет умножение на $\widehat{W}^T$ справа. Эта матрица имеет вид

$$\begin{pmatrix}
w_0 & w_1 &  & & & & \\
w_{-1} & w_0 & w_1  & & & &  \\
& w_{-1} & w_0  & w_1 & &&  \\
& & w_{-1}  & w_0  & \ddots & &  \\
& &  & w_{-1}  & \ddots & w_1 &  \\
& &  &   & \ddots & w_0 & w_1  \\
& &  &   &  & w_{-1} & w_0 \\
\end{pmatrix}$$

Она тоже соответствует свёртке, только:

* с симметричным исходному ядром $(w_1, w_0, w_{-1})$;
* с дополнением вектора $\nabla_{X_0\ast W}$ нулями (это как раз соответствует неполным столбцам: можно считать, что «выходящие» за границы матрицы и отсутствующие в ней элементы умножаются на нули).

**Вопрос на подумать**. Поменяется ли что-нибудь, если исходный вектор не дополнять нулями?

### Общий случай

Рассмотрим теперь двумерную свёртку, для простоты нечётного размера и без свободного члена

$$(X\ast W)_{ijc} = \sum_{p=1}^{c_{\text{in}}}\sum_{k_1 = -k}^k\sum_{k_2=-k}^kW^{c}_{k+1+k_1, k+1+k_2, p}X_{i + k_1, j + k_1, p}$$

1. Продифференцируем по $X_{stl}$:
	
$$
\frac{\partial\mathcal{L}}{\partial X_{stl}} = \sum_{i, j, c}\frac{\partial (X\ast W)_{ijc}}{\partial X_{stl}}\cdot\frac{\partial\mathcal{L}}{\partial(X\ast W)_{ijc}}
$$

Разберёмся с производной $\frac{\partial (X\ast W)_{ijc}}{\partial X_{stl}}$. Во всей большой сумме из определения свёртки для $(X\ast W)_{ijc}$ элемент $X_{stl}$ может встретиться в позициях $X_{i+k_1, j+k_2, l}$ при $i + k_1 = s$, $j + k_2 = t$ и всевозможных $c$, причём это возможно лишь если $k_1 = s - i\in\{-k,\ldots,k\}$, $k_2 = t - j\in\{-k,\ldots,k\}$ (для всех остальных $(X\ast W)_{ijc}$ производная по $X_{stl}$ нулевая). Соответствующий коэффициент при $X_{stl}$ будет равен $W_{k + 1 + k_1, k + 1 + k_2, c}$. Таким образом, производная будет иметь вид:

$$
\frac{\partial\mathcal{L}}{\partial X_{stl}} = \sum_{c=1}^{c_{\text{out}}}\sum_{k_1=-k}^k\sum_{k_2=-k}^kW_{k + 1 + k_1, k + 1 + k_2, c}\cdot\frac{\partial\mathcal{L}}{\partial(X\ast W)_{s - k_1, t - k_2, c}}
$$
Легко заметить, что это тоже свёртка, но поскольку индексы $k_1, k_2$ в $W$ и в $\frac{\partial\mathcal{L}}{\partial(X\ast W)}$ стоят с разными знаками, получаем, что
$$
\color{blue}{\nabla_{X}\mathcal{L} = W\text{[::-1,::-1,:]}\ast\nabla_{X\ast W}\mathcal{L}}
$$

2. Продифференцируем по $W^q_{ab}$:
	
$$
\frac{\partial\mathcal{L}}{\partial W^q_{ab}} = \sum_{i, j, c}\frac{\partial (X\ast W)_{ijc}}{\partial W^q_{ab}}\cdot\frac{\partial\mathcal{L}}{\partial(X\ast W)_{ijc}}
$$

В формуле для $(X\ast W)_{ijc}$ элемент $W^q_{ab}$ может встретиться в позициях $W^q_{k + 1 + k_1, k + 1 + k_2}$, для $k + 1 + k_1 = a$, $k + 1 + k_2 = b$, с коэффициентами $X_{i + k_1, j + k_2, p}$ (для любых $p$). Значит, производная будет иметь вид:

$$
\frac{\partial\mathcal{L}}{\partial W^q_{ab}} = \sum_{p=1}^{c_{\text{in}}}\sum_{i=1}^H\sum_{j=1}^WX_{a - k - 1, b - k - 1, p}\cdot\frac{\partial\mathcal{L}}{\partial(X\ast W)_{a - k - 1, b - k - 1, q}}
$$

В этой формуле тоже нетрудно узнать свёртку:

$$\color{blue}{\nabla_{W}\mathcal{L} = X\ast\nabla_{X\ast W}\mathcal{L}}$$

**Вопрос на подумать**. Если всё-таки есть свободные члены, как будет выглядить градиент по $b_c$?

## Остальные важные блоки свёрточных нейронных сетей

Наигравшись с нашими мысленными экспериментами, давайте обратимся к опыту инженеров и исследователей, который копился с 2012 года – [alexnet](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf). Он поможет нам разобраться  с тем, как эффективней всего строить картиночные нейронки. Здесь будут перечислены самые важные (на момент написания и по мнению автора) блоки.

### Max pool

Каждая из $C$ свёрток очередного свёрточного слоя — это новая карта признаков для нашего изображения, и нам, конечно, хотелось бы, чтобы таких карт было побольше: ведь это позволит нам выучивать больше новых закономерностей.  

Но для картинок в высоком разрешении это может быть затруднительно: слишком уж много будет параметров. Выходом оказалось использование следующей эвристики: сначала сделаем несколько свёрток с $C_1$ каналами, а затем как-нибудь уменьшим нашу карту признаков в 2 раза и одновременно увеличим количество свёрток во столько же.  

Посчитаем, как в таком случае изменится число параметров: было $H \times W \times K \times K \times C_1 \times C_1$, стало $(H/2) \times (W/2) \times K \times K \times (C_1 \times 2) \times (C_1 \times 2) = H \times W \times K \times K \times C_1 \times C_1$, то есть, ничего не изменилось, а количество фильтров удвоилось, что приводит к выучиванию более сложных зависимостей.

Осталось разобраться, как именно можно понижать разрешение картинки. Тривиальный способ — взять все пиксели с нечетными индексами. Такой подход будет работать, но, как может подсказать здравый смысл, выкидывать пиксели — значит терять информацию, а этого не хотелось бы делать.

Здесь есть много вариантов: например, брать среднее/максимум по обучаемым весам в окне `2x2`, которое идет по карте признаков с шагом 2. Экспериментально выяснилось, что максимум — хороший выбор, и, в большинстве архитектур, используют именно его. Обратите внимание, что максимум берется для каждого канала независимо.

Еще одно преимущество — увеличение receptive field. Получается, что он увеличивается в 2 раза:

![21_maxpool_b19a0a26b2_dbd86d51dd.svg](https://yastatic.net/s3/education-portal/media/21_maxpool_b19a0a26b2_dbd86d51dd_acdf90632c.svg)

Операция понижения разрешения со взятием максимума в окне называется **max pooling**, а со взятием среднего — **average pooling**.

**Вопрос на подумать**. Как будет преобразовываться градиент во время error backpropagation для maxpool с окном и шагом 2x2? А для average pool? 

Кстати, ещё один способ уменьшать размер карт признаков по ходу применения свёрточной сети — использование **strided convolution**, в которых ядро свёртки сдвигается на каждом шаге на некоторое большее единицы число пикселей (возможно, разное для осей `H` и `W`; обычная свёртка получается, если установить параметр `stride=(1,1)`).

![gif_conv_template1_5c2a2c90c6.svg](https://yastatic.net/s3/education-portal/media/gif_conv_template1_5c2a2c90c6_8f14127d42.svg)

### Global average pool

Как свёрточные слои, так и пулинг превращают картинку в «стопку» карт признаков. Но если мы решаем задачу классификации или регрессии, то в итоге нам надо получить число (или вектор логитов, если речь про многоклассовую классификацию).  

Один из способов добиться этого — воспользоваться тем, что свёртка без дополнения нулями и пулинг уменьшают размер карты признаков, и в итоге при должном терпении и верном расчёте мы можем получить тензор `1x1xC` (финальные, общие признаки изображения), к которому уже можно применить один или несколько полносвязных слоёв. Или же можно, не дождавшись, пока пространственные измерения схлопнутся, «растянуть» всё в один вектор и после этого применить полносвязные слои (именно так, как мы не хотели делать, не правда ли?). Примерно так и происходило в старых архитектурах (alexnet, vgg).

**Вопрос на подумать**. Попробуйте соорудить конструкцию из свёточных слоёв и слоёв пулинга, превращающую изображение размера `128x128x3` в тензор размера `1x1xC`.

Но у такого подхода есть как минимум один существенный недостаток: для каждого размера входящего изображения нам придётся делать новую сетку.

Позднее было предложено следующее: после скольких-то свёрточных слоёв мы будем брать среднее вдоль пространственных осей нашего последнего тензора и усреднять их активации, а уже после этого строить MLP. Это и есть Global Average Pooling. У такого подхода есть несколько преимуществ:

1. радикально меньше параметров;
2. теперь мы можем применять нейронку к картинку любого размера;
3. мы сохраняем «магию» инвариантности предсказаний к сдвигам.

![23_globalpool_8b3ea2ca37_797f26c48e.svg](https://yastatic.net/s3/education-portal/media/23_globalpool_8b3ea2ca37_797f26c48e_55f0fc2755.svg)


### Residual connection

Оказывается, что, если мы будем бесконтрольно стекать наши свёртки, то, несмотря на использование relu и batch normalization, градиенты все равно будут затухать, и на первых слоях будут почти нулевыми. Интересное решение предлагают авторы архитектуры resnet: давайте будем «прокидывать» признаки на предыдущем слое мимо свёрток на следующем:

![24_residual_bb9168282a_b20cdede5b.svg](https://yastatic.net/s3/education-portal/media/24_residual_bb9168282a_b20cdede5b_7417348fb7.svg)

Таким образом получается, что градиент доплывет даже до самых первых слоев, что существенно ускоряет сходимость и качество полученной модели. Вопрос: почему именно сумма? Может, лучше конкатенировать? Авторы densenet именно такой подход и предлагают (с оговорками), получая результаты лучше, чем у resnet. Однако, такой подход получается вычислительно сложным и редко используется на практике.

## Регуляризация

Несмотря на наши ухищрения со свёртками, в современных нейронных сетях параметров все равно оказывается больше, чем количество картинок. Поэтому часто оказывается важным использовать различные комбинации регуляризаторов, которых уже стало слишком много, чтобы все описывать в этом параграфе, так что мы рассмотрим лишь несколько наиболее важных.

### Классические

Почти все регуляризаторы, которые использовались в классической машинке и полносвязных сетях, применимы и здесь: l1/l2, dropout и так далее.

**Вопрос на подумать**. Насколько разумно использовать dropout в свёрточных слоях? Как можно модифицировать метод, чтобы он стал «более подходящим»?

### Аугментации

Это один из самых мощных инструментов при работе с картинками. Помогает, даже если картинок несколько тысяч, а нейронная сеть с миллионами параметров. Мы уже выяснили, что смещение\поворот\прочее не меняют (при разумных параметрах) факта наличия на картинке того или иного объекта.  

На самом деле, есть огромное множество операций, сохраняющих это свойство:

1. сдвиги, повороты и отражения;
2. добавление случайного гауссового шума;
3. вырезание случайно части картинки (cutout);
4. перспективные преобразования;
5. случайное изменение оттенка\насыщенности\яркости для всей картинки;
6. и многое другое.

Пример хорошой библиотеки с аугментациями: [Albumentations](https://github.com/albumentations-team/albumentations).

### Label smoothing

Часто оказывается, что нейронная сеть делает «слишком уверенные предсказания»: 0.9999 или 0.00001. Это становится головной болью, если в нашей разметке есть шум — тогда градиенты на таких объектах могут сильно портить сходимость.  

Исследователи пришли к интересной идее: давайте предсказывать не one-hot метку, а ее сглаженный вариант. Итак, пусть у нас есть $K$ классов:

$$y_{ohot}=(0, 0, \dots, 1, \dots, 0)$$

$$y_{ls}=\left(\frac{\varepsilon}{k-1},\frac{\varepsilon}{k-1}, \dots, 1-\varepsilon,\frac{\varepsilon}{k-1}, \dots, \frac{\varepsilon}{k-1}\right)$$

$$\sum_i y^i_{ohot}=\sum_i y^i_{ls}=1$$

Обычно берут $\varepsilon=0.1$. Тем самым модель штрафуется за слишком уверенные предсказания, а шумные лейблы уже не вносят такого большого вклада в градиент.

### Mixup

Самый интересный вариант. А что будет, если мы сделаем выпуклую комбинацию двух картинок и их лейблов:

![ml_6_1_b4c9489f19.svg](https://yastatic.net/s3/education-portal/media/ml_6_1_b4c9489f19_1770e11a3e.svg)

где $\alpha$ обычно семплируется из какого-нибудь Бета распределения. Оказывается, что такой подход заставляет модель выучивать в каком-то смысле более устойчивые предсказания, так как мы форсируем некую линейность в отображении из пространства картинок в пространство лейблов. На практике часто оказывается, что это дает значимое улучшение в качестве модели.

## Бонус №1: знаковые архитектуры в мире свёрточных нейронных сетей для задачи классификации изображений

Дисклеймер: это мнение _одного_ автора. Приведённые в этом разделе вехи связаны преимущественно с архитектурами моделей, а не способом их оптимизации.

Здесь перечислены знаковые архитектуры, заметно повлиявшие на мир свёрточных нейронных сетей в задаче классификации картинок (и не только). К каждой архитектуре указана ссылка на оригинальную статью, а также комментарий автора параграфа с указанием _ключевых_ нововведений. Значение метрики error rate на одном из влиятельных датасетов [imagenet](https://image-net.org/) указано для финального ансамбля из нейросетей, если не указано иное.

Зачем это полезно изучить (вместе с чтением статей)? Основных причин две:

1. Общее развитие. Полезно понимать, откуда взялись и чем мотивированы те или иные компоненты.
2. Этот вопрос задают на собеседовании, когда не знают, что еще спросить :)

### lenet (1998)

[Ссылка на статью](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)

7 слоев

Первая свёрточная нейронная сеть, показавшая SOTA (State Of The Art) результаты на задаче классификации изображений цифр MNIST. В архитектуре впервые успешно использовались свёрточные слои с ядром `5x5`. В качестве активации использовался tanh, а вместо max pool в тот момент использовался average.

### alexnet (2012)

[Ссылка на статью](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)

11 слоев

Первая CNN (Convolutional Neural Network), взявшая победу на конкурсе imagenet. Автор предложил использовать ReLU вместо сигмоид (чтобы градиенты не затухали) и популяризовал max-pool вместо average. Что самое важное, обучение модели было перенесено на несколько GPU, что позволило обучать достаточно большую модель за относительное небольшое время (6 дней на двух видеокартах того времени). Также автор обратил внимание, что глубина нейросети важна, так как выключение хотя бы одного слоя стабильно ухудшало качество на несколько процентов. 

### network in network (2013)

[Ссылка на статью](https://arxiv.org/pdf/1312.4400.pdf)

В статье не привели интересных SOTA результатов, но зато ввели два очень популярных впоследствии модуля. Первый — это GAP (Global Average Pooling), который стоит после последнего свёрточного слоя и усредняет все активации вдоль пространственных осей. Второй — стекинг `1x1` свёрток поверх `3x3`, что эквивалентно тому, что вместо линейной свёртки используется полносвязный слой.

### vgg (2014)

[Ссылка на статью](https://www.robots.ox.ac.uk/~vgg/publications/2015/Simonyan15/simonyan15.pdf)

19 слоев

Авторы предложили декомпозировать большие свёртки (`5x5`, `7x7` и выше) на последовательное выполнение свёрток `3x3` с нелинейностями между ними. Впоследствии, за нечастым исключением, свёртки `3x3` стали стандартом в индустрии (вместе со свёртками `1x1`).

### googleLeNet aka Inception (2014)

[Ссылка на статью](https://arxiv.org/pdf/1409.4842.pdf)

22 слоя

Ввели inception слой, просуществовавший довольно продолжительное время. Сейчас сам слой уже не используется, но идея лежащая в его основе, эксплуатируется. Идея следующая: будем параллельно применять свёртки с разным пространственными размерами ядер, чтобы можно было одновременно обрабатывать как low-, так и high-level признаки. Еще полезной для сообщества оказалась идея с dimensionality reduction: перед тяжелой операцией поставим свёртку 1x1, чтобы уменьшить количество каналов и кратно ускорить вычисление.

### batch normalization (2015)

[Ссылка на статью](https://arxiv.org/pdf/1502.03167.pdf)

Авторы внедрили вездесущую batch normalization, которая стабилизирует сходимость, позволяя увеличить шаг оптимизатора и скорость сходимости. Применив идею к архитектуре inception, они превзошли человека на imagenet.

### kaiming weight initialization (2015)

[Ссылка на статью](https://arxiv.org/pdf/1502.01852.pdf)


В статье предложили использовать инициализацию весов, берущую во внимание особенность активации ReLU (в предыдущих работах предполагалось, что $Var[x] = \mathbb{E}[x^2]$, что, очевидно, нарушается для $\hat{x} = max(0, x)$). Применение этой и других «свистелок» на VGG19 позволило существенно уменьшить ошибку на imagenet.

### ResNet (2015)

[Ссылка на статью](https://arxiv.org/pdf/1512.03385.pdf)

152 слоя

Архитектура, которая на момент написания этого параграфа до сих пор бейзлайн и отправная точка во многих задачах. Основная идея — использование skip connections, что позволило градиенту протекать вплоть до первых слоев. Благодаря этому эффекту получилось успешно обучать очень глубокие нейронные сети, например, с 1202 слоями (впрочем, результаты на таких моделях менее впечатляющие, чем на 152-слойной). После этой статьи также стали повсеместно использоваться GAP и уменьшение размерности свёртками `1x1`.

### MobileNet (2017)

[Ссылка на статью](https://arxiv.org/pdf/1704.04861.pdf)

Очень популярная модель для быстрого инференса (на мобильных устройствах или gpu). По качеству хоть и немного проигрывает «монстрам», но в индустрии, оказывается, зачастую этого достаточно (особенно если брать последние варианты модели).  

Основная деталь — это использование depthwise convolutions: параллельный стекинг свёрток `3x3x1x1` — то есть таких, в которых вычисление для каждого $с_{\text{out}}$ канала просходит только на основе признаков одного $c_{\text{in}}$ канала. Чтобы скомбинировать фичи между каналами, используется классическая `1x1` свёртка.

### EfficientNet (2019)

[Ссылка на статью](https://arxiv.org/pdf/1905.11946.pdf)

Одна из первых моделей, полученных при помощи NAS (Neural Architecture Search), которая взяла SOTA на imagenet. После этого, модели, где компоненты подбирались вручную, уже почти не показывали лучших результатов на классических задачах.

## Бонус №2: не классификацией единой

Свёрточными нейронными сетями можно решать большой спектр задач, например:

1. Сегментация. Если убрать в конце слои GlobalAveragePool или flatten, то можно делать предсказания для каждого пикселя в отдельности (подумайте, что делать, если в сети есть maxpool) — получаем сегментацию картинки. Проблема — долгая и дорогая разметка.
2. Детекция. Часто намного дешевле получить разметку объектов обрамляющими прямоугольниками. Здесь уже можно для каждого пикселя предсказывать размеры прямоугольника, который обрамляет объект, к которому принадлежит пиксель. Проблемы — нужен этап агрегации прямоугольников + много неоднозначностей во время разметки + много эверистик на всех этапах + данных нужно больше.
3. Понимание видео. Добавляем в тензор новый канал — временной, считаем четырехмерные свёртки — и получаем распознавание сцен на видео.
4. Metric learning. Часто мы не можем собрать все интересующие нас классы, например, в задаче идентификации человека по лицу (или товара на полке). В этом случае используют такой трюк: научим модель в некотором смысле (обычно по косиносному расстоянию) разделять эмбеддинги существующих классов (уникальных людей). Если на руках была репрезентативная выборка, то модель, скорее всего (а обычно — всегда), выучит генерировать дискриминативные эмбеддинги, которые уже позволят различать между собой ранее невиданные лица.
5. и многое другое

## Итого

Мы разобрались, что для картинок эффективно использовать свёрточные фильтры в качестве основных операторов. Выяснили, какие основные блоки есть почти в каждой картиночной нейронной сети и зачем они там нужны. Разобрались, какие методы регуляризаторы сейчас самые популярные и какая за ними идея. 

И наконец — рассмотрели знаковые архитектуры в мире свёрточных нейронных сетей.

  ## handbook

  Учебник по машинному обучению

  ## title

  Свёрточные нейросети

  ## description

  *null*

- 
  ## path

  /handbook/ml/article/nejroseti-dlya-raboty-s-posledovatelnostyami

  ## content

  В этом разделе вы познакомитесь с нейросетями для работы с данными, имеющими вид последовательностей некоторых токенов. Это может быть музыка или видео, временные ряды или траектория движения робота, последовательности аминокислот в белке или много чего ещё, но одним из самых богатых источников таких данных является Natural Language Processing (NLP).

Как следует из названия, Natural Language Processing (обработка естественного языка) — это область data science, посвященная анализу текстов, написанных на естественных (человеческих) языках. С задачами обработки текста мы встречаемся каждый день, например, когда просим Siri или Алису включить любимую песню или добавить напоминание в календарь, когда используем автодополнение при вводе поискового запроса или проверяем орфографию и пунктуацию с помощью специальных программ.

Вот ещё несколько примеров задач, относящихся к обработке естественного языка:

* классификация документов (по темам, рубрикам, жанрам и так далее);
* определение спама;
* определение частей речи;
* исправление орфографических ошибок и опечаток;
* поиск ключевых слов, синонимов / антонимов в тексте;
* распознавание именованных сущностей (имен, названий географических объектов, дат, номеров телефонов, адресов);
* определение эмоциональной окраски текста (sentiment analysis);
* поиск релевантных документов по запросу, а также их ранжирование;
* задача суммаризации (автоматическое составление краткого пересказа текста);
* автоматический перевод с одного языка на другой (машинный перевод);
* диалоговые системы и чат-боты;
* вопросно-ответные системы (выбор ответа из нескольких предложенных вариантов или вопросы с открытым ответом);
* кроме того, к NLP также относят задачу распознавания речи (Automated Speech Recognition, ASR).

Для работы с такими данными есть несколько возможных режимов:

1. **Many-to-one**. На вход подается последовательность объектов, на выходе один объект. Пример 2: классификация текстов или видео. Пример 2: тематическая классификация. По предложению нефиксированной длины генерируем вектор вероятностей упоминания заранее фиксированных тем во входном предложении. Размерность выходного вектора постоянна и равна количеству тем.
2. **One-to-many**. На вход подается один объект, на выходе последовательность объектов. Пример: генерация заголовка к изображению (image captioning).
3. **Many-to-many**. На входе и выходе последовательности нефиксированной длины. Примеры: машинный перевод, суммаризация текста, генерация заголовка к статье.
4. **Синхронизированный вариант many-to-many**. На входе и выходе последовательности одинаковой длины, токены входной явно сопоставлены соответствующим токенам выходной. Пример: генерация покадровых субтитров к видео, PoS-tagging (part of speech tagging, для каждого слова в предложении предсказываем, что это за часть речи).

![sec](https://yastatic.net/s3/education-portal/media/sec_architectures_b7bc32c3d6_1_b98e97b208_f99654e141.svg)

Мы начнём с архитектур, в которых размер выхода предсказуемым образом зависит от размера входа: many-to-one и синхронизованном варианте many-to-many — но в итоге доберёмся и до остальных.

## Word Embeddings

Перед тем, как рассказать об архитектурах, которые часто используются для работы с текстами, надо разобраться, каким образом можно кодировать текстовые данные: ведь нужно их превратить во что-то векторное, прежде чем подавать на вход нейросети. К векторизации текстов есть два базовых подхода:

* векторизовать текст целиком, превращая его в один вектор;
* векторизовать отдельные структурные единицы, превращая текст в последовательность векторов.

Первые, статистические подходы к векторизации следовали первому подходу и рассматривали текст как неупорядоченный набор («мешок») токенов (обычно токенами являются слова). Тем самым, тексты «Я не люблю ML» и «Я люблю не ML» получали одинаковые векторизации, то есть по ходу терялась существенная информация. Поэтому мы лишь коротко упомянем о них.

{% cut "Немного о статистических подходах" %}

Самый очевидный вариант так и называется — Bag-of-Words («мешок слов»). Текст предлагается представить в виде вектора частот встречаемости каждого токена, кроме элементов заранее заданного списка «стоп-слов», в которые обычно включают самые вездесущие токены: личные местоимения, артикли и так далее.

Чуть более усложненной версией является TF-IDF (Term Frequency-Inverted Document Frequency). Этот подход использует не только информацию из текста, но и пытается соотнести её с *контекстом* — остальными текстами из имеющейся у нас коллекции $D$. Представление текста $d$ состоит из произведений $TF(t, d)\cdot IDF(t, D)$ по всем токенам $t$. Разберёмся отдельно с каждым из сомножителей:

* $TF(t,d) = \dfrac{n_t}{\sum_k n_k}$, где $n_t$ — число вхождений токена $t$ в документ, а в знаменателе стоит общее число слов в данном документе $d$. Это частота вхождения токена в документ.
* $IDF(t,D) = \log \dfrac{\vert D \vert}{\vert\{d_i \in D \vert t \in d_i\}\vert}$, где $\{d_i \in D \vert t \in d_i\}$ — число документов в текстовой коллекции $D$, в которых встречается слово $t$. Этот множитель штрафует компоненты, отвечающие слишком распространённым токенам, и повышает вес специфических для отдельных текстов (и, вероятно, информативных) слов.

{% endcut %}

Обратимся теперь к другому подходу и подумаем, как сопоставить векторы (**эмбеддинги**) словам.

Допустим, что у нас одно и то же слово будет представлено одним и тем же вектором во всех текстах и в любых позициях. Как заключить в векторе его смысл, содержающуюся в нём информацию? Ответ предвосхищает одну из основных идей обучения представлений: нужно использовать *контекст*. Если, читая книгу на иностранном языке, вы встречаете незнакомое слово, вы нередко можете угадать его значение по контексту, что оно значит. Можно сказать, что смысл слова — это те слова, которые встречаются с ним рядом.

Одним из воплощений такого подхода является Word2vec. Впервые он был предложен Т.Миколовым в 2013 году в [статье](https://arxiv.org/abs/1301.3781) Efficient Estimation of Word Representations in Vector Space.

Для обучения авторы предложили две стратегии: Skip-gram и CBOW (Сontinuous bag-of-words):

* В архитектуре CBOW модель учится предсказывать данное (центральное) слово по контексту (например, по двум словам перед данным и двум словам после него).
* В архитектуре Skip-gram модель учится по слову предсказывать контекст (например, каждого из двух соседей слева и справа);

![cbow](https://yastatic.net/s3/education-portal/media/cbow_vs_skipgram_e3e32c275b_pngcbow_vs_skipgram_e3e32c275b_64c438dc03_0b778979eb.svg)

Авторы предложили для каждого слова $w$ обучать два эмбеддинга: $\color{#FFC100}{v_u}$ и $\color{#97C804}{v_{w}}$, первое из которых используется, когда $w$ является центральным, а второе — когда оно рассматривается, как часть контекста. В модели CBOW при фиксированном контексте $\color{#97C804}{\text{context}}$ вычисляются логиты

$$logits_u = \langle\sum_{w\in\color{#97C804}{\text{ context }}}\color{#97C804}{v_{w}},\color{#FFC100}{v_u}\rangle
$$

после чего «вероятности» всевозможных слов $u$ быть центральным словом для контекста $\color{#97C804}{\text{context}}$ вычисляются как $\text{softmax}(logits)$. Модель учится с помощью SGD на кросс-энтропию полученного распределения с истинным рапределением центральных слов.

![CBOW](https://yastatic.net/s3/education-portal/media/CBOW_a659160b42_1_ee8406db0b_a4873a5818.svg)

В модели Skip-gram по данному центральному слову $u$ для каждой позиции контекста $\color{#97C804}{\text{context}}$ независимо предсказывается распределение вероятностей. В качестве функции потерь выступает сумма кросс-энтропий распределений слов контекста с их истинными распределениями.

![Skip](https://yastatic.net/s3/education-portal/media/Skip_gram_b385df32ae_2_ba3f7892c2_56dcf3930a.svg)

Размерность эмбеддинга в каждой из архитектур — это гиперпараметр и подбирается эмпирически. В оригинальной [статье](https://arxiv.org/abs/1301.3781) предлагается взять размерность эмбеддинга 300. Полученные представления центральных слов могут дальше использоваться в качестве эмбеддингов слов, которые сохраняют семантическую связь слов друг с другом.

Мы не будем здесь останавливаться подробно на деталях работы Word2vec и его современных модификациях и предложим читателю обратиться к соответствующей лекции в [учебнике](https://lena-voita.github.io/nlp_course/word_embeddings.html) Лены Войта по NLP. А мы лишь продемонстрируем, что он работает.

**Примеры**. Возьмём несколько слов и посмотрим, как выглядят топ-10 слов, ближайших к ним в пространстве эмбеддингов (обученных на одном из датасетов Quora Questions с помощью word2vec):

1. **quantum**: electrodynamics, computation, relativity, theory, equations, theoretical, particle, mathematical, mechanics, physics;
2. **personality**: personalities, traits, character, persona, temperament, demeanor, narcissistic, trait, antisocial, charisma;
3. **triangle**: triangles, equilateral, isosceles, rectangle, circle, choke (догадаетесь, почему?), quadrilateral, hypotenuse, bordered, polygon;
4. **art**: arts, museum, paintings, painting, gallery, sculpture, photography, contemporary, exhibition, artist.

**Вопрос на подумать**. В реальных текстах наверняка будут опечатки, странные слова и другие подобные неприятности. Word2vec же учится для фиксированного словаря. Что делать, если на этапе применения вам попадается неизвестное слово? Да и вообще, хорошо ли учить вложения для редких слов или слов с нетривиальными опечатками, которые, может быть, только раз встретятся в тексте?

{% cut "Ответ (не открывайте сразу; сначала подумайте сами!)" %}

Начнём с последнего вопроса: наверное, не очень хорошо. Словарь может получиться слишком большим и займёт всю оперативную память мира. Нередко для достаточно редких слов вовсе не учат специального эмбеддинга, вместо этого для всех них обучая представление одного единственного токена `UNK` (unknown). В таком случае и всем незнакомым словам, встреченным на этапе применения, также можно сопоставить этот же эмбеддинг.

В реальных сервисах, имеющих дело с текстами (например, в автоматических переводчиках) зачастую вовсе не имеют дела со словами, предпочитая дополнительно разбивать их на subword units. Самым популярным на данный момент решением является BPE (Byte pair encoding). Верхнеуровневая идея состоит в том, что мы фиксируем размер словаря (обычно не очень большой, несколько тысяч или десятков тысяч единиц), добавляем в него все символы, после чего повторяем, пока словарь не заполнится:

* находим самую часто встречающуюся вместе пару токенов;
* добавляем их конкатенацию в словарь.

Более подробно о BPE вы можете прочитать в [учебнике](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html#bpe) Лены Войта.

{% endcut %}

**Вопрос на подумать**. В некоторых случаях всё же полезно уметь строить эмбеддинги не отдельных слов, а текстов (например, для поиска похожих документов). Можете ли вы, вдохновившись идеей word2vec, придумать более тонкий способ сделать это, чем BoW или TF-IDF?

{% cut "Ответ (не открывайте сразу; сначала подумайте сами!)" %}

Самый простой способ это сделать — сложить или усреднить эмбеддинги отдельных слов, но так мы теряем их порядок, и вместе с ним значительную часть смысла (очень похоже на bag-of-words, не так ли?). Другой подход, развивающий идеи word2vec, был предложен в другой<a href="https://arxiv.org/abs/1405.4053"> статье</a> Т. Миколова и носит название doc2vec. Он также имеет CBOW-подобную и Skip-gram-подобную версии. Остановимся на первой. Если в оригинальном CBOW мы предсказывали центральное слово по контексту, то теперь добавляется ещё дополнительный вектор $\color{#5002A7}{v_{document}}$, уникальный для каждого документа и кодирующий присутствующую в нём смысловую специфику:

![doc2vec](https://yastatic.net/s3/education-portal/media/doc2vec_eff509e41a_4789671af6.webp)

{% endcut %}

## Рекуррентные нейронные сети

Итак, мы представили текст в виде последовательности векторов, соответствующих словам или их кусочкам. Как с ней работать? Один из вариантов мы уже рассматривали: можно посмотреть на последовательность из $k$ векторов размерности $d$ как на «изображение» $k\times 1$ с $d$ «каналами», после чего использовать уже знакомые нам свёрточные нейросети, только с одномерными свёртками вместо двумерных.

В каких-то случаях это действительно будет работать, но всё же есть несколько сомнительных моментов:

* Хотя изображения тоже могут быть разного размера, всё же в датасете редко попадаются рядом картинки $1920\times 1080$ и $3\times 3$, а среди, скажем, отзывов на ресторан могут попадаться как труды, сопоставимые по размеру с «Войной и миром», так и безликие «Да, вроде норм». И если обработать слишком длинное предложение нам поможет (с потерей информации, конечно) global pooling, слишком короткое может что-нибудь поломать, особенно если мы забываем про паддинг.
* Слегка философское соображение. Изображение однородно, в нём нет предпочтительных направлений, тогда как текст пишется и читается последовательно. Нам может показаться, что это стоит использовать: при обработке очередного токена обращаться к предыдущим, как к его контексту.

В последнем соображении уже непосредственно видна идея рекуррентных нейронных сетей (recurrent networks, RNN):

![ecurrent](https://yastatic.net/s3/education-portal/media/ecurrent_many_to_many_08a3506cd8_fb323d4098_5eecbe06a7.svg)

Давайте разберёмся, что тут происходит. Чтобы хранить информацию о предыдущих токенах, мы вводим понятие внутренней памяти или **скрытого состояния** (**hidden state**, векторы $\color{#5002A7}{h_n}$). В простейшем случае оно выражается одним вектором фиксированной размерности. На каждом (дискретном) шаге в сеть подаются данные (например, эмбеддинг токена), при этом происходит обновление скрытого состояния.

Пример:

$$\color{#5002A7}{h_n} = \text{tanh}(\color{#5002A7}{h_{n-1}}\color{#292183}{W_1} +  \color{#97C804}{x_n}\color{#292183}{W_2})
$$

после чего по скрытому состоянию предсказывается выходной сигнал, к примеру, следующим образом:

$$\color{#FFC100}{y_n} =  \color{#5002A7}{h_n}\color{#292183}{W_3}
$$

Обратите внимание, что веса $\color{#292183}{W_i}$ одинаковы на всех итерациях, то есть вы можете представлять себе, что очередные $\color{#97C804}{x_n}$ и $\color{#5002A7}{h_{n-1}}$ подаются на вход одного и того же слоя, зацикленного на себе.

Рекуррентную сеть можно обучать на ошибку, равную суммарному отклонению по всем выходных сигналам $\color{#FFC100}{y_n}$ нашей сети.

**Вопрос на подумать**. Как инициализировать веса $\color{#292183}{W_i}$ мы, наверное, понимаем (про это можно почитать в [параграфе](https://academy.yandex.ru/handbook/ml/article/tonkosti-obucheniya) про тонкости обучения нейросетей). А как инициализировать начальное скрытое состояние $\color{#5002A7}{h_0}$? Можно ли инициализировать его нулём?

{% cut "Ответ (не открывайте сразу; сначала подумайте сами!)" %}

Важно разобраться, что в данном случае значит «инициализировать». Сравнение с инициализацией $\color{#292183}{W_i}$ не работает, поскольку $\color{#5002A7}{h_0}$ пока не объявлялся обучаемым параметром (хотя его, конечно, можно и обучать, если очень хочется). Можно всегда полагать $\color{#5002A7}{h_0}$ нулём: как во время обучения, так и во время применения. И так действительно можно делать: в конце концов, скрытое состояние хранит информацию о предыдущих элементов последовательности, а до первого шага никакой информации нет.

{% endcut %}

Нетрудно представить себе и нейросеть с несколькими рекуррентными слоями: первый слой RNN будет принимать на вход исходную последовательность, вторая RNN — выходы первой сети, третья — выходы второй и т.д. Такие сети называют глубокими рекуррентными сетями.

Вот пример схему глубокой рекуррентной сети:

![recurrent](https://yastatic.net/s3/education-portal/media/recurrent_many_layers_a91371b370_b5fae618d9_210eb6cabb.svg)

Вы, наверное, заметили, что описанная выше архитектура RNN решает синхронизованную версию задачи many-to-many. Её, впрочем, легко переделать для решения задачи many-to-one: достаточно убрать все выходы, кроме последнего:

![recurrent](https://yastatic.net/s3/education-portal/media/recurrent_many_to_one_94825f5fbf_b9c2eb6d43_a53a1c465e.svg)

### Bidirectional RNN

Стандартная RNN учитывает только предыдущий контекст. Но ведь слово в предложении связано не только с предыдущими, но и с последующими словами. В таких случаях имеет смысл использовать двунаправленную рекуррентную сеть (bidirectional RNN, BRNN).

Как следует из названия, в bidirectional RNN есть две рекуррентных подсети: прямая (forward, токены в нее подаются от первого к последнему) и обратная (backward, токены подаются в обраттном порядке).

Вот пример такой архитектуры:

![recurrent](https://yastatic.net/s3/education-portal/media/recurrent_bidirectional1_f474990a9d_ca882b2008_25a0ba0240.svg)

Конечно, формула для $\color{#FFC100}{y_n}$ может быть и другой. Например, выходы обеих рекуррентных сетей могут агрегироваться путем усреднения, или суммирования, или любым другим способом.

Обратите внимание, что двунаправленная рекуррентная сеть работает с входом фиксированного размера, и по-прежнему не может решать не синхронизованный вариант задачи many-to-many. Backward RNN должна точно знать, где заканчивается входная последовательность, чтобы начать её обрабатывать с конца. Зато такая архитектура может помочь в решении задачи определения именованных сущностей или частей речи, использоваться в качестве энкодера в машинном переводе и так далее.

### Взрыв и затухание градиента в RNN

При всех неоспоримых плюсах описанной выше глубокой рекуррентной архитектуры, на практике обычно используется её модифицированный вариант, который позволяет бороться с проблемой затухания или зашкаливания (взрыва) градиентов. Давайте разберёмся подробнее, почему она возникает.

Рассмотрим функцию потерь $\mathcal{L}_n = L(y_n, \hat{y}_n)$, измеряющую отклонение предсказанного $n$-го выхода от истинного (напомним, что архитектура many-to-many обучается на $\sum_{n=1}^N \mathcal{L}_n$, а архитектура many-to-one — на $\mathcal{L}_N$). Выход $y_n$ зависит от скрытого состояния $h_n$, а то, в свою очередь, от всех $h_i, i < n$. Обновление градиента при переходе через преобразование $h_i = \text{tanh}(h_{i-1}W_1 + x_iW_2)$ имеет, как мы хорошо знаем, вид

$$\nabla_{h_{i-1}}L = \left(\nabla_{h_{i-1}}L\right)W_1^T\odot \text{tanh}'(h_{i-1}W_1 + x_iW_2) = 
$$

То есть в ходе вычисления $\nabla_{W_1}\mathcal{L}_n$ мы $(n-1)$ раз будем умножать на $W_1^T$. Если у $W_1^T$ есть собственные значения, по модулю большие $1$, и нам не посчастливится попадать в их окрестность, градиент будет стремиться к бесконечности («взрываться»).

Такие градиенты делают обучение нестабильным, а в крайнем случае значения весов могут стать настолько большими, что произойдет численное переполнение, и значения весов перестанут обновляться. Если же у $W_1^T$ есть маленькие собственные значения, градиент может затухать. В любом случае, эти проблемы делают получение информации от далеких по времени состояний затруднительным.

Теоретические выкладки о том, почему RNN без модификаций не могут достаточно хорошо учитывать долговременные зависимости, появились ещё в 90х. Их можно прочесть в [статье](http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf) Y.Bengio, 1994 или [диссертации](http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf)  Josef Hochreiter, 1991.

Но как бороться с этой проблемой?

Простым инженерным решением является **gradient clipping**. Эта техника устанавливает максимально возможное значение градиента и заменяет все значения выше выбранного порога на это значение. При обратном распространении ошибки пробрасывается «ограниченный» градиент:

$$\|\nabla \mathcal{L}\|_{clip} =
\begin{cases}
	\|\nabla\mathcal{L}\|,\ \text{if}\ \|\nabla \mathcal{L}\| < \tau \\
       	\tau,\ \text{otherwise}
\end{cases}$$

где $\tau$ — гиперпараметр, подбираемый порог.

Но сам по себе **gradient clipping** это довольно грубый инструмент. Поэтому были придуманы сложные модификации рекуррентных сетей, позволяющие им выучивать длинные зависимости.

### LSTM

Вдохновение при написании этого параграфа черпалось из [статьи](https://colah.github.io/posts/2015-08-Understanding-LSTMs) в блоге исследователя Кристофера Олаха, из него же взяты иллюстрации.

Сеть с долговременной и кратковременной памятью (Long short term memory, LSTM) частично решает проблему исчезновения или зашкаливания градиентов в процессе обучения рекуррентных сетей методом обратного распространения ошибки. Эта архитектура была [предложена](http://www.bioinf.jku.at/publications/older/2604.pdf) Hochreiter & Schmidhuber в 1997 году. LSTM построена таким образом, чтобы учитывать долговременные зависимости.
Рассмотрим подробнее архитектуру LSTM.

Все рекуррентные сети можно представить в виде цепочки из повторяющихся блоков. В RNN таким блоком обычно является один линейный слой с гиперболическим тангенсом в качестве функции активации. В LSTM повторяющийся блок имеет более сложную структуру, состоящую не из одного, а из четырех слоев. Кроме скрытого состояния $h_n$, в LSTM появляется понятие состояния блока (cell state, $c_n$).

Cell state $c_n$ будет играть роль внутренней, закрытой информации LSTM-блока, тогда как скрытое состояние $h_n$ теперь становится передаваемым наружу (не только в следующий блок, но и на следующий слой или выход всей сети) значением. LSTM может добавлять или удалять определенную информацию из cell state с помощью специальных механизмов, которые называются **gates** (ворота или вентили в русскоязычной литературе).

Рассмотрим этот механизм подробнее.

Основное назначение вентиля — контролировать количество проходящей через него информации. Для этого матрица, проходящая по каналу, который контролирует вентиль, поточечно умножается на выражение вида

$$\sigma(W_1 h_{n-1} + W_2 x_n)
$$

Сигмоида выдает значение от $0$ до $1$. Оно означает, какая доля информации сможет пройти через вентиль. Рассмотрим типы гейтов в том порядке, в каком они применяются в LSTM.

**Forget gate** (вентиль забывания). Он позволяет на основе предыдущего скрытого состояния $h_{t-1}$ и нового входа $x_t$ определить, какую долю информации из $c_{n-1}$ (состояния предыдущего блока) стоит пропустить дальше, а какую забыть.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/img6_18a64eb712_39fef32627.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    LSTM: вентиль забывания.<a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs">Источник</a>
  </figcaption>
</figure>

Доля $f_t$ сохраняемой информации из $c_{t-1}$ вычисляется следующим образом:

$$f_t = \sigma(h_{t-1} W^f_1 + x_t W^f_2 + b_f)
$$

Дальше $f_t$ поэлементно умножается на $c_{t-1}$.

Следующий шаг — определить, что нового мы внесём в cell state. Для этого у нас есть отличная кандидатура — уже привычное:

$$\tilde{C_t} = \text{tanh}(h_{t-1}W^С_1 + x_tW^C_2 + b_c)
$$

Но мы не уверены, что вся эта информация достаточно релевантна и достойна переноса в cell state, и хотим взять лишь некоторую её долю. Какую именно — поможет узнать наш следующий персонаж.

**Input gate** (вентиль входного состояния). Вычислим

$$i_t = \sigma(h_{t-1}W^i_1 + x_tW^i_2 + b_i)
$$

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/img7_de3b5ff967_3d4bc6d106.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    LSTM: вентиль входного состояния. <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs">Источник</a>
  </figcaption>
</figure>

и умножим почленно на $\tilde{c_t}$, чтобы получить информацию, которая поступит в cell state от $h_{t-1}$ и $x_t$. А именно, новое состояние cell state будет равно:

$$c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c_t}
$$

где $\odot$ — это поэлементное умножение. Первое слагаемое отвечает за «забывание» нерелевантной информации из $c_{t-1}$, а второе — за привнесение новой, релевантной.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/img8_2d75ae0b16_a128461583.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    LSTM: обновление состояния блока. <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs">Источник</a>
  </figcaption>
</figure>

Как мы уже отмечали, роль выходного вектора LSTM-блока будет играть $h_n$. Он вычисляется по cell state с помощью последнего вентиля.

**Output gate** (вентиль выходного состояния). Он отвечает на вопрос о том, сколько информации из cell state следует отдавать на выход из LSTM-блока. Доля вычисляется следующим образом:

$$o_t = \sigma(h_{t-1}W^o_1 + x_tW^o_2 + b_o)
$$

Теперь пропускаем cell state через гиперболический тангенс, чтобы значения были в диапазоне от $-1$ до $1$, и умножаем полученный вектор на o_n, чтобы отфильтровать информацию из cell state, которую нужно подать на выход:

$$h_t = o_t \odot \text{tanh}(c_t)
$$

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/img9_dda717d896_3c3f8599f2.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    LSTM: вентиль выходного состояния. <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs">Источник</a>
  </figcaption>
</figure>

Описанная архитектура выглядит несколько сложно. Кроме того, вычисление четырех различных типов гейтов может быть вычислительно невыгодным. Поэтому были разработаны различные вариации LSTM, одна из самых популярных (Gated Recurrent Unit, GRU) освещена ниже.

#### Gated Recurrent Unit (GRU)

Gated Recurrent Unit был предложен в [статье](https://arxiv.org/pdf/1406.1078v3.pdf) Cho et al. в 2014 году. GRU объединяет input gate и forget gate в один **update gate**, также устраняет разделение внутренней информации блока на hidden и cell state. Вот общий вид GRU-блока:

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/img10_27a924e83a_a86b927eeb.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    GRU. <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs">Источник</a>
  </figcaption>
</figure>

Внимательно посмотрев на структуру LSTM, можно заметить, что функции forget gate и input gate похожи. Первый механизм определяет, какие значения $c_{t-1}$ надо забыть, а второй — какие значения нового вектора $\tilde{c_t}$ нужно использовать для обновления старого cell state $c_{t-1}$. Давайте объединим эти функции воедино: грубо говоря, будем забывать только те значения, которые собираемся обновить. Такую роль в GRU выполняет update gate ($z_t$):

$$z_t = \sigma(h_{t-1}W^z_1 + x_tW^z_2 + b_z)
$$

Новый тип гейта, который появляется в GRU — **reset gate** ($r_t$). Он определяет, какую долю информации из $h_{t-1}$ с прошлого шага надо «сбросить», инициализировать заново.

$$r_t = \sigma(h_{t-1}W^r_1 + x_tW^r_2 + b_r)
$$

Теперь мы вычисляем потенциальное обновление для скрытого состояния

$$\tilde{h}_n = tanh((r_t\odot h_{t-1})W^h_1 + x_tW^h_2 + b_{h})
$$

и, наконец, решаем, что из старого забыть, а что из нового добавить:

$$h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t
$$

В итоге GRU имеет меньше параметров, чем LSTM (в GRU нет output gate) и при прочих равных, быстрее учится. GRU и LSTM показывают сопоставимое качество на многих задачах, включая генерацию музыки, распознавание речи, многие задачи обработки естественного языка.

Модификации RNN, которые помогают лучше моделировать долгосрочные зависимости (LSTM, GRU) — важная веха развития нейросетей в NLP. Следующий большой этап в развитии — механизм внимания — мы рассмотрим чуть ниже.

## Seq2seq

Вы, должно быть обратили внимание, что мы пока не касались задач, связанных с порождением последовательностей (синхронизованный варианты many-to-many не в счёт).

Действительно: имевшиеся у нас пока инструменты не позволяли генерировать последовательности произвольной длины. Но как тогда переводить с одного языка на другой? Ведь мы не знаем, какой должна быть длина перевода фразы, да и однозначного соответствия между словами исходного предложения и его перевода обычно нет.

Естественным решением для задачи sequence-to-sequence (seq2seq) является использование архитектуры **энкодер-декодер**, состоящей из кодировщика (**энкодера**) для кодирования информации об исходной последовательности в **контекстном векторе** (context vector) и декодировщика (**декодера**) для превращения закодированной энкодером информации в новую последовательность.

![prob](https://yastatic.net/s3/education-portal/media/prob_ML_1_NN_copy_653b463de1_b4f83f4eda.svg)

Очевидным выбором на роль энкодера и декодера являются рекуррентные сети, например, LSTM. Простейшая архитектура будет иметь вид:

![Teacher](https://yastatic.net/s3/education-portal/media/Teacher_forcing_crop_fbebd95d04_89999c96e1.gif)

Рассмотрим подробнее энкодер и декодер.

Энкодер читает входное предложение токен за токеном и обрабатывает их с помощью блоков рекуррентной сети. Hidden state последнего блока становится контекстным вектором. Часто энкодер читает предложение в обратном порядке. Это делается для того, чтобы последний токен, который видит энкодер, совпал (или примерно совпал) с первыми токенами, которые будет генерировать декодер. Таким образом, декодеру проще начать процесс воссоздания предложения. Несколько первых правильных токенов сильно упрощают процесс дальнейшей генерации.

Архитектура декодера аналогична энкодеру. При этом каждый блок декодера должен учитывать токены, сгенерированные к текущему моменту, и также информацию о предложении на исходном языке. Вектор скрытого состояния в нулевом блоке декодера ($g_0$) инициализируется с помощью контекстного вектора.

Таким образом, декодер получит сжатое представление исходного предложения. Предложение генерируется следующим образом: в первый блок подаем метку начала последовательности (например, <BOS>-токен, begin of sentence), на выходе первого блока получаем первый токен новой последовательности, и затем подаем его на вход следующего блока декодера. Повторяем аналогичную процедуру до тех пор, пока не сгенерируется метка конца последовательности (например, <EOS>, end of sentence) или не будет достигнута максимально возможная длина предложения. Таким образом, декодер работает в режиме языковой модели, генерируя предложение токен за токеном и учитывая предыдущий контекст.

Разумеется, энкодер может быть и более сложным. Например, можно использовать многослойную двунаправленную сеть, лишь бы выходом её был один вектор контекста. С декодером сложнее: он должен порождать слова по одному, в одном направлении.

Далее мы очень коротко остановимся на нетривиальных моментах обучения и применения такой модели.

### Тонкости применения

В предыдущих разделах мы не останавливались подробно на том, что происходит с выходами $y_n$, но сейчас всё-таки попробуем разобраться. Если мы решаем задачу машинного перевода, то на очередном этапе декодер выдаёт нам условное распределение

$$p(y_n\mid x, y_{<n})
$$

на словах (или каких-то subword unit, например, BPE), из которого мы будем выбирать самое вероятное слово $y_n$ и подавать его на вход следующего блока. Но эта, жадная, стратегия может и подвести. Легко представить себе ситуацию, в которой самое вероятное на данный момент слово приведёт дальше к менее вероятной подпоследовательности:

![why](https://yastatic.net/s3/education-portal/media/why_beam_search_0fc77ca043_849022bc86_f40d132234.svg)

Чтобы справиться с этим, на этапе применения модели используют **beam search**. В каждый момент времени мы поддерживаем некоторое количество $B$ самых вероятных гипотез, на $n$-м шаге пытаясь продолжать все сохранённые, а из продолжений выбирая топ-$B$ по метрике

$$\prod_{t=1}^np(y_t\mid x, y_{<t})
$$

![beam](https://yastatic.net/s3/education-portal/media/beam_search_cc5e683c74_24ea396a53_0f1d106ca2.gif)

Число $B$ нет смысла делать большим (это и вычислительно будет тяжко, и может привести к более плохим результатам), можете брать в пределах $10$.

### Тонкости обучения

Как уже было сказано выше, на каждом шаге декодер предсказывает распределение вероятностей $p(y_n\mid x, y_{<n})$. Вся модель учится на сумму по всем $n$ кросс-энтропиям этих распределений с истинными $y_n$.

Одна из сложностей такого обучения состоит в том, что единожды ошибившись и предсказав неправильный $\widehat{y}_n$ вместо истинного $y_n$, модель скорее всего и следующие токены предскажет неверно, а это сделает всё дальнейшее обучение малополезным: ведь мы будем учить декодер предсказывать правильное продолжение неправильного начала. Одним из способов борьбы с этим является **teacher forcing**. Суть его в том, что на этапе обучения мы подаём на вход декодера не предсказанный им на предыдущем этапе токен, а истинный:

![teacher](https://yastatic.net/s3/education-portal/media/teacher_forcing_a7ff9db2da_c679a636d5_1309a466d6.svg)

### А как же one-to-many?

У нас остался лишь один неразобранный тип задач: one-to-many. К счастью, чтобы с ним справиться, ничего нового не нужно: достаточно уже знакомой модели энкодер-декодер, лишь с корректировкой энкодера.

Рассмотрим для примера задачу генерации подписей к изображениям (image captioning). Если мы уже умеем как-то превращать картинки в векторы, то эти векторы мы можем напрямую подавать в декодер в качестве векторов контекста:

![one](https://yastatic.net/s3/education-portal/media/one_to_many1_ebca52150a_49f363a9ac_e4ce962093.svg)

Более подробно о том, как строить векторизации для изображений, вы узнаете в параграфе про [обучение представлений](https://academy.yandex.ru/handbook/ml/article/obuchenie-predstavlenij).

А если у вас есть все данные мира, то вы можете в качестве энкодера взять свёрточную нейросеть и обучать её вместе с декодером end-to-end:

![one](https://yastatic.net/s3/education-portal/media/one_to_many2_260aa93598_04b35d18d3_a5dd66af79.svg)

## Механизм внимания (attention)

Как человек переводит предложения с одного языка на другой? Обычно переводчик уделяет особое внимание слову, которое записывает в данный момент. Хочется сообщить аналогичную интуицию нейронным сетям. Рассмотрим, как можно реализовать такой механизм на примере машинного перевода.

Внимательно посмотрим на seq2seq модель для машинного перевода. Вся информация о предложении на исходном языке заключена в контекстном векторе, но разные слова в предложении могут иметь разную смысловую значимость и следовательно, должны учитываться с разными весами. Кроме того, при генерации разных частей перевода следует обращать внимание на разные части исходного предложения. Например, первое слово переведенной фразы нередко связано с первыми словами в предложении, поданном на вход энкодеру, а порой одно слово перевода передаёт смысл нескольких слов, разбросанных по исходному предложению (вдруг кто-нибудь сталкивался с отделяемыми приставками в немецком?).

**Механизм внимания** (**attention**) реализует эту интуицию путем предоставления декодеру информации обо всех токенах исходного предложения на каждом шаге генерации. Рассмотрим классическую модель внимания, предложенную Bahdanau et al. в 2014 году.

Обозначим скрытые состояния энкодера $(h_0, h_1, …, h_n)$, а скрытые состояния декодера $(s_0, s_1, …, s_m)$. Важно отметить, что $h_n = s_0$, это контекстный вектор. На каждом шаге декодера будем считать **attention scores**, умножая $s_i$ на вектор скрытого состояния каждого блока энкодера  $(h_0, h_1, …, h_n)$. Таким образом, получаем $n$ значений, указывающих, насколько каждый из токенов c номерами $(0...n)$ из исходного предложения важен для генерации токена $i$ из перевода:

$$e_i = [\langle s_i, h_0\rangle, \langle s_i, h_1\rangle, …, \langle s_i, h_n\rangle] =
$$

$$=[s_ih_0^T,\ldots,s_ih_n^T]
$$

(здесь $s_i$ и $h_j$, как обычно, являются строками, так что $s_ih_j^T$ — скаляр).

Теперь превращаем эти значения в attention distribution, применив к ним softmax:

$$\alpha_i = \text{softmax}(e_i)
$$

Используем $\alpha_i$ в качестве весов для нахождения окончательного вектора внимания $a_i$:

$$a_i = \sum_{j=0}^{n}\alpha_jh_j
$$

Теперь в декодере на шаге i вместо вектора скрытого состояния $(h_0, h_1, ..., h_n)$ будем использовать вектор $[s_i, a_i]$ -- конкатенацию скрытого состояния блока и соответствующего attention вектора. Таким образом, на каждом шаге декодер получает информацию о важности всех токенов входного предложения. Данная схема вычисления attention представлена на следующем рисунке.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/self_attention_calculation_b1dc069cb7_10215a20fb.svg" loading="lazy" decoding="async" alt="">
  <figcaption>
    Вычисление attention в seq2seq модели
  </figcaption>
</figure>

Существует много разных видов механизмов внимания, например:

1. Базовый dot-product, рассмотренный ранее: $e_i = [s_ih_j^T]_{j=0}^n$
2. Мультипликативный: $e_i = [s_iWh_j^T]_{j=0}^n$, где $W$ — обучаемая матрица весов.
3. MLP: $e_{ij} = \text{tanh}(h_jW_1 + s_iW_2) v$, где $W_1$, $W_2$ — обучаемые матрицы весов, $v$ — обучаемый вектор весов

Важной особенностью механизма внимания является то, что его веса несут в себе информацию о связях слов в двух языках, участвующих в переводе. Визуализировав веса механизма внимания, получаем таблицу взаимосвязей между словами:

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/bernoulli_entropy_copy_4_68b411bd14_48253cbcf3.svg" loading="lazy" decoding="async" alt="">
  <figcaption>
    Пример визуализации весов attention
  </figcaption>
</figure>

Весьма логично, что слово *dogs* теснее всего связано со словом *собак*, а слову *очень* соответствуют целых два слова: *very* и *much*.

## Self-attention

В предыдущем разделе мы обсуждали применение механизма внимания во время работы декодера, но оказывается, что и энкодеру это может быть полезно.

**Механизм внутреннего внимания** (**self-attention**) используется, чтобы посмотреть на другие слова во входной последовательности во время кодирования конкретного слова. Изначально этот механизм был представлен в [статье](https://arxiv.org/abs/1706.03762) Attention is all you need как элемент архитектуры «**трансформер**» (Transformer).

Эффективность трансформера демонстировалась на примере задачи машинного перевода. Сейчас трансформеры и self-attention обрели огромную популярность и используются не только в NLP, но и в других областях (например, в компьютерном зрении: [Vision Transformer](https://arxiv.org/pdf/2010.11929.pdf), [Video Transformer](https://arxiv.org/pdf/1906.02634.pdf), [Multimodal Transformer for Video Retrieval](https://arxiv.org/pdf/2007.10639.pdf) и так далее).

Более подробный обзор архитектуры Трансформер оставим [курсу](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html) Лены Войта по NLP, а пока остановимся на механизме внутреннего внимания. Пусть на вход нейросети пришли два предложения «Мама мыла раму. Она держала в руках тряпку». Местоимение «она» относится к маме или к раме? Для человека это очень простой вопрос, но для модели машинного обучения — нет. Self-attention помогает выучить взаимосвязи между токенами в последовательности, моделируя «смысл» других релевантных слов в последовательности при обработке текущего токена.

Что происходит внутри self-attention-модуля? Для начала, из входного вектора (например, эмбеддинга каждого токена) формируются три вектора: Query (запрос), Key (ключ) и Value (значение). Они получаются с помощью умножения входного вектора на матрицы $W_Q$, $W_K$ и $W_V$, веса которых учатся вместе со всеми остальными параметрами модели с помощью обратного распространения ошибки.

Выделение этих трех абстракций нужно, чтобы разграничить эмбеддинги, задающие «направление» внимания (query, key) и смысловую часть токена (value). Вектор query задает модальность «начальной точки» механизма внутреннего внимания (от какого токена направлено внимание), вектор key — модальность «конечной точки» (к какому токену направлено внимание). Таким образом, один и тот же токен может выступать как «начальной», так и «конечной» точкой направления внимания: self-attention вычисляется между всеми токенами в выбранном фрагменте текста.

Процесс происходит так: по очереди фиксируется каждый токен (становится query) и просчитывается степень его связанности со всеми оставшимися токенами. Для этого поочередно key-вектора всех токенов скалярно умножаются на query-вектор текущего токена. Полученные числа будут показывать, насколько важны остальные токены при кодировании query токена в конкретной позиции.

Дальше полученные числа надо нормализовать и пропустить через софтмакс, чтобы получить распределение. Затем подсчитывается взвешенная сумма value векторов,где в качестве весов используются полученные на предыдущем шаге вероятности. Полученный вектор и будет выходом слоя внутреннего внимания для одного токена. Изложенную выше схему вычисления self-attention вектора для одного токена можно представить простой схемой:

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/self_attention_calculation_b1dc069cb7_10215a20fb.svg" loading="lazy" decoding="async" alt="">
  <figcaption>
    Вычисление self-attention для одного токена. <a href="http://jalammar.github.io/illustrated-transformer/">Источник</a>
  </figcaption>
</figure>

На практике self-attention не вычисляется для каждого токена по отдельности, вместо этого используются матричные вычисления. Например, вместо вычисления query, key и value векторов для каждого токена, настакаем эмбеддинги входных токенов в матрицу $X$ и посчитаем матрицы $Q = W_Q * X$, $K = W_K * X$ и $V = W_V * X$.

Затем происходит повторение описанных в предыдущем абзаце шагов, только для матриц. Посчитаем итоговую матрицу $Z$, подав матрицы Q, K и V в формулу:

$$Z=softmax(\frac{Q\dot{K}^T}{\text{norm const}})\dot{V}
$$

В оригинальной [статье](https://arxiv.org/pdf/1706.03762.pdf) Vaswani et al., 2017 в качестве нормализующей константы выбрали число 8 (квадратный корень размерности key-векторов). Нормализация приводила к более стабильным градиентам в процессе обучения.

Интересно, что обычно используют параллельно несколько self-attention блоков. Такая схема называется multi-head self-attention. Вычисление self-attention происходит несколько раз с разными матрицами весов, затем полученные матрицы конкатенируются и умножаются на еще одну матрицу весов $W_O$ (см. схему).

Это позволяет разным self-attention головам фокусироваться на разных взаимосвязях, например, одна голова может отвечать за признаковые описания, другая за действия, третья за отношения «объект-субъект». Разные головы могут вычисляться параллельно, при этом входная матрица эмбеддингов отображается в разные подпространства представлений, что значительно обогащает возможности внутреннего внимания моделировать взаимосвязи между словами. В виде формулы вычисление multihead self-attention можно представить так:

$MultiHead(Q,K,V) = Concat(head_1, head_2, ..., head_n) \dot W^O$,\
где $head_i(Q,K,V)=softmax(\frac{Q\dot{K}^T}{\text{norm const}})\dot{V}$

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/multi_head_self_attention_f9cd5dbd2e_06bc861586.svg" loading="lazy" decoding="async" alt="">
  <figcaption>
    Схема вычисления multi-head self-attention.  <a href="http://jalammar.github.io/illustrated-transformer/">Источник</a>
  </figcaption>
</figure>

Есть много реализаций self-attention ([PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html), TensorFlow). Также советуем ознакомиться с [jupyter-ноутбуком](http://nlp.seas.harvard.edu/2018/04/03/attention.html) от Гарвардской NLP-группы, в котором представлена реализация архитектуры «трансформер» с подробными объяснениями. Еще один отличный источник, позволяющий подробнее разобраться с self-attention и трансформером, - это [статья](https://jalammar.github.io/illustrated-transformer/) Jay Alammar под названием «Illustrated Transformer».

## Особенности работы с текстами

### Предобработка текстов

Перед тем, как применять описанные выше архитектуры (или даже использовать простые подходы, вроде TF-IDF или word2vec),  нужно разобраться, как делать предобработку текстов.

Первым делом надо научиться представлять связный текст в виде последовательности. Для начала имеет смысл разбить текст на предложения, а дальше уже на слова или символьные n-граммы. Этот процесс называется токенизацией. Можно делать токенизацию вручную, например, с помощью регулярных выражений, или воспользоваться готовыми методами из библиотеки NLTK.

Представим, что мы получили упорядоченный список слов, из которых состоит текст. Но это еще не все. Обычно тексты содержат разные грамматические формы одного и того же слова. Привести все словоформы к начальной форме можно с помощью лемматизации.  Лемматизация - это алгоритм приведения слова к его начальной форме с использованием морфологическего анализа и знаний об особенностях конкретного языка.

Пример работы лемматизатора:
«собаки, собака, с собакой, собаками -\> собака»

Другой способ приведения всех словоформ к одной форме - это стемминг. Стемминг — это более грубый процесс на основе эвристик, который действует без знания контекста, словарей и морфологии. Стеммер не поймет, что слова с чередованием имеют один и тот же корень (только если прописать в явном виде такую эвристику) или что слова «есть», «буду» и «был» - это формы глагола «быть». Стемминг - менее аккуратный процесс по сравнению с лемматизацией, зато гораздо более быстрый.

Еще один важный этап предобработки текстов - это удаление стоп-слов. Стоп-словами называют междометия, союзы, предлоги, артикли, в общем все слова, которые будут вносить шум в работу алгоритма машинного обучения. Иногда дополнительно убирают слова общей лексики, оставляя только специфические термины. Универсального списка слов не существует, но для начала можно использовать список стоп-слов из библиотеки NLTK.

### Аугментации для текстов

Аугментации данных часто используются, чтобы увеличить количество данных в обучающей выборке, а также повысить обобщаемость модели. И если для компьютерного зрения аугментации относительно простые и могут выполняться на лету (масштабирование, обрезка, вращение, добавление шума и т.д.), то для текстов в виду грамматической структуры, синтаксиса и особенностей языка все не так просто.

Аугментации текста менее «автоматические», в идеале нужно понимать смысл фразы и иметь под рукой отлично работающий механизм перефразирования. Рассмотрим несколько популярных способов аугментации текстовых данных:

1. Обратный перевод. Переводим исходный текст на какой-то язык, и затем переводим его обратно. Это помогает сохранить контекст, но при этом получить синонимичную формулировку.
2. Замены слова на синонимичное/близкое по смыслу. Для этого можно использовать словари синонимов либо искать близкое слово в пространстве эмбеддингов, минимизируя расстояние между соответствующими векторами. В качестве таких эмбеддингов можно взять привычный word2vec, [fasttext](https://fasttext.cc/) или контекстуализированные эмбеддинги на основе претренированных моделей ([BERT](https://arxiv.org/abs/1810.04805), [ELMO](https://arxiv.org/abs/1802.05365), [GPT-2](https://openai.com/blog/gpt-2-1-5b-release/)/[GPT-3](https://arxiv.org/abs/2005.14165) и так далее).
3. Вставка синонима слова в случайное место в предложении.
4. Замена сокращения на полное наименование и обратно. Для английского языка этот способ более актуален, чем для русского.
5. Случайная вставка/удаление/замена/перемена местами слов в предложении.
6. Случайная перестановка местами предложений.
7. Случайное изменение букв на произвольные/ближайшие на клавиатуре, добавление/исправление орфографических и пунктуационных ошибок, изменение регистра.
8. [MixUp](https://arxiv.org/abs/1905.08941) для текстов. В задаче классификации смешиваем признаковые описания двух объектов и с такими же весами смешиваем их метки классов, получаем новый объект с признаками $x_{ij}$ и меткой класса $y_{ij}$:

$$x_{ij} = \lambda x_i + (1 - \lambda) x_j
$$

$$y_{ij} = \lambda y_i + (1 - \lambda) y_j
$$

Для текстов признаковые описания можно смешивать на уровне слов (выбирать ближайшее слово в пространстве word embeddings) или на уровне предложений. Еще один вариант: сэмплировать слова из двух разных текстов с вероятностями $\lambda$ и $1-\lambda$.
9\. Аугментации с использованием синтаксического дерева предложения.
10\. Генерация текста языковыми моделями. Например, генерация текста с помощью упоминавшейся ранее модели GPT-3.

Подробнее про некоторые методы аугментации текстов можно почитать в [статье](https://arxiv.org/abs/1901.11196) Easy Data Augmentation (EDA). Многие из описанных выше и в статье методов реализованы в библиотеке [NLPAug](https://github.com/makcedward/nlpaug), использование которой сильно упрощает задачу аугментации текстовых данных на практике.

  ## handbook

  Учебник по машинному обучению

  ## title

  Нейросети для работы с последовательностями

  ## description

  *null*

- 
  ## path

  /handbook/ml/article/grafovye-nejronnye-seti

  ## content

  ## Введение

Наряду с обработкой табличных, текстовых, аудио данных и изображений, в глубинном обучении довольно часто приходится решать задачи на данных, имеющих **графовую** структуру. К таким данным относятся, к примеру, описания дорожных и компьютерных сетей, социальных графов и графов цитирований, молекулярных графов, а также графов знаний, описывающих взаимосвязи между сущностями, событиями и абстрактными категориями.

![graph_examples_d518b5ab5c.webp](https://yastatic.net/s3/education-portal/media/graph_examples_d518b5ab5c_1819e5c6d3.webp)

В этом параграфе мы с вами познакомимся с основными задачами, которые возникают при обработке графов, а также поговорим о **графовых свертках** и **графовых нейронных сетях** — специальном классе обучаемых преобразований, способных принимать в качестве входа графы и решать задачи на них.

## Описание графовых данных

Граф $G=(V, E)$ принято представлять двумя множествами: множеством $V$, содержащим вершины и их признаковые описания, а также множеством $E$, содержащим связи между вершинами (то есть рёбра) и признаковые описания этих связей. Для простоты математических выкладок и изложения дальнейшего материала давайте считать, что мы всегда работаем с ориентированными графами. Если граф содержит ненаправленное ребро, мы его заменяем на пару направленных ребер. Кроме того, давайте обозначать окрестность вершины как $N(v)=\{\hat{v}\vert(\hat{v}, v) \in E\}$.

![2_rgb_split_rooster_d20d157ef7.svg](https://yastatic.net/s3/education-portal/media/2_rgb_split_rooster_d20d157ef7_03b8cb7672.svg)


Графовые данные довольно разнообразны. Они могут отличаться между собой в следующих моментах:

* По размеру, т.е. *количеству вершин и/или ребер*.
* По *наличию признаковых описаний вершин и рёбер*. В зависимости от решаемой задачи, графы могут содержать информацию только в вершинах, только в ребрах, либо же и там и там.  
* Кроме того, графы могут быть **гомо- и гетерогенными** — в зависимости от того, имеют ли вершины и ребра графа одну природу либо же нет. 

Например, социальные графы содержат огромное количество вершин и ребер, часто измеряющееся в тысячах, содержат информацию в вершинах и очень редко в ребрах, а также являются гомогенными, так как все вершины имеют один тип. В то же время, молекулярные графы — это пример графов с, как правило, средним количеством вершин и ребер; вершины и связи в молекулярных графах имеют признаковое описание (типы атомов и ковалентных связей, а также информацию о зарядах и т.п.), но при этом также являются гомогенными графами. К классу гетерогенных графов относятся, например, графы знаний, описывающие некоторую систему, различные сущности в ней и взаимодействия между этими сущностями. Вершины (сущности) и связи (ребра) такого графа могут иметь различную природу: скажем, вершинами могут быть сотрудники и подразделения компании, а рёбра могут отвечать отношениям «Х работает в подразделении Y», «X и Z коллеги» и так далее.

![1_rgb_split_rooster_94ec31a0a3.svg](https://yastatic.net/s3/education-portal/media/1_rgb_split_rooster_94ec31a0a3_5089b8a719.svg)


## Задачи на графах

Разнообразие графовых данных закономерно породило множество разнообразных задач, которые решаются на этих данных.

Среди них можно встретить классические постановки классификации, регрессии и кластеризации, но есть и специфичные задачи, не встречающиеся в других областях — например, задача восстановления пропущенных связей внутри графа или генерации графов с нужными свойствами. Однако даже классические задачи могут решаться на различных *уровнях*: классифицировать можно весь граф (**graph-level**), а можно отдельные его вершины (**node-level**) или связи (**edge-level**).

![1_rgb_split_rooster_1_40fb5a1496.svg](https://yastatic.net/s3/education-portal/media/1_rgb_split_rooster_1_40fb5a1496_82899b6adb.svg)

Так, в качестве примера *graph-level* задач можно привести классификацию и регрессию на молекулярных графах. Имея датасет с размеченными молекулами, можно предсказывать их принадлежность к лекарственной категории и различные химико-биологические свойства. 


![1_rgb_split_rooster_2_b78144bcb1.svg](https://yastatic.net/s3/education-portal/media/1_rgb_split_rooster_2_b78144bcb1_f7faaa0c8f.svg)


На *node-level*, как правило, классифицируют вершины одного огромного графа, например, социального. Имея частичную разметку, хочется восстановить метки неразмеченных вершин. Например, предсказать интересы нового пользователя по интересам его друзей.

![1_rgb_split_rooster_3_1_dad355ba22.svg](https://yastatic.net/s3/education-portal/media/1_rgb_split_rooster_3_1_dad355ba22_889c0bacf0.svg)

Часто бывает такое, что граф приходит полностью неразмеченным и хочется *без учителя* разделить на компоненты. Например, имея граф цитирований, выделить в нем подгруппы соавторов или выделить области исследования. В таком случае принято говорить о *node-level* кластеризации графа.

![graph_clustering_684c9b863f.webp](https://yastatic.net/s3/education-portal/media/graph_clustering_684c9b863f_677385fa41.webp)

Наконец, довольно интересна задача предсказания пропущенных связей в графе. В больших графах часто некоторые связи отсутствуют. Например, в социальном графе пользователь может добавить не всех знакомых в друзья. А в графе знаний могут быть проставлены только простые взаимосвязи, а высокоуровневые могут быть пропущены.  

![1_rgb_split_rooster_4_d2140019b4.svg](https://yastatic.net/s3/education-portal/media/1_rgb_split_rooster_4_d2140019b4_bc419c7590.svg)

В конце, хотелось бы отметить очень важные особенности всех задач, связанных с графами. Алгоритмы решения этих задач должны обладать двумя свойствами.

- Во-первых, графы в датасетах, как правило, могут отличаться по размерам: как по количеству вершин, так и по количеству связей. Алгоритмы решения задач на графах должны уметь принимать графы различных размеров.
- Во-вторых, алгоритмы должны быть инварианты к перестановкам порядка вершин. То есть если взять тот же граф и перенумеровать его вершины, то алгоритмы должны выдавать те же предсказания с учетом этой перестановки.

## Графовые нейронные сети

Развитие глубинного обучения повлияло на подходы к решению задач на графовых данных. Был предложен концепт **графовых нейронных сетей**, которые в последнее время либо полностью заменили классические алгоритмы обработки графов, либо породили мощные синергии с этими алгоритмами.

![1_rgb_split_rooster_5_d0b351ce6e.svg](https://yastatic.net/s3/education-portal/media/1_rgb_split_rooster_5_d0b351ce6e_edfded2077.svg)

Графовые нейронные сети по принципу работы и построения идейно очень похожи на сверточные нейронные сети. Более того, забегая немного вперед, графовые нейроные сети являются *обобщением* сверточных нейронных сетей.

На вход графовой нейронной сети подается граф. В отличие от сверточных нейронных сетей, которые требуют, чтобы все картинки в батче были одинакового размера, графовые нейронные сети допускают разные размеры у объектов батча. Кроме того, в отличие от картинок, у которых информация довольно однородна (это, как правило, несколько цветовых каналов) и хранится в пикселях, у графов информация может также храниться в вершинах и/или ребрах. Причем в одних задачах информация может быть только в вершинах, в других только в ребрах, а в третьих и там, и там. Сама информация может быть довольно разнородной: это могут быть и вещественные значения, и дискретные значения, в зависимости от природы графа и от типа решаемой задачи. Поэтому, довольно часто первым слоем в графовых нейронных сетях идут Embedding слои, которые переводят дискретные токены в вещественные векторы.


$$h^{n}_{0} = Emb(V),\quad h^{e}_{0} = Emb(E)$$


Однако, сама суть работы у графовых и сверточных сетей совпадает. В графовой нейронной сети по очереди применяются слои, которые собирают информацию с соседей  и обновляют информацию в вершине. То же самое делают и обычные свертки. Поэтому такие слои и называются **графовыми свертками**. Графовая свертка принимает на вход граф со скрытыми состояниями у вершин и ребер и выдает тот же граф, но уже с обновленными более информативными скрытыми состояниями. 

В отличие от сверточных нейронных сетей, при обработке графа pooling слои вставляют редко, в основном в graph-level задачах, при этом придумать разумную концепцию графового пулинга оказалось нелегко. Если вам станет интересно, вы можете познакомиться с несколькими вариантами графовых пулингов в следующих статьях:

* [Learning Spectral Clustering](https://www.di.ens.fr/~fbach/nips03_cluster.pdf)
* [Kernel k-means, Spectral Clustering and Normalized Cuts](https://www.cs.utexas.edu/users/inderjit/public_papers/kdd_spectral_kernelkmeans.pdf)
* [Weighted Graph Cuts without Eigenvectors](https://www.cs.utexas.edu/users/inderjit/public_papers/multilevel_pami.pdf)

В большинстве же архитектур пулинги не используются, и структура графа на входе и выходе графовой нейронной сети совпадает.

![1_rgb_split_rooster_6_29a3696ede.svg](https://yastatic.net/s3/education-portal/media/1_rgb_split_rooster_6_29a3696ede_a4f5668691.svg)



Полученная после череды сверток информация с вершин и ребер в конце обрабатывается с помощью полносвязных сетей для получения ответа на задачу. Для node-level классификации и регрессии полносвязная сеть применяется к скрытым состояниям вершин $h^{n}_{K}$, а для edge-level, соответственно, к скрытым состояниям ребер $h^{e}_{K}$. Для получения ответа на graph-level уровне информация с вершин и ребер сначала агрегируется с помощью <b>readout</b> операции. На месте readout операции могут располагаться любые инвариантные к перестановкам операции: подсчет максимума, среднего или даже обучаемый self-attention слой.


![gnn_classification_af4200451c.webp](https://yastatic.net/s3/education-portal/media/gnn_classification_af4200451c_d7d5b98042.webp)

Как говорилось ранее, графовые нейронные сети являются обобщением сверточных. Если представить пиксели изображения вершинами графа, соединить соседние по свертке пиксели ребрами и предоставить относительную позицию пикселей в информации о ребре, то графовая свертка на таком графе будет работать так же, как и свертка над изображением.

![gnn_cnn_f56ac106b0.webp](https://yastatic.net/s3/education-portal/media/gnn_cnn_f56ac106b0_b8a2d0408a.webp)

К графовым нейронным сетям, как и к сверточным, применим термин **receptive field**. Это та область графа, которая будет влиять на скрытое состояние вершины после N сверток. Для графов receptive field после N графовых сверток — это все вершины и ребра графа, до которых можно дойти от фиксированной вершины не более чем за N переходов. Знание receptive field полезно при проектировании нейронной сети - имея представление о том, с какой окрестности вершины надо собрать информацию для решения задачи, можно подбирать нужное количество графовых сверток.

![receptive_field_b6f6d395f3.webp](https://yastatic.net/s3/education-portal/media/receptive_field_b6f6d395f3_4406384972.webp)

Многие техники стабилизации обучения и повышения обобщаемости, такие как Dropout, BatchNorm и Residual Connections, применимы и к графовым нейронным сетям. Однако стоит помнить про их особенности. Эти операции могут независимо применяться (или не применяться) к вершинам и ребрам. Так, если вы применяете Dropout, то вы вправе поставить для вершин и для рёбер различные значения dropout rate. Аналогично и для Residual Connections - они могут применяться только для вершин, только для ребер или же и там и там.


Кроме того, стоит иметь ввиду, что графы различных размеров будут неравноценно влиять на среднее и дисперсию в BatchNorm слое. Более стабильной альтернативой BatchNorm в обработке графов, например, являются [LayerNorm](https://arxiv.org/pdf/1607.06450.pdf) и [GraphNorm](https://arxiv.org/pdf/2009.03294.pdf), которые производят нормировку активаций по каждому графу независимо. 

LayerNorm, по сути, применяет BatchNorm для каждого графа: 

$$
\mathbf{x}^{\prime}_i = \frac{\mathbf{x} -
        \textrm{E}[\mathbf{x}]}{\sqrt{\textrm{Var}[\mathbf{x}] + \epsilon}}
        \odot \gamma + \beta
$$

A вот GraphNorm содержит несколько обучаемых параметров и является более гибким вариантом нормализации:

$$\mathbf{x}^{\prime}_i = \frac{\mathbf{x} - \alpha \odot
        \textrm{E}[\mathbf{x}]}
        {\sqrt{\textrm{Var}[\mathbf{x} - \alpha \odot \textrm{E}[\mathbf{x}]]
        + \epsilon}} \odot \gamma + \beta$$

## Парадигмы построения графовых сверток

Важно отметить, что в отличие от свертки, применяемой для изображений, являющейся четко определенной операцией, графовая свертка представляет собой именно концепт, абстрактную операцию, обновляющую скрытые представления объектов графа, используя доступную информацию с соседей и ребер. На практике, конкретный механизм графовой свертки разрабатывается для конкретной задачи, и различные реализации графовых сверток могут очень сильно отличаться между собой. И если зайти на сайты популярных фреймворков глубинного обучения на графах (например, [PyG](https://www.pyg.org)), то можно обнаружить десятки различных реализаций графовых сверток.

Во-первых, графовые свертки отличаются между собой по тому набору информации, которые они могут использовать. Есть свертки, которые используют только скрытые представления вершин, игнорируя информацию на ребрах. Существуют свертки, которые по разному обрабатывают информацию от ребер различного типа. А есть свертки, которые используют информацию с ребер и вершин, обновляя одновременно и те и другие. 

Во-вторых, и что более важно, графовые свертки можно разделить на два семейства, которые отличаются математической парадигмой, в которой они работают. Есть spatial (пространственный) и spectral (спектральный) подходы. Пространственные свертки основываются на message-passing парадигме, в то время как спектральные работают с графовым лапласианом и его собственными векторами. 

На практике, спектральные свертки чаще применяются и показывают лучшие результаты в задачах связанных с обработкой одного большого графа, где важно понимать относительное месторасположение вершины в этом большом графе. Например, графа соцсетей или графа цитирований. Пространственные свертки показывают хорошие результаты в остальных задачах, где для решения задачи важно находить локальные подструктуры внутри графа.

Несмотря на принципиальную противоположность этих двух подходов, активно предпринимаются попытки их совмещения в одну парадигму, например, в этой [работе](https://arxiv.org/pdf/2107.10234.pdf).

Давайте разберемся с этими двумя парадигмами.

![gnn_family_107283d83b.webp](https://yastatic.net/s3/education-portal/media/gnn_family_107283d83b_680ac24afd.webp)

### Пространственная парадигма

Пространственная (spatial) парадигма основывает на алгоритме передачи сообщений (message passing) между вершинами графа.

Концепт этого подхода заключается в следующем - каждая вершина графа имеет внутреннее состояние. Каждую итерацию это внутреннее состояние пересчитывается, основываясь на внутренних состояниях соседей по графу. Каждый сосед влияет на состояние вершины, так же как и вершина влияет на состояния соседей.

![1_rgb_split_rooster_7_4a73b7b411.svg](https://yastatic.net/s3/education-portal/media/1_rgb_split_rooster_7_4a73b7b411_89853ccf75.svg)

Итерация работы Message passing подхода для одной вершины можно описать следующим абстрактным алгоритмом. Для каждой вершины $v$ собираются все тройки $(x_v, x_w, e_{wv})$ состоящие из скрытых представлений текущей вершин $x_v$ и ее соседа $x_w$, а также из типа ребра $e_{wv}$,соединяющего текущую вершину и её соседа. Ко всем этим тройкам применяется обучаемое преобразование $M$ (от слова message), которая считает сообщение — информацию, которая идет от соседа к вершине. Посчитанные сообщения агрегируются в одно, обозначаемое $m_v$. Сообщения могут быть сагрегированы любой ассоциативной операцией, например взятием поэлементного минимума, максимума или среднего. Далее, агрегированное сообщение и текущее внутреннее состояние вершины подаются на вход обучаемой операции $U$ (от слова update), которая обновляет внутреннее состояние вершины. 

Конкретные имплементации операций $M, U$ непосредственно зависят от алгоритма и той задачи, которую он решает. 

![1_rgb_split_rooster_8_9253c51bed.svg](https://yastatic.net/s3/education-portal/media/1_rgb_split_rooster_8_9253c51bed_335a7a5c8a.svg)

Одним из самых известных классических алгоритмов, построенных на пространственной парадигме, является PageRank. Алгоритм [PageRank](https://ru.wikipedia.org/wiki/PageRank) проходит по графу веб страниц и выставляет каждой веб-странице значение ее "важности" $PG$, которое впоследствии можно использовать для ранжирования поисковой выдачи. Формула подсчета PageRank выражается через коэффициент затухания $d$, а также значения PageRank соседей $N(A)$ вершины и количество исходящий ссылок из этих соседей $LN(A)$ следующим образом:

$$ PR(A) = (1 - d) + d \sum_{B \in N(A)} \frac{PR(B)}{LN(B)} $$

В такой постановке операции подсчета сообщений $M$ и операции обновления $U$ имеют следующий вид:

$$ M(B) = \frac{PR(B)}{LN(B)} \\
m_{a} = \sum_{B \in N(A)} M(B) \\
U(A) = (1 -d) + d  m_{a} \\
PR(A) = U(A) $$

Графовые свертки, работающие на парадигме передачи сообщений, как правило делают $M$ и $U$ обучаемыми преобразованиями.

Рассмотрим несколько конкретных примеров архитектур.

#### GraphSAGE

Свертка [GraphSAGE](https://arxiv.org/abs/1706.02216) работает по следующему принципу. Для каждой вершины вычисляется набор скрытых представлений соседних вершин $h_w^t$, из которых идут связи в текущую. Далее, собранная информация агрегируется с помощью некоторой коммутативной операции $AGGR$ в вектор фиксированного размера. В качестве операции агрегации авторы предлагают использовать операции взятия средних или максимальных значений скрытых представлений объектов из набора. Далее агрегированный вектор объединяется со скрытым представлением вершины $h_v^t$, они домножаются на обучаемую матрицу $W$ и к результату умножения поэлементно применяется сигмоида. Обучаемые параметры данного слоя, как и в случае GCN, содержат только одну матрицу.

$$m_v^{t+1}=AGGR (\{h_w^t,w \in N(v)\})\\
h_v^{t+1}=\sigma(W^{t+1} CONCAT(m_v^{t+1},h_v^t))$$

Данная свертка использует только скрытые представления вершин, однако уделяет больше внимания локальному окружению вершины, нежели её глобальному положению во всем графе. Авторы показали высокое качество данной архитектуры в задачах, связанных с выучиванием представлений вершин, однако использование данной свертки можно встретить и в других задачах, связанных с обработкой графов, не содержащих дополнительной информации о рёбрах.

#### GAT

Свертка [GAT](https://arxiv.org/abs/1710.10903) (Graph ATtention) является развитием идеи GraphSAGE. В качестве механизма агрегации эта архитектура предлагает использовать механизм внимания, у которого матрицы преобразования для ключей, значений и запросов совпадают и обозначены в формуле буквой $W$. Как и в GraphSAGE, агрегированное сообщение проходит через сигмоиду, но не домножается перед этим на обучаемую матрицу.

$$\alpha_{v*}=softmax(act(a^T
CONCAT(Wh_v^t,Wh_∗^t)))\\
h_v^{t+1}=\sigma\left(\sum_{w \in N(v)} [\alpha_{vw} Wh_w^t] \right)$$

Здесь act — некоторая функция активации. Как и в случае механизма внимания для последовательностей, в момент обновления представления для вершины $v$ attention «смотрит» на все остальные вершины $w$ и генерирует веса $\alpha_{vw}$, которые указывают, информация из каких вершин $w$ «важнее» для нас.

Благодаря мощности и гибкости механизм внимания, эта свертка показала отличные результаты на множестве задач и является одной из самых популярных сверток. По умолчанию, эта свертка, как и GraphSAGE, использует только признаки вершин, однако, в некоторых проектах можно встретить модификации свертки, в которых механизм внимания учитывает ещё и для информацию ребер.

#### RGCN

Наконец, есть специально разработанные свертки для обработки графов, ребра которых могут быть нескольких типов. Одна из них называется [RGCN](https://arxiv.org/abs/1703.06103) (Relational Graph Convolutional Networks). Она суммирует скрытые представления соседей, однако каждое представление соседа домножается на матрицу, зависящую от типа ребра, которое соединяет соседа с текущей вершиной. Если в графе присутствует ребра $N$ типов, то данная свертка будет учить $N$ матриц - по одной для каждого типа связи.

$$h_v^{t+1}=\sigma\left(\sum_{r \in R}\sum_{w \in N_r(v)} \frac{1}{c_{i,r}}
W_r^l  h_w^t + W_0^l h_v^t\right)$$

### Спектральная парадигма

Противоположностью пространственной парадигме является спектральная (spectral) парадигма. В своей постановке спектральная парадигма опирается на анализ процесса диффузии сигнала внутри графа и анализирует матрицы, описывающих граф — матрицу смежности и матрицу, которая называется Лапласианом графа.
 
![spectral_dec5f54b96.webp](https://yastatic.net/s3/education-portal/media/spectral_dec5f54b96_579b110e73.webp)

Лапласиан графа — это матрица $L=D-A$, где $D$ — диагональная матрица, хранящая в $i$-й диагональной ячейке количество исходящих из $i$-й вершины рёбер, а $A$ — матрица смежности графа, $(i,j)$-й элемент которой равен числу рёбер, соединяющих $i$-ю и $j$-ю вершину.

Лапласиан графа имеет неотрицательные собственные значения. Количество нулевых собственных значений всегда совпадает с количеством компонент связности. Потрясающим свойством Лапласиана является то, что его собственные векторы, соответствующие положительным собственным значениям, в порядке возрастания собственных значений, описывают разрезы графа — его разделения пополам таким образом, чтобы между разделенным половинами было как можно меньше ребер. 

Так, собственный вектор, соответствующий наименьшему положительному собственному значению, будет описывать кластеризацию графа на два подграфа. Все индексы, соответствующие положительным элементам вектора задают вершины, которые должны оказаться в первом кластере, а отрицательные элементы будут соответствовать вершинам, которые должны оказаться во втором кластере. 

Этим свойством Лапласиана графа пользуются для того, чтобы проводить кластеризацию графа без учителя. Для этого надо:

1. Посчитать Лапласиан $L$ матрицы $A$
2. Посчитать $k$ собственных векторов, соответствующих наименьшим собственным значениям
3. Сформировать из них матрицу размера $N \times k$, каждая строка которой описывает вершину $k$ признаками
4. Кластеризовать объекты, описываемые этой матрицей (например, c помощью K-Means)

Таким образом, спектральный подход отлично подходит для того, чтобы находить в графе  компоненты,  вершины которых связаны друг с другом и имеют похожие свойства.



#### GCN

Свертка [GCN](https://arxiv.org/abs/1609.02907), основанная на спектральной парадигме, использует только скрытые состояния вершин $h$ и матрицу смежности $A$ — она учитывает лишь наличие или отсутствие ребра в графе, но не признаки ребер. 

С математической точки зрения, GCN очень проста и представляет собой один шаг итеративного процесса поиска собственных значений Лапласиана графа: мы берем скрытые представления вершин и домножаем их на нормированную матрицу смежности — матрицу $A$, домноженную слева и справа на матричный корень матрицы $D$. Этот шаг применяется ко всем каналам скрытого представления вершины. После этого шага, обновленные скрытые представления ещё домножаются на обучаемую матрицу $\theta$:

$$h^{t+1} = \theta D^{-1/2} (A + I) D^{−1/2} h^{t}$$

Здесь $h^{j}$ — это матрица размера (число вершин)$\times$(длина вектора представления), то есть к каждому «каналу» представлений свёртка применяется отдельно. Если же мы хотим работать с несколькими каналами, то есть вместо $h^{t}$ у нас матрица $H^{t}$ размера (число вершин)$\times$(число каналов), и ещё добавить нелинейность $f$, формула переписывается следующим образом:

$$H^{t+1} = f\left(D^{-1/2} (A + I) D^{−1/2} H^{t}\Theta\right).$$

Авторы данной свертки показали отличное качество работы в задачах классификации вершин графов цитирования и графа знаний. Однако, различные модификации данной свертки применяются и в других задачах, например, для выучивания векторных представлений вершин и для кластеризации вершин графа.

{% cut "Математическая интуиция за формулами" %}

Попробуем пояснить подробнее, откуда берётся такая формула для обновления $h^t$.

Пусть $h$ — некоторый скалярный «сигнал» на графе, который мы запишем в виде вектора (длина которого равна числу вершин графа).

Мы будем работать не с обычным Лапласианом, а с нормализованным, равным $\widetilde{L} = D^{-1/2} L D^{−1/2} = I - D^{-1/2} A D^{−1/2}$. Нормализованный лапласиан — симметричная матрица, так что у него есть ортонормированный базис $u_1,u_2,\ldots$ из собственных векторов. Запишем базис $u_i$ в (ортогональную) матрицу $U$. Тогда произведение $U^Th$ — это вектор, состоящий из координат сигнала $h$ в базисе $u_i$. Казалось бы, мы делаем тривиальные вещи из линейной алгебры, но на самом деле мы занимается анализом Фурье, а именно:

* $u_i$ — гармоники;
* $U^Th$ — коэффициенты Фурье;
* умножение на $U^T$ — преобразование Фурье;
* умножение на $U$ — обратное преобразование Фурье.

Действительно, преобразование Фурье — это, грубо говоря, всего лишь разложение по какому-то удобному ортонормированному базису. В данном случае — по базису $u_i$. 

В мире функций, функциональных свёрток и обычного преобразования Фурье $\mathcal{F}$ свёртка удовлетворяет такому соотношению:

$$g\ast h = \mathcal{F}^{-1}\left(\mathcal{F}(g)\odot\mathcal{F}(h)\right),$$

где $\odot$ — поэлементное умножение. То есть при переходе в мир коэффициентов Фурье свёртка функций превращается в поэлементное умножение векторов.

Для графов кажется логичным свёртку двух сигналов $g$ и $h$ определить похожим образом:

$$g\ast h = U\left((U^Tg)\odot(U^Th)\right) = U\theta U^Th,$$

где через $g_{\theta}$ мы обозначили диагональную матрицу с диагональю $U^Tg$.

Итак, мы дали новое определение свёртки на графе — через лапласиан и его собственные векторы. Проблема в том, что вычислять $u_i$ очень долго. Поэтому мы линеаризуем задачу. Рассмотрим свёртку $h\ast g$ как функцию от нормализованного лапласиана $\widetilde{L}$ и разложим её в ряд Тейлора в точке $I$ (единичная матрица):

$$g\ast h\approx \theta'_0h + \theta'_1(\widetilde{L} - I)h$$

Теперь для простоты ещё больше огрубим модель и предположим, что $\theta'_0 = -\theta'_1$ (обозначим это число буквой $\theta$). Тогда остаётся

$$g\ast h\approx \theta(I + D^{-1/2} A D^{−1/2})h$$

А для улучшения численной устойчивости мы перепишем это так:

$$g\ast h\approx \theta D^{-1/2} (I + A) D^{−1/2}h$$

Получилась та самая формула, которую вы видели выше.

Более подробно о том, как устроен анализ Фурье на графах, вы можете прочитать, например, в [этой статье](https://arxiv.org/pdf/1211.0053.pdf). Кроме того, рекомендуем заглянуть в [оригинальную статью про GCN](https://arxiv.org/abs/1609.02907) за более подробным изложением вывода формул.

{% endcut %}

  ## handbook

  Учебник по машинному обучению

  ## title

  Графовые нейронные сети

  ## description

  *null*

- 
  ## path

  /handbook/ml/article/nejroseti-dlya-oblakov-tochek

  ## content

  Облако точек — это неструктурированный набор векторов вида $(x, y, z)$. Опционально каждой точке может также соответствовать вектор $f$, который содержит дополнительные признаки, например, RGB цвет точки, интенсивность полученного сенсором сигнала и т.д.

![ml_6_4_1_1c33d89642.svg](https://yastatic.net/s3/education-portal/media/ml_6_4_1_1c33d89642_c8dff90440.svg)

Облака точек возникают во многих задачах из реального трёхмерного мира и позволяют нам понять, как он выглядит. Например, беспилотный автомобиль воспринимает окружающие его объекты в виде облака точек и строит между ними безопасный маршрут. Современные смартфоны с помощью сенсоров, которые возвращают облака точек, способны восстановить точную геометрию окружающего пространства.

Интуитивно кажется, что облака точек похожи на трёхмерные изображения, но есть важное отличие: облако точек — это неупорядоченный, неструктурированный массив, и чтобы извлечь из него пространственную структуру, нужно ещё постараться.  

Поэтому, мы не сможем напрямую использовать для работы с облаками точек архитектуры, созданные для изображений. В этом параграфе мы расскажем о том, как извлекать признаки из облаков точек и какие end-to-end архитектуры существуют для популярных задач: классификации и сегментации.  

Но прежде, чем мы переедем к описанию конкретных архитектур, давайте разберёмся, откуда берутся облака точек.

## Сенсоры для получения облаков точек

### LiDAR
Один из самых популярных сенсоров для получения облаков точек — это LiDAR. Принцип его действия состоит в следующем: посылается луч, он отражается от поверхности и возвращается на детектор. Зная скорость света и время, за которое луч вернулся, мы можем посчитать расстояние до объекта.

![lidar_point_cloud_33ff1c6eca.webp](https://yastatic.net/s3/education-portal/media/lidar_point_cloud_33ff1c6eca_0096d52a15.webp)

Благодаря используемой технологии у каждой точки помимо позиции $(x, y, z)$ будет известна ещё интенсивность. Она показывает, насколько сильным был отклик от поверхности. Например, от стекол или снега мы можем ожидать околонулевой мощности отражения, в то время как от дорожных знаков отражение будет очень сильным. Таким образом, интенсивность может оказаться важным признаком при построении предсказательной модели над облаками.

Большим преимуществом лидара является способность получать отклик на расстоянии 200 метров и больше при сохранении высокой точности.

### Камера RGB-D

RGB-D камера — это камера, которая способна возвращать глубину каждого пикселя помимо RGB значения. Зная глубину и математическую модель камеры, мы можем восстановить облако точек. Большим преимуществом такого сенсора является наличие цвета для каждой точки. Это может быть полезно при построении моделей сегментации, детекции и так далее.

RGB-D камеры, как правило, обладают меньшей точностью, чем LiDAR. Также такие сенсоры не позволяют оценивать глубину дальше 10-15 метров.

### RGB-камеры

Облако точек можно получить и с помощью обычной камеры, если воспользоваться алгоритмами компьютерного зрения для вычисления глубины. Существуют алгоритмы оценки глубины по одному кадру (monocular depth estimation), оценки глубины по нескольким кадрам (multi view stereo), восстановление облака точек окружающих объектов с помощью движения камеры (Structure From Motion). В зависимости от алгоритма, камеры и среды качество итоговых облаков может различаться. В каждом отдельном практическом приложении нужно смотреть, насколько тот или иной сенсор (или метод получения облака точек) подходит для задачи.


## Архитектуры для обработки облаков точек

В этой части мы рассмотрим различные подходы к построению нейросетей для работы с облаками точек. Мы обсудим детали архитектур, их минусы и плюсы, а также варианты использования этих архитектур в приложениях.

### PointNet

Давайте подумаем как, именно мы можем обрабатывать облака точек.

Во-первых мы можем применять некоторую функцию, например, MLP (полносвязную нейросеть) к вектору признаков каждой точки нашего облака.  

Проблема с таким подходом в том, что мы работаем не с облаком, а с отдельными точками. У такой архитектуры не будет пространственного контекста. В терминах CNN, receptive field каждой точки будет равен 1 — это эквивалентно свёрточной архитектуре, где все ядра размера 1x1.

Значит нам нужен некий механизм, с помощью которого точки смогут обмениваться информацией друг с другом. Таких механизмов можно придумать много: это и построение графа над облаком, и пересылка сообщений между вершинами, это может быть механизм внимания.  

Авторы архитектуры [PointNet](https://arxiv.org/pdf/1612.00593.pdf) предложили максимально простой механизм: имея для каждой точки некоторый вектор признаков $f_i \in \mathbb{R}^p$, давайте применим к этим векторам некоторую глобальную агрегацию AGGR, например, GlobalAveragePooling или GlobalMaxPooling:  

$$\text{AGGR}:\ ({f_i}_{i=1}^n) \mapsto f \in \mathbb{R}^p$$

Таким образом из набора векторов для точек мы получим общий вектор признаков, описывающий всё облако.

![ml_6_4_2_c2f224bb96.svg](https://yastatic.net/s3/education-portal/media/ml_6_4_2_c2f224bb96_382fb840d4.svg)

Полученный вектор описывает облако в целом, но точки при таком подходе по-прежнему не обмениваются информацией: ни одна из них не «знает», что происходит вокруг.

Чтобы исправить недостатки двух описанных выше подходов, объединим их, рассмотрев следующий базовый «слой» преобразования облака:

* Агрегируем векторы признаков точек $f_i$, получаем общий вектор признаков $f$ для облака в целом;
* Для каждой точки конкатенируем её вектор признаков с вектором признаков облака и строим новый вектор признаков $g_i = \text{concat}(f_i, f)$;
* Применим MLP к вектору признаков каждой точки: $f'_i = \text{MLP}(g_i)$.

Такие слои можно применять последовательно, формируя сколь угодно глубокую архитектуру.

Теперь, если мы хотим решить задачу классификации над облаком, мы можем взять очередной вектор $f$, полученный после агрегации всех векторов признаков точек, применить к нему MLP, получить логиты и обучать полученную сеть на cross-entropy loss.

Если же мы хотим решить задачу сегментации (то есть классификации для каждой точки), мы можем точно после очередного слоя применить MLP с необходимыми размерами к каждому вектору $f_i$ и получить логиты.

Авторы архитектуры PointNet предложили ещё одно небольшое усложнение описанной выше архитектуры. Давайте внимательно посмотрим на схему из статьи:

![point_net_model_ef646172ed.webp](https://yastatic.net/s3/education-portal/media/point_net_model_ef646172ed_40b8897e9f.webp)

Синий прямоугольник наверху соответствует одному описанному выше «слою», но вместо одного MLP здесь последовательность двух MLP, между которыми вставлена операция feature transform. Эта операция состоит в следующем:

* с помощью дополнительной сети по облаку в целом строим матрицу;
* эта матрица умножается на вектор признаков каждой из точек.

Тем самым точки дополнительно обмениваются информацией. Во многих последующих статьях эту часть не включают, и практика показывает, что для PointNet это не ключевой элемент архитектуры, и им можно пренебречь.

PointNet — это достаточно простая архитектура. Она использует геометрию облака, потому что координаты точек входят в векторы признаков, но работает не с локальной окрестностью точки, а со всем облаком в целом. Если мы обратимся к аналогиям с изображениями, то такая архитектура будет эквивалентна CNN со свёртками размера всего изображения. Такая архитектура с трудом улавливает локальные паттерны и детали, и именно это будет основным направлением дальнейшего улучшения этой архитектуры.

### PointNet++

Продолжением статьи PointNet стала статья [PointNet++](https://arxiv.org/pdf/1706.02413.pdf) от той же научной группы. Новая архитектура развивает идею локальности, о который мы писали в предыдущем параграфе.

Давайте подумаем, каким образом мы можем получить локальные, зависящие только от окрестности точки операции над облаком точек. Первым делом нам необходимо понять, что такое локальность. В облаке точек очевидным решением кажется определить окрестность точки исходя из евклидового расстояния между точками — как шар некоторого радиуса — и проводить операции внутри этого шара.

Какие операции мы будем проводить внутри шара? У нас уже есть PointNet, который по произвольному набору точек может построить вектор, описывающий этот набор в целом. С его помощью мы можем описать многослойную архитектуру, напоминающую энкодер свёрточной сети, каждый слой которой будет выглядеть следующим образом:

1. сэмплируем $N$ ключевых точек в облаке, где $N$ меньше размера облака (как правило в несколько раз);
2. вокруг каждой ключевой точки фиксируем шар радиуса $R$;
3. для каждого шара находим все точки, которые в нём лежат;
4. запускаем PointNet с одними и теми же весами для каждого шара;
5. получаем новое облако из $N$ точек, где каждой точке присвоен вектор признаков, полученный из PointNet.

Далее мы можем повторять эту процедуру, пока у нас не останется одна точка, то есть один вектор признаков для всего облака в целом. Полученный эмбединг можно использовать для решения различных задач.


![ml_6_4_3_4a76819408.svg](https://yastatic.net/s3/education-portal/media/ml_6_4_3_4a76819408_26b7e39379.svg)

Обсудим детали реализации этой архитектуры.

В качестве алгоритма сэмплирования предлагается Farthest Point Sampling (FPS). Он заключается в том, что мы жадно сэмплируем точку, которая максимально удалена от текущего насэмплированного множества. Этот процесс мы повторяем, пока не наберём достаточное количество точек. Благодаря FPS мы можем в некоторой степени гарантировать, что покроем равномерно все облако и не пропустим какие-то мелкие, но удалённые от основного облака объекты. В этом ценное отличие FPS от случайного сэмплирования, при котором детали, содержащие мало точек, могут быть легко потеряны.

Выбор радиуса шара зависит, как правило, от плотности облака. Стоит посмотреть, сколько примерно точек попадает в каждую окрестность, и исходя из этого фиксировать радиус. Также этот параметр можно подобрать с помощью валидации. Для удобства реализации фиксируют не только радиус, но и максимальное количество объектов внутри шара.  

Это делается для того, чтобы при реализации архитектуры можно было манипулировать тензорами, избегая итерации по всем точкам каждого шара. Например, мы фиксируем количество шаров $N$, количество точек внутри шара $K$ и размерность вектора признаков для каждой точки $P$. Тогда размер тензора будет: $N \times K \times P$. Если в окрестности шара оказалось больше точек, чем мы заранее определили, то возьмем ближайшие $K$. В случае, если точек в тензоре меньше, чем $K$, тензор паддится нулями до нужного размера.

Внутри каждого слоя веса PointNet одинаковы для всех шаров. На окрестностях с одинаковой локальной структурой мы хотим получать одинаковые результаты, поэтому мы не можем использовать абсолютные значения $(x, y, z)$.  

В этом плане мы хотим, чтобы наши слои были похожи на свёртки, результат работы которых тоже не зависят от положения пикселя на изображении. Чтобы этого добиться, координаты $(x, y, z)$ каждой точки преобразуются в локальные координаты $(x - \overline{x}, y - \overline{y}, z - \overline{z})$, где $(\overline{x}, \overline{y}, \overline{z})$ — координаты центра шара.

Отдельно стоит обсудить, как получить поточечные признаки для решения задачи сегментации. Нам предстоит обратить энкодер и получить архитектуру декодера. Для этого каждый раз, когда мы делаем downsampling облака, то есть переход от вектора для каждой точки к общему вектору для некоторой окрестности, мы будем запоминать, какие точки принадлежали к какому шару.  

В процессе upsampling, то есть перехода от вектора окрестности к поточечным векторам, мы будем конкатенировать вектор окрестности с исходными векторами признаков точек. После каждой операции upsampling мы можем запускать MLP, чтобы точка, получив информацию с более глубокого уровня, могла извлечь оттуда информацию.

### Воксельные архитектуры

Мы разобрали архитектуру PointNet++, которая построена на обработке локальных окрестностей точек, определённых исходя из евклидового расстояния. У этой архитектуры есть свои проблемы.  

Во-первых, поиск ближайших соседей и FPS — это медленные процедуры. В результате могут возникнуть проблемы, когда мы захотим использовать такую сеть в real-time приложениях.  

Во-вторых, в зависимости от расстояния до сенсора у нас может меняться паттерн разреженности: если вблизи сенсора в шаре радиуса $\epsilon$ мы можем найти в среднем 100 точек, то, как правило, чем больше расстояние до наблюдателя — тем меньше точек мы будем находить в таком же объёме. Это усложняет обучение и может привести к тому, что с расстоянием качество будет сильно деградировать.

Давайте попробуем напрямую применить свёрточные нейронные сети в нашей задаче. Идея следующая: возьмём прямоугольный параллелепипед, который накрывает интересующую нас область пространства. Далее разобьём этот прямоугольный параллелепипед на значительно меньшие прямоугольные параллелипипеды одинакового размера. Назовем эти прямоугольные параллелепипеды **вокселями**. В результате внутри каждого вокселя окажется некоторый набор точек. Нам нужно каким-то образом превратить точки внутри каждого вокселя в один вектор. После этого мы можем применить обычные свёрточные сети и решать любые задачи.

Чтобы из набора точек внутри вокселя получить вектор признаков, мы можем просто применить PointNet. Таким образом, мы получим тензор размера $H\times W \times L \times P$, где первые три размерности пространственные, а последняя размерность — размерность вектора признаков.

Далее мы можем использовать 3D-свёртки для того, чтобы обработать этот тензор. Но проблема в том, что ядро в 3D-свёртках имеет на одну размерность больше, чем в 2D-свёртках, что делает их дорогими для вычисления.

![ml_6_4_5_19d87b7f08.svg](https://yastatic.net/s3/education-portal/media/ml_6_4_5_19d87b7f08_3770a74047.svg)

В статьях были предложены различные подходы для того, чтобы ускорить эту архитектуру. Часто размерность по высоте и размерность признаков объединяют в одну: тензор размерности $W\times L\times H\times P$ превращается в тензор размерности $W\times L\times (H\times P)$, как будто мы смотрим на него сверху (bird’s eye view). Для работы с ними мы можем использовать 2D-свёртки.

![ml_6_4_4_f15d8f8d19.svg](https://yastatic.net/s3/education-portal/media/ml_6_4_4_f15d8f8d19_a75c28fbbe.svg)

Ещё одним популярным трюком является использование высокой степени разреженности этого тензора: большое количество «столбиков» не содержат ни одной точки. Давайте возьмём все «столбики», содержащие хотя бы одну точку; предположим, что их $B$ штук. Соберём из них тензор размера $B\times (H\times P)$ и применим к нему линейный слой, который уменьшит число каналов, то есть высоту «столбиков».  

Получится тензор $B\times N$ для выбранного нами $N < H\times P$. Его столбцы мы снова расставим на их исходные места, получив тензор $W\times L\times N$. «Столбики» этого тензора, которые соответствуют пустым «столбикам» исходного тензора $W\times L\times (H\times P)$, мы заполним нулями.

Воксельный подход очень популярен в задаче детекции, потому что позволяет напрямую переиспользовать некоторые подходы к 2D детектированию. Но у него есть и недостатки. Например, нельзя сколько угодно плотно покрыть вокселями сколь угодно большой объём, потому что это напрямую влияет на размер тензора, а значит — на время работы, что может быть критично в real-time приложениях. В итоге мы можем потерять какие-то важные удалённые объекты или не справиться с мелкими деталями.


### Архитектуры с цилиндрической проекцией

Основная проблема воксельных архитектур — это неспособность обрабатывать облака произвольного размера и дальности. У вас может быть самый лучший LiDAR в мире, который видит на 300 метров во все стороны, но если ваша архитектура не будет способна обрабатывать полученное облако точек в режиме реального времени, то такой сенсор бесполезен.

Чтобы преодолеть эту проблему, мы можем воспользоваться тем фактом, что лидар сканирует окрестность из одной точки. Давайте окружим сенсор виртуальным цилиндром и спроецируем все точки на этот цилиндр. Далее цилиндр можно развернуть в прямоугольник и получить представление с которым могут работать свёрточные сети.

Обычно точки параметризуют двумя углами:

* pitch — наклон по вертикали,
* yaw — угол по горизонтали.

Мы можем дискретизовать эти углы и таким образом получить для каждой точки координаты пикселя, в который она должна быть помещена. В каналы нашего тензора мы можем записать: расстояние до точки, интенсивность сигнала, абсолютную высоту точки. Далее такое представление может быть обработано любой 2D архитектурой.

![frontal_projection_0a5a37da5b.webp](https://yastatic.net/s3/education-portal/media/frontal_projection_0a5a37da5b_fc0c5685e1.webp)

Важно отметить, что такое представление не будет работать для других методов получения облаков точек. Лидар сканирует окрестность из одной точки, и потому в направлении каждого луча у нас будет только одна точка. Это означает, что мы не видим за препятствиями. В то время облака, полученные, например, с помощью Structure from Motion, в направлении одного луча могут содержать несколько точек, что лишает нас возможности без потерь спроецировать всё облако на 2D холст.

Такое представление снимает проблему с производительностью, так как размер обрабатываемых данных не зависит от разброса облака. Но, к сожалению, появляются новые проблемы.

Во-первых, объекты, которые находятся далеко друг от друга в трёхмерном пространстве, могут оказаться рядом в этой проекции — в итоге, информация о таких объектах будет смешиваться, и в результате границы в задаче сегментации могут получиться размытыми. Воксельные- или PointNet-архитектуры не имеют этой проблемы — в них далёкие друг от друга объекты не будут так сильно взаимодействовать.

Во-вторых, объекты при таком представлении будут иметь разные размеры в зависимости от расстояния, и модели придётся адаптироваться к этому. Воксельные и PointNet архитектуры, опять же, лишены этой проблемы, так как их представления инвариантны к переносу.

Обе эти проблемы могут приводить к существенной деградации качества. Именно поэтому архитектуры, построенные на цилиндрической проекции, часто проигрывают другим архитектурам по качеству.


### Объединение информации с облака и изображения

В некоторых приложениях у нас может быть несколько различных сенсоров: например, лидар и камера. Тогда возникает задача: построить архитектуру, которая использует информацию с обоих сенсоров.  

Конечно, мы бы могли отдельно подготовить архитектуру для работы с изображением, отдельно архитектуру для работы с облаком и затем каким-то образом объединить результаты. Минус такого подхода в том, что нам требуется в два раза больше вычислительных мощностей, а нейронные сети никак не делятся информацией на промежуточных слоях. Возможно, нейронная сеть могла бы извлечь более богатые представления из объединенных данных. Например, разрешить некоторую неопределённость в облаке с помощью изображений.

Оказывается, что задача смешивания различных сенсоров очень непростая, и в литературе долгое время не могли для задачи 3D детекции получить результаты, которые бы превосходили только лидарный подход. Мы обсудим идеи, как именно можно смешивать изображения и 3D информацию.

Прежде чем мы перейдем к описанию подходов, давайте обсудим одну важную техническую деталь. Смешивание информации с разных сенсоров всегда предполагает, что сенсоры скалиброваны. Это означает что нам известна матрица перехода из системы координат одного сенсора в систему координат другого. Таким образом, у нас есть необходимая информация, для того, чтобы спроецировать лидарную точку на плоскость изображения.

Архитектура, которая первой показала действительно значительный прирост качества при смешивании сенсоров, называется [PointPainting](https://arxiv.org/pdf/1911.10150.pdf). Идея была следующей:

* Делаем сегментацию изображения;
* Для каждой лидарной точки найдём соответствующий ей пиксель на изображении и присвоим этой точке соответствующий класс из сегментации;
* Для архитектуры, которая работает с облаком точек, этот класс будет еще одним дополнительным признаком на входе.

Важно отметить, что сопоставление точки класса не будет идеальным, например, из-за того, что камера и лидар находятся не в одной точке.

![point_painting_98332dd991.webp](https://yastatic.net/s3/education-portal/media/point_painting_98332dd991_bdbaa7c1d9.webp)

Идея очень простая, но при этом позволяет сильно улучшить метрики. Проблема в том, что данный подход не решает проблему возросших вычислений при обработке данных с двух сенсоров. Более того, мы передаем в 3D архитектуру лишь одно число для каждой точки и теряем много информации об изображении.

Другой подход был описан в статье [LaserNet++](https://arxiv.org/pdf/1904.11466.pdf). Авторы используют цилиндрическую проекцию для обработки облака. Перед началом обработки они пропускают изображение через небольшую свёрточную сеть, получая в результате некоторый вектор признаков в каждом пикселе.  

Далее для каждой лидарной точки находим соответствующий пиксель на изображении и добавляем его признаки к признакам точки. Отличие от PointPainting в том, что в данном подходе обе нейронные сети обучаются end-to-end. Это позволяет нам извлечь более богатое представление из изображения.

В статье авторы сообщают о росте метрик на in-house наборе данных. К сожалению, на открытых данных этот подход не смог продемонстрировать превосходства над lidar-only моделями.

![laser_net_plus_4ac43808f3.webp](https://yastatic.net/s3/education-portal/media/laser_net_plus_4ac43808f3_d50c122ff8.webp)

В этом параграфе мы разобрали основные идеи работы с облаками точек. Верхнеуровнево все архитектуры можно разделить на следующие группы:

- Облако как граф (PointNet и многие другие архитектуры);
- Вокселизация (VoxelNet, PointPillars и другие);
- Проекция лидарного облака на 2D поверхность (RangerNet, LaserNet, LaserNet++ и другие).

Каждый подход имеет свои плюсы и минусы и выбор конкретного подхода должен быть обусловлен решаемой задачей.

  ## handbook

  Учебник по машинному обучению

  ## title

  Нейросети для облаков точек

  ## description

  *null*

- 
  ## path

  /handbook/ml/article/transformery

  ## content

  Наверное, ни один из рассказов про современные нейросети не обойдётся без упоминания трансформер-моделей: в самом деле, почти все нашумевшие достижения в глубинном обучении последних лет так или иначе опираются на эту архитектуру. Что же в ней такого особенного и почему трансформеры успешно применяются в самых разных задачах? 

Давайте разбираться. 

## Зачем нам внимание

Для начала вспомним, что основным подходом для работы с последовательностями до 2017 года (выхода оригинальной статьи про архитектуру «трансформер») было использование рекуррентных нейронных сетей, или RNN. Однако у такого подхода есть несколько известных минусов:

- Во-первых, RNN содержат всю информацию о последовательности в скрытом состоянии, которое обновляется с каждым шагом. Если модели необходимо «вспомнить» что-то, что было сотни шагов назад, то эту информацию необходимо хранить внутри скрытого состояния и не заменять чем-то новым. Следовательно, придется иметь либо очень большое скрытое состояние, либо мириться с потерей информации.
- Во-вторых, обучение рекуррентных сетей сложно распараллелить: чтобы получить скрытое состояние RNN-слоя для шага $i+1$, вам необходимо вычислить состояние для шага $i$. Таким образом, обработка батча примеров длиной $1000$ должна потребовать $1000$ последовательных операций, что занимает много времени и не очень эффективно работает на GPU, созданных для параллельных вычислений.

Обе этих проблемы затрудняют применение RNN к по-настоящему длинным последовательностям: даже если вы дождетесь конца обучения, ваша модель по своей конструкции будет так или иначе терять информацию о том, что было в начале текста. Хочется иметь способ «читать» последовательность так, чтобы в каждый момент времени можно было обратиться к произвольному моменту из прошлого за константное время и без потерь информации. Таким способом и является лежащий в основе трансформеров механизм self-attention, о котором далее пойдет речь. Как мы узнаем позже, благодаря своей универсальности и масштабируемости этот механизм оказался применим к множеству задач помимо обработки естественного языка.

Ниже приведено устройство архитектуры «трансформер» из оригинальной [статьи](https://arxiv.org/pdf/1706.03762.pdf):

![ml_transformery_1_d962e897a7.svg](https://yastatic.net/s3/education-portal/media/ml_transformery_1_d962e897a7_29a7a070f1.svg)

Слева на схеме представлено устройство энкодера. Он по очереди применяет к исходной последовательности $N$ блоков:

![ml_transformery_1_c190108d7e.svg](https://yastatic.net/s3/education-portal/media/ml_transformery_1_c190108d7e_0cf50c39c9.svg)

Каждый блок выдаёт последовательность такой же длины. В нём есть два важных слоя, multi-head attention и feed-forward. После каждого из них к выходу прибавляется вход (это стандартный подход под названием residual connection) и затем активации проходят через слой layer normalization: на рисунке эта часть обозначена как “Add & Norm”. 

У декодера схема похожая, но внутри каждого из $N$ блоков два слоя multi-head attention, в одном из которых используются выходы энкодера.

Давайте подробнее обсудим каждую из составляющих частей этого механизма.

## Слой внимания

Первая часть transformer-блока — это слой self-attention. От обычного внимания его отличает то, что выходом являются новые представления для элементов той же последовательности, что мы подали на вход, причем каждый элемент этой последовательности напрямую взаимодействует с каждым. 

Если говорить более подробно, то в вычислении внимания для последовательности будет участвовать три обучаемых матрицы $W_Q,\ W_K,\ W_V$. Представление $x_i$ каждого элемента входной последовательности мы умножаем на $W_Q,\ W_K,\ W_V$, получая вектор-строки $q_i, k_i, v_i$ ($i$ — номер элемента), которые соответственно называются *запросами*, *ключами* и *значениями* (query, key и value). Их роли можно условно описать следующим образом:

- $q_i$ — запрос к базе данных;
- $k_i$ — ключи хранящихся в базе значений, по которым будет осуществляться поиск;
- $v_i$ — сами значения.

![queries_keys_values_7a1ffcfa39.webp](https://yastatic.net/s3/education-portal/media/queries_keys_values_7a1ffcfa39_0b5b83a4c4.webp)

Близость запроса к ключу можно определять, например, с помощью скалярного произведения:

$$\text{self-attention weights}_i=\text{softmax}\left(\frac{q_ik_1^T}{C},\frac{q_ik_2^T}{C},\ldots\right),$$

где $C$ — некоторая нормировочная константа. Именно так и делали в исходной статье; в качестве нормировочной константы брался корень $\sqrt{d_k}$ из размерности ключей и значений. Теперь мы складываем значения $v_i$ с полученными коэффициентами. Это и будет выходом слоя self-attention. В векторизованном виде можно записать:

$$\text{self-attention}(Q, K, V)=\text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V,$$

где $Q$, $K$, $V$ — матрицы запросов, ключей и значений соответственно, в которых по строкам записаны $q_i$, $k_i$, $v_i$, а $\text{softmax}$ берётся построчно.

### Особенности слоя внимания в декодере

Как мы уже упоминали выше, в декодере один из attention-слоёв является слоем кросс-внимания (cross-attention), в котором запросы берутся из выходной последовательности, а ключи и значения — из входной (то есть из результатов работы энкодера).

![decoder_attention_03be436fa1.webp](https://yastatic.net/s3/education-portal/media/decoder_attention_03be436fa1_b4c942b0ae.webp)

Также стоит учитывать, что в описанном выше виде внимания каждый токен будет «смотреть» на всю последовательность, что нежелательно для декодера. Действительно, на этапе генерации мы будем порождать по одному токену за шаг, и доступ к последующим шагам на этапе обучения приведёт к утечке информации в декодере и низкому качеству модели. Чтобы избежать этой проблемы, при обучении к вниманию нужно применять авторегрессивную маску, вручную обращая в $-\infty$ веса до softmax для токенов из будущего, чтобы после softmax их вероятности стали нулевыми. Как можно увидеть на рисунке внизу, эта маска имеет нижнетреугольный вид.  

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/ml_transformery_4_5594f40524_c4d9db63fd.svg" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="http://nlp.seas.harvard.edu/annotated-transformer/">Источник</a>
  </figcaption>
</figure>

### Multi-head attention

Один набор $Q$, $K$ и $V$ может отражать только один вид зависимостей между токенами, и матрицы извлекают лишь ограниченный набор информации из входных представлений. Чтобы скомпенсировать эту неоптимальность, авторы архитектуры предложили подход с несколькими «головами» внимания (multi-head attention): по сути вместо одного слоя внимания мы применяем несколько параллельных с разными весами, а потом агрегируем результаты. Рисунок ниже показывает, как выглядит multi-head attention:

![ml_transformery_2_4254512be8.svg](https://yastatic.net/s3/education-portal/media/ml_transformery_2_4254512be8_ffdde2ee96.svg)

### Эффективность

Подход к обработке последовательностей целиком через внимание позволяет избавиться от такого понятия, как скрытое состояние, обновляющееся рекуррентно: каждый токен может напрямую «прочитать» любую часть последовательности, наиболее полезную для предсказания. В частности, отсутствие рекуррентности означает, что мы можем применять слой ко всей последовательности одновременно, так как матричные умножения прекрасно параллелятся.

Однако стоит помнить о затратах памяти и времени: поскольку каждый элемент последовательности взаимодействует с каждым, легко показать, что сложность self-attention составляет $O(n^2)$ по длине последовательности, а простые реализации, формирующие полную матрицу внимания, будут расходовать ещё и $O(n^2)$ памяти. С оптимизацией вычислительной сложности внимания связано множество работ как инженерного, так и архитектурного плана: в частности, есть подходы, которые позволяют сократить время работы self-attention до линейного или существенно уменьшают константы за счёт учёта иерархии памяти GPU.

Например, на графиках ниже сравнивается время работы и потребление памяти трансформера со стандартным вниманием и с механизмом из [статьи](https://arxiv.org/abs/2004.05150) Longformer:

![longformer_plots_1aa1cd4475.webp](https://yastatic.net/s3/education-portal/media/longformer_plots_1aa1cd4475_cc09216035.webp)

## Полносвязный слой и нормализация

Вторая часть трансформерного блока называется feed-forward network (FFN) и представляет собой два обычных полносвязных слоя, применяемых независимо к каждому элементу входной последовательности. В последних архитектурах размер промежуточного представления (то есть выхода первого слоя) бывает весьма большим — в 4 раза больше выходов блока.  

Из-за этого вычислительной стоимостью FFN не стоит пренебрегать: несмотря на квадратичную асимптотику внимания, в больших моделях или на коротких последовательностях FFN может занимать существенно больше времени по сравнению с self-attention. В виде формулы применение FFN можно представить так:

$$
\text{FFN}(x)=\text{act}\left(xW_1+b_1\right)W_2+b_2,
$$

Промежуточные активации $\text{act}$ в FFN бывают разными: начиналось всё с широко известной ReLU, но в какой-то момент сообщество перешло на [GELU (Gaussian Error Linear Unit)](https://arxiv.org/abs/1606.08415v4) с формулой $x\Phi(x)$, где $\Phi$ — функция распределения стандартной нормальной случайной величины.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/ml_transformery_5_e006e36b24_685f47dccb.svg" loading="lazy" decoding="async" alt="">
  <figcaption>
    Сравнение ReLU, ELU и GELU. <a href="https://arxiv.org/abs/1606.08415v4">Источник</a>
  </figcaption>
</figure>

Скажем ещё пару слов о layer normalization: как было показано в [ряде](https://arxiv.org/abs/2002.04745) [работ](https://aclanthology.org/2020.emnlp-main.463/), их положение внутри residual-ветки довольно важно. В стандартной архитектуре используется формулировка PostLN, где нормализация применяется после остаточной связи. Однако такое применение нормализации оказывается довольно нестабильным при обучении моделей с большим числом слоёв: вместо этого предлагается использовать PreLN (справа на рисунке снизу), где нормализация применяется ко входу residual-ветки.

![ml_transformery_6_94683ebb85.svg](https://yastatic.net/s3/education-portal/media/ml_transformery_6_94683ebb85_fc02da13f3.svg)

## Кодирование позиций

Внимательный читатель может заметить, что все операции внутри трансформер-блока, строго говоря, инвариантны к порядку элементов в последовательности. Например, результат внимания зависит от скалярных произведений между эмбеддингами токенов, но расположение этих токенов внутри текста значения не имеет. Таким образом, итоговые представления каждого токена на выходе из модели будут одинаковыми вне зависимости от порядка слов, что вряд ли нас устроит. Как с этим справиться?

На помощь приходит такая вещь, как позиционные эмбеддинги. Это вспомогательные представления, которые прибавляются к обычным эмбеддингам токенов входной последовательности и позволяют слоям внимания различать одинаковые токены на разных местах.  

Исторически первым подходом были фиксированные эмбеддинги, однозначно кодирующие позицию тригонометрическими функциями (ниже $pos$ — номер позиции, $i$ — индекс элемента в векторе, кодирующем эту позицию, $d$ — размерность эмбеддинга):

$$
\begin{align*}
\textrm{PE}_{(pos,2i)}&=\sin\left(\frac{pos}{10000^{2i/d}}\right),\\
\textrm{PE}_{(pos,2i+1)}&=\cos\left(\frac{pos}{10000^{2i/d}}\right).
\end{align*}
$$

С момента появления архитектуры «трансформер», однако, появилось множество других способов кодировать позиции токенов. Например, можно просто сделать позиционные эмбеддинги обучаемыми наряду с эмбеддингами токенов. Иной подход — напрямую учесть тот факт, что нам важны не абсолютные позиции токенов, а расстояние между ними, и обучать [*относительные*](https://arxiv.org/abs/1803.02155) позиционные представления: подобный подход заметно улучшает качество на чувствительных к порядку слов задачах, а его более современные [модификации](https://arxiv.org/abs/2108.12409) регулярно используются в самых мощных моделях.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/positional_embeddings_6582114cd8_750074ea6f.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
  Позиционное кодирование ALiBi: метод добавляет необучаемые константы к весам внимания в зависимости от расстояния между токенами ключа и значения. 
    <a href="https://arxiv.org/abs/2108.12409">Источник</a>
  </figcaption>
</figure>

## Про BERT и GPT

Несомненно, трансформер-модели не были бы так интересны, если бы практически все задачи NLP сейчас не решались бы с помощью этой архитектуры. Главными факторами, повлиявшими на бурный рост популярности идеи self-attention, послужили два семейства хорошо всем известных архитектур — BERT и GPT, которые в некотором роде являются энкодером и декодером трансформера, которые зажили своей жизнью.

Модель **GPT** (**Generative Pretrained Transformer**) хронологически [появилась](https://openai.com/research/language-unsupervised) раньше. Она представляет собой обычную языковую модель, реализованную в виде последовательности слоев декодера трансформера.  

В качестве задачи при обучении выступает обычное предсказание следующего токена (то есть многоклассовая классификация по словарю). Важно, что в качестве маски внимания как раз выступает нижнетреугольная матрица: в противном случае возникла бы утечка в данных из-за того, что токены из «прошлого» будут видеть «будущее». Полученную модель можно использовать для генерации текстов и всех задач, которые на это опираются. Даже ChatGPT, обученная на специальных инструкциях, по своей сути незначительно отличается от базовой модели.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/ml_transformery_7_24995dbcc6_060b20218a.svg" loading="lazy" decoding="async" alt="">
  <figcaption>
    Иллюстрация задачи при обучении GPT. <a href="https://twitter.com/MishaLaskin/status/1481767762141888512">Источник</a>
  </figcaption>
</figure>

Как понятно из названия, модель **Bidirectional Encoder Representations from Transformers** (или **BERT**) отличается от GPT двунаправленностью внимания: это значит, что при обработке входной последовательности все токены могут использовать информацию друг о друге.  

Это делает такую архитектуру более удобной для задач, где нужно сделать предсказание относительно всего входа целиком без генерации, например, при классификации предложений или поиске пар похожих документов. Важно, что при этом BERT не учится генерировать тексты с нуля: одна из его задач при обучении — это masked language modeling (предсказание случайно замаскированных слов по оставшимся, изображено на рисунке ниже), а вторая — next sentence prediction (предсказание по паре текстовых фрагментов, следуют они друг за другом или нет).

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/bert_a4e464d873_4038d8fbdc.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
  Пример masked language modeling.
  </figcaption>
</figure>

Заметим, что самое ключевое отличие в моделях BERT и GPT (а не в задачах для обучения или применениях) можно свести к использованию разных видов внимания, изображенных на рисунке снизу.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/bert_vs_gpt_1d8df8bea4_9539ededda.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
  Отличия между вниманием в BERT и GPT. <a href="https://arxiv.org/abs/1810.04805">Источник</a>
  </figcaption>
</figure>

## Тонкости обучения

К сожалению, если вы просто напишете код Transformer-нейросети и попробуете сразу обучить что-то содержательное, используя привычные для других архитектур гиперпараметры, то вас с большой вероятностью постигнет неудача. Оптимизационный процесс для таких моделей зачастую требуется изменить, и недостаточное внимание к этому может повлечь за собой существенные потери в итоговом качестве или вообще привести к нестабильному обучению.

Первый момент, на который стоит обратить внимание, — размер батча для обучения. Практически все современные Transformer-модели обучаются на больших батчах, которые для самых больших языковых моделей могут достигать миллионов токенов. Разумеется, ни одна современная GPU не может обработать столько данных за один шаг: на помощь приходят распределенное обучение и чуть более универсальный [трюк](https://sebastianraschka.com/blog/2023/llm-grad-accumulation.html#what-is-gradient-accumulation) с аккумуляцией градиентов по микробатчам.  

Также в последних статьях зачастую прибегают к [увеличению размера батча](https://openreview.net/forum?id=B1Yy1BxCZ) по ходу обучения: идея заключается в том, что на ранних этапах важнее быстрее совершить много шагов градиентного спуска, а на поздних становится важнее иметь точную оценку градиента.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/training_hours_5890f926f3_0b80856616.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
  Размер батча может играть большую роль даже для сравнительно маленьких моделей. <a href="https://arxiv.org/abs/1804.00247">Источник</a>
  </figcaption>
</figure>

</figure>

Второй немаловажный фактор — выбор оптимизатора и расписания для learning rate. Обучить трансформер стандартным SGD, скорее всего, не выйдет: в оригинальной статье в качестве оптимизатора использовался Adam, и де-факто он остаётся стандартом до сих пор.  

Однако стоит заметить, что для больших размеров батча Adam порой работает плохо: из-за этого порой приходится прибегать к алгоритмам наподобие [LAMB](https://arxiv.org/abs/1904.00962), нормализующим обновления весов для каждого слоя.

## Трансформеры не для текстов

Разумеется, успех этого семейства архитектур на множестве текстовых задач не мог остаться незамеченным для исследователей в других доменах. Одним из наиболее ярких примеров областей, в которой Transformer-модели нашли новое приложение, несомненно, является компьютерное зрение.  

К примеру, [архитектура](https://arxiv.org/abs/2010.11929) ViT (Vision Transformer) в свое время побила рекорды качества по классификации изображений, задействуя идею self-attention для картинок, разделенных на множество «лоскутных» (patches) сегментов квадратной формы.  

Как пишут авторы статьи, идея использовать Transformer-архитектуру в зрении пришла к ним после наблюдения за успехами таких моделей в NLP: использование такого общего подхода, как self-attention, позволяет избежать необходимости явно закладывать в архитектуру особенности задачи (это ещё называют inductive bias) при достаточном времени обучения, числе параметров и размере выборки.

![Vi_T_c7f1bfbedc.webp](https://yastatic.net/s3/education-portal/media/Vi_T_c7f1bfbedc_e047c467f5.webp)

Также именно на трансформерах базируется генеративная часть DALL-E — модели, положившей начало активным исследованиям последних лет в генерации изображений по тексту. Концептуально DALL-E довольно проста: её можно рассматривать как авторегрессивную «языковую модель», генерирующую изображение по одному «визуальному токену» за шаг. 

Применяют трансформеры и к обучению с подкреплением: ярким примером является [работа](https://arxiv.org/abs/2106.01345) Decision Transformer, в которой предлагают использовать авторегрессивное моделирование с использованием этой архитектуры для построения агента.  

Авторы показали, что такой же подход, который используют для генерации текстов, можно использовать для предсказания действий в динамической среде: как показано на рисунке ниже, модель последовательно принимает стандартные тройки из закодированных состояний, текущих действий и наград и в качестве ответа на каждом шаге выдаёт следующее действие.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/ml_transformery_8_5f6f1a3105_1ba6071898.svg" loading="lazy" decoding="async" alt="">
  <figcaption>
  Архитектура Decision Transformer. <a href="https://arxiv.org/abs/2106.01345">Источник</a>
  </figcaption>
</figure>

  ## handbook

  Учебник по машинному обучению

  ## title

  Трансформеры

  ## description

  *null*

- 
  ## path

  /handbook/ml/article/obuchenie-predstavlenij

  ## content

  Объекты, с которыми работает машинное обучение, очень разнообразны и часто состоят из большого количества низкоуровневых сигналов: это может быть цвет отдельного пикселя, амплитуда звукового сигнала в отдельно взятый момент времени или буква в тексте.  

Каждый из таких слабых сигналов в отдельности несёт крайне мало информации про объект, но все вместе слабые сигналы складываются в музыку, изображение или текст.  

В _сыром_ виде такие объекты сложно анализировать, поэтому наша цель — находить _хорошие_ представления данных, удобные для анализа и решения разных задач. Любой способ построения признакового описания объекта мы будем назвать алгоритмом построения представлений, а настройку такого алгоритма по данным — обучением.

Так, все методы понижения размерности, например, SVD — это методы обучения представлений, а методы обучения представлений часто можно проинтерпретировать как методы понижения размерности. 

Представим, что нам хочется уметь искать похожие музыкальные треки и использовать эту технологию в музыкальном сервисе для функции «играть похожие треки». 

Каждый трек в нашей базе хранится в формате WAV с частотой 44kHz и длится 3 минуты. Другими словами, трек будет описываться вектором из 7920000 чисел (44000 Hz * 60 секунд * 3 минуты).

Однако небольшие изменения треков (сдвиг по времени, изменение громкости) могут соответствовать существенному изменению положения вектора представлений в пространстве. Поэтому простые расстояния в таком пространстве, вероятно, не будут отражать представление людей о схожести треков. Искать похожие треки, используя такие, _сырые_ представления, проблематично, и необходимо научится строить представления, с помощью которых будет удобно решать разные высокоуровневые задачи.   

Выученные представления или модели для вычисления представлений можно использовать для:  

- Поиска изображений: по представлению изображения искать похожие изображения. 
- Рекомендаций: по представлениям пользователя определять наиболее интересные фильмы или товары. 
- Чат-ботов: по представлению диалога уметь продолжать диалог, отвечать на вопросы и так далее.
- Систем видеосвязи: уметь восстанавливать кадры по компактным представлениям, сохраняя высокое качество картинки. 
- и многого другого.

Для каждой из этих задач будут свои хорошо отлаженные трюки, однако ключевых идей, которые часто переиспользуются, повторяются и модифицируются, не очень много. Мы постараемся рассказать про такие идеи. 

**Предупреждение:** перед прочтением этой главы стоит освежить в памяти главу про
<a href="https://academy.yandex.ru/handbook/ml/article/nejronnye-seti">нейронные сети</a>.

## Нейронные сети и выучивания представлений

Нейронные сети можно рассматривать, как механизм автоматического выучивания представлений, поэтому современные методы выучивания представлений существенно сконцентрированы на использовании нейросетей.  

Напомним, что нейронная сеть состоит из набора дифференцируемых преобразований, примененных друг за другом к объекту $x$ для получения предсказания целевой переменной $y$. Обычно преобразования содержат обучаемые параметры, которые настраиваются в процессе обучения по данным.  

Преобразования в литературе часто называют слоями. Результат применения преобразования к его входу мы будем называть скрытыми представлениями или активациями.  

### Representations 

Активации любого слоя можно использовать как представления объекта. Представления с разной глубины нейронной сети будут обладать разными свойствами.  

Рассмотрим свёрточные нейронные сети для изображений. Активации первых слоёв обычно видят только маленькие части исходной картинки, другими словами, имеют маленький _receptive field_. Такие активации могут реагировать — принимать большие значения — только на низкоуровневые детали, маленькие фрагменты изображения.  

По мере увеличения глубины receptive field становится больше, а активации начинают реагировать на более высокоуровневые абстракции, такие как формы и части объектов. Активации последних слоёв имеют большой receptive field и реагируют на уже на объекты и группы объектов. 

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/lecun_feat_level_8318bd67b3_33d314f27b.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://youtu.be/cWzi38-vDbE?t=312">Источник</a>
  </figcaption>
</figure>

На изображении ниже показаны части картинок (патчи), каждая группа из 9 изображений максимизирует значение определенной активации в обученной нейронной сети. Размер патча зависит от receptive field активации, а максимизация ведется по датасету реальных изображений:  выбираем топ-9 патчей из датасета по значению активации. 

Для активаций, взятых с ранних слоев, нейроны реагируют на низкоуровневые детали. По мере увеличения глубины нейроны начинают реагировать на более высокоуровневые объекты.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/Zeiler2013_ef78a7d040_4f0d0156c3.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://arxiv.org/abs/1311.2901">Источник</a>
  </figcaption>
</figure>

Большая часть методов, которые мы рассмотрим ниже, за исключением матричной факторизации (в зависимости от того, как на это взглянуть), будут использовать активации нейросети в качестве представлений. Поэтому, обучить представления и обучить нейросеть это почти синонимы. Большинство отличий будет состоять в том, как эти нейронные сети обучаются и какую архитектуру имеют. 

### Дообучение 

Нейронные сети можно обучать из случайной инициализации, а можно стартовать с вектора весов, обученного на внешнем датасете.  

К примеру, если вы решаете задачу классификации изображений, часто инициализация части вашей нейронной сети весами, предобученными на популярном датасете ImageNet, ускорит и улучшит обучение.  

Такой процесс называется fine-tuning («дообучение» / «файнтюнинг»):

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/finetune_da81330a6f_e5b55838c9.svg" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://d2l.ai/chapter_computer-vision/fine-tuning.html">Источник</a>
  </figcaption>
</figure>


Как можно усложнять эту схему:
- Добавлять в модель много новых, обучающихся с нуля слоёв (на картинке мы добавляем один, но можно и больше);
- Не обязательно копировать все слои, можно копировать только сколько-то первых.
- Дообучать как все веса модели, так и какую-то часть. К примеру, можно заморозить скопированные слои и дообучать только новые части модели.
- Для файнтюнинга часто используют постепенное увеличение (warm-up) learning rate на первых эпохах обучения. Это позволяет сетке «привыкнуть» к новой задаче и архитектуре. Пример:

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/warmup_cosine_schedule_6935e2bddd_d2c763f3ce.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://huggingface.co/transformers/main_classes/optimizer_schedules.html">Источник</a>
  </figcaption>
</figure>

### Prior

В некотором смысле хорошая инициализация работает как праер на функции, которые могут быть выучены после дообучения.  

Поэтому дообучение часто требует в разы меньше данных, чем обучение со случайной инициализации.  

## Supervised обучение

### Обучение представлений через решение supervised задачи

Обучение представлений через решение supervised задач — это простой и популярный способ обучения представлений. Рассмотрим его на примере задачи поиска изображений (image retrieval) 

**Задача**: Рассмотрим задачу поиска изображений. Каждое изображение хочется закодировать вектором признаков (представлением) так, чтобы вектора признаков похожих изображений были близки. 

**При чем тут обучение представлений?** Image retrieval часто рассматривается как задача обучения представлений. Хочется получить алгоритм, который по изображению выдаст вектор (представление объекта) так, чтобы близость векторов по какой-то простой (скажем, евклидовой) метрике означала схожесть объектов.  

**Идея**: Возьмём активации с последнего слоя из нейросети, предобученной на большом размеченном датасете. 
Для задач зрения почти всегда имеется ввиду предобучение на задаче классификации. Также мы предполагаем, что высокоуровневая разметка собрана человеком.

**Почему такие представления могут адекватно решать задачу поиска изображений?** Классификационная сеть будет неявно поощрять, чтобы у похожих изображений векторы активаций тоже были близки. К примеру, перед последнем слоем классификационной сети активации кошек и собак должны быть распознаны линейными классификаторами — активации картинок одного класса должны быть близки друг к другу. А за счет похожих паттернов визуально похожие коты будут находиться ближе друг к другу, чем непохожие.

#### Решение
Для начала нам нужно обучить нейросеть на большом размеченном датасете картинок/текста/звука/... (_pretext problem_)  

Обычно лучше всего работает предобучение на задачах классификации. Почему так происходит? Пока непонятно. Возможно, это связано с тем, что для классификации удобнее собирать датасеты, а возможно это хорошие свойства задачи или CrossEntropy функции потерь.  

- Для изображений часто используется предобучение на датасете [ImageNet](https://www.image-net.org/) — классификация на 1000 классов, 1.3M изображений в обучающей выборке, ~ 150 GiB.  
- Для текстов, обычно решают задачу языкового моделирования, на набирающем популярность датасет [The Pile](https://pile.eleuther.ai/) ~ 825 GiB.  

Затем мы дообучаем нейросеть на более похожем на нашу задачу размеченном датасете <span style="color:gray"> (если такой есть; если нет пропускаем этот шаг)</span>.  

После — оставляем только первые $L$ слоёв. Aктивации слоя $L$ берём в качестве представлений объектов   

Агрегируем активации по пространственным координатам, чтобы получить _вектор_ для каждого объекта. Часто используется покомпонентное среднее или максимум (скажем, глобальный пулинг для изображений).  

Наконец, используем признаки или предобученные веса для решения целевой задачи (_downstream problem_).

#### Об алгоритме

Supervised подход можно применять для различных типов данных. Всё, что нужно — это большой размеченный датасет, хоть и отдалённо, но похожий на целевые данные. Для музыки это может быть задача классификации жанра, для видео — задача классификация действий, для текста — классификация тематик.

#### Достоинства и недостатки

👍 Благодаря выученным представлениям мы сможем решать целевую задачу, не имея для неё большого датасета;
👎 Нужен большой размеченный датасет, близкий для целевой задачи;
👎 Оптимальные представления для датасета, на котором мы предобучаемся, могут сколь угодно плохо подойти для целевой задачи. К примеру, представления, полученные на ImageNet, плохо подойдут для медицинских изображений (<a href="https://arxiv.org/abs/1902.07208">Raghu2019</a>).

#### Эксперименты

На рисунке ниже показан пример того, как представления помогают решать задачу поиска. Запрос находится в самом левом столбике. Зеленым отмечены верно найденные изображения, красным — неверно найденные, синим — изображения из стоп-листа.

Как видно, система вполне неплохо решает задачу поиска изображений. Подробнее про такой подход для поиска изображений можно почитать в работах  (<a href="https://link.springer.com/content/pdf/10.1007/978-3-319-10590-1_38.pdf">Babenko 2014</a>, <a href="https://openaccess.thecvf.com/content_iccv_2015/papers/Babenko_Aggregating_Local_Deep_ICCV_2015_paper.pdf">Babenko 2015</a>). 

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/babenko2015_140d789f33_4d5d979f52.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://openaccess.thecvf.com/content_iccv_2015/papers/Babenko_Aggregating_Local_Deep_ICCV_2015_paper.pdf">Источник</a>
  </figcaption>
</figure>

#### Советы

<a href="https://arxiv.org/abs/1912.11370">Статья</a> Big Transfer (BiT): General Visual Representation Learning (Kolesnikov at el. 2019) даёт ряд важных советов, о том, на что именно стоит обратить внимание при supervised обучении c целью переноса представлений и весов модели на новые задачи.

Рассмотрим их подробнее.

- **Большие и разнообразные датасеты**: Увеличение размера pretex-датасета вносит существенный вклад в качество решения downstream задачи. Авторы продемонстрировали, что при предобучении переход от 1М изображений (ImageNet) к 14М изображений (ImageNet21k) к 300M изображений (JFT), стабильно улучшает качество дообучения на новой задаче с маленьким числом размеченных примеров. Да, тут мы заходим на территорию, когда ImageNet рассматривается как маленький датасет. 

- **Большие pretext модели**: Увеличение датасета при недостаточном размере модели может навредить. Нужно одновременно иметь большие модели и большие датасеты. Одно из возможных объяснений такое: с увеличением датасета модель должна предсказывать правильные ответы на трейне для огромного числа точек. При этом нельзя работать очень плохо хоть на каких-то точках, ведь когда гибкости недостаточно, моделька настраивается «так себе» во многих областях пространства, что может ухудшить финальное качество алгоритма.    
- **Долгое обучение**: Большие модели требуют много шагов оптимизации.  

{% cut "Оговорка" %}

Большие модели часто учатся на десятках, сотнях или даже тысячах вычислителей, таких как GPU и TPU, и требуют много памяти для обучения. Как правило, это происходит из-за большого размера моделей: на один вычислитель часто помещаются только маленькие минибатчи (1-10 примеров). Маленькие минибатчи работают плохо с популярной Batch Normalization или требуют дорогой синхорнизации между вычислителями. 

В статье [(Kolesnikov at el. 2019)](https://arxiv.org/abs/1912.11370) Batch Normalization заменяется на Group Normalization, которая позволяет использовать батчи маленького размера на каждом отдельном вычислительном девайсе.<a href="https://paperswithcode.com/method/group-normalization"> Group Normalization </a> используется в комбинации c <a href='https://paperswithcode.com/method/weight-standardization'> Weight Standardization </a>, что позволяет улучшить обучение.

{% endcut %}  

### Обучение метрических эмбедингов с использование разметки (triplet loss)

**Мотивация:** После supervised обучения расстояния между эмбеддингами не обязаны хорошо отражать треубуемую для решения нашей задачи _«похожесть»_. Поэтому хочется, чтобы _«похожесть»_ моделировалась известным расстоянием (к примеру евклидовым).

Для этого была [предложена](https://arxiv.org/abs/1503.03832) триплетная фунция потерь или **triplet loss** (Schroff at el. 2015). Триплетный лосс обучается на тройках объектов (якорный объект, негативный к якорному, позитивный к якорному). Информация о позитивных и негативных объектах – это один из видов разметки. Этот лосс может использоваться как для обучения с нуля, так и для дообучения.  

Отметим, что объекты не обязательно должны быть из одного домена: к примеру, якорный объект может быть картинкой, а позитивные и негативые объекты текстами. Таким образом, мы сможем находить «подходящие» тексты к картинкам и наоборот. 

#### Как будет устроено обучение

- рассмотрим тройки объектов $(\text{obj}_i,  \text{pos}_i, \text{neg}_i)$, где $\text{pos}_i$ — позитивный пример к $\text{obj}_i$, $\text{neg}_i$ — негативный пример к $\text{obj}_i$
- будем притягивать $emb(\text{obj}_i)$ и $emb(\text{pos}_i)$ и отталкивать $emb(\text{obj}_i)$ и $emb(\text{neg}_i)$
- одним из популярных лоссов для решения такой задачи является triplet loss:

$$L=\sum_i^N\left[\left\|emb_\theta(\text{obj}_i)-emb_\theta(\text{pos}_i)\right\|_2^2 - \left\|emb_\theta(\text{obj}_i)-emb_\theta(\text{neg}_i)\right\|_2^2+\alpha\right]_+.$$

- $emb$ представляется нейронной сетью $emb_\theta(\cdot)$
- $\alpha$ — параметер зазора — в некотором смысле усложняет задачу:
	- при $alpha=0$ достаточно, чтобы позитивный эмбединг был ближе якорному, чем негативный;
	- с параметром $alpha=0.5$ мы начинаем требовать, чтобы позитивный был ближе, чем негативный, как минимум на $0.5$.
- лосс $L$ оптимизируем по параметрам $\theta$.

![triplet_15aa014e42.webp](https://yastatic.net/s3/education-portal/media/triplet_15aa014e42_01339a3157.webp)

{% cut "Почему бы просто не притягивать позитивные примеры? Нужны ли нам негативные?" %}

Если будем только притягивать, то любой константный вектор будет хорошим решением. К примеру $emb(x) = [1,\dots, 1]$.

{% endcut %}  

#### Алгоритм формирования троек

Обучение с триплет лоссом сильно зависит от алгоритма формирования троек. 
Если формировать тройки случайно, то большинство троек будут слишком легкими, не информативными. Негативные объекты будет слишком легко отличить от позитивных, поэтому обучающего сигнала от таких троек будет мало. 

Поэтому хочется собрать наиболее сложные тройки из всех объектов в датасете или минибатче.  

Такой процесс называется hard negative/positive mining и часто используется для обучения с триплетной функцией потерь. 

🧪 **_Примеры_**:
#### Примеры
- Диалоговая система: 
  - $\text{obj}_i$ — фраза;
  - $\text{pos}_i$ — подходящий ответ;
  - $\text{neg}_i$ — ответ не в тему.
- Верификация лица: 
  - $\text{obj}_i$ — лицо которое хотим верифицировать;
  - $\text{pos}_i$ — тот же человек, что и в $\text{obj}_i$, но с других ракукрсов, в другом освещении, ...;
  - $\text{neg}_i$ — лица других людей.

![triplet_2_dea8ab1697.webp](https://yastatic.net/s3/education-portal/media/triplet_2_dea8ab1697_0f2d5e2fa5.webp)

#### Достоинства и недостатки

👍 Обучение метрических эмбедингов (metric learning), в отличие от supervised подхода, использует информацию о метрике, что позволяет выучить более релевантные представления для целевой задачи. 

👎 Все еще требует разметки (на тройки объектов).

👎 Обучение с триплетным лоссом часто ведет себя нестабильно (еще нестабильнее, чем обучение нейросетей для других задач).

## Self-supervised обучение

В этом разделе мы хотим показать, что нейронные сети и представления можно предобучать без рукотвороной разметки.  

**_Мотивация_** Мы разобрали supervised предобучение нейронных сетей и их использование для извлечения признаков. Однако supervised подходы не всегда эффективны. Supervised обучение требует больших размеченных датасетов. 

Разметка данных — это трудоёмкий и дорогой процесс, на выходе от которого всё равно получается шумная, и зачастую смещенная разметка. Поэтому от ручной разметки данных хочется уйти или хотя бы постараться её минимизировать. 

Этого можно добиться, если научиться использовать неразмеченные данные для предобучения. Неразмеченные данные генерируются в огромном количестве, и их значительно проще собирать. Это позволит нам обучаться на  **огромных** коллекциях данных, размер которых был бы недостижим при необходимости сбора разметки.  

Также в каждом объекте, изображении, звуке или тексте содержится **в разы больше информации, которую можно учитывать при обучении**, чем закодировано в одном таргете.  

К примеру, один из тысячи классов можно закодировать всего десятью битами, а изображение содержит мегабайты внутренней полезной информации, котрую можно использовать для обучения. Поэтому подходы, которые могут обучаться без разметки, но с использованием внтурненнией информации, потенциально могут выучивать более хорошие представления, чем supervised подходы. 

💡Основная идея self-supervised обучения — обучение через решение синтетических supervised задач (pretext problems), источником разметки в которых является сам объект (текст, изображение, или видео). Отсюда и приставка "self" в названии подхода.

### Примеры pretext задач

1. предсказание объекта по его компактному описанию;
2. предсказание слова по контексту;
3. предсказание закрытой части изображения по открытой;
4. предсказание будущих кадров по прошлым. 

Если всё это кажется вам supervised-задачами, вы правы! Приставлка self- означает отсутствие внешней разметки.

Признаки и веса, выученные для решения, казалось бы, бесполезных pretext задач, на практике работают как очень хороший претрейнинг для решения supervised задач (downstream problems). Это позволяет достигать отличного качества, используя в сотни раз меньше размеченных данных по сравнению с чисто supervised подходами. 

Осталось ответить на вопрос: какие pretext задачи использовать?  

Универсального ответа нет, но оказывается, что многие pretext задачи используют контекст для обучения. Подробнее об этом расскажем далее.

### Использование контекста для обучения

Почему контекст так важен для обучения? Обучение людей, как и обучение алгоритмов, неразрывно связано с использованием контекста.  

При изучении иностранного языка часто прибегают к упражнениям вида «Вставте правильные слова в текст». Чтобы выполнить такое упражнение, человеку нужно учитывать контекст и предсказывать значения незнакомых слов, если это необходимо для понимания текста. 

![context_lang_9296220c0c.webp](https://yastatic.net/s3/education-portal/media/context_lang_9296220c0c_8b99244d22.webp)

Предложенная профессором Южно-Калифорнийского университета [Стивином Крашенйном](https://en.wikipedia.org/wiki/Stephen_Krashen) «гипотезы входного материала» (input hypothesis) предполагают, что для эффективного изучения языка человеку нужно читать и слушать текст, который немного превышает его текущий уровень. Скажем, содержит 10-15% незнакомых слов, но при этом остается понятным. Такой способ обучения требует восстановления значения незнакомых слов из контекста.

Визуальный контекст также широко используется при обучении детей. Вы можете помнить упражнения, в которых нужно было найти лишний предмет, закончить рисунок или раскрасить изображение. Такие задания требуют учета визуального контекста для решения задачи: важно уметь понимать принадлежность разных рисунков к одной группе, генерировать изображение, наблюдая только некоторую его часть и так далее. 

![context_vison_56ec078727.webp](https://yastatic.net/s3/education-portal/media/context_vison_56ec078727_cc7f1cfa3b.webp)

Self-supervised обучение представлений и моделей глубокого обучения использует похожие идеи обучения из контекста. 

К примеру, модель  word2vec [(Mikolov et al. 2013)](https://arxiv.org/abs/1301.3781) и BERT [(Devlin et al. 2018)](https://arxiv.org/abs/1810.04805) выучивают эмбединги слов, решая задачу предсказания слов по контексту. С философией word2vec вы уже познакомились в [параграфе](https://academy.yandex.ru/handbook/ml/article/beta-nejroseti-dlya-raboty-s-posledovatelnostyami) про нейросети для работы с последовательностями.

А некоторые модели для картинок решают пазлы [(Doersch](https://arxiv.org/abs/1505.05192), [Noroozi et al. 2017)](https://arxiv.org/abs/1603.09246), дополняют изображения или звуки [(van den Oord et al. 2018)](https://arxiv.org/abs/1807.03748), раскрашивают фотографии [(Zhang et al. 2016)](https://arxiv.org/abs/1603.08511) и ищут похожие объекты [(Chen et al. 2020)](https://arxiv.org/abs/2002.05709). 

Последнюю из них — SimCLR – мы подробно разберём ниже. 

### Self-supervised предобучение для изображений 

SimCLR — метод, который первым продемонстрировал, что self-supervised предобучение может достигать того же качества что и supervised обучение.  

Он основан на контрастивной функции потерь (**contrastive loss**), и в некотором смысле решает задачу поиска похожих объектов.  

Также мы разберем метод self-supervised предобучения для vision tranformer, который, в некотором смысле, дорисовывает картинку, а также демонстрирует, что методы self-supervised предобучения для изображений и текстов во многом похожи.  

Стоит отметить, что pretext задачи, которые мы будем обсуждать ниже, не являются «серебряной пулей».  

Известно, что такие задачи работают как хороший претрейнинг. Другими словами, позволяют получить хорошее качество для некоторых downstream задач (классификация, детекция, сегментация) после дообучения на небольшом количестве размеченных примеров.  

Хорошего понимания, почему эти методы работают, в области пока нет. Скорее всего, разные типы задач (downstream problems) будут требовать разных методов претрейнинга, но это мы поймём только в ближайшие несколько лет. 

#### A Simple Framework for Contrastive Learning of Visual Representations (SimCLR)

SimCLR решает синтетическую задачу поиска похожих  изображений. Вот как он работает на верхнем уровне:  

1. Для каждого изображения в минибатче генерируются две аугментации;
2. Выбирается одно из изображений; одна из его аугментаций считается запросом, вторая — позитивным ответом, аугментации остальных объектов — негативными примерами;
3. Цель модели — для каждого «запроса» найти позитивный пример.  
4. Выученные веса могут быть использованны для «дообучения»/«файнтюнинга» сети под финальную задачу.  

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/simclr_simple_80aaea53a0_073ffd6638.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    Иллюстрация задачи SimCLR для одного запросса из минибатча
  </figcaption>
</figure>

{% cut "А вот как работает SimCLR на низком уровне" %}

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/simclr_3ee8844008_a6cda5e945.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://generallyintelligent.ai/understanding-self-supervised contrastive-learning.html">Источник</a>
  </figcaption>
</figure>


{% endcut %}

#### Лосс 

SimCLR оптимизирует контрастив лосс (contrastive loss), который фактически является кросс энтропией на positive-negative разметке:

$$L_{positive}^{query}=-log\frac{\exp\left(sim(emb(query),emb(positive))\right)}{\sum_{z\in\mathrm{Neg}(query)}\exp\left(sim(emb(query),emb(z))\right)},$$

где $sim(\cdot,\cdot)$ — это косинусное расстояние, a лосс $L$ работает следующим образом: 

- Контрастивная функция потерь $L_{positive}^{query}$ притягивает друг к другу эмбединги запроса $emb(\color{#5180e6}{\textit{query}})$ и позитивного примера $emb(\color{#72bd44}{\textit{positive}})$, в то же самое время отталкивая эмбединги негативных примеров $emb(\color{#fd0007}{z})$;
- Максимум $sim(\color{#5180e6}{\textit{query}}, \color{#72bd44}{\textit{positive}})$ будет достигается в точке $\color{#5180e6}{\textit{query}}=\color{#72bd44}{\textit{positive}}$, поэтому эмбединги аугментаций одной и той же картинки будут притягиваться;
- Знаменатель требует, чтобы негативные эмбединги были далеко от запроса.

**Интуиция**: На контрастивную функцию потерь можно смотреть как на поиск ответа по запросу, который ведется только среди всех эмбедингов в текущем минибатче. Такая задача требует сохранения информации про контент на изображении (что, вообще говоря, не очень просто) и в то же время понижения размерности, так как эмбединги $f(\cdot)$ обычно имеют сравнительно низкую размерность. 

**Размер минибатча**: Размер минибатча влияет на количество отрицательных примеров. Чем больше отрицательных примеров — тем более сложную задачу мы ставим перед нейросетью. Существует некоторый баланс между сложностью задачи и качеством выученных представлений. Слишком простые задачи (то есть маленькие батчи) обычно не позволяют выучить хороших представлений: простая задача может хорошо решаться даже с помощью «плохих» представлений. Поэтому SimCLR обучается хорошо только на очень больших мини-батчах (с тысячами примеров). 

#### Что нам нужно иметь перед началом обучения

- неразмеченный датасет изображений $X = {x_0, ..., x_N}$
- операцию аугментации изображения $aug(x_j)$
- энкодер $f_\theta(\cdot): Image \to R^M$ (типичные значения M~2048)
- проекция $g_\psi(\cdot): R^M \to R^K$  (типичные значения K~128)

✍️ В примере сверху $emb (x) = g \odot f (x)$

#### Как мы обучаемся

1. Семплируем мини-батч объектов $\hat{Х} \sim X$;

2. Для каждого объекта в минибатче $\hat{Х}$:  
	— Cемплируем две аугментации ${v}_i, {v^\prime}_i = aug(\hat{Х}_i), aug(\hat{Х}_i)$;  

	— Вычисляем эмбединги $y_i, y^\prime_i =  f_\theta(v_i), f_\theta(v^\prime_i)$;  

	— Вычисляем проекции  $z_i, z^\prime_i =  g_\psi(v_i), g_\psi(v^\prime_i)$;  

3. Вычисляем contrastive loss $\mathcal{L}=\sum_i l^{z_i}_ {z^\prime_i} + \sum_i l^{z^\prime_i}_{z_i}$, используя $sim(\cdot,\cdot) = \frac{u^Tv}{\|\|u\|\|\ \|\|v\|\|}$.

	$$l_p^q=-log\frac{\exp(sim(q,p))}{\sum_{z_i,z_i'\neq q}[\exp(sim(q,z_j))+\exp(sim(q,z_j'))]}$$

	— В $\mathcal{L}$ два слагаемых из-за того, что в паре (изображение, аугментация), вообще говоря, любой элемент можно выбрать в качестве запроса (другой тогда будет позитивным примером). Тем самым из одного мини-батча картинок мы можем сделать два мини-батча для обучения SimCLR.  

	— Функция потерь $\mathcal{L}$ вычисляется для низкоразмерных проекций $z_i, z^\prime_i  \in R^K$.

4. Делаем шаг по градиенту $\nabla_{\theta,\psi} \mathcal{L}$, повторяем с шага 1 пока не сойдёмся; 

5. Используем $f_{\theta}(\cdot)$ для генерации эмбедингов или файнтюнинга под supervised задачу. 

#### Почему это вообще работает?

Точно никто не знает, но приведем следующую гипотезу:

Контрастивная функция потерь требует различать аугментации разных изображений. При этом эмбеддинги должны содержать информацию о контенте изображения, чтобы осуществлять поиск аугментаций одинаковых изображений по ключу. Этот процесс позволяет создать представления изображений, сохраняющие достаточно много информации про контент, чтобы решать не только задачу поиска аугментаций, но и другие задачи.

#### Результаты

Претрейнинг, который мы обсудили выше, позволяет эффективно дообучать модели и получать качество, сравнимое с supervised обучением, используя в 100 раз меньше размеченных примеров. 

![simclrv2_7e663fa0ad.PNG](https://yastatic.net/s3/education-portal/media/simclrv2_7e663fa0ad_88ce6f57eb.PNG)

Оговорка в том, что эти результаты получены второй весрсией метода SimCLRv2 [(Chen at. el, 2020)](https://arxiv.org/pdf/2006.10029.pdf).
SimCLRv2 TLDR; модели больше, глубже сеть проекции, улучшение качества происходит за счет дистиляции.


{% cut "Немного вопросов и ответов про SimCLR" %}

**Какие аугментации выбрать?** 

Авторы предлагают использовать resize random crop, random flip, color distortions, Gaussian blur. Такая комбинация была найдена небольшим перебором. Для разных данных оптимальный набор аугментаций может получаться разным.
	
![sim_clr_augs_e5e2164aa3.PNG](https://yastatic.net/s3/education-portal/media/sim_clr_augs_e5e2164aa3_6de7de26af.PNG)

**Сложно ли такое учится?**

Да, но сложности в основном технические. Вы быстро оказываетесь наедине c размером батча 2048 на ImageNet (не забудьте ещё, что на каждую картинку 2 аугментации, поэтому реальный размер батча 4048). Даже с не самой большой сеткой, например, ResNet50 (25М параметров) приходится использовать не один десяток ГПУ с 32 Гб памяти в каждой и долго ждать, пока всё обучится. Процесс чтения 2k картинок с диска тоже может занимать намного больше времени, чем вы предпологали, а одновременное вычисление 4k аугментаций быстро создает bottleneck в CPU. Но если у вас много GPU, батч успевает грузиться быстро, и аугментации не упираются в CPU, так что основные сложности позади.  

**Можно ли обойтись без contrastive loss/негативных примеров?** 

Да можно: так делают авторы [статьи](https://arxiv.org/pdf/2006.07733.pdf) "Bootstrap your own latent: A new approach to self-supervised Learning" (Grill et al.). Но пока метод достаточно новый, и его рано добавлять в учебник. Если вам очень хочется узнать про этот метод, рекомендуем посмотреть разбор с анализом на [канале](https://youtu.be/YPfUiOMYOEE) Yannic Kilcher.

{% endcut %}  

#### Vision Transformer и BERT-like обучение

Одна из самых популярных self-supervised задач в NLP — это предсказание замаскированных токенов (masked tokens prediction [Devlin at el. 2019](https://arxiv.org/pdf/1810.04805.pdf)). При обучении такая модель (обычно transformer) видит текст, в котором некоторые токены заменены на специальный токен [MASK]; задача модели — правильно предсказать замаскированные токены по контексту.  

Оказывается, такой претрейнинг позволяет очень хорошо адаптировать модель для решения разных задач, таких как классификация текстов, используя при этом мало размеченных примеров. 

{% cut "Оговорка" %}

На самом деле, модель будет способна не только предсказывать замаскированные токены, но и для оригинальных предсказывать «более логичные», на её взгляд замены: например, с её помощью можно исправлять опечатки. Это можно использовать на этапе обучения модели, чтобы показывать ей, что токена [MASK] может и не быть в тексте. Это может оказаться полезным на этапе файнтюнинга, где уже нет маскировки токенов.  

Подробнее про Transformer, BERT, и masked tokens prediction можно прочитать в курсе [NLP for You](https://lena-voita.github.io/nlp_course.html).
 
{% endcut %}

Можно ли использовать такой self-supervised подход для изображений? Оказывается, что да! В этом помогает vision transformer.

В последнее время модели на основе **vision transformer** (ViT) [(Dosovitskiy at el. 2020)](https://arxiv.org/pdf/2010.11929.pdf) бурно развиваются и  компьютерного зрения. 

![vit_6c038fe6ea.webp](https://yastatic.net/s3/education-portal/media/vit_6c038fe6ea_eb16e4b2cc.webp)

В supervised режиме для задачи классификации vision transformer обучается следующим образом:
1. Изображение нарезается на квадратные патчи одинакового размера;
2. Затем патчи вытягиваются в последовательность;
3. Каждый патч вытягивается в столбец пикселей;
4. Каждый стобец проецируется обучаемой матрицей; 
5. К каждому вектору с шага 4 добавляются positional encoding (без позиционных эмбеддингов трансформер не учитывает позицию токена в последовательности, а positional encoding кодирует позицию токена и позволяют трансформеру учитывать эту информацию);
6. Векторы с шага 5 подаются в трансформер;
7. Классификационный токен на выходе предсказывает распределение на классы;
8. Вычисляется кросс-энтропия, делается шаг по её градиенту.

ViT не используют локальные операции, такие как свёртки. Как следствие, такие модели требуют заметно больше данных и параметров для обучения (300M изображений по сравнению со стандартным размером размеченного датасета 1.3М). Но оказывается, что **BERT-like self-supervised обучение применимо и для моделей vision transformer**, и позволяет обучать их без использовния гиганских датасетов. 

Какие self-supervised задачи на замаскированных патчах решают авторы статьи:
1. Предсказание среднего цвета в замаскированном патче;
2. Предсказание патча низкого разрешения и одновременное предсказание цвета;
3. Предсказание патча высокого разрешения разрешение с использованием $L^2$-лосса.

Во всех случаях обучается и файнтюнится вся сеть целиком. Этот интересный пример показывает, что pretext-задачи придуманные, для NLP-сетей могут быть применены и к задачам зрения.  

## Послесловие
Глубинное обучение — в существенной степени наука о представлениях сложных объектов. В этом параграфе мы лишь слегка затронули несколько важных тем: supervised предобучение, self-supervised предобучение, и metric learning. Self-supervised предобучение — это важный новый раздел глубинного обучения, который,  вероятно, поможет серьезно сократить количество необходимой разметки во многих приложениях. Генеративные модели VAE/inverse-GANs также широко используются для получения и обработки представлений. О них вы сможете прочитать в следующих параграфах. 

## Почитать по теме
1. [Contrastive Representation Learning](https://lilianweng.github.io/lil-log/2021/05/31/contrastive-representation-learning.html), Lilian Weng, May 2021.
2. [Self-Supervised Representation Learning](https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html), Lilian Weng, Nov 2019.
3. [Самообучение (Self-Supervision)](https://dyakonov.org/2020/06/03/%D1%81%D0%B0%D0%BC%D0%BE%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5-self-supervision/), Александр Дьяконов, Июнь 2020.
4. [_Self-Supervised Learning \| ICLR_](https://youtu.be/8TTK-Dd0H9U), Yann LeCun, May 2020.
5. [_Self-Supervised Learning \| UC Berkeley, CS294-158 Deep Unsupervised Learning_](https://youtu.be/dMUes74-nYY), Aravind Srinivas, Spring 2020.
6. [_Unsupervised Representation Learning \| DeepMind x UCL_](https://youtu.be/f0s-uvvXvWg).

  ## handbook

  Учебник по машинному обучению

  ## title

  Обучение представлений

  ## description

  Обучение представлений

- 
  ## path

  /handbook/ml/article/distillyaciya-znanij

  ## content

  В этом параграфе вы познакомитесь с продвинутой техникой машинного обучения, получившей название дистилляции знаний. Дистилляция знаний (knowledge distillation) — это способ обучения в первую очередь нейросетевых моделей машинного обучения, направленный на передачу знаний от модели-учителя к модели-ученику.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/Artboard_30_f40ffa108f_4980246392.svg" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://arxiv.org/abs/2006.05525">Источник</a>
  </figcaption>
</figure>

Слишком абстрактное определение? Соглашусь, но в последние годы дистилляция знаний как поле исследований сильно разрослась и стала включать в себя множество новых и, возможно, даже неожиданных сценариев применения. Так, авторы [статьи 2020 года](https://arxiv.org/abs/2006.00555) утверждают, что смогли добиться примерной инвариантности выходов полносвязной сети к сдвигу входа-картинки с помощью дистилляции в неё знаний из сверточной сети.  

Таким образом получается, что дистилляция знаний может применяться для того, чтобы передавать так называемые inductive biases от одной сети к другой. Схожие доводы встречаются и в статьях безумно популярного на момент написания данного параграфа направления трансформеров для компьютерного зрения.  

Так, использование дистилляции знаний [оказалась](https://arxiv.org/abs/2012.12877) важным компонентом для получения хорошего качества предсказания на ImageNet от ViT без использования дополнительных данных. Впоследствии данный подход [использовался](https://arxiv.org/abs/2104.01136) и в других трансформерах для компьютерного зрения, например, в LeViT.

Тем не менее, среди всего разнообразия применений дистилляции знаний наиболее ярко выделяется одно — сжатие моделей.

## Сжатие моделей

Задача сжатия моделей проистекает из следующего наблюдения. Неоднократно было замечено, что в широком диапазоне практически значимых задач машинного обучения точность предсказания модели существенно зависит от её размера. При этом зачастую данная зависимость выглядит достаточно тривиально: последовательное увеличение размеров модели позволяет последовательно улучшать точность её предсказаний.  

Однако такой безграничный рост приводит к ряду проблем, связанных с практическим применением итоговых моделей. Сюда относятся рост времени обучения больших моделей и повышенные аппетиты таких моделей к размерам и качеству обучающей выборки. Кроме того, большие модели нередко требуют более дорогостоящего вычислительного оборудования для эффективного применения, особенно если мы говорим об обработке большого количества запросов в сжатые сроки. А для некоторых сценариев, таких как предсказание в реальном времени и/или на мобильных устройствах, применение большой модели может оказаться вовсе невозможным.

Эти проблемы породили каждая свою ветвь исследований. Так в последние годы де-факто стандартным способом обучения даже относительно компактных моделей стало использование mixed-precision training, которое позволяет ускорить обучение более или менее любых сетей на современных графических процессорах, при этом практически без потерь в итоговом качестве. Для борьбы с недостатком обучающих данных была предложена целая плеяда методов self-supervised pretraining, и новые появляются до сих пор. Сжатие моделей же концентрируется на решении проблем, связанных с этапом применения уже обученных моделей.

Как можно догадаться из названия, задача сжатия моделей заключается в том, чтобы взять большую модель, и сжать её в как можно более компактную модель при этом по возможности минимально пожертвовав качеством предсказания.  

Исторически задачу сжатия моделей пытались решать множеством разных способов. Классическим примером здесь может служить прунинг, где модель обучается специальным образом (например, с использованием L2 регуляризации) так, чтобы часть весов в итоге можно было занулить и исключить из итоговой модели.  

Однако методы данного семейства, как правило, страдают от двух основных проблем.  

- Во-первых, простое удаление части весов каждого из слоёв обычно показывает лишь незначительное ускорение в применении итоговой модели за исключением случаев использования специализированной аппаратуры
- Во-вторых, наивный прунинг нередко приводит к существенной просадке в качестве предсказания сжатой модели, причём соотношение степени сжатия и качества итоговой модели едва ли возможно контролировать. Чтобы обойти данные ограничения, и была предложена техника дистилляции знаний.

## Хинтоновская дистилляция знаний

Первой статьёй, в которой можно встретить дистилляцию знаний в современном виде является [статья](https://arxiv.org/abs/1503.02531)  Хинтона и др. 2014 года. 

В ней авторы рассматривают задачу классификации картинок и предлагают использовать предсказания большой заранее обученной модели-учителя в качестве новой, *мягкой*, разметки, которую будет пытаться повторить компактный ученик. 

### Формулировка

Авторы предлагают использовать дивергенцию Кульбака-Лейблера между предсказаниями учителя и ученика в качестве дополнительного слагаемого к стандартной функции потерь — кросс-энтропии между предсказанием ученика и *жёсткой* разметкой:

$$
\mathcal{L}_{KD} = \frac1N \sum_{i=1}^N \left( - \sum_{j=1}^K y_{ij} \log p_{ij} + \lambda D_{KL}(\mathbf{p}_i \| \mathbf{q}_i) \right) =
$$

$$
= \frac1N \sum_{i=1}^N \left( - \sum_{j=1}^K y_{ij} \log p_{ij} + \lambda \sum_{j=1}^K q_{ij} \log \frac{q_{ij}}{p_{ij}} \right) \sim
$$

$$
\sim - \frac1N \sum_{i=1}^N \left( \sum_{j=1}^K y_{ij} \log p_{ij} + \lambda \sum_{j=1}^K q_{ij} \log p_{ij} \right)
$$

Здесь через $\mathcal{L}_{KD}$ обозначена функция потерь для дистилляции знаний. Под $N$ подразумевается число объектов, а под $K$ — классов, представленных в обучающей выборке. Через $y_{ij}$ обозначена жёсткая разметка:

$$
y_{ij} = \begin{cases}
1, & \text{если $i$-ый объект принадлежит $j$-ому классу}, \\
0, & \text{в противном случае}.\end{cases}
$$

Через $p_{ij}$ обозначены вероятности классов, предсказанные моделью-учеником, а через $q_{ij}$ — моделью-учителем. Коэффициент $\lambda$ позволяет настраивать баланс между решением исходной задачи и повторением предсказаний учителя.

В последнем переходе учтено, что логарифм частного раскладывается в разность логарифмов, после чего один из членов можно исключить, поскольку он не зависит от оптимизируемых значений $p_{ij}$. В дальнейшем для упрощения выкладок под $\mathcal{L}_{KD}$ будет подразумеваться именно последнее выражение.

### Мотивация

Свой выбор функции потерь авторы мотивируют следующим образом. Широко известным фактом является то, что при классификации картинок на достаточно больших и разнообразных датасетах большие нейронные сети стабильно показывают лучшие результаты по сравнению с компактными. Однако также хорошо известно, что даже сравнительно небольшие нейронные сети способны приближать очень широкий спектр функций.  

В таком случае можно предположить, что проблема обучения компактных сетей заключается не в том, что компактная модель не способна приблизить ту же функцию, что и большая, а в том, что компактная модель не способна самостоятельно выучить данную функцию из исходных данных. В таком случае потенциально мы можем подтолкнуть компактную модель к выучиванию более информативного представления путем модификации функции потерь.  

Как этого добиться? Давайте возьмем заведомо более информативное представление, выученное большой моделью-учителем, и добавим в функцию потерь слагаемое, которое будет учить модель-ученика повторять его. В случае решения задачи классификации KL-дивергенция является именно таким слагаемым.

Есть и другой способ взглянуть на хинтоновскую дистилляцию знаний. Минимизация $\mathcal{L}_{KD}$ отличается от стандартного обучения, тем, что мы дополнительно минимизируем расстояние между предсказаниями ученика и учителя. От стандартной разметки такая целевая переменная отличается наличием так называемых *теневых знаний* (dark knowledge), которые состоят из вероятностей принадлежности объекта ко всем классам, помимо истинного.  

Благодаря теневым знаниям модели-ученику во время обучения доступна дополнительная информация о взаимоотношениях между представленными в обучающей выборке классами, а также схожести отдельных объектов и классов.  

Чтобы проверить данную гипотезу, авторы проводят следующий эксперимент. Сначала они обучают модель-учителя классифицировать картинки на датасете [MNIST](http://yann.lecun.com/exdb/mnist/). После этого авторы обучают компактную модель-ученика с помощью ранее полученного учителя на тех же данных, но опуская при этом все картинки цифры $3$. После этого авторы показывают, что, если исправить коэффициент сдвига для данного класса в последнем слое сети-ученика с помощью небольшой валидационной выборки, сеть способна верно определить $98.6\%$ картинок тройки, несмотря на то, что во время обучения она не видела ни одного примера.  

Также косвенным подтверждением данной гипотезы можно считать и тот факт, что при использовании довольно популярной сейчас [техники сглаживания разметки (label smoothing)](https://arxiv.org/abs/1906.02629), эффективность дистилляции знаний заметно падает. Именно теневые знания на данный момент являются де-факто стандартным объяснением эффекта Хинтоновской дистилляции знаний.

### Использование температуры при подсчете KL-дивергенции

В качестве дополнительной эвристики авторы также предлагают перед взятием KL-дивергенции сглаживать распределения учителя и ученика с помощью температуры $T$, то есть вместо $\mathbf{p}_i$ и $\mathbf{q}_i$ считать KL-дивергенцию между $\mathbf{p}_i^T$ и $\mathbf{q}_i^T$, где:

$$\mathbf{p}_i^T = \text{softmax}\left(\frac{\mathbf{z}_i}{T}\right)$$

Здесь с помощью $\mathbf{z}_i$ обозначены логиты классов, предсказанные моделью-учеником. Формула для $\mathbf{q}_i^T$ выглядит аналогично.

Зачем нужна температура? Давайте рассмотрим формулу дополнительного слагаемого функции потерь. Для простоты выкладок я ограничусь функцией потерь для единственного объекта под номером $i$, а также опущу постоянный множитель $\frac{\lambda}{N}$, который также не существенен для данного повествования.

$$L_i = - \sum_{j=1}^K q_{ij} \log p_{ij}$$

Вспомним, что коэффициенты $q_{ij}$ приходят из предобученной модели-учителя, а значит являются константными с точки зрения процесса оптимизации.  

В таком случае несложно видеть, что мы имеем дело с чем-то очень близким к стандартной кросс-энтропийной функции потерь, но таргет $y_{ij}$ — это уже не one-shot закодированные номера классов, а что-то более интересное. В таком случае компоненты предсказания ученика, которые отвечают классам, оценённым учителем, как наиболее вероятные, получат большие веса и сформируют каркас итоговой функции потерь.  

В то же время все прочие компоненты получат околонулевые коэффициенты и влияния на функцию потерь практически не окажут. В какой-то степени эффект от этого может быть позитивным. Действительно, так как для преобразования предсказания нейронной сети в распределение вероятностей мы используем $\text{softmax}$, итоговая модель не может предсказать строго нулевую вероятность. Поэтому типичное предсказание обученной сети содержит в себе множество практически нулевых значений.  

При этом порядок между данными значениями определяется в первую очередь не похожестью объекта на представителей данных классов, а конкретным исходом стохастического процесса обучения данной модели. В таком случае нам вовсе не хотелось бы вынуждать ученика воспроизводить данный порядок, если ценой тому будет ухудшение точности предсказания истинного класса.

С другой стороны, нейронные сети являются зачастую излишне уверенными в себе классификаторами: их предсказание часто содержит ярко выраженный максимум, вероятность которого близка к единице даже в тех случаях, когда модели стоило бы усомниться. К сожалению, для нас это значит, что при дистилляции знаний из такой модели мы рискуем попасть в ситуацию, что итоговые веса $q_{ij}$ настолько малы для всех классов, кроме истинного, что наше дополнительное слагаемое по сути повторяет стандартную кросс энтропию и не способно внести хоть сколь-нибудь заметный вклад в обучение модели-ученика.  

Этот эффект можно нивелировать путем сглаживания предсказания учителя таким образом, чтобы сделать распределение $q_{ij}$ ближе к равномерному, для чего, собственно и используется температура.

В таком случае функция потерь задается следующим образом:

$$L_{KD} = - \frac1N \sum_{i=1}^N \left( \sum_{j=1}^K y_{ij} \log p_{ij} + \lambda \sum_{j=1}^K q_{ij}^T \log p_{ij}^T \right)$$

Но в данную формулу незаметно закралась одна неприятная деталь. Давайте рассмотрим градиент второго слагаемого в скобках. Как и в прошлый раз, для простоты выкладок я ограничусь случаем единственного объекта под номером $i$ и опущу константный множитель $\frac{\lambda}{N}$:

$$L_i = - \sum_{j=1}^K q_{ij}^T \log p_{ij}^T$$

Здесь легко узнаётся формула кросс-энтропийной функции потерь, градиент которой по логитам считается следующим образом:

$$\frac{\partial L_i}{\partial z_{ik}} = \frac{1}{T} (p_{ik}^T - q_{ik}^T)$$


{% cut "Доказательство формулы для градиента кросс-энтропийной функции потерь." %}

Так как мы будем искать частную производную функции потерь по логитам, давайте сначала выразим через них саму функцию потерь:

$$L_i = - \sum_{j=1}^K q_{ij}^T \log p_{ij}^T =$$

$$= - \sum_{j=1}^K q_{ij}^T \log \frac{\exp(z_{ij}/T)}{\sum\limits_{l=1}^K \exp(z_{il}/T)} =$$

$$= \sum_{j=1}^K \left(q_{ij}^T \log \sum_{l=1}^K \exp(z_{il}/T) - q_{ij}^T \log \exp(z_{ij}/T)\right) =$$

$$= \sum_{j=1}^K \left(q_{ij}^T \log \sum_{l=1}^K \exp(z_{il}/T) - q_{ij}^T z_{ij} / T\right)$$

Теперь мы готовы брать производную:

$$\frac{\partial L_i}{\partial z_{ik}} = \sum_{j=1}^K \left( q_{ij}^T \frac{1}{\sum\limits_{l=1}^K \exp(z_{il}/T)} \frac{1}{T} \exp(z_{ik}/T) - \frac{1}{T} q_{ik}^T \right) =$$

$$= \frac{1}{T} \left(p_{ik} \sum_{j=1}^K q_{ij}^T - q_{ik}\right)$$

Поскольку $q_{ij}^T$ для каждого фиксированного $i$ является вектором вероятностей, то $\sum_{j=1}^K q_{ij}^T = 1$, откуда мы и получаем искомую формулу.

{% endcut %}

Можно видеть, что при изменении температуры $T$ баланс между слагаемыми функции потерь (качеством решения задачи и качеством повторения предсказания учителя) нарушается. Действительно, если раньше мы настраивали его путём выбора подходящего коэффициента $\lambda$, то теперь мы приходим к тому, что при изменении температуры коэффициент $\lambda$ необходимо также менять: иначе при взятии градиента одно слагаемое функции потерь будет разделено на $T$, а другое останется неизменным.  

Разумным кажется ввести множитель $T$ в формулу для $\mathcal{L}_{KD}$ явным образом. Однако прежде, чем мы сделаем это, давайте ещё раз внимательно посмотрим на получившийся градиент:

$$\frac{\partial L_i}{\partial z_{ik}} = \frac{1}{T} (p_{ik}^T - q_{ik}^T) = \frac{1}{T} \left(\frac{\exp(z_{ik}/T)}{\sum_{l=1}^K \exp(z_{il}/T)} - \frac{\exp(v_{ik}/T)}{\sum_{l=1}^K \exp(v_{il}/T)}\right),$$

где через $v_{ij}$ обозначены логиты, предсказанные моделью-учителем.

Давайте теперь устремим $T$ к бесконечности. Раскладывая экспоненты в ряд Тейлора до первого слагаемого, получаем:

$$\frac{\partial L_i}{\partial z_{ik}} \approx \frac{1}{T} \left(\frac{1 + z_{ik}/T}{N + \sum_{l=1}^K z_{il}/T} - \frac{1 + v_{ik}/T}{N + \sum_{l=1}^K v_{il}/T}\right)$$

Вспомним теперь, что результат применения преобразования $\text{softmax}$ не зависит от сдвига на константу, поэтому на выходе из нейронной сети мы можем вычитать из логитов среднее значение таким образом, чтобы $\sum_{l=1}^K z_{il} = \sum_{l=1}^K v_{il} = 0$. В таком случае, предыдущая формула упрощается до:

$$\frac{\partial L_i}{\partial z_{ik}} \approx \frac{1}{NT^2} (z_{ik} - v_{ik})$$

Из этой формулы следует два вывода.  

- Во-первых, можно видеть, что для соблюдения баланса второе слагаемое в $\mathcal{L}_{KD}$ правильнее будет домножить не на $T$, а на $T^2$.  
- Во-вторых, в данной формуле можно узнать градиент квадратичной функции потерь между векторами логитов.  

То есть при стремлении температуры $T$ к бесконечности градиент второго слагаемого в $\mathcal{L}_{KD}$ стремится к градиенту квадрата нормы разности между логитами модели-ученика и модели-учителя.

Таким образом, мы приходим к финальной версии функции потерь:

$$L_{KD} = - \frac1N \sum_{i=1}^N \left( \sum_{j=1}^K y_{ij} \log p_{ij} + \lambda T^2 \sum_{j=1}^K q_{ij}^T \log p_{ij}^T \right)$$

Описанная выше статья произвела настоящий фурор в 2014 году. Дистилляция знаний путем минимизации KL-дивергенции между предсказаниями ученика и учителя хорошо зарекомендовала себя на практике и породила целый ряд исследований, направленных на использование и усовершенствование предложенного подхода.  

Вместе с методом прижилось и понятие теневых знаний, и его довольно часто можно встретить в статьях, посвящённых данной тематике. Кроме того, зародилась традиция изучения дистилляции знаний на примере задачи классификации картинок.  

Дальше по ходу параграфа мы ещё не раз столкнёмся с тем, что авторы различных методов часто прилагают результаты экспериментов на таких датасетах, как [CIFAR-10, CIFAR-100](https://www.cs.toronto.edu/~kriz/cifar.html), [ImageNet](https://www.image-net.org) и так далее.  

Тем не менее, сети для работы с данными других модальностей тоже дистиллируют, и начнем мы с разбора статьи, которая использует предложенный метод для решения задачи языкового моделирования (language modelling).

### DistilBERT как пример хинтоновской дистилляции

Одним из наиболее выдающихся примеров применения Хинтоновской дистилляции можно считать модель [DistilBERT](https://arxiv.org/abs/1910.01108), которая сохраняет 97% качества модели BERT (согласно бенчмарку [GLUE](https://arxiv.org/abs/1804.07461)), используя при этом на 40% меньше параметров и требуя на 60% меньше времени при применении. При этом столь выдающийся результат авторы получают, используя хинтоновский подход практически без изменений.

По аналогии с тем, как это делалось для модели-учителя (в роли которого выступает BERT), авторы обучают свою модель решать задачу маскированного языкового моделирования. В дополнение к хинтоновской функции потерь использовалось ещё косинусное расстояние между итоговыми векторными представлениями токенов, полученными с помощью ученика и учителя, *разворачивая* представлений ученика в сторону направлений, задаваемых представлениями модели-учителя.

Ещё одна интересная деталь в этой статье — способ инициализации модели-ученика. Действительно, в качестве архитектуры для своей сети, авторы решили переиспользовать архитектуру самого BERT, но с уменьшенным вдвое числом слоёв для ускорения.  

Авторы замечают, что большинство операций, которые используются в трансформерах, уже достаточно хорошо оптимизированы во всех популярных библиотеках, поэтому изменение размера внутренних представлений оказывает существенно меньшее влияние на итоговое время применения сети, нежели изменение количества слоёв. Поэтому в статья фокусировалась на сжатии модели именно в глубину, оставляя ширину неизменной. 

Поскольку веса слоёв модели-ученика имеют при таком подходе такие же размерности, что и веса слоёв модели-учителя, последние можно использовать при инициализации. Ровно так авторы и поступают, копируя веса каждого второго слоя исходной модели для инициализации DistilBERT. 

Может показаться, что умная инициализация не критична и наихудшим следствием использования более примитивной стратегии будет всего лишь увеличение времени, требуемого для обучения модели-ученика. Но авторы провели ablation study и выяснили, что обучение без умной инициализации приводит к потере почти $4.8$ процентных пункта итоговой метрики (обученная без неё модель сохраняет лишь $92.2\%$ качества модели-учителя).

Для сравнения, исключение из функции потерь кросс-энтропии между предсказанием ученика и истинной разметки приводит к потере лишь $0.4$ процентных пункта итоговой метрики, а исключение KL-дивергенции приводит к потере $3.8$ процентных пункта.

Интересно, что двумя годами позднее вышла независимая [статья](https://arxiv.org/abs/2106.05945), авторы которой показали, что хинтоновская дистилляция — это очень сложная оптимизационная задача со множеством локальных минимумов, которые сильно усложняют поиск глобального. 

Поскольку статья была написана независимо другими авторами, конкретный пример DistilBERT там не изучается, однако в целом авторы приходят к выводу, что умная инициализация может быть ключевым элементом для успеха дистилляции знаний.

## Дополнительные источники знаний для дистилляции

Несмотря на широкий успех хинтоновского подхода, дистилляция знаний им не ограничивается. 

Одно из наиболее очевидных направлений улучшения предложенного метода — это использование дополнительных способов передачи знаний от модели-учителя к модели-ученику. Действительно, в хинтоновской постановке единственный канал передачи знаний — это выходы с последнего слоя модели-учителя. 

Однако в случае нейронных сетей это отнюдь не единственный доступный нам источник информации. Например, можно использовать веса модели-учителя для умной инициализации, как при обучении DistilBERT. К сожалению, поскольку дистилляция знаний практически всегда сопряжена со сжатием модели, не всегда получается непосредственно использовать веса учителя, и в каждом отдельном случае приходится изобретать специализированные трюки. 

По этой причине DistilBERT — это единственная известная автору этого параграфа модель, в которой удалось добиться улучшения результатов благодаря использованию весов модели-учителя для умной инициализации.

Тем не менее, в нейронных сетях можно найти и другие источники информации. Хинтоновская дистилляция использует только выходы с последнего слоя сети. Почему бы нам дополнительно не использовать выходы промежуточных слоев? И действительно, [исследования показывают](https://arxiv.org/abs/1412.6550), что использование выходов промежуточных слоев позволяет улучшить результаты дистилляции знаний.

Для получения прироста качества авторы предлагают выбрать один или несколько промежуточных слоев модели-учителя, сопоставить каждому из них промежуточный слой модели-ученика, после чего использовать квадрат нормы разности выходов итоговых пар слоев в качестве дополнительного слагаемого к хинтоновской функции потерь.

К сожалению, несмотря на кажущуюся прямолинейность данного подхода, здесь возникают две сложности.

### Сложность №1

Мы явным образом предполагаем наличие заранее выбранных пар слоёв, оставляя за бортом вопрос о том, каким образом их собственно стоит выбирать. Поскольку дополнительные слагаемые функции потерь по сути обучают модель-ученика повторять промежуточные представления модели-учителя, разумным кажется сохранять порядок слоёв: слои из середины модели-учителя сопоставлять со слоями из середины модели-ученика, а слои, находящиеся ближе к концу модели-учителя, — со слоями, находящимися ближе к концу модели-ученика.

В частности, авторы оригинальной статьи просто берут средний слой в каждой из моделей и используют их в качестве своей единственной пары, однако это в большей степени связано с тем, что статья была написана в 2014 году и рассматривала достаточно маленькие по современным меркам модели. Более свежие статьи, как правило, работают с более глубокими сетями, а потому используют большее количество пар слоёв. 

Так, авторы следующей работы [рассматривают](https://arxiv.org/abs/1612.03928) глубокие сверточные сети с промежуточными связями (residual connections) и предлагают разбивать каждую из моделей на группы блоков с промежуточной связью таким образом, чтобы итоговое количество групп совпало. Пример такой разбивки можно видеть на картинке ниже.

Здесь к каждой группе относится по три блока в модели-учителе и по два блока в модели-ученике. После этого выходы каждой такой группы можно сопоставить друг другу и использовать для дистилляции знаний.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/Artboard_36_98f8d5dccd_5060fc72dc.svg" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://arxiv.org/abs/1612.03928">Источник</a>
  </figcaption>
</figure>

После того, как пары слоёв были выбраны, перед нами может возникнуть и второе препятствие.

### Сложность №2

Что, если выходы выбранных слоёв различаются по размерам? Такая ситуация запросто может случиться, ведь мы хотим, чтобы модель-ученик была поменьше, а один из способов сжатия — как раз уменьшение количества нейронов в полносвязных слоях.

В таком случае авторы оригинальной статьи [предлагают](https://arxiv.org/abs/1412.6550) использовать дополнительное преобразование, чтобы придать выходам модели-ученика нужные размеры (например, линейный слой).

Такие слои обучаются совместно с моделью-учеником, а после исключаются из сети при применении. В более поздних работах встречаются и другие, более продвинутые преобразования.  

Несмотря на кажущуюся интуитивность дистилляции промежуточных выходов, практическое применение это метода, к сожалению, осложняется необходимостью выбора целого ряда гиперпараметров. Скажем, оптимальные тактики выбора пар слоёв для дистилляции или дополнительных преобразований для выравнивания размерностей выходов до сих пор являются предметами активных исследований, точно так же, как и функции потерь для оптимизации.

## Иерархия методов дистилляции знаний

Выше мы рассмотрели два подхода к дистилляции знаний: хинтоновскую дистилляцию и дистилляцию промежуточных представлений. Как мы уже упоминали ранее, в последние годы область применения дистилляции знаний сильно разрослась, и новые методы появляются день ото дня.

Это породило довольно естественное желание систематизировать предложенные методы в некоторую иерархию. Мы рассмотрим две классификации методов:

- по режиму дистилляции,
- по области применения.

### Режимы дистилляции знаний

Различные подходы к дистилляции знаний принято делить по так называемым режимам. Выделяют три основных режима дистилляции знаний: offline-, online- и самодистилляция.

#### Offline-дистилляция знаний

Все рассмотренные выше статьи так или иначе следуют некоторой общей схеме: в качестве учителя используется большая заранее обученная модель, знания из которой дистиллируются в ученика, в то время как сам учитель остается неизменным. Дистилляция в таком режиме получила название offline-дистилляции знаний.

Но что делать, если большой предобученный учитель для вашей задачи не доступен? Что если модель-учитель не помещается на доступную нам видеокарту, из-за чего обучение или вовсе невозможно, или требует в десятки раз больше времени, по сравнению с обучением желаемой модели-ученика? Что, если набор данных, описывающий вашу задачу, невелик, и большая модель может переобучиться на нём, делая дистилляцию знаний как минимум неэффективной, а возможно и вредной для итогового качества ученика?  

Тут на помощь приходит online-дистилляция знаний.

#### Online дистилляция знаний

В качестве альтернативы авторы [этой статьи](https://arxiv.org/abs/1706.00384) предлагают брать в качестве учителя модель такой же архитектуры, что и ученик, и обучать обе модели одновременно.  

То есть вместо одной модели мы случайно инициализируем две, после чего на каждом шаге обучения для каждой из моделей мы минимизируем $\mathcal{L}_{KD}$, где в качестве учителя выступает другая модель.  

В таком случае в начале обучения градиент дистилляционного члена не будет иметь какого-то чёткого направления, а обучение обеих моделей будет происходить преимущественно за счет минимизации обычной функции потерь. На поздних же этапах обучения в дело включится и KL-дивергенция, что позволит дополнительно повысить качество каждой из моделей.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/dml_ebbd86d63c_123acde9f5.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://arxiv.org/abs/1706.00384">Источник</a>
  </figcaption>
</figure>

Почему данный подход работает? Широко известно, что в ряде задач ансамблирование нескольких одинаковых нейронных сетей, одинаково обученных на одних и тех же данных, но из разных случайных инициализаций, [дает прирост](https://arxiv.org/abs/2012.09816) в итоговой метрике.  

Этот факт подталкивает нас к выводу о том, что в зависимости от инициализации одна и та же нейронная сеть вычленяет из данных разные закономерности. Опираясь на данный вывод, авторы вышеупомянутой статьи утверждают, что в предложенной постановке каждая из моделей в процессе обучения может воспользоваться информацией, которая иначе была бы доступна только модели, стартовавшей из другой инициализации.

Авторы проводят ряд экспериментов с моделями разных размеров, обучая их на датасетах CIFAR-100 и [Market-1501](https://ieeexplore.ieee.org/document/7410490), и показывают, что использование даже одной дополнительной модели позволяет добиться заметного улучшения в качестве предсказаний обучаемой модели.  

Так на датасете CIFAR-100 совместное обучение ансамбля из двух моделей практически во всех экспериментах дает прирост в $1--2$ процентных пункта к итоговой точности предсказания, причем метод позволяет достигнуть положительного эффекта даже для самой большой из рассмотренных моделей при её совместном обучении с самой малой моделью. Кроме того, авторы проводят ряд экспериментов, в которых сравнивают offline-дистилляцию большей модели в меньшую с их совместным обучением и показывают, что предложенный метод позволяет добиться лучших результатов.

Online-постановка естественным образом обобщается на случай большего числа моделей в обучаемом ансамбле. В таком случае в качестве дистилляционного слагаемого авторы предлагают минимизировать среднее значение KL-дивергенций от текущей модели до предсказаний каждой из других моделей в ансамбле, поскольку минимизация KL-дивергенции до усредненных вероятностей приводит к худшему результату.  

При этом авторы в своих экспериментах показывают, что увеличение числа моделей в ансамбле приводит к улучшению результатов обучения. Кроме того авторы отмечают, что для ускорения обучения можно достаточно эффективно использовать несколько видеокарт, поскольку на каждом шаге между устройствами передавать необходимо только результаты предсказания.

#### Самодистилляция

В качестве отдельного режима дистилляции знаний принято выделять также самодистилляцию (self distillation), при которой учитель и ученик являются одной и той же моделью. Самодистилляция включает в себя две основные группы методов.

Первая группа методов направлена на использование информации, которая накапливается в модели во время обучения, для дополнительного улучшения качества предсказаний той же самой модели. Методы данной группы являются как бы продолжением идей online дистилляции знаний, поскольку учитель и ученик обучаются одновременно.  

Хороший пример метода из данной группы можно найти в [этой статье](https://arxiv.org/abs/1905.08094), где авторы пытаются заставить представления менее глубоких слоёв быть эквивалентными представлениям более глубоких слоёв. А именно, авторы предлагают разделить сеть на несколько частей ($4$ в статье) и после каждой такой части добавить небольшую предсказательную голову. Все такие головы обучаются путем минимизации суммы трёх слагаемых:

- кросс-энтропии с истинной разметкой; 
- KL-дивергенции с предсказаниями полной сети;
- квадратичной функции потерь между промежуточными представлениями данной головы и выходом последней части сети.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/byot_85061c03aa_4a95b5f9f5.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://arxiv.org/abs/1905.08094">Источник</a>
  </figcaption>
</figure>


Таким образом авторы добиваются от ResNet50 $81.04\%$ точности предсказания на тестовой выборке CIFAR-100 с минимальным замедлением обучения.  

Для сравнения, стандартное обучение такой же сети позволяет добиться лишь $77.68\%$ точности предсказания, а дистилляция из ResNet152 (которая, в свою очередь, показывает точность предсказания в $79.21\%$) позволяет улучшить данный показатель лишь до $79.31\%$.  

При этом обучение в предложенном режиме занимает $5.87$ часов (обычное обучение занимает $4.03$ часа), а дистилляция из ResNet152 занимает уже $12.31$ часов без учета обучения модели учителя (что требует дополнительных $14.67$ часов).

Вторая группа методов по сути заключается в offline дистилляции из обученной модели в новую модель такой же архитектуры. То есть мы выбираем некоторую архитектуру нейронной сети, обучаем одну модель стандартным образом, а затем обучаем точно такую же модель из новой случайной инициализации с использованием хинтоновской дистилляции из ранее обученной модели.  

Стоит заметить, что с хинтоновской точки зрения данное действие едва ли способно улучшить итоговое качество модели. Действительно, будучи точно такой же моделью, ученик обладает идентичной способностью к обучению, а значит учитель едва ли может предоставить ему какую-либо дополнительную информацию во время обучения.  

Поэтому такая самодистилляция изначально была предложена как метод изучения процесса хинтоновской дистилляции знаний, поскольку в такой постановке у задачи минимизации KL-дивергенции гарантированно есть глобальный минимум, причем мы даже знаем точку, в которой он достигается. В частности именно с помощью данного метода авторы [ранее упомянутой статьи](https://arxiv.org/abs/2106.05945) демонстрируют, что хинтоновская дистилляция знаний является сложной оптимизационной задачей.

Тем удивительнее, что авторы статьи 2018 года [обнаружили](https://arxiv.org/abs/1805.04770), что самодистилляция в предложенной выше постановке позволяет получить прирост в обобщающей способности итоговой модели. Так, они проводят ряд экспериментов с моделями [DenseNet](https://arxiv.org/abs/1608.06993) и [Wide-ResNet](http://www.bmva.org/bmvc/2016/papers/paper087/index.html) на датасете CIFAR-100 и показывают, например, что самодистилляция DenseNet-112-33 позволяет повысить точность предсказания на тестовой выборке с $81.75\%$ до $83.05\%$.

Вопрос об источнике прироста качества в данном случае до сих пор в значительной степени открыт. Авторы статьи приписывают данный эффект комбинации умного сглаживания разметки и внесения в обучение информации о взаимоотношении классов в датасете. Но на наш взгляд эксперименты, которые предъявляют в статье в качестве доказательства этих гипотез, едва ли можно назвать убедительными.

Возможно, здесь в очередной раз проявляется то, что одинаковые модели могут вычленять из одних и тех же данных разные закономерности в зависимости от случайной инициализации, и дистилляция одной такой модели в другую позволяет ученику увидеть ранее недоступные ему связи. 

Также хочется обратить внимание на интересную [статью](https://arxiv.org/abs/2002.05715), вышедшую в 2020 году. В ней показывается, что в случае обучения с L2-регуляризацией предложенная выше самодистилляция производит неявный отбор признаков.

Ну и раз мы проходили мимо самодистилляции, здесь никак нельзя не упомянуть [статью](https://arxiv.org/abs/1911.04252) 2019 года, которая в течении практически года держала почетный статус SOTA на датасете ImageNet.

Её авторы предлагают подход, который во многом очень близок к описанному выше. Они обучают модель на исходном наборе данных, после чего используют её для разметки новых данных, взятых в данном случае из стороннего обширного набора данных JFT-300M (закрытый набор данных, который нередко упоминается в статьях от Google).

После этого авторы отбрасывают картинки, для которых модель дает неуверенные предсказания, чтобы избежать данных out-of-domain. Кроме того, они выравнивают размеры классов, чтобы избежать связанных с этим спецэффектов (согласно авторам статьи, модели меньшего размера показали себя более чувствительными к данной оптимизации).

Таким образом, авторы получают большое количество дополнительных шумно размеченных данных, на которых, совместно с основным набором, они обучают новую модель. Эту модель, в свою очередь, можно использовать для получения новой разметки для дополнительных данных, с помощью которых обучается следующая модель, и такие итерации можно продолжать произвольное количество раз.

В качестве разметки авторы предлагают использовать мягкую разметку, задаваемую моделью-учителем, но и бинаризованная разметка показывает схожие результаты на данных in-domain. Ключевая деталь здесь — что предсказание на новых данных производится без аугментаций, в то время как ученик учится воспроизводить разметку уже с высоким уровнем аугментации данных, а также с применением других техник регуляризации, таких как dropout и stochastic depth.

Авторы утверждают, что ученик обучается лучше переносить свои знания на новые данные. Предложенный метод позволил авторам добиться от модели [EfficientNet-L2](https://arxiv.org/abs/1905.11946) точности предсказания в $88.4\%$ на тестовом наборе данных ImageNet, существенно улучшив результат исходной модели в $85.5\%$ и обновив мировой рекорд.

### Области применения дистилляции знаний

#### Сжатие генеративных состязательных сетей

Подавляющее большинство рассмотренных выше статей так или иначе ограничиваются задачей классификации картинок. Такой выбор, хоть и не является случайным, всё же несёт больше исторический, нежели практический характер.

На самом деле, многие предложенные выше методы достаточно тривиально могут быть обобщены и на другие задачи машинного обучения. Например, метод дистилляции промежуточных представлений по сути вовсе никак не зависит от природы итоговых выходов модели, а потому может использоваться при сжатии практически любой модели.

В частности данный метод может быть использован для сжатия генеративных состязательных сетей. Так авторы довольно популярной статьи в данной области [демонстрируют](https://arxiv.org/abs/2003.08936) $9$-, $10$- и даже $29$-кратное ускорение для ряда популярных pix2pix генеративных сетей, при этом не теряя в качестве генерации.

Как уже упоминалось ранее, авторы используют дистилляцию промежуточных представлений: модель-ученик учится минимизировать L2-расстояние между своим промежуточным представлением и промежуточным представлением модели-учителя. Но, так как данные представления имеют различное количество каналов (ученик выучивает более сжатое представление) авторы используют дополнительную свертку 1х1 над представлением ученика для сопоставления тензоров друг с другом.

Помимо дистилляции промежуточных представлений, авторы также пользуются наличием модели-учителя для того, чтобы получить парную картинку в случае обучения на неспаренных данных (как это происходит, например, в CycleGAN).  

Парная картинка используется для минимизации L1 нормы разности с предсказанием модели. Кроме того, авторы во время обучения минимизируют и стандартную для генеративных состязательных сетей функцию потерь, при этом для модели-ученика используется такой же дискриминатор, что и для модели-учителя, что позволяет авторам инициализировать веса с помощью весов оригинального дискриминатора.

Таким образом авторы предлагают следующий рецепт для сжатия генеративных состязательных сетей. Сначала необходимо обучить модель-учителя. После этого нужно сконструировать сжатый генератор-ученика, скопировать (вместе с весами) дискриминатор и обучить получившуюся систему с помощью минимизации взвешенной суммы трёх функций потерь: 

- Стандартной функции потерь генеративных состязательных сетей.
- L1-расстояния между предсказанием генератора и парной картинки. При этом если в данных парной картинки нет, вместо неё используется результат генерации моделью-учителем.
- L2-расстояния между промежуточными представлениями двух генераторов. 

Этот метод хорошо себя зарекомендовал на практике и получил широкое распространение в своей нише.

#### Дистилляция знаний при квантизации моделей

Ещё одно интересное применение дистилляции знаний — улучшение результатов квантизации моделей. Техника квантизации нейронных сетей заключается в том, чтобы перевести часть весов или даже всю сеть из полной точности (как правило, float32) во float8 или даже float4.

Помимо очевидной экономии памяти, такое представление нередко позволяет использовать специальные ядра современных графических ускорителей или специальные регистры процессоров для достижения существенного ускорения при применении квантизованных моделей.

К сожалению, бесплатный сыр бывает только в мышеловке. Сжатое представление на то и называется сжатым, что является менее богатым, нежели полная точность. Поэтому большинство весов сети приходится изменять при сжатии, чтобы они попали на более грубую сетку. Разумеется изменения каждого отдельного веса может показаться незначительным, однако когда все веса сети незначительно изменяются, итоговый результат подсчетов может оказаться вовсе неузнаваемым.

Чтобы смягчить данный эффект, модель принято доучивать после квантизации. И вот здесь на помощь приходит дистилляция знаний: например, из сети полной точности в квантизованного ученика. Ровно к такой схеме приходят авторы [этой статьи](https://arxiv.org/abs/1711.05852).

Итоговая схема выглядит следующим образом: мы обучаем сеть в полной точности, квантизуем ее веса и доучиваем ее в квантизованном виде с использованием дистилляции знаний из сети в полной точности.

#### Дистилляция знаний за пределами сжатия моделей

Хочется отметить, что сжатие моделей — это хоть и основное, но всё же не единственное применение дистилляции знаний. Так, раньше в этом параграфе уже упоминалась самодистилляция, которая позволяет получать прирост в обобщающей способности обучаемой модели без использования моделей большего размера.

Самодистилляцией, однако, примеры применения дистилляции знаний без сжатия модели не ограничиваются. Так, в 2020 году был [предложен](https://arxiv.org/abs/2006.07733) метод BYOL-предобучения без учителя, основанный на дистилляции знаний.

Метод BYOL направлен на предобучение моделей компьютерного зрения и основан на идее так называемого контрастивного предобучения (contrastive pretraining). Суть методов данного семейства заключается в том, чтобы обучать модель выдавать схожие представления для различных аугментаций одной и той же картинки.

Действительно, случайные патчи, вырезанные из фотографии автомобиля, скорее всего также являются фотографиями автомобиля. При этом, если в наших данных присутствует достаточное количество фотографий различных автомобилей, мы можем надеяться на то, что модель выучит некоторое общее понимание концепта автомобиля даже несмотря на то, что мы можем вовсе не знать, на каких конкретно картинках автомобили присутствовали, а на каких - нет.

Однако если мы хотим добиться от такой модели осмысленных представлений сначала нам необходимо преодолеть проблему коллапса представлений.  

Действительно, у предложенной выше задачи есть тривиальное решение, в котором выход обучаемой сети не зависит от ее входа. В таком случае представления для произвольных аугментаций любой картинки будут совпадать, то есть функция потерь окажется нулевой. Тем не менее сами представления при этом окажутся совершенно бесполезными.

Поэтому различные методы контрастивного обучения отличаются в первую очередь как раз способами борьбы с проблемой коллапса представлений. Так, авторы метода BYOL часто сравнивают свои результаты с довольно свежим на момент написания статьи методом [SimCLR](https://arxiv.org/abs/2002.05709), в котором предлагается обучать модель одновременно минимизируя расстояния между парами представлений для различных аугментаций одной картинки и максимизируя расстояния между представлениями для различных картинок.  

При этом, для повышения эффективности такого обучения, во время генерации батча данных авторы сначала выбирают некоторое количество картинок из набора данных, затем для каждой картинки производят две различные случайные аугментации, после чего полученные картинки используются для создания одной *позитивной пары*, расстояние между представлениями которой будет минимизироваться, а также для создания множества *негативных пар* с аугментациями других картинок в батче, расстояния между представлениями которых будут максимизироваться.

Авторы BYOL подвергают данный подход критике, показывая, что для эффективного обучения SimCLR требует большого размера батча, а также довольно агрессивных аугментаций. В противном же случае качество обучаемых представлений заметно падает. Авторы BYOL объясняют данный эффект тем, что сам подход использования негативных пар является субоптимальным, поскольку требует аккуратного выбора негативных примеров. Поэтому свой метод авторы конструируют таким образом, чтобы модель обучалась только на позитивных парах картинок.

В таком случае каким образом авторам удается решить проблему коллапса представлений? Для этого, вместо минимизации расстояния между представлениями обучаемой сети для двух аугментаций одной картинки, авторы обучают свою модель минимизировать расстояние между представлением обучаемой (online) сети для одной аугментации и представлением для второй аугментации, которое задается уже другой, целевой (target) сетью. То есть в некотором смысле здесь происходит дистилляция знаний из целевой сети в обучаемую.

Последней важной деталью является природа целевой сети. Авторы BYOL замечают, что даже использование произвольно инициализированной сети в качестве целевой для предложенного выше метода обучения приводит к выучиванию обучаемой моделью осмысленных представлений.  

Подробнее, линейный классификатор, обученный на основе выученных таким образом представлений картинок из набора данных ImageNet показывает $18.8\%$ тестовой точности предсказания, в то время как использование представлений задаваемых самой целевой сетью позволяет добиться лишь $1.4\%$ точности. Мотивированные данным наблюдением, авторы предлагают в качестве целевой использовать такую же сеть, что и обучаемая.  

При этом:

- градиенты не текут через целевую модель, и она не обновляется на шаге градиентного спуска;
- обучаемая модель заканчивается дополнительным двухслойным перцептроном, который используется для преобразования её финальных представлений в представления целевой модели;
- веса целевой модели не меняются на шаге градиентного спуска, а вместо этого они обновляются между шагами с помощью экспоненциального сглаживания весов обучаемой модели:

$$\xi \mapsto \tau\xi + (1 - \tau)\theta,$$

где, следуя обозначениям из статьи, мы обозначили через $\xi$ и $\tau$ веса целевой и обучаемой моделей соответственно, а $\tau$ — вещественный параметр.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/Artboard_31_3b7e513b94_b630bc04df.svg" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://arxiv.org/abs/2006.07733">Источник</a>
  </figcaption>
</figure>


Предложенный метод позволяет авторам добиться уже $79.6\%$ тестовой точности от линейного классификатора на наборе данных ImageNet, заметно превосходя предложенные ранее методы self-supervised предобучения, и практически преодолевая разрыв между self-supervised и supervised обучением классификаторов на основе ResNet.

Стоить заметить, что сценарии применения дистилляции знаний отнюдь не ограничиваются выше рассмотренными. На данный момент уже существует множество различных подходов и алгоритмов, так или иначе связанных с дистилляцией знаний, и их количество растет день ото дня.  

Данный параграф не ставит своей целью полный обзор таких методов. Вместо этого всем заинтересовавшимся я рекомендую обратить внимание на довольно исчерпывающий [обзор](https://arxiv.org/abs/2006.05525) от 2020 года. Здесь можно найти множество ссылок на актуальные к тому моменту статьи, в числе которых присутствует и большинство статей, упомянутых в этом параграфе.

### Открытые проблемы

Завершим параграф упоминанием открытых проблем в области дистилляции знаний. 

Действительно, несмотря на впечатляющие результаты, дистилляция знаний всё же не является идеальным методом, и ряд вопросов до сих пор остаются без ответа.

Например, с ростом популярности дистилляции знаний [выяснилось](https://arxiv.org/abs/1910.01348), что использование учителя с большей обобщающей способностью не всегда приводит к улучшению обобщающей способности ученика. В какой-то степени данный эффект можно списать на то, что компактная модель-ученик упирается в пределы своего качества предсказания, и тогда использование более умного учителя уже не приносит дополнительной пользы.

Но это не объясняет, почему использование более точной модели в качестве учителя может приводить даже к ухудшению итоговой точности модели-ученика. В чём причина данного эффекта и как выбрать оптимального учителя для фиксированного ученика, до сих пор открытый вопрос.

И как уже упоминалось ранее, дистилляция знаний из одной сети в точно такую же нередко [приводит](https://arxiv.org/abs/1805.04770) к росту обобщающей способности ученика по сравнению со своим учителем. С точки зрения хинтоновской теории, которая является де-факто стандартным способом объяснения дистилляции знаний, это звучит абсурдно. 

Модель-ученик гарантированно способна приблизить ту же функцию, что и модель-учитель. Тем не менее, этого не происходит, а модель-ученик выучивает свое собственное представление, которое нередко качественно превосходит представление учителя. Данный факт уже сложно объяснить в парадигме передачи знаний от учителя к ученику, потому что здесь ученик оказывается в состоянии получить больше знаний, нежели учитель способен передать. Несмотря на то, что на данную тему написана уже не одна статья, исчерпывающего объяснения пока нет.

Так или иначе, дистилляция знаний неоспоримо работает и является основным практическим подходом к сжатию нейросетевых моделей на данный момент.

  ## handbook

  Учебник по машинному обучению

  ## title

  Дистилляция знаний

  ## description

  *null*

- 
  ## path

  /handbook/ml/article/vvedenie-v-generativnoe-modelirovanie

  ## content

  До этого вы изучали модели машинного обучения, которые в основном предсказывают какие-то характеристики объектов.Например, метки класса или регрессионные метки. Подобные задачи называют дискриминативным моделированием.

В то же время, существуют обратные задачи, в которых по какой-то характеристике нужно создать объект или оценить плотность распределения объектов. Это называется генеративным моделированием — его нюансы мы и рассмотрим в этом разделе. 

Обучение генеративных моделей существенно сложнее обучения дискриминативных моделей. Последние работают с намного более простыми распределениями. Например, предсказать вероятность конкретной цифры, нарисованной на картинке, гораздо проще, чем создать картинку с нужной цифрой. При этом генеративные модели в последние годы достигли невероятных успехов и позволяют генерировать изображения, которые трудно отличить от настоящих фотографий.


![generative_vs_discriminative_models_593f8fe604.webp](https://yastatic.net/s3/education-portal/media/generative_vs_discriminative_models_593f8fe604_cc2dc8db48.webp)

Генеративные модели помогают решать множество задач, которые мы рассмотрим далее. Самая основная задача — это приближение распределения данных и генерация новых данных. 

Допустим у нас есть набор картинок с нарисованными от руки числами. Будем считать, что мы получили этот набор из генеральной совокупности (то есть из всех возможных изображений). Нам бы хотелось так или иначе смоделировать распределение этой генеральной совокупности.

Мы можем это сделать двумя подходами:

1. Явное моделирование. В этом случае мы построим и как-то оценим функцию плотности распределения данных $p(x)$. Из этого распределения мы сможем семплировать новые объекты. Примеры таких моделей: авторегрессионные модели (например, [PixelCNN++](https://arxiv.org/pdf/1701.05517.pdf), [Video Transformer](https://arxiv.org/pdf/1906.02634.pdf)), [диффузионные модели](https://education.yandex.ru/handbook/ml/article/diffuzionnye-modeli), модели на основе нормализующих потоков и [вариационные автокодировщики](https://education.yandex.ru/handbook/ml/article/variational-autoencoder-(vae)).
2. Неявное моделирование. При неявном моделировании мы доступ к функции плотности не получим. Но мы сможем из этого распределения сэмплировать новые объекты. В случае нашего примера с нарисованными числами мы сможем генерировать такие изображения. Примерами таких моделей являются [генеративно-состязательные сети](https://education.yandex.ru/handbook/ml/article/generativno-sostyazatelnye-seti-(gan)).

Рассмотрим дискриминативные и генеративные задачи чуть более формально. При дискриминативном моделировании для объекта $x$ и характеристики $y$ мы обычно хотим получить плотность распределения $p(y \mid x)$.

При генеративном моделировании ставится противоположная задача: восстановить плотность $p(x)$ или $p(x \mid y)$. В качестве $y$ тут может выступать как метка класса, так и другой объект. Например, если мы хотим уметь генерировать изображения на основе текстового описания, то изображения будут являться $x$, а текст — $y$.



## Интерполяции в латентном пространстве

Большинство моделей генеративного моделирования позволяют семплировать новые объекты. Как правило, в результате обучения генеративной модели мы получаем генератор — функцию, которая на выходе выдаёт объект. 

В таких моделях, как генеративные состязательные нейронные сети, диффузионные модели, вариационные автокодировщики, генератор на вход принимает вектор случайных значений из простого вероятностного распределения (например, нормального или равномерного). Получается, что $x = G(z)$, где $x$ — объект, $G$ — функция генератора, а $z$ — вектор случайных значений. Пространство, в котором располагается $z$, называется латентным.

Обычно распределение $z$ задаётся ещё до обучения модели и не меняется в процессе. Поскольку мы знаем распределение, мы можем семплировать из него сколько угодно разных $z$.

Рассмотрим два вектора $z_1$ и $z_2$ из латентного пространства и два соответствующих им сгенерированных объекта $x_1 = G(z_1)$ $x_2 = G(z_2)$. Так как $z_1$ и $z_2$ — это две точки в латентном пространстве, между ними можно провести линию.  

Точки, лежащие на этой линии, будут так же принадлежать этому пространству. Если двигаться по этой линии и использовать точки с неё в качестве входа для генератора, то можно получить плавно изменяющийся сгенерированный объект.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/interpolation_example_f6ce073e6a_d71e9e4faf.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <p>Пример изображений, полученных с помощью интерполяции в латентном пространстве.
    <a href="https://arxiv.org/abs/1707.05776">Источник</a></p>
  </figcaption>
</figure>


В примере выше мы рассмотрели движение вдоль линии, однако на практике интерполяция может быть по более сложной траектории.  

Манипуляции с латентным пространством позволяют не только создавать плавные переходы между объектами, но так же редактировать объекты. Обычно в таких случаях требуется найти направления в латентном пространстве, которое отвечает за нужное свойство сгенерированных объектов.  

Например, направление, отвечающее за цвет волос или улыбку человека. Подробнее такие методы мы рассмотрим в параграфах про конкретные модели.

## Применения генеративных моделей

Зачем может понадобиться генерировать новые данные или восстанавливать их плотность? Самый простой пример – это аугментация набора данных, которая мешает переобучению и улучшает обобщаемость модели.

Простые аугментации данных (случайные сдвиги, повороты, масштабирование, изменения цвета и контраста) активно используются почти во всех методах машинного обучения. Генеративные же модели представляют собой более сложный вид аугментации данных, который способен существенно расширить датасет, или обогатить его совершенно новыми элементами. 

Например, генеративную модель, которая переносит стиль одного изображения на другое (style transfer), можно использовать для обучения более робастных моделей классификации. В [статье](https://www.nature.com/articles/s41598-019-52737-x) Sandfort et al. используют аугментацию генеративными нейросетями, чтобы улучшить качество сегментации компьютерной томографии. 

Помимо этого, у генеративных моделей есть ряд других применений для редактирования изображений. Их используют, чтобы повысить разрешение картинок (задача super-resolution). 

На изображении ниже оригинальную картинку (original) сначала сжали в четыре раза, а потом попробовали восстановить до исходных размеров разными методами. Видно, что метод SRGAN, метод на основе генеративных состязательных нейронных сетях работает гораздо лучше бикубической интерполяции (bicubic), которая обычно применяется по умолчанию и смазывает картинку. 

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/superresolution_f3c0eb057c_af074e8b4e.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://arxiv.org/abs/1609.04802">ссылка на источник картинки</a>
  </figcaption>
</figure>

С помощью генеративных моделей можно закрашивать пропущенные куски изображений. Это полезно, когда мы хотим удалить с фото других людей, и нам нужно закрасить участки, образовавшиеся после их удаления. Эта функция представлена в некоторых современных смартфонах.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/inpainting_lama_cb60a0cc2f_9f287d916a.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://arxiv.org/abs/2109.07161">ссылка на источник картинки</a>
  </figcaption>
</figure>

В последние несколько лет хорошо стали работать модели, которые генерируют изображения на основе их текстового описания. Среди таких моделей:

- Stable Diffusion ([Демо](https://huggingface.co/spaces/stabilityai/stable-diffusion)). Модель с открытым [исходным кодом](https://github.com/CompVis/stable-diffusion)
- [DALLE 2](https://openai.com/dall-e-2/). Доступ по платному API
- [Midjourney](https://www.midjourney.com/). Доступ через Discord
- [Imagen](https://imagen.research.google/)

![stable_diffusion_1_1_137e2f686a.webp](https://yastatic.net/s3/education-portal/media/stable_diffusion_1_1_137e2f686a_1d77b97e70.webp)

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/stable_diffusion_2_1_f10c87953c_870d4758eb.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <p>Примеры генерации изображений из текстового описания. Модель stable diffusion
    <a href="https://github.com/CompVis/stable-diffusion">Источник</a></p>
  </figcaption>
</figure>

Появились даже специальные базы изображений, сгенерированных нейронными сетями: [Lexica](https://lexica.art/), [Openart](https://openart.ai/).


Доступность таких моделей приводит к появлению множества приложений:
- [Иллюстрации для книг](https://www.reddit.com/r/midjourney/comments/x4kk0r/i_created_a_graphic_novel_using_mj_and_now_its_on/)
- [Создание логотипов](https://jacobmartins.com/posts/how-i-used-dalle2-to-generate-the-logo-for-octosql/)
- [Создание дизайнов помещений](https://interiorai.com/)
- [Генерация тату](https://www.tattoosai.com/)

Кроме этого, некоторые модели позволяют совместить несколько задач и делать закрашивание изображения на основе текстового описания. Например, удалять какую-то область и говорить модели, что там должно быть нарисовано.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/stable_diffusion_inpainting_1_fd77de9693_aabc45e095.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <p>Пример закрашивания части изображения на основе текстового описания.
    <a href="https://github.com/runwayml/stable-diffusion">Источник</a></p>
  </figcaption>
</figure>

На основе этой технологии появились редакторы изображений с генеративными моделями внутри: [Neural love](https://neural.love/), [Photoroom](https://www.photoroom.com/), [ZMO](https://www.zmo.ai/). 

Современные генеративные модели достигли очень хорошего качества и уже стали использоваться в реальных задачах, о которых мы вам рассказали. В следующих параграфах этой главы мы рассмотрим основные методы генеративного обучения более детально.

  ## handbook

  Учебник по машинному обучению

  ## title

  Введение в генеративное моделирование

  ## description

  *null*

- 
  ## path

  /handbook/ml/article/variational-autoencoder-(vae)

  ## content

  В машинном обучении есть довольно широкая область, посвящённая обучению генеративных моделей. Их задача — выучить распределение, из которого могли бы быть насемплированы объекты обучающей выборки.

Обученная генеративная модель способна семплировать из выученного распределения новые объекты, не принадлежащие исходным данным. Чаще всего это связано с задачей генерации новых изображений: от изображений рукописных чисел до замены лиц на видео с помощью deepfake.

Модель, о которой пойдёт речь в данном параграфе, называется «Вариационный автоэнкодер» или VAE (variational autoencoder). Она относится к семейству генеративных моделей. Коротко расскажем, что вас ждёт дальше.

- В разделах «Постановка задачи» и «Обучение VAE» мы опишем построение и обучение VAE в классическом описании. Этих двух разделов достаточно для общего представления о VAE. 
- Раздел «Обзор статей» для первоначального понимания не обязателен, но может быть интересен тем, кто захочет узнать о недавних интересных работах, связанных с VAE. 

Прежде чем двинуться дальше — небольшое напоминание: большинство картинок в тексте кликабельны, и при клике вы сможете перейти к источнику, из которого была заимствована картинка.

## Постановка задачи

Давайте представим себе, что нам нужно нарисовать лошадь. Как бы мы это сделали?

Наверное, сначала наметили бы общий силуэт лошади, её размер и позу, а затем стали бы добавлять детали: гриву, хвост, копыта, выбирать окраску шерсти и так далее. Кажется, что в процессе обучения рисованию мы учимся выделять для себя основной набор каких-то **факторов**, наиболее важных для генерации нового изображения: общий силуэт, размер, цвет и тому подобное, а во время рисования уже просто подставляем какие-то **значения** факторов.

При этом одинаковые сочетания одних и тех же факторов могут привести к разным картинкам — ведь нарисовать что-то два раза абсолютно одинаково вы, скорее всего, не сможете.

Попробуем формализовать описанный выше процесс. Пусть у нас есть датасет $D$ в многомерном пространстве исходных данных $X^N$, — объектов, которые мы желаем генерировать, — и пространство $Z^M$ **скрытых (латентных) переменных** меньшей размерности, которыми кодируются скрытые факторы в данных. Тогда генеративный процесс состоит из двух последовательных стадий (см. картинку ниже):

1. Семплирование $z \in Z^M$ из распределения $p(z)$ (красное)
2. Семплирование $x \in X^N$ из распределения $p(x \| z)$ (синее)

![1.](https://yastatic.net/s3/education-portal/media/lvm_diagram_ce53ef5812_fdf30bc1e1.webp)

То есть, рассуждая в терминах рисования картинок с лошадками, мы сначала мысленно семплируем некоторое $z$ (размер, форму, цвет, ...), затем дорисовываем все необходимые детали, то есть семплируем из распределения $p(x \| z)$, и в итоге надеемся, что получившееся будет напоминать лошадку.

Таким образом, построить генеративную модель в нашем случае — значит уметь семплировать с помощью описанного двустадийного процесса объекты, близкие к объектам из обучающей выборки $D$. 

Говоря более формально, нам бы хотелось, чтобы наша модель максимизировала правдоподобие $p(x)$ элементов обучающего множества $D$ при описанной процедуре генерации: 

$$
p(x) = \int \limits_{Z^M} p(x | z) p(z) dz \to \max
$$

Предположим, что совместное распределение $p(x, z)$ параметризовано некоторым параметром $\theta \in \Theta$ и выражается непрерывной по $\theta$ функцией при каждых фиксированных $x$ и $z$:

$$
p_\theta(x, z) = p(x, z | \theta) \in C(\Theta)
$$

Тогда

$$
p_\theta(x, z) = p(x| z, \theta) p(z | \theta) = p_\theta(x | z) p_\theta(z),
$$

и мы можем записать следующую задачу оптимизации:

$$
\begin{equation}
    p_\theta(x) = \int \limits_{Z^M} p_\theta(x | z) p_\theta(z) dz \to \max_{\theta \in \Theta} \tag{1}
\end{equation}
$$

Решив её, мы построим нашу генеративную модель.

**Замечание 1**. После приведённой выше аналогии с обучением рисованию может ошибочно показаться, что в скрытые переменные всегда заложен некоторый хорошо интерпретируемый смысл. Но на практике это всё же не обязано быть так: те скрытые переменные, которые мы найдём, могут как иметь простую интерпретацию, так и не иметь. С помощью объяснений выше мы прежде всего хотели проиллюстрировать понятие «скрытые переменные».

**Замечание 2**. Может показаться, что $p(x)$ нам откуда-то уже известно, и тогда не ясно, зачем все эти сложности с введением латентных переменных и интегралами. На самом деле, мы действительно можем построить [статистическую оценку](https://en.wikipedia.org/wiki/Density_estimation) $\hat p(x)$ по данным $D$ и даже пытаться генерировать новые данные с помощью таких моделей (как, например, делается [тут](https://scikit-learn.org/stable/modules/density.html)). Но у статистических методов есть разные ограничения, наиболее серьёзным из которых представляется проклятие размерности: чем больше измерений у ваших данных, тем больше разнообразных примеров вам нужно для построения адекватной оценки $\hat p(x)$. О проклятии размерности мы поговорим чуть подробнее далее.

**Замечание 3**. Также может возникать вопрос — а зачем вообще нужно вводить латентные переменные, моделировать совместное распределение $p(x, z)$, а целевое распределение $p(x)$ определять как маргинализацию $p(x, z)$ по $z$? Почему такой подход в принципе должен работать? Ответ состоит в том, что, даже имея относительно простые выражения для $p(z)$ и $p(x \| z)$, можно описать достаточно сложное распределение $p(x)$, что достаточно наглядно проиллюстрировано в примере ниже.

{% cut "Пример: смесь гауссиан" %}

Представьте себе, что у вас есть таблица с конечным числом строк, в $k$-й строке которой записано два числа — среднее $\mu_k$ и дисперсия $\sigma^2_k$ нормального распределения. Пусть на индексах строк этой таблицы определено дискретное распределение $p(z)$, такое что:

$$
    p(z = k) = \lambda_k
$$

Пусть мы насемплировали индекс $k$, взяли параметры распределения из соответствующей ему строки и насемплировали с этими параметрами объект $x$. Распределение, из которого был получен $x$, равно:

$$
    p(x | z = k) = \mathcal{N}(\mu_k, \sigma_k^2)
$$

Распределение $p(x)$ получается маргинализацией совместного распределения $p(x, z)$ по $z$:

$$
    p(x) =  \sum_{k = 1}^K p(x, z = k) = \sum_{k = 1}^K p(x | z = k) p(z = k) = \sum_{k = 1}^K \lambda_k \mathcal{N}(\mu_k, \sigma_k^2)
$$

Получилось, что $p(x)$ описывается смесью гауссиан и имеет более сложный вид, чем $p(z)$ и $p(x \| z)$:

![2](https://yastatic.net/s3/education-portal/media/discrete_gaussian_mixture_757e189f6f_eb17ce766e.webp)

Ясно, что чем больше гауссиан в нашей сумме, тем более сложную форму может иметь $p(x)$. Так, имея простые $p(z)$ и $p(x \| z)$, мы можем моделировать сложные мультимодальные распределения. А теперь представим себе, что априорное распределение $p(z)$ имеет уже не дискретные, а непрерывные значения. Рассмотрим, например, такой случай:

$$
    p(z) = \mathcal{N}(0, 1),
$$

$$
    p(x | z) = \mathcal{N}(\mu(z), \sigma^2(z))
$$

Распределение $p(x)$, аналогично случаю с дискретным априорным распределением, будет получено интегрированием $p(x, z)$ по $z$ и будет как бы «бесконечной» смесью гауссиан:

![2](https://yastatic.net/s3/education-portal/media/infinite_gaussian_sum_40324553c2_29d8e7bc3f.webp)

С этим разобрались. В следующей главе продолжим говорить о задаче оптимизации, о которой мы начали разговор чуть выше — не теряйтесь!

{% endcut %}

## Обучение VAE

Прежде чем пытаться решать задачу оптимизации  [$ (1) $](#eq:main_problem), давайте подумаем, а как мы вообще могли бы посчитать такой интеграл? Первое, что приходит на ум, — попробовать получить его приближённое значение методом Монте-Карло:

$$
p_\theta(x) = \int \limits_{Z^M} p_\theta(x | z) p_\theta(z) dz = \mathbb{E}_{z \sim p_\theta(z)} [p_\theta(x | z)] \approx \frac{1}{K} \sum_k p_\theta(x | z_k),
$$

где в последнем переходе мы используем сэмплы $z_k \sim p_\theta(z)$. Однако, если $z \in Z^M$ и $M$ — достаточно большое, мы столкнёмся с *проклятием размерности* — количество семплов, необходимых для того, чтобы хорошо покрыть $Z^M$, растёт экспоненциально с ростом $M$:

![3](https://yastatic.net/s3/education-portal/media/curse_of_dimensionality_9ba358f72a_d8f618cf0a.webp)

Есть ли способ как-то сократить число необходимых семплов для подсчёта  [$ (1) $](#eq:main_problem)? На самом деле, часто оказывается, что далеко не все возможные $z$ отображаются в элементы $D$, и вклад большинства $z$ в оценку $p_\theta(x \| z)$ практически нулевой. Это наводит на мысль, что для каждого $x$ нам может пригодиться знание распределения $q(z \| x)$ таких $z$, которые являются прообразами $x$. Мы можем предположить, что распределение $q$ параметризовано некоторым семейством параметров $\Phi$:

$$
q(z | x) = q_{\phi}(z | x), \phi \in \Phi
$$

Зная распределение $q_\phi(z \| x)$, мы могли бы семлировать уже только из него, а не из всего $p_\theta(z)$, и, если распределение $q$ окажется достаточно хорошим, число необходимых семплов значительно сократится.

О том, как построить $q_\phi$, мы поговорим позже. Сейчас стоит обратить внимание на то, что процессы семплирования из распределений $q_\phi(z \| x)$ и $p_\theta(x \| z)$ взаимно обратны друг к другу: первое отображает элементы датасета в подмножество латентного пространства $Z^M$, то есть действует как *энкодер*, а второе отображает латентные переменные в подмножество $X^N$, то есть действует как *декодер*:

![2](https://yastatic.net/s3/education-portal/media/autoenc_pic_b9155df393_020bce3d93.webp)

Так как оба эти распределения будут участвовать в обучении VAE, возникает аналогия между VAE и [моделями-автоэнкодерами](https://en.wikipedia.org/wiki/Autoencoder#:~:text=Machine%20learninganddata%20mining.%20v.%20t.,network%20to%20ignore%20signal%20“noise”), имеющими похожую структуру.

### Вывод функции потерь

Сейчас у нас всё готово для того, чтобы записать общий вид функции потерь для обучения вариационного автоэнкодера. Напомним, что мы обучаем модель путём максимизации правдоподобия $p_{\theta}(x)$ по $\theta$. Для удобства мы перейдём к логарифму правдоподобия:

$$
\log p_{\theta}(x) = \log \int_{Z^M} p_{\theta}(x | z) p_{\theta}(z) d z \to \max_{\theta \in \Theta}
$$

Оптимизировать напрямую это выражение тяжело из-за проклятия размерности, обсуждавшегося в прошлом разделе. Чтобы победить проклятие размерности, мы хотели бы заменить семплирование из априорного распределения $p_{\theta}(z)$ на семплирование из $q_\phi(z \| x)$, для чего придётся осуществить некоторый трюк. Для любого $q_\phi(z \| x)$, отличного от нуля для всех $z \in Z^M$, мы можем выписать следующую цепочку равенств:

$$
\log p_\theta(x) = \mathbb E_{q_\phi(z | x)} [\log p_\theta (x)] = \\
$$

$$
= \mathbb E_{q_\phi(z | x)} \left[ 
    \log \left( 
        \frac{p_\theta (x, z)}{p_\theta(z | x)} 
    \right) 
\right] = \\
$$

$$ 
= \mathbb E_{q_\phi(z | x)} \left[ 
    \log \left( 
        \frac{p_\theta (x, z)}{q_\phi(z | x)} \frac{q_\phi(z | x)}{p_\theta(z | x)}
    \right) 
\right] = \\
$$

$$ 
= \underbrace{
    \mathbb E_{q_\phi(z | x)} \left[ 
        \log \left( 
            \frac{p_\theta (x, z)}{q_\phi(z | x)}
        \right) 
    \right]
    }_{\mathcal L_{\theta, \phi}(x) \\ (ELBO)} + 
    \underbrace{
    \mathbb E_{q_\phi(z | x)} \left[ 
        \log \left( 
            \frac{q_\phi(z | x)}{p_\theta(z | x)}
        \right) 
    \right]
    }_{D_{KL}(q_{\phi}(z | x) \parallel p_\theta (z | x))}
$$

Второе слагаемое в последнем равенстве — [$KL$-дивергенция](http://www.machinelearning.ru/wiki/images/d/d0/BMMO11_6.pdf) между $q_\phi(z \| x)$ и $p_\theta(z \| x)$, которая, как известно, неотрицательна:

$$
    D_{KL}(q_{\phi}(z | x) \parallel p_\theta (z | x)) \ge 0
$$

А первое слагаемое — это величина, именуемая в английской литературе **evidence lower bound (ELBO)**:

$$
\mathcal L_{\theta, \phi} (x) =
\mathbb E_{q_\phi(z | x)} \left[ 
    \log p_\theta (x, z) - \log q_\phi(z | x)
\right] = \\
$$

$$
= \underbrace{ \mathbb E_{q_\phi(z | x)} \left[ 
    \log p_\theta (x | z)
\right]}_{\text{reconstruction loss}} - \underbrace{D_{KL} (q_\phi(z | x) \parallel p_\theta (z))}_{\text{regularization term}}
$$

Первое слагаемое в последнем переходе обычно называют **reconstruction loss**, так как оно оценивает качество восстановления декодером объекта $x$ из его латентного представления $z$. А второе играет роль регуляризационного члена и подталкивает распределение, генерируемое энкодером, быть ближе к априорному распределению.

Так как $KL$-дивергенция неотрицательна, ELBO является нижней границей для логарифма правдоподобия данных:

$$
\mathcal L_{\theta, \phi} (x) =  \log p_\theta (x) - D_{KL}(q_\phi (z | x) \parallel p_\theta(z | x)) \le \log p_\theta (x)
$$

Посмотрим повнимательнее на равенства, которые мы выписали.
- Функцию $\mathcal L_{\theta, \phi}$ можно оптимизировать градиентным спуском (SGD), предварительно выбрав удобный вид для $p_\theta (x \| z)$, $q_\phi(z \| x)$ и $ p_\theta (z)$. Максимизируя $\mathcal L_{\theta, \phi}$, мы растим $\log p_\theta (x)$, тем самым улучшая нашу генеративную модель. Оптимизацию ELBO с помощью SGD мы будем подробно обсуждать в следующем разделе.
- Максимизируя $\mathcal L_{\theta, \phi}$, мы одновременно минимизируем $D_{KL}(q_\phi(z \| x) \parallel p_\theta (z \| x))$. Распределение $p_\theta (z \| x)$ оценивает, из каких $z$ мог бы быть сгенерирован объект $x$, и заранее оно нам не известно. Но если мы выберем достаточно большую модель для $q_\phi(z \| x)$, то $q_\phi(z \| x)$ в процессе оптимизации может очень сильно приблизиться к $p_\theta (z \| x)$, и тогда мы будем напрямую оптимизировать $\log p_\theta(x)$. Заодно мы получаем приятный бонус: для оценки распределения прообразов $x$ мы сможем использовать $q_\phi(z \| x)$ вместо невычислимого $p_\theta (z \| x)$. То есть $q_\phi$, которое мы при выводе формулы ввели в рассмотрение как произвольное распределение, действительно будет играть роль энкодера для модели.

{% cut "Альтернативный вывод выражения для ELBO" %}

В рассуждениях выше введение $q_{\phi}(z \| x)$ в рассмотрение могло показаться довольно формальным. Поэтому мы приведём здесь ещё один подход к выводу выражения для ELBO, который может показаться более естественным. Он состоит в последовательном применении приёма, называемого [importance sampling](https://statweb.stanford.edu/~owen/mc/Ch-var-is.pdf), и неравенства Йенсена.

![2](https://yastatic.net/s3/education-portal/media/importance_sampling_b8e2228d10_09295299ff.webp)

Во многих практических задачах возникает ситуация, в которой мы хотим вычислить $\mu = \mathbb E[ f(X) ]$, но при этом $f(x)$ близка к нулю вне некоторой области $A$, а вероятность попасть в эту важную область очень мала: $P(X \in A) \approx 0$.

Множество $A$ может либо иметь слишком маленькую мощность, либо быть в хвосте распределения случайной величины $X$. Обычное семплирование по методу Монте-Карло может почти не сгенерировать примеров, которые бы попадали в множество $A$. Проблемы такого типа довольно часто встречаются в физике высоких энергий, байесовском выводе, прогнозировании опасных природных явлений и во многих других областях.

Достаточно интуитивным выглядит решение, состоящие в том, чтобы попробовать как-то искусственно увеличить долю важных примеров среди всех остальных. Это можно сделать, используя распределение, дающее больше веса примерам из важной области. Отсюда и название метода — **importance sampling** (выборка по значимости).

Итак, пусть наша задача — вычислить математическое ожидание $\mu = \mathbb E_p\left[ f(x) \right] = \int_{\mathcal D} f(x) p(x) dx$, где 
- $p$ — плотность распределения на множестве $\mathcal D \in \mathbb R^d$, 
- $f$ — некоторая интегрируемая функция. 

Пусть $q$ — функция плотности вероятности, определённая и положительная на $\mathcal D$, позволяющая осуществлять семплирование примеров из некоторого интересующего нас узкого подмножества. Наша задача — перейти от семплирования из $p$ к семплированию из $q$ для оценки $\mu$. Поскольку среднее $\mathbb E_q\left[ f(x) \right]$, вообще говоря, не равно $\mu$, запишем следующее:

$$
\mu = \mathbb E_p\left[ f(x) \right] = \int_{\mathcal D} f(x) p(x) dx = \int_{\mathcal D} \frac{f(x) p(x)}{q(x)} q(x) dx = \mathbb E_q\left[ \frac{f(x) p(x)}{q(x)} \right]
$$

Исходная плотность $p$ называется номинальной (nominal distribution), а плотность $q$ — смещённой (importance distribution). Отношение правдоподобия $\frac{p(x)}{q(x)}$ компенсирует смещение, возникающее при переходе от $p$ к $q$.

Напомним также формулировку неравенства Йенсена для случайных величин.

Напомним также формулировку неравенства Йенсена для случайных величин: если $\xi$ — случайная величина с конечным математическим ожиданием и $g(x)$ — выпуклая функция, то:

$$
\mathbb E[g (\xi)] \ge g(\mathbb E [\xi])
$$

Теперь вернёмся к исходной задаче. Снова, для любого $q_\phi(z \| x)$, отличного от нуля для всех $z \in Z^M$, мы можем записать:

$$
    \log p_\theta (x) = \log \int_{Z^M} p_\theta(x | z) p_{\theta}(z) dz = \\
$$

$$
    = \log \mathbb{E}_{p_\theta(z)} [p_\theta(x | z)] = \\
$$

$$
    = \log \mathbb{E}_{q_\phi(z | x)} \left[ \frac{p_\theta(x | z) p_\theta(z)}{q_\phi(z | x)} \right] \ge \\
$$

$$
    \ge \mathbb E_{q_\phi(z | x)} \log \left[ \frac{p_\theta(x | z) p_\theta(z)}{q_\phi(z | x)} \right] = \\
$$

$$
   = \mathbb E_{q_\phi(z | x)} \log \left[p_\theta(x | z) \right] - \mathbb E_{q_\phi(z | x)} \log \left[\frac{q_\phi(z | x)}{p_\theta(z)} \right] = \\
$$

$$
   = \mathbb E_{q_\phi(z | x)} \log \left[p_\theta(x | z) \right] - D_{KL} (q_\phi(z | x) \parallel p_\theta (z))
$$

В результате проведённых выкладок мы, как можно заметить, снова получили выражение для ELBO. На третьем переходе мы применили importance sampling, а на четвёртом — неравенство Йенсена для $g(x) = \log (x)$.

Минус данного подхода состоит в том, что он, в отличие от предыдущего способа, не позволяет выписать в явном виде формулу для разности между $\log p_\theta (x)$ и ELBO:

$$
    \log p_\theta (x) - ELBO = D_{KL}(q_{\phi}(z | x) \parallel p_\theta (z | x))
$$

Но зато данный вывод естественным образом следует из более общих методов, не требуя применения искусственных трюков.

{% endcut %}

### Обучение VAE с помощью градиентного спуска

Важное свойство ELBO в том, что его можно оптимизировать градиентным спуском относительно параметров $\phi$ и $\theta$. Если объекты датасета $D$ независимы и одинаково распределены, то $\mathcal L_{\theta, \phi} (D)$ запишется как сумма (или среднее) значений $\mathcal L_{\theta, \phi} (x)$ на объектах $x \in D$:

$$
    \mathcal L_{\theta, \phi} (D) = \sum_{x \in D} \mathcal L_{\theta, \phi} (x)
$$

Значения $\mathcal L_{\theta, \phi} (x)$ и их градиенты $\nabla \mathcal L_{\theta, \phi} (x)$ в общем случае вычислить невозможно, однако можно получить их несмещённые оценки, что позволит нам использовать стохастический градиентный спуск.

Оценку для градиента по параметрам $\theta$ получить несложно:

$$
    \nabla_\theta \mathcal L_{\theta, \phi}(x)
    = \nabla_\theta \mathbb E_{q_\phi(z | x)} \left[ 
        \log p_\theta(x, z) - \log q_\phi(z | x)
    \right] = \\
$$

$$
    = \mathbb E_{q_\phi(z | x)} \left[ 
        \nabla_\theta(\log p_\theta(x, z) - \log q_\phi(z | x))
    \right] = \\
$$

$$
    = \mathbb E_{q_\phi(z | x)} \left[ 
        \nabla_\theta \log p_\theta(x, z)
    \right] \approx \\
$$

$$
    \approx \frac{1}{K} \sum_k \nabla_\theta \log p_\theta(x, z_k),
$$

где в последней строчке $z_k \sim q_\phi(z \| x)$. Однако оценку на градиент по параметрам $\phi$ получить сложнее, ведь они также участвуют и в семплировании:

$$
     \nabla_\phi \mathcal L_{\theta, \phi}(x)
    = \nabla_\phi \mathbb E_{q_\phi(z | x)} \left[ 
        \log p_\theta(x, z) - \log q_\phi(z | x)
    \right] \ne \\
$$

$$
    \ne \mathbb E_{q_\phi(z | x)} \left[ 
        \nabla_\phi(\log p_\theta(x, z) - \log q_\phi(z | x))
    \right]
$$

В общем случае эта проблема не разрешима. Однако некоторые распределения позволяют применить **репараметризацию (reparameterization trick)**: представить переменную $z$ как обратимую дифференцируемую функцию от случайного шума, параметров $\phi$ и переменнной $x \in D$:

$$
    z = g(\varepsilon, \phi, x)
$$

Здесь распределение $\varepsilon \sim p_\varepsilon$ не зависит от $\phi$ и $x$. Например, пусть $\varepsilon \sim \mathcal N (0, I)$. Тогда $g$ может иметь следующий вид:

$$
    z = g(\varepsilon, \phi, x) = \mu_\phi(x) + \varepsilon \cdot \sigma_\phi(x) \sim \mathcal N (\mu_\phi(x), \sigma^2_\phi(x))
$$

После такой замены мы сможем получить оценку на градиент по $\phi$:

$$
     \nabla_\phi \mathcal L_{\theta, \phi}(x)
    = \nabla_\phi \mathbb E_{q_\phi(z | x)} \left[ 
        \log p_\theta(x, z) - \log q_\phi(z | x)
    \right] = \\
$$
$$
    = \nabla_\phi \mathbb E_{p_\varepsilon} \left[ 
        \log p_\theta(x, g(\varepsilon, \phi, x)) - \log q_\phi(g(\varepsilon, \phi, x) | x)
    \right] = \\
$$
$$
    = \mathbb E_{p_\varepsilon} \left[ 
        \nabla_\phi \big( 
            \log p_\theta(x, g(\varepsilon, \phi, x)) - \log q_\phi(g(\varepsilon, \phi, x) | x) 
        \big)
    \right] \approx \\
$$
$$
    \approx \frac{1}{K} \sum_{k} \nabla_\phi \big( 
        \log p_\theta(x, g(\varepsilon_k, \phi, x)) - \log q_\phi(g(\varepsilon_k, \phi, x) | x)
    \big),
$$

где в последней строчке $\varepsilon_k \sim p_\varepsilon$. Репараметризация хорошо иллюстрируется следующей картинкой:

![2](https://yastatic.net/s3/education-portal/media/reparametrization_92f25ad7f1_6321ae51f3.webp)

Здесь $f$ — функция потерь. Значения $f$ на обеих схемах одинаковы, но на левой картинке градиенты по $\phi$ рассчитать не получится, так как мы не можем дифференцировать по случайной переменной $z$.

Однако на правой картинке источник случайности перемещается во входные данные благодаря репараметризации, а градиенты вычисляются по детерминированным переменным. Таким образом, мы получили сетап, типичный для оптимизации с помощью SGD: там мы приближаем градиент функции потерь по случайным батчам входных данных, а здесь роль случайных батчей играют одновременно батчи из переменных $x$ и случайных переменных $\varepsilon$.

Кроме нормального распределения, есть довольного много примеров распределений, допускающих репараметризацию. Их можно найти [по ссылке](https://arxiv.org/pdf/1312.6114.pdf) в разделе "The reparameterization trick". Однако большая часть реализаций VAE используют именно нормальное распределение.

В итоге примерный алгоритм обучения VAE такой:

``` python
dataset = np.array(...)
epsilon = RandomDistribution(...)

# Энкодер q_phi(z|x) — нейронная сеть с параметрами phi
encoder = Encoder()

# Декодер p_theta(x|z) — нейронная сеть с параметрами theta
decoder = Decoder()

for step in range(max_steps):
     # Семплируем батч исходных данных и случайного шума
     batch_x = sample_batch(dataset)
     batch_noise = sample_batch(epsilon)
    
     # Считаем параметры распределения q(z | x) с помощью энкодера
     latent_distribution_parameters = encoder(batch_x)
     
     # Делаем репараметризацию (семплируем из q(z | x))
     z = reparameterize(latent_distribution_parameters, batch_noise)

     # Декодер отдаёт параметры выходного распределения
     output_distribution_parameters = decoder(z)

     # Вычисляем ELBO и обновляем параметры моделей
     L = -ELBO(
        latent_distribution_parameters, 
        output_distribution_parameters, 
        batch_x
     )
     L.backward()
```

Стоит подчеркнуть, что декодер выдаёт именно параметры выходного распределения, а не конкретный семпл из этого распределения. Например, если вы моделируете выходные изображения с помощью нормального распределения $\mathcal{N}(\mu(z), \sigma^2(z))$, то декодер на выходе предскажет некоторые $\hat \mu(z)$ и $\hat \sigma(z)$, которые вместе с параметрами латентного распределения (выход энкодера) будут поданы в ELBO.

Для генерации конкретной картинки на этапе инференса нужно будет либо честно провести семплирование из $\mathcal{N}(\hat \mu(z), \hat \sigma^2(z))$, либо, как часто делают, просто взять среднее $\hat \mu(z)$ в качестве выходного изображения. В общем случае конкретный способ проведения инференса зависит от вида используемого выходного распределения.

### Выбор вида используемых распределений

Пришло время привести примеры конкретных $p_\theta(x \| z)$, $q_\phi(z \| x)$ и $p_\theta(z)$, с которыми можно построить VAE. Для начала предположим, что $p_\theta(z)$ можно положить равным стандартному нормальному распределению:

$$
    p_\theta(z) = \mathcal N(0, I)
$$

Заметим, что в этом случае у априорного распределения $z$ отсутствует зависимость от параметров $\theta$. 

Распределение $p_\theta(x \| z)$ зависит от того, к какому распределению принадлежат ваши данные. Если ваши данные имеют непрерывное распределение, то $p_\theta(x \| z)$ можно задать, например, как гауссовское распределение:

$$
    p_\theta(x | z) = \mathcal N(f_\theta(z), \sigma^2)
$$

Вектор средних в этом примере определяется функцией $f$ с переменными $\theta$ и $z$, а матрица ковариаций определяется постоянной диагональной матрицей. Функцию $f$ можно задать с помощью нейронной сети с параметрами $\theta$. При желании, матрицу ковариаций тоже можно задавать некоторой функцией и не ограничивать её вид только постоянными матрицами. Если же ваши данные дискретны, то может подойти категориальное распределение:

$$
    p_\theta(x | z) = \operatorname{Categorical}(f_\theta(z)),
$$

в котором вектор вероятностей $f_\theta(z) = (p_1, \ldots, p_n)$ — выход нейросети после применения $\text{softmax}$. Если у вас бинарные данные, вы можете использовать бернуллиевское распределение:

$$
    p_\theta(x | z) = \operatorname{Bernoulli}(f_\theta(z)),
$$
 
где $f_\theta(z) = p$ — выход нейронной сети после применения сигмоиды.

Распределение $q_\phi(z \| x)$ может, в принципе, быть любым, но в самом простом случае оно имеет вид гауссовского распределения c диагональной матрицей ковариаций:

$$
    q_\phi(z | x) = \mathcal N(\mu_\phi(x), \sigma_\phi^2(x))
$$

Такое распределение позволяет, в частности, применить репараметризацию, обсуждавшуюся выше. Если выбрать $z$ двумерным, то распределения, определямые $q$, хорошо визуализируются:

![2](https://yastatic.net/s3/education-portal/media/encoder_vae_diagram_dad0562e49_1e0f9bbb40.svg)
А теперь вспомним, как определяется ELBO:

$$
    \mathcal L_{\theta, \phi} (x) 
    = \mathbb E_{q_\phi(z | x)} \left[ 
        \log p_\theta (x | z)
    \right] - D_{KL} (q_\phi(z | x) \parallel p_\theta (z))
$$

Вычислим его для приведённых выше распределений. 

Начнём с $D_{KL} (q_\phi(z \| x) \parallel p_\theta (z))$. $KL$-дивергенция между распределениями $\mathcal N(\mu, \Sigma)$ и $\mathcal N(0, I)$ равна:

$$
     D_{KL} ( N(\mu, \Sigma) \parallel \mathcal N(0, I)) = \frac{1}{2} \left(
        \mu^T \mu + tr \Sigma - M - \log (\det \Sigma)
     \right),
$$

где $M$ — размерность этих распределений. Вывод этого соотношения можно найти [здесь](https://mr-easy.github.io/2020-04-16-kl-divergence-between-2-gaussian-distributions/). В нашем случае $\mu_\phi(x) = (\mu_1, \ldots, \mu_M)$, $\sigma^2_\phi(x) = \operatorname{diag}(\sigma^2_1, \ldots, \sigma^2_M)$ и

$$
    D_{KL} (q_\phi(z \| x) \parallel p_\theta (z)) = D_{KL} (\mathcal N(\mu_\phi(x), \sigma_\phi^2(x)) \parallel \mathcal{N}(0, I)) = \\
$$

$$
    = \frac{1}{2} \sum_{j = 1}^M (\sigma_j^2 + \mu_j^2 - 1 - \ln \sigma_j^2)
$$

Тогда ELBO будет вычисляться как:

$$
    \mathcal L_{\theta, \phi} (x) = \mathbb E_{q_\phi(z | x)} \left[ 
        \log p_\theta (x | z)
    \right] - D_{KL} (q_\phi(z | x) \parallel p_\theta (z)) = \\
$$
$$
    = \mathbb E_{\mathcal N(\mu_\phi(x), \sigma_\phi^2(x))} \left[ 
        \log p_\theta (x | z)
    \right] - \frac{1}{2} \sum_{j = 1}^M (\sigma_j^2 + \mu_j^2 - 1 - \ln \sigma_j^2) \approx \\
$$
$$
    \approx \frac{1}{K} \sum_{k = 1}^K \log p_\theta (x | z_k) + \frac{1}{2} \sum_{j = 1}^M (1 + \ln \sigma_j^2 - \mu_j^2 -\sigma_j^2),
$$

где $z_k \sim \mathcal N(\mu_\phi(x), \sigma_\phi^2(x))$. Как было упомянуто в [этой статье](https://arxiv.org/pdf/1312.6114.pdf) от авторов VAE в разделе 2.3, число семплирований $K$ можно положить равным единице при достаточно большом размере батча (например, 100). 

Если вы выберете биномиальное $p_\theta (x \| z)$, то 

$$
    \log p_\theta (x | z) = \sum_{j = 1}^D \log p_\theta(x_j | z) = \sum_{j = 1}^D \log \operatorname{Bernoulli}(x_j, p_j) = \\
$$
$$
    = \sum_{j = 1}^D x_j \log p_j + (1 - x_j) \log(1 - p_j)
$$

Если гауссовское $\mathcal N(f_\theta(z), \sigma^2)$, то

$$
    \log p_\theta (x | z) = \sum_{j = 1}^D \log p_\theta(x_j | z) = \sum_{j = 1}^D \log \left( 
        \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left( 
            -\frac{(x_j - f_{\theta, j}(z))^2}{2 \sigma^2}
        \right)
    \right) = \\
$$
$$
    = - \frac{D}{2} \log 2 \pi - D \log \sigma - \frac{1}{2 \sigma^2} \sum_{j = 1}^D (x_j - f_{\theta, j}(z))^2
$$

Пример реализации обучения и применения VAE на датасете MNIST на Keras можно найти [здесь](https://blog.keras.io/building-autoencoders-in-keras.html), а на PyTorch — [здесь](https://github.com/pytorch/examples/blob/master/vae/main.py).

### Инференс обученной модели

Когда мы обучили VAE, мы сможем генерировать новые семплы, просто подавая $z \sim \mathcal{N}(0, I)$ на вход декодеру:

![2](https://yastatic.net/s3/education-portal/media/vae_decoder_diagram_385be2e566_6c396a28e8.svg"></a>

Энкодер для генерации новых семплов не нужен. Однако нам может понадобиться оценить $p(x) = \int p(x \| z) p(z) dz$ для $x$ из тестового множества, чтобы понять, с какой вероятностью модель сможет сгенерировать $x$. Для оценки интеграла нам нужно насемплировать некоторое количество $z$, и если брать семплы из $z \sim \mathcal{N}(0, I)$, то оценка может плохо сойтись. Но можно снова использовать ELBO как нижнюю границу для $\log p(x)$ и оценивать уже её, семплируя из распределения $q_\phi(z \| x)$. Такая оценка сойдётся быстрее и даст примерное представление о том, насколько хорошо модель справляется с конкретным примером $x$.

Также интересно бывает взглянуть на то, как распределены коды обучающих примеров в латентном пространстве. Так, например, может выглядеть распределение латентных кодов цифр MNIST для обученного VAE в двумерном латентном пространстве:

![2](https://yastatic.net/s3/education-portal/media/vae_classes_plane_0fa7689113_4e6b5e1037.webp)

Разные типы цифр обозначены разными цветами (соответствие цифр и цветов показано на шкале сбоку). Здесь видно, что лучше всего модель различает нули и единицы, а восьмёрки и тройки — хуже всего. Стоит, конечно, отметить, что латентное пространство выбрано двумерным в целях визуализации, и при большей его размерности модель могла бы научиться различать цифры более качественно.

Для двумерного латентного пространства есть ещё один интересный способ визуализировать структуру многообразия, выученного VAE. Можно взять равномерную сетку на единичном квадрате и отобразить её в латентное пространство, применив к ней функцию, обратную к CDF нормального распределения. 

{% cut "Почему это сработает" %}

Узлы равномерной сетки $u_{ij}$ можно в некотором приближении считать семплами из равномерного распределения: $u_{ij} \sim \text{Uniform}([0, 1])$. Поэтому семплы $\Phi^{-1}(u_{ij})$ приближённо подчиняются нормальному распределению:

$$
    \mathbb P(\Phi^{-1}(u_{ij}) \le t) = \mathbb P(u_{ij} \le \Phi(t)) = \Phi(t)
$$

{% endcut %}

Полученные семплы можно подать в декодер и посмотреть, какие картинки будут соответствовать узлам сетки:

![2](https://yastatic.net/s3/education-portal/media/manifolds_6b5b85515c_c695dfba8d.webp)

Здесь изображены примеры, сгенерированные для датасетов Frey Face и MNIST (оба доступны по [ссылке](https://cs.nyu.edu/~roweis/data.html)). Такая визуализация позволяет увидеть плавный переход латентных кодов одних объектов в коды других, а также взаимное расположение латентных кодов.

Для MNIST снова видно, в частности, что коды нулей и единиц модель разнесла далеко друг от друга, а коды троек и восьмёрок очень близки. А ещё интересно наблюдать плавный переход от шестёрок к нулям и от семёрок к единицам. Для Frey Face видно, что весёлые лица расположены далеко от грустных, а по главной диагонали квадрата можно проследить плавный переход от серьёзного лица к улыбающемуся.

Ещё интересно посмотреть на то, как меняется качество генерируемых цифр в зависимости от размерности латентного пространства (на картинках просто случайные семплы из модели):

![2](https://yastatic.net/s3/education-portal/media/latent_spaces_129c8124ab_024fd5632c.webp)

Заметный переход виден между размерностями 2 и 5, дальнейший рост размерности почти не оказывает значимого эффекта.

### Conditional VAE (CVAE)

Иногда мы можем захотеть сгенерировать не просто какой-то произвольный объект из датасета, а относящийся к конкретной группе или классу. Ранее мы выписывали уравнение для $\log p_\theta(x)$:

$$
    \log p_\theta(x) = \mathbb E_{q_\phi(z | x)} \left[ 
        \log p_\theta (x | z)
    \right] - D_{KL} (q_\phi(z | x) \parallel p_\theta (z)) + D_{KL}(q_{\phi}(z | x) \parallel p_\theta (z | x))
$$

Все распределения, участвующие в этом уравнении, мы можем сделать обусловленными по переменной $y$:

$$
    \log p_\theta(x | y) = \mathbb E_{q_\phi(z | x, y)} \left[ 
        \log p_\theta (x | z, y)
    \right] - D_{KL} (q_\phi(z | x, y) \parallel p_\theta (z | y)) + D_{KL}(q_{\phi}(z | x, y) \parallel p_\theta (z | x, y))
$$

Переменная $y$ может быть лейблом объекта $x$ или вообще произвольным тензором, как-то характеризующим $x$. Вместо $p_\theta(z)$, единого для всех $x$ из обучающей выборки, для каждого значения $y$ теперь будет отдельное априорное распределение $p_\theta(z \| y)$.

Переменная $y$ может принимать и дискретные, и непрерывные значения. Она может даже, например, быть половиной изображения, которую модели предлагается дополнить. На всякий случай подчеркнём, что обучение CVAE — это не то же самое, что обучение нескольких независимых VAE, так как веса CVAE общие для всех классов.

На уровне имплементации это реализуется довольно просто: нужно всего лишь сконкатенировать входы энкодера и декодера с тензором, соответствующим $y$. Если $y$ имеет категориальные значения, то бывает полезно предварительно закодировать их one-hot векторами. Алгоритм будет примерно таким:

``` python
dataset, labels = np.array(...), np.array(...)
epsilon = RandomDistribution(...)

# Энкодер q_phi(z|x) — нейронная сеть с параметрами phi
encoder = Encoder()

# Декодер p_theta(x|z) — нейронная сеть с параметрами theta
decoder = Decoder()

for step in range(max_steps):
     # Семплируем батч исходных данных, лейблов и случайного шума
     batch_x = sample_batch(dataset)
     batch_y = sample_batch(labels)
     batch_noise = sample_batch(epsilon)

     # Подаём в энкодер конкатенацию входных данных и лейблов
     encoder_input = concatenate([batch_x, batch_y])
    
     # Считаем параметры распределения z с помощью энкодера
     latent_distribution_parameters = encoder(encoder_input)
     # Делаем репараметризацию
     z = reparameterize(latent_distribution_parameters, batch_noise)

     # Конкатенируем полученный случайный вектор и лейблы
     decoder_input = concatenate([z, batch_y])

     # Декодер отдаёт нам выходное изображение
     output_distribution_parameters = decoder(decoder_input)

     # Вычисляем ELBO и обновляем параметры
     L = -ELBO(
        latent_distribution_parameters, 
        output_distribution_parameters, 
        batch_x
     )
     L.backward()
```

Реализацию CVAE на PyTorch и Tensorflow можно найти, например, [здесь](https://github.com/wiseodd/generative-models/tree/master/VAE/conditional_vae).

Если визуализировать распределение латентных кодов для цифр MNIST, полученных после обуславливания модели на класс цифры, то можно увидеть что-то такое:

![2](https://yastatic.net/s3/education-portal/media/z_dist_cvae_b651c517c3_b275364e3a.webp)

Мы видим непонятную смесь из точек вместо явных кластеров, которые выделяла обычная модель VAE. Однако дело тут в том, что, вместо того, чтобы пытаться размещать все цифры в одном пространстве $p(z) \sim \mathcal N (0, I)$, модель использует отдельное латентное пространство $p(z \| y) \sim \mathcal N (0, I)$ для каждой цифры:

![2](https://yastatic.net/s3/education-portal/media/cvae_manifold_6_becdec52d5_1e4e21995e.webp)
![2](https://yastatic.net/s3/education-portal/media/cvae_manifold_7_b98915c786_33c732547a.webp)

На картинке справа — априорные распределения для цифр 6 и 7, а слева — визуализация структуры выученных многообразий для этих цифр, построенная так же, как аналогичная визуализация для VAE. Качество изображений каждой отдельной цифры заметно повышается:

![2](https://yastatic.net/s3/education-portal/media/samples_from_cvae_60f0f65726_44246fd857.webp)

Видно, что вариабельность генерации цифр теперь тоже заметно выросла, и модель может имитировать написание цифр разными почерками.

## Обзор статей

Кроме стандратного описания работы VAE, приведём результаты нескольких недавних интересных работ, базирующихся на идее VAE.

### VQ-VAE и VQ-VAE-2

Модели [VQ-VAE](https://arxiv.org/pdf/1711.00937v2.pdf) и [VQ-VAE-2](https://arxiv.org/pdf/1906.00446.pdf) интересны тем, что в них в качестве априорных распределений были задействованы дискретные распределения. В каких ситуациях дискретные распределения могут быть более применимы, чем непрерывные? Например, если мы имеем дело с токенам в задачах NLP или фонемами в обработке речи. Картинки также можно было бы кодировать некоторым набором из целых чисел: например, одно число могло бы кодировать тип объекта, другое — его цвет, третье — цвет фона и так далее:

![2](https://yastatic.net/s3/education-portal/media/discrete_enc_dec_2dba0b17a0_6f46c2a61b.webp)

Кроме того, существуют довольно мощные алгоритмы (например, [Трансформер](https://arxiv.org/abs/1706.03762)), предназначенные для работы с дискретными данными. Выучивание хороших дискретных представлений даёт возможность эффективно использовать такие алгоритмы для, например, задачи генерации картинок.

#### VQ-VAE

Авторы VQ-VAE вводят дискретное латентное пространство в виде $K$ вещественных векторов $e_1, \ldots, e_K$ размерности $D$. Векторы из этого пространства называются **кодовыми векторами** или **кодами**. На рисунке ниже приведена примерная схема обучения предлагаемой модели.

![2](https://yastatic.net/s3/education-portal/media/vq_vae_40e8acafa9_1e58753f42.webp)

Энкодер принимает на вход картинку $x$ и выдаёт на выходе тензор $z_e(x)$. На рисунке этот тензор имеет размерность $M \times M \times D$: последняя размерность совпадает с длиной кодовых векторов, а $M \times M$ — это пространственная размерность выхода CNN (для простоты мы здесь не пишем явно размерность батчей).

Каждый из $M \times M$ векторов из $z_e(x)$ отображается в ближайший к нему по $L_2$-расстоянию кодовый вектор. После такой процедуры тензор $z_e(x)$ переходит в тензор $z_q(x)$, состоящий из $M \times M$ кодовых векторов. Декодер получает на вход тензор $z_q(x)$ и отображает его в исходную картинку. Для работы с речью и текстами авторы использовали двумерный тензор $z_e(x)$ вместо трёхмерного.

Выходное распределение энкодера $q(z \| x)$ определено здесь следующим образом:

$$
q(z=k|x)=\begin{cases}1,&k=\arg\min_j\|z_e(x)-e_j\|_2,\\0,&\text{иначе}\end{cases}
$$

Во время обучения в качестве априорного распределения в латентном пространстве используется равномерное распределение $p(z) = \frac{1}{K}$, поэтому слагаемое $D_{KL} (q(z \| x) \parallel p (z))$ оказывается постоянным и равным $\log K$:

$$
    D_{KL} (q(z | x) \parallel p (z)) = - \sum_{k = 1}^K q(z = k | x) \log \left( \frac{p(z)}{q(z = k | x)} \right) = \log K
$$

В точках, где $q(z = k \| x) = 0$, предпоследнее выражение продолжается нулём по непрерывности. Таким образом, ELBO для таких распределений примет вид

$$
    ELBO(x) = \mathbb{E}_{q(z | x)} [\log p_\theta (x | z_e(x))] - D_{KL} (q(z | x) \parallel p (z)) = \log p_\theta (x | z_q(x)) - \log K,
$$

где $\theta$ — параметры декодера. При оптимизации $\log K$ можно не учитывать. Отображение выхода энкодера в кодовые векторы не дифференцируемо, поэтому при обучении применяется следующий трюк: при обратном проходе градиент копируется напрямую из декодера в энкодер, пропуская при этом слой, отображающий выходы энкодера в кодовые векторы.

Этот трюк очень близок к приёму, известному как **straight-through estimator**, впервые предложенному в этой [статье](https://arxiv.org/pdf/1308.3432.pdf) (а его простое описание можно найти [тут](https://www.hassanaskary.com/python/pytorch/deep%20learning/2020/09/19/intuitive-explanation-of-straight-through-estimators.html)). Использование straight-through estimator, однако, не позволяет обучать сами кодовые векторы, так как по ним не будут вычисляться градиенты. Поэтому лосс для обучения модели складывается из трёх компонент:

$$
    \mathcal{L} = \log p(x | z_q(x)) + \| sg[z_e(x)] - z_q(x) \|_2^2 + \beta \| z_e(x) - sg[z_q(x)] \|_2^2
$$

Здесь $sg[\cdot]$ обозначает оператор остановки дифференцирования: через его аргумент не текут градиенты.
{% cut "Замечание по поводу обозначений" %}

В статье лосс записан несколько иначе:

$$
\mathcal{L} = \log p(x | z_q(x)) + \| sg[z_e(x)] - e \|_2^2 + \beta \| z_e(x) - sg[e] \|_2^2
$$

Эти обозначения кажутся несколько путающими по двум причинам:
1. Буква $e$ в нижнем индексе $z_e(x)$ призвана обозначить только то, что это выход энкодера, а не наличие связи между кодовыми векторами $e$ и параметрами энкодера. Но второе довольно легко для себя предположить. 
2. Вычитание $e$ обозначает вычитание не всех элементов словаря из соответствующей позиции тензора $z_e(x)$, а только лишь ближайшего соседа к элементу $z_e(x)$ на этой позиции. То есть по факту вычитание $e$ в этой записи равносильно вычитанию $z_q(x)$. Это не уточняется в статье, но можно [увидеть](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/nets/vqvae.py#L113) в официальной реализации.

{% endcut %}

Первое слагаемое — это ELBO с точностью до константы. Второе слагаемое отвечает за сдвиг кодовых векторов в сторону выходов энкодера. Чтобы не получилось так, что выходы энкодера всё время меняют кодовые векторы за счёт второй компоненты лосса, а сами на каждой итерации выдают векторы, далёкие от текущих кодовых векторов, добавляется третье слагаемое. Оно отвечает за то, чтобы энкодер стремился выдавать векторы, близкие к кодовым векторам, а его значимость регулируется с помощью коэффициента $\beta$.

Однако при обучении мы потеряли регуляризационное слагаемое $D_{KL} (q(z \| x) \parallel p (z))$, из-за чего распределение энкодера не было обязано приближать собой априорное распределение и осталось его узким подмножеством. Из-за этого с наибольшей вероятностью при семплировании из равномерного категориального распределения мы будем получать просто шумы вместо хороших картинок:

![2](https://yastatic.net/s3/education-portal/media/bad_samples_75f081105c_8c86b457d5.webp)

{% cut "Чуть подробнее" %}

При обучении обычного VAE мы минимизируем расстояние между априорным распределением и распределением, которое выдаёт энкодер, с помощью регуляризационного слагаемого $D_{KL} (q(z \| x) \parallel p (z))$.

Благодаря нему, например, двумерные латентные коды цифр MNIST приближённо распределяются по шарику — априорному нормальному распределению. А если каждой цифре выделить собственное латентное пространство (провести обуславливание на класс цифры), то априорное условное распределение для каждой цифры очень близко к нормальному.

А в случае VQ-VAE мы не можем заставить распределение, предсказываемое энкодером, быть равномерным категориальным, и получаем просто какое-то категориальное распределение с неизвестной параметризацией. Это напоминает ситуацию с обычным автоэнкодером: он тоже переводит входные картинки в латентное пространство, но семплировать из такого пространства мы не можем. 

{% endcut %}

Чтобы исправить эту проблему, авторы предлагают с помощью дополнительной модели выучить априорное распределение $p(z)$ тех латентных переменных, которые модель научилась генерировать в процессе обучения. Поскольку любое кодовое представление можно вытянуть в последовательность, а самих кодов — конечное наперёд заданное число, то эта задача близка к задаче обучения языковой модели.

Действительно, ведь там мы должны по последовательности предыдущих слов предложения предсказать следующее слово из доступного словаря, а в нашем случае — по входной последовательности дискретных латентных кодов предсказать следующий латентный код.

Для картинок авторы предложили моделировать априорное распределение латентных кодов с помощью PixelCNN. Детали архитектуры и обучения этой модели можно найти в оригинальной [статье](https://arxiv.org/pdf/1606.05328v2.pdf), здесь мы опишем только общую идею.

PixelCNN последовательно генерирует пиксели картинки, двигаясь из верхнего левого угла в правый нижний. Она проходит все ряды последовательно от верхнего до нижнего, а внутри каждого ряда движется слева направо:

![2](https://yastatic.net/s3/education-portal/media/pixelcnn_a09ba4e935_35e466109f.webp)

Для цветных картинок каналы (R, G, B) также моделируются последовательно: канал B при генерации зависит от R и G, а G — только от R. При предсказании значения каждого следующего пикселя модель использует значения уже сгенерированных соседей из некоторого окружающего квадрата. Чтобы модель не могла читать пиксели, идущие после текущего предсказываемого пикселя, используется специальная маска, пример которой изображён на правой части рисунка.

В случае VQ-VAE обучение PixelCNN происходит не на пикселях, а на латентных кодах. Семплирование из выученного априорного распределения выглядит гораздо лучше, чем попытки семплировать из равномерного:

![2](https://yastatic.net/s3/education-portal/media/pixel_cnn_prior_7b056530b7_943d1fafc4.webp)

Для аудио вместо PixelCNN авторами используется [WaveNet](https://deepmind.com/blog/article/wavenet-generative-model-raw-audio). При обучении моделей априорных распределений есть возможность подавать метки классов, чтобы потом можно было семплировать из этих классов (принцип тот же, что и для CVAE).

Результаты реконструкции картинок из ImageNet с помощью VQ-VAE выглядят довольно неплохо (под реконструкцией понимается выход полной модели, состоящей из энкодера и декодера):

![2](https://yastatic.net/s3/education-portal/media/vq_vae_reconstruction_9edc867133_30318b6a2d.webp)

А так выглядят результаты семплирования из VQ-VAE с априорным распределением, выученным PixelCNN:

![2](https://yastatic.net/s3/education-portal/media/vq_vae_samples_7ee45b4e87_c5c9d1b905.webp)

#### VQ-VAE-2

Модель VQ-VAE-2 — это расширение VQ-VAE. Она показывает значительный скачок по качеству генерируемых изображений:

![2](https://yastatic.net/s3/education-portal/media/vq_vae_2_samples_5afee645f4_bc4a494bcb.webp)

Впечатляет то, что на картинке именно результат семплирования из выученного моделью распределения, а не результат реконструкции. Первое основное отличие модели VQ-VAE от VQ-VAE-2 — использование иерархических латентных переменных:

![2](https://yastatic.net/s3/education-portal/media/hierarchical_latents_1d8c41404a_b87a6935e4.webp)

Прежде чем перейти к описанию архитектуры, хочется сделать небольшой дисклеймер: когда в тексте далее будет говориться «тензор размера $M \times M$», то будет иметься в виду, что тензор имеет шейп $(B, M, M, C)$, где первая размерность соответствует батчам, а последняя — каналам.

На картинке показан пример двухуровневой архитектуры (хотя уровней может быть и больше). Каждому уровню соответствуют свои энкодер, декодер и набор кодовых векторов (общей размерности $D$ для всех уровней). Обозначим нижний и верхний энкодеры как $Enc_{\text{bottom}}$ и $Enc_{\text{top}}$, а декодеры — как $Dec_{\text{bottom}}$ и $Dec_{\text{top}}$.

- $Enc_{\text{bottom}}$ принимает на вход трёхканальную картинку размера $256 \times 256$ пикселей, отображает её в тензор размера $64 \times 64$ и передаёт на вход $Enc_{\text{top}}$. $Enc_{\text{top}}$ выдаёт тензор размера $32 \times 32$, который затем отображается в тензор из кодовых векторов $z_{\text{top}}$ (квантизуется)
- $z_{\text{top}}$ передаётся на вход $Dec_{\text{top}}$, затем выходы $Enc_{\text{bottom}}$ и $Dec_{\text{top}}$ конкатенируются и квантизуются в $z_{\text{bottom}}$ 
- $z_{\text{top}}$ и $z_{\text{bottom}}$ конкатенируются и передаются на вход $Dec_{\text{bottom}}$, который отображает их в исходную картинку

Для обучения модели используется почти такой же лосс, как для VQ-VAE. Для VQ-VAE он имел вид:

$$
    \mathcal{L} = \log p(x | z_q(x)) + \| sg[z_e(x)] - z_q(x) \|_2^2 + \beta \| z_e(x) - sg[z_q(x)] \|_2^2
$$

Для VQ-VAE-2 первое и третье слагаемые сохраняют свой вид, а второе слагаемое заменяется на обновление кодовых векторов $e_i$ с помощью экспоненциального скользящего среднего. Пусть $E(x)^{(t)}$ — выход энкодера на шаге $t$, выпрямленный в двумерный тензор, последняя размерность которого равна размерности $D$ кодовых векторов.

Пусть $\{ E^{(t)}_{i, 1}, \ldots, E^{(t)}_{i, n_i^{(t)}} \}$ — множество из $n_i^{(t)}$ векторов, для которых на шаге $t$ ближайшим оказался кодовый вектор $e_i^{(t - 1)}$. Тогда обновление $e_i$ на шаге $t$ происходит по следующим формулам:

$$
    e_i^{(t)} = \frac{m_i^{(t)}}{N_i^{(t)}}
$$

$$
    m_i^{(t)} = m_i^{(t - 1)} \cdot \gamma + \sum_j^{n_i^{(t)}} E(x)_{i, j}^{(t)} (1 - \gamma)
$$

$$
    N_i^{(t)} =  N_i^{(t - 1)} \cdot \gamma + n_i^{(t)} (1 - \gamma)
$$

Здесь $\gamma$ — некоторый вещественный параметр.

Так же, как и для VQ-VAE, априорное распределение для VQ-VAE-2 выучивается отдельно уже после обучения основной модели, но в случае VQ-VAE-2 оно имеет иерархическую структуру. На картинке изображён пример такого распределения для двухуровневой архитектуры:

![2](https://yastatic.net/s3/education-portal/media/hierarchical_prior_55f4d6aa56_21a33b2263.webp)

Для каждого уровня обучается отдельная модель PixelCNN: одна — на кодовых векторах первого уровня, вторая — на кодовых векторах первого и второго уровней. Обе модели также принимают на вход метку класса, изображение из которого нужно насемплировать. 

Семплирование из финальной модели происходит так: 
- семплируются векторы $e_{\text{top}}$ из верхнего распределения
- из нижнего распределения семплируются векторы $e_{\text{bottom}}$ при условии векторов $e_{\text{top}}$
- декодер принимает на вход векторы $e_{\text{top}}$ и $e_{\text{bottom}}$ и выдаёт финальную картинку

Результаты семплирования из двухуровневой модели VQ-VAE-2, обученной на ImageNet:

![2](https://yastatic.net/s3/education-portal/media/vq_vae_2_image_net_11e8448202_72cfc6fb5a.webp)

А это — результаты семплирования из трёхуровневой модели VQ-VAE-2, обучавшейся на [FFHQ](https://paperswithcode.com/dataset/ffhq):

![2](https://yastatic.net/s3/education-portal/media/vq_vae_2_ffhq_4c83b1aab6_4367922715.webp)

### DALL-E

Одна из недавних работ, связанных с VAE, — это [DALL-E](https://arxiv.org/pdf/2102.12092.pdf) от OpenAI. Они обучили модель с 12 миллиардами параметров, генерирующую картинки по их текстовому описанию. Для обучения авторами был собран датасет, состоящий из 250 миллионов пар картинок и их описаний. Вот примеры работы этой модели:

![2](https://yastatic.net/s3/education-portal/media/avocado_329095ee68_9f7a46934f.webp)

![2](https://yastatic.net/s3/education-portal/media/open_ai_store_0110955e3f_3a7446d885.webp)

В [блог-посте](https://openai.com/blog/dall-e/) OpenAI, посвящённом DALL-E, есть возможность самостоятельно составлять текстовые описания из некоторого ограниченного словаря и смотреть на результаты. Осторожно, это затягивает :)

![2](https://yastatic.net/s3/education-portal/media/capybara_1d7ed6f4c8_e3e460924c.webp)

DALL-E идейно основывается на результатах VQ-VAE: сначала выучиваются кодовые векторы для картинок, а затем обучается Трансформер, моделирующий совместное априорное распределение текстов и кодовых векторов. Подробнее о трансформерах мы [рассказывали](https://education.yandex.ru/handbook/ml/article/transformery) в главе 6.3 этого хендбука.

В DALL-E задействована архитектура, основанная на декодер-части исходной архитектуры Трансформера, поэтому стоит также [почитать](https://jalammar.github.io/illustrated-gpt2/) про модель GPT-2, работающую аналогичным образом.

Обучение проходит в две стадии:

- Сначала обучается дискретизованный VAE (**dVAE**) c энкодером для сжатия RGB-картинок размера $256 \times 256$ в тензор из $32 \times 32 = 1024$ кодовых векторов. Эта стадия обучения очень напоминает VQ-VAE, но вместо добавления в лосс дополнительных слагаемых для кодовых векторов авторы DALL-E используют *релаксацию Гумбеля* — трюк, позволяющий проводить честное дифференцирование по параметрам энкодера. Об обучении dVAE мы будем говорить подробнее далее.
- Затем обучается Трансформер (точнее, только декодер-часть исходной архитектуры Трансформера), задача которого — выучить совместное распределение картинок и их текстовых описаний. Он принимает на вход конкатенацию из эмбеддингов текстовых токенов и кодовых векторов картинок и учится для каждой входной последовательности предсказывать её продолжение. О некоторых деталях обучения Трансформера также будет рассказано далее.

Инференс обученной модели происходит так: эмбеддинги текстового описания картинки подаются на вход Трансформеру, и он авторегрессионно предсказывает кодовые векторы картинки, соответствующей этому описанию, а затем полученные кодовые векторы пропускаются через декодер dVAE. 

#### dVAE

Обучение dVAE происходит путём максимизации ELBO для картинок $x$ и их дискретных латентных представлений $z$:

$$
\ln p_{\theta}(x) \ge \mathbb E_{q_\phi(z | x)} \left[ 
    \log p_\theta (x | z) 
\right] - \beta \, D_{KL} (q_\phi(z | x) \parallel p(z)),
$$

где $\phi$ и $\theta$ — параметры энкодера и декодера дискретизованного VAE, a $p(z)$ — равномерное категориальное распределение над кодовыми векторами. Здесь можно заметить дополнительный коэффициент $\beta$, который в стандартном VAE всегда равен 1. Однако авторы DALL-E ввели дополнительный параметр $\beta$, опираясь на результаты [статьи](https://arxiv.org/pdf/1904.10509.pdf) о $\beta$-VAE. Но, в отличие от исходной статьи, в их экспериментах значение $beta$ постепенно понижается в ходе обучения.

Энкодер dVAE отображает картинки размера $256 \times 256$ в тензор $z_e(x)$ с шейпом $32 \times 32 \times 8192$, где $8192$ — число кодовых векторов. То есть каждой из $32 \times 32$ позиций энкодер сопоставляет категориальное распределение над $8192$ кодовыми векторами, параметризованное выходными логитами.

Для получения тензора $z_q(x)$ из кодовых векторов можно было бы сначала применить $\text{softmax}$ к распределениям на каждой из $32 \times 32$ позиций, а затем сопоставить каждой позиции кодовый вектор, номеру которого соответствует максимальная вероятность (взять $\text{argmax}$ для этой позиции).

Однако операция $\text{argmax}$ не дифференцируема, и, к тому же, в концепции VAE на вход декодеру должен пойти семпл из распределения, предсказываемого энкодером, а взятие $\text{argmax}$ на каждой позиции не является семплированием из предсказанного распределения.

Поэтому нам потребуется применение некоторых трюков, которые позволят нам одновременно:

- аппроксимировать семплирование из $\text{softmax}$
- сделать семплирование дифференцируемым

##### Gumbel-Max Trick и Gumbel-Softmax

Первый трюк известен в англоязычной литературе как Gumbel-Max Trick. Представим, что у нас есть логиты-выходы сетки $x_1, \ldots, x_k$, и мы хотим с их помощью получить семпл из категориального распределения, то есть стохастически предсказать класс. Для этого мы обычно применяем к логитам $\text{softmax}$, чтобы получить вероятности $\pi_i$:

$$
    \pi_i = \frac{\exp{x_i}}{\sum_j \exp{x_j}},
$$

а затем из получившегося категориального распределения $\left\{ \pi_1, \ldots, \pi_k \right\}$ семплируем класс. Оказывается, этим двум шагам будет эквивалентна следующая процедура:

- насемплировать числа $g_1, ..., g_k$ из стандартного распределения [Гумбеля](https://en.wikipedia.org/wiki/Gumbel_distribution),
- прибавить к каждому из логитов $x_i$ семпл $g_i$,
- выбрать класс $j$, такой что $j = \text{argmax}_i(x_i + g_i)$.

О том, почему это действительно так, можно почитать [здесь](https://lips.cs.princeton.edu/the-gumbel-max-trick-for-discrete-distributions/). Однако сам по себе Gumbel-Max Trick нам не поможет — ведь операция так и не стала дифференцируемой. Поэтому придётся использовать ещё один трюк, предложенный практически одновременно в двух статьях ([первая](https://arxiv.org/pdf/1611.01144.pdf) и [вторая](https://arxiv.org/pdf/1611.00712v3.pdf)) и названный Gumbel-Softmax в одной из них.

Чтобы описать этот трюк, отметим, что результат операции $\text{argmax}$ — это индекс некоторого класса $j$. Такой индекс можно описать one-hot кодированием, то есть вектором длиной $k$, в котором все элементы равны нулю, кроме $j$-го, который равен единице.

Gumbel-Softmax состоит в том, чтобы вместо взятия $\text{argmax}$ на последнем этапе Gumbel-Max Trick делать следующее:

- вычислить $y_i = \frac{\exp\left( (x_i + g_i) / \tau \right)}{\sum_{j = 1}^k \exp \left( (x_j + g_j) / \tau \right)}$, $i = 1, \ldots, k$, — аппроксимацию one-hot при помощи $\text{softmax}$ с температурой
- сложить кодовые векторы $e_i$ с весами $y_i$: $z = \sum_i y_i e_i$
- выдать вектор $z$ в качестве латентного вектора для данной позиции

На самом деле авторы DALL-E не уточняли, как выходной вектор $z$ агрегируется из кодовых векторов и $y_i$, но такой подход применён в [реализации](https://github.com/lucidrains/DALLE-pytorch/tree/main/dalle_pytorch) DALL-E на PyTorch.

При $\tau \to 0$ семплирование из распределения $\frac{\exp\left( (x_i + g_i) / \tau \right)}{\sum_{j = 1}^k \exp \left( (x_j + g_j) / \tau \right)}$ стремится к $\text{argmax}$, и в процессе обучения dVAE авторы постепенно уменьшали значение $\tau$. На следующей картинке слева — просто Gumbel-Max Trick, а справа — дифференцируемый вариант Gumbel-Max Trick:

![2](https://yastatic.net/s3/education-portal/media/gumbel_reparametrization_426251e2a8_6336997622.webp)

Таким образом, для обучения кодовых векторов для dVAE не требуется дополнительных слагаемых в лоссе относительно ELBO, а также копирования градиентов из декодера в энкодер (как было в VQ-VAE).

Кроме того, стоит отметить, что $D_{KL} (q_\phi(z | x) \parallel p(z))$ в данном случае не вырождается в константу, а действительно действует как регурялизатор, заставляя категориальное распределение, параметризованное логитами энкодера, быть ближе к равномерному распределению над кодовыми векторами.

##### Распределение Logit-Laplace

Ещё один трюк в обучении dVAE касается выходного распределения $p_\theta (x \| z)$. Авторы DALL-E подметили проблему, возникающую при часто встречающемся выборе лапласовского и гауссовского распределений в качестве $p_\theta (x \| z)$: оба они определены на всей вещественной прямой, в то время как пиксели принимают значения из ограниченного интервала. Таким образом, часть плотности при моделировании «теряется», оказываясь вне возможных границ значений пикселей.

Чтобы исправить эту проблему, авторы предлагают использовать распределение, которое они назвали “Logit-Laplace”. Его плотность определена на интервале $(0, 1)$ и выражается следующей формулой:

$$
    f(x | \mu, b) = \frac{1}{2 b x (1 - x)} \exp \left( -\frac{|\text{logit}(x) - \mu|}{b} \right),
$$

$$
    \text{logit}(x) = \frac{x}{1 - x}
$$

Эта плотность соответствует случайной переменной, полученной применением сигмоиды к распределённой по Лапласу случайной переменной. Выражение для распределения Logit-Laplace можно получить по стандартной формуле для плотности случайной величины, полученной применением монотонной дифференцируемой функции к другой случайной величине (см. формулу, например, [тут](https://en.wikipedia.org/wiki/Probability_density_function)). Логарифм этой плотности подставляется в ELBO вместо $\ln p_\theta(x \| z)$.

Декодер на выходе выдаёт 6 тензоров: первые три соответствуют $\mu$ для RGB-каналов, оставшиеся три соответствуют $\ln b$, и эти 6 тензоров используются для подсчёта лосса. При подаче в энкодер значения картинок нормируются функцией $\phi: [0, 255] \to (\varepsilon, 1 - \varepsilon)$:

$$
    \phi: x \mapsto \frac{1 - 2 \varepsilon}{255} x + \varepsilon
$$

Этим авторы добиваются того, чтобы декодер моделировал значения из $(\varepsilon, 1 - \varepsilon)$, что позволяет нивелировать вычислительные проблемы, связанные с делением на $x(1 - x)$ в формуле плотности. Во время инференса реконструкция $\hat x$ картинки $x$ вычисляется по формуле: 

$$
    \hat x = \phi^{-1}(\text{sigmoid}(\mu)),
$$

где $\mu$ — первые три тензора из выхода декодера. Выходы, соответствующие $\ln b$, при этом не используются.
 
#### Априорное распределение на текстах и картинках

На втором этапе авторы фиксируют параметры $\phi$ и $\theta$ и моделируют совместное распределение картинок и их текстовых описаний с помощью [Sparse Transformer](https://openai.com/blog/sparse-transformer/) с 12 миллиардами параметров. На вход он получает конкатенацию из текстового описания картинки и её кодовых векторов. Картинка представляется 1024 кодовыми векторами, получаемыми из энкодера $q_\phi$, причём при семплировании кодовых последовательностей используется обычный $\text{argmax}$ без добавления шума из распределения Гумбеля.

Текстовое описание токенизируется с помощью процедуры BPE (см. раздел про BPE [здесь](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)), и каждому токену ставится в соответствие представляющий его вектор из вещественных чисел (эмбеддинг). Для представления текста используется не более 256 токенов, а размер используемого словаря — 16 384 токена.

Задача Трансформера во время обучения — для каждого начального отрезка входной последовательности предсказать следующий за ним токен. Это может быть как текстовый токен, так и кодовый вектор картинки. Поскольку кодовые векторы картинок всегда идут за текстовыми токенами, при генерации кодовых векторов attention-механизм учитывает также и все предыдущие текстовые токены.

Кроме того, маска attention для кодовых векторов учитывает, что исходно они расположены не линейно друг за другом, а на прямоугольной сетке. В статье приводится несколько вариантов геометрических паттернов, которые использовались для attention-маски на кодовых векторах.

В качестве лосса используется взвешенная сумма кросс-энтропии для текстовых токенов и кросс-энтропии для кодовых векторов картинок c весами $\frac{1}{8}$ и $\frac{7}{8}$ соответственно (больший приоритет отдаётся генерации картнок, отсюда и больший вес для лосса).

Конечно, огромный Трансформер обучить крайне непросто, и очень существенная часть статьи посвящена трюкам, которые авторы применили для обучения такой большой модели.

![2](https://yastatic.net/s3/education-portal/media/duck_meme_2752e650d6_a025afbc70.webp)

#### Инференс

На этапе инференса в модель подаются токены текстового описания картинки, и на их основании модель авторегрессионно предсказывает кодовые векторы:

![2](https://yastatic.net/s3/education-portal/media/dalle_transformer_bcdbc306bb_5be86e20e3.webp)

Кодовые векторы картинки подаются в декодер dVAE, который отображает их в финальную картинку:

![2](https://yastatic.net/s3/education-portal/media/dalle_inference_pic_4ae38121df_950e745022.webp)

Для повышения качества предсказания авторы сначала генерируют 512 картинок для каждого текстового описания, а затем выбирают лучшую картинку из предсказанных. Разные наборы кодовых векторов для одного и того же текста можно получить, например, случайно выбирая на каждом шаге генерации какой-то кодовый вектор согласно предсказанному Трансформером распределению. Ранжирование полученных 512 картинок осуществляется с помощью [CLIP](https://openai.com/blog/clip/) — большой нейросети, обучавшейся в режиме без учителя на большом количестве данных моделировать совместное распределение картинок и текстов.

## Заключение

Итак, в этом параграфе мы поговорили о том, как устроен VAE в классическом смысле, — с непрерывным распределением латентных переменных, а также поговорили о работах, основанных на идеях использования дискретных распределений для VAE. 

Конечно, различные модификации VAE не исчерпываются только лишь отказом от непрерывных латентных переменных в пользу дискретных. Есть множество других возможных направлений для улучшения модели: использование иерархических латентных распределений (которые мы, кстати, видели в контексте VQ-VAE-2), использование функций потерь, отличающихся от ELBO, выбор различных форм латентных пространств, применение adversarial-обучения и многое другое. 

Хороший список различных статей, посвящённых модификациям VAE, можно найти [здесь](https://jmtomczak.github.io/blog/4/4_VAE.html#There-are-many,-many-more!). Из недавних работ, связанных с применением иерархических распределений, интересной кажется [NVAE](https://arxiv.org/pdf/2007.03898.pdf) — семплы из модели выглядят весьма впечатляюще. Про неё есть хороший [видеообзор](https://www.youtube.com/watch?v=x6T1zMSE4Ts) от Yannic Kilcher. 

На этом мы завершаем рассказ о VAE. Будем надеяться, что он дал вам общее представление и об исходных идеях, из которых выросла модель VAE, и о наиболее интересных последних результатах, связанных с ней.

А в следующем параграфе мы поговорим о генеративно-состязательных сетях.

  ## handbook

  Учебник по машинному обучению

  ## title

  Variational Autoencoder (VAE)

  ## description

  Variational Autoencoder (VAE)

- 
  ## path

  /handbook/ml/article/generativno-sostyazatelnye-seti-(gan)

  ## content

  ## Введение

Генеративно-состязательные сети (Generative Adversarial Networks, GAN) – это большой класс генеративных моделей, общая черта которых заключается в том, что они обучаются одновременно с другой сетью, которая старается отличить сгенерированные объекты от настоящих. В этом параграфе мы рассмотрим основы основ GAN-ов, интуитивное объясним принципы их работы, а также детально погрузимся в многочисленные приёмы и модификации оригинального подхода, которые применяются в наиболее успешных моделях. Мы также приведём примеры нескольких типов практических задач, в которых применяются генеративно-состязательные сети.

![mnist_digits_73edc82188.webp](https://yastatic.net/s3/education-portal/media/mnist_digits_73edc82188_86cd92299c.webp)  

Генеративно-состязательные сети — это неявная генеративная модель. То есть она не восстанавливает плотность данных в явном виде, но умеет сэмплировать из распределения данных. Самый простой и эффективный дизайн генеративных моделей, которые умеют только сэмплировать, но не умеют оценивать плотность, – это отображение одних случайных величин в другие. 

Подобного вида модель после обучения работает следующим образом: пусть $x$ – случайная величина, обозначающая сэмпл из распределения нужных нам данных (например, картинок с нарисованными цифрами), а $z$ – сэмпл из какого-то распределения, который нам легко получить (например, каждая его компонента берётся из стандартного нормального). Тогда, если у нас есть обученная функция $G$, которая переводит сэмплы из $p(z)$ в сэмплы из $p(x)$, то процесс генерации происходит в два этапа: сначала мы случайным образом получаем вектор $z \sim p(z)$, а затем отображаем его в $\hat{x} = G(z)$:

$$
z \sim p(z),\quad \hat{x} = G(z):\quad \hat{x} \sim p(x)
$$

Ключевым вопросом в таких моделях является соотношение размерностей $z$ и $x$. Есть генеративные модели, где $\text{dim}(z) \approx \text{dim}(x)$. Примером таких подходов являются, например, нормализующие потоки. В случае генеративных состязательных сетей (как и другого класса популярных генеративных моделей, вариационных автоэнкодеров), $\text{dim}(z) \ll \text{dim}(x)$. Поэтому работу этих моделей можно рассматривать как поиск многообразия размерности $\text{dim}(z)$ среди всех случайных примеров из домена, на котором определяется $p(x)$. Например, в случае генерации цифр это соответствует поиску в домене $[0, 1]^{H \times W}$, где $H$ – это ширина картинки, а $W$ – её высота, подмножества, в котором каждый элемент изображает какую-либо цифру. Таким образом, задача обучения генеративных состязательных сетей может рассматриваться как задача компрессии данных в низкоразмерное представление.

## Основы обучения GAN-ов

![counterfeiter_analogy_8df968ecec_a40b7b9c4a.svg](https://yastatic.net/s3/education-portal/media/counterfeiter_analogy_8df968ecec_a40b7b9c4a_0cc3c5a1fe.svg)

Классическая аналогия того, как учатся GANы — это фальшивомонетчик и полицейский. Задача фальшивомонетчика — научиться создавать купюры, которые полицейский не сможет отличить от реальных. Задача полицейского — научиться отличать купюры фальшивомонетчика от настоящих.

Чтобы понять, как обучаются GANы, надо представить себе следующий мысленный эксперимент. Допустим, фальшивомонетчик и полицейский — друзья, которые решили поучиться друг у друга. Фальшивомонетчик создаёт несколько фальшивых купюр и показывает полицейскому. Полицейский говорит фальшивомонетчику, какие из его купюр, по его мнению, поддельные, а какие — настоящие. Фальшивомонетчик запоминает отзыв полицейского и в следующий раз улучшит свои купюры на основе отзыва от полицейского. Сам полицейский при этом тоже учится: он запоминает, что купюры, которые он видел — поддельные.

В нашем мысленном эксперименте представим, что фальшивомонетчик взаимодействует с полицейским много раз. Что получается в результате? С каждым разом купюры фальшивомонетчика всё труднее отличить от настоящих. И с каждым разом умение выявлять поддельные купюры у полицейского выше.

Важный вопрос для понимания работы GANов: в какой момент мы можем утверждать, что фальшивомонетчик хорошо подделывает купюры?

Ответ:
Когда фальшивомонетчик сможет обманывать сильного полицейского. В начале нашего эксперимента полицейский плохо отличает подделку от оригинала. Поэтому обмануть его можно купюрами плохого качества. Нам же интересно получить фальшивомонетчика, который будет выдавать купюры, неотличимые от оригинала даже профессионалом.

Рассмотрим задачу обучения более формально. Пусть у нас есть генератор $G_\theta$ (фальшивомонетчик) с параметрами $\theta$, и дискриминатор $D_\phi$ (полицейский) с параметрами $\phi$. Генератор отображает векторы $z \sim \mathcal{N}(0, I)$ в $\hat{x} \sim q(x)$, распределение которых приближает реальное распределение данных $p(x)$. Дискриминатор каждому реальному сэмплу $x$ и фейковому $\hat{x}$ ставит в соответствие вероятность $D(x)$, которая оценивает степень принадлежности $x$ к реальным данным, т.е. он решает задачу бинарной классификации. Самый простой способ это сделать – при помощи минимизации бинарной кросс-энтропии:

$$
    \min_\phi\ \mathbb{E}_{x \sim p(x)} - \log D_\phi(x) + \mathbb{E}_{\hat{x} \sim q(x)} - \log \big[ 1 - D_\phi(\hat{x}) \big].
$$

Учитывая обозначение $\hat{x} = G_\theta(z)$, и то, что мы пытаемся максимизировать вероятность принадлежности к реальным данным, как её оценивает дискриминатор, задачу, которую решает генератор, можно расписать следующим образом (используя свойство выпуклости логарифма):

$$
\begin{aligned}
    \theta^* =\ & \arg \max_\theta\ \mathbb{E}_{z\sim p(z)} D_\theta \big( G_\theta(z) \big) \\
             =\ & \arg \min_\theta\ \mathbb{E}_{z\sim p(z)} - D_\theta \big( G_\theta(z) \big) \\
             =\ & \arg \min_\theta\ \mathbb{E}_{z\sim p(z)} \big[ 1 - D_\theta \big( G_\theta(z) \big) \big] \\
             =\ & \arg \min_\theta\ \mathbb{E}_{z\sim p(z)} \log \big[ 1 - D_\theta \big( G_\theta(z) \big) \big] \\
             =\ & \arg \max_\theta\ \mathbb{E}_{z\sim p(z)} -\log \big[ 1 - D_\theta \big( G_\theta(z) \big) \big].
\end{aligned}
$$

Это равенство позволяет записать задачи, которые решают генератор и дискриминатор, вместе. (Мы также избавимся от лишних минусов, сделав так, чтобы дискриминатор решал задачу максимизации.)

$$
   \min_\theta \max_\phi\ \mathbb{E}_{x \sim p(x)} \log D_\phi(x) + \mathbb{E}_{z \sim p(z)} \log \big[ 1 - D_\phi\big(G_\theta(z)\big) \big].
$$

Получается, что на самом деле генератор и дискриминатор пытаются оптимизировать одну функцию: генератор её минимизирует, а дискриминатор максимизирует. Обозначим эту функцию (минус бинарную кросс-энтропию) как $\mathcal{L}_{\ \theta, \phi}$. Тогда эту задачу оптимизации можно записать в сокращённом виде:

$$
    \min_\theta \max_\phi\ \mathcal{L}_{\ \theta, \phi}.
$$

По параметрам дискриминатора минимум бинарной кросс-энтропии (или минимум $\mathcal{L}_{\ \theta, \phi}$ по $\phi$) достигается на следующей функции – оптимальном дискриминаторе для фиксированного генератора:

$$
    D_{\phi^*}(x) = \frac{p(x)}{p(x) + q(x)}.
$$

Её оптимальность нетрудно проверить, используя выпуклость логарифма. Учитывая это, и формулу для $\mathcal{L}$, интуицию работы метода обучения GANов со стороны генератора можно сформулировать следующим образом:

1. Мы замеряем, насколько реалистичными являются сгенерированные сэмплы $\\{\hat{x}_1, \dots, \hat{x}_2\\}$, используя для этого оптимальный дискриминатор $D\_{\phi^*}(x)$.
2. Мы хотим увеличить отклик дискриминатора на каждом сэмпле, т.е. пытаемся модифицировать каждый предсказанный элемент $\hat{x}_i$ так, чтобы на нём стало выше значение $D\_{\phi^*}(\hat{x}_i)$.

Ещё более простую интуицию для этой задачу можно сформулировать следующим образом. Как нужно модифицировать плотность $q(x)$, чтобы она стала ближе к $p(x)$, если к плотности распределения мы имеем доступ только через сэмплы из него? Визуализацию желаемых градиентов по случайным сэмплам для задачи сопоставления двух гауссиан можно видеть на графике ниже, где $f(x) = D\_{\phi^*}(x)$.

![Artboard_33_f123d6e9af.svg](https://yastatic.net/s3/education-portal/media/Artboard_33_f123d6e9af_bb7a95aff1.svg) 

Направленные вниз стрелки показывают, насколько нужно уменьшить координаты точек из распределения $q(x)$, чтобы получилось нечто максимально похожее на $p(x)$. То есть на самом деле точки будут сдвигаться на то же самое расстояние влево.

Формализуем эту интуицию, и заодно поймём, почему вообще такой метод должен работать. Подставив выражение для оптимального дискриминатора в $\mathcal{L}$, мы можем избавиться от внутренней максимизации в исходной задаче и оставить только внешнюю минимизацию по параметрам генератора. Тем самым, мы получим в явном виде функцию потерь, которую минимизирует генератор (обозначим её за $\mathcal{D}_\theta$). Для неё мы распишем математическое ожидание через интеграл и упростим дроби:

$$
\begin{aligned}
    \mathcal{D}_\theta 
    =\ & \! \max_\phi \mathcal{L}_{\theta, \phi} \\
    =\ & \mathbb{E}_{x \sim p(x)} \log D_{\phi^*}(x) + \mathbb{E}_{x \sim q(x)} \log \big[ 1 - D_{\phi^*}(x) \big] \\
    =\ & \mathbb{E}_{x \sim p(x)} \log \frac{p(x)}{p(x) + q(x)} + \mathbb{E}_{x \sim q(x)} \log \bigg[ 1 - \frac{p(x)}{p(x) + q(x)} \bigg] \\
    =\ & \int p(x) \log \frac{p(x)}{p(x) + q(x)} dx + \int q(x) \log \frac{q(x)}{p(x) + q(x)} dx \\
\end{aligned}
$$

Упростим выражение для $\mathcal{D}(x)$ ещё раз, прибавив и отняв константу $\log4$, а также учитывая, что $\int p(x)\ dx = 1$ и $\int q(x)\ dx = 1$:

$$
\begin{aligned}
    \mathcal{D}_\theta
    =\ & - \log 4 + \int p(x) \log \frac{2p(x)}{p(x) + q(x)} dx + \int q(x) \log \frac{2q(x)}{p(x) + q(x)} dx \\
    =\ & - \log 4 + \text{KL} \bigg( p \;\Big\|\; \frac{p + q}{2} \bigg) + \text{KL} \bigg( q \;\Big\|\; \frac{p + q}{2} \bigg) \\
    =\ & - \log 4 + 2 \cdot JSD(p\;\|\; q).
\end{aligned}
$$

Здесь $\text{KL}(P \;\|\; Q)$ означает, как обычно, KL-дивергенцию

$$
\begin{equation}
    \text{KL}(P \;\|\; Q) = \int P(x) \log \frac{P(x)}{Q(x)} dx,
\end{equation}
$$

которая показывает, насколько два распределения отличаются друг от друга. Через $JSD(p \;\|\; q)$ обозначает ещё один вид дивергенции (её называют дивергенцией Йенсена-Шеннона). Получается, что при оптимальном дискриминаторе генератор, решая внешнюю задачу оптимизации, уменьшает расстояние между распределениями реальных и фейковых данных, действительно приближая их друг к другу!

Исходя из этого, и в предположении достаточной capacity генератора и дискриминатора (т.е. предполагая, что их параметризация позволяет достичь оптимума), мы можем сформулировать первый, наивный алгоритм обучения генеративно-состязательных сетей.

1. Решить внутреннюю задачу максимизации по $\phi$, повторяя шаги ниже до сходимости по параметрам дискриминатора $\phi$ к оптимальному значению $\phi^*$:  
  
    — Составить мини-батч сэмплов шума $\\{z_1, \dots, z_n\\}$ из $p(z)$.  
    — Составить мини-батч сэмплов данных $\\{x_1, \dots, x_n\\}$ из $p(x)$.  
    — Обновить дикриминатор, сделав шаг вверх по его градиенту:  
    $$
        \nabla_\phi \frac{1}{n} \sum_{i=1}^n \Big[ \log D_\phi(x_i) + \log \Big( 1 - D_\phi\big(G_\theta(z_i)\big) \Big) \Big] \\
    $$

2. Сделать шаг SGD для внешней задачи минимизации по $\theta$:  
  
    — Составить мини-батч сэмплов шума $\{z_1, \dots, z_n\}$ из $p(z)$.  
    — Обновить генератор, сделав шаг вниз по его градиенту:  
    $$
        \nabla_\theta \frac{1}{n} \sum_{i=1}^n \log \Big( 1 - D_{\phi^*}\big(G_\theta(z_i)\big) \Big) = \frac{1}{n} \sum_{i=1}^n - \frac{\nabla_\theta f\big(G_\theta(z_i)\big) }{1 - f\big(G_\theta(z_i)\big)},
    $$

    где через $f$ мы для краткости обозначили $D_{\phi^*}(x)$.

Какие у этого наивного подхода могут быть недостатки? Во-первых, он очень медленный, потому что необходимо обучать дискриминатор до сходимости, чтобы сделать всего один шаг по градиенту генератора. Но вторая проблема намного серьёзнее: функция потерь генератора может насыщаться и выдавать близкие к нулю градиенты. Проиллюстрируем это на примере обучения простой модели, которая будет сэмплировать из одномерной гауссианы с заданными параметрами.

![Artboard_35_00b6473069.svg](https://yastatic.net/s3/education-portal/media/Artboard_35_00b6473069_48b468655d.svg)

Распределение $p(x)$ в этом случае известно, а распределение $q(x)$ мы можем получить с помощью методов оценки плотности по сэмплам. Визуализируем эти плотности, а также градиент по сэмплам из генератора (a). Видно, что в случае, когда пики распределений плохо пересекаются друг с другом, градиент будет равен нулю на большинстве сэмплов, которые выдаёт генератор, т.е. они никак не будут использоваться для обучения. Чтобы понять причину происходящего, давайте посмотрим на градиент функции потерь генератора. На точках, далёких от основной «массы» $p(x)$, дискриминатор выдаёт что-то близкое к нулю, то есть знаменатель градиента практически не будет ни на что влиять, а в числителе тоже будет практически ноль: ведь если мы немного поменяем параметры генератора, то «плохие» точки по-прежнему будут далеки от $p(x)$, так что изменение лосса будет пренебрежимо малым, и градиент тоже.

Это приводит к тому, что обучение происходит недостаточно эффективно: мы тратим время на вычисление сэмплов, которые не делают никакой вклад в обновление параметров генератора. Но более существенная проблема возникает в вырожденном случае: если изначально два распределения практически не пересекаются своими плотностями (b): в этом случае процесс обучения практически не идёт. Часто ли встречается такая вырожденная ситуация на практике? Довольно часто! Достаточно представить себе ситуацию, когда мы хотим генерировать реалистичные изображения лиц, а генератор в начале обучения вместо этого выдаёт случайный шум.  Из-за наличия такой проблемы описанная выше функция потерь генератора называется «сатурирующей».

В оригинальном подходе по обучению генеративно-состязательных сетей было предложено два решения этой проблемы. Во-первых, мы можем обучать дискриминатор на каждой итерации не до сходимости, а с небольшим фиксированным числом шагов $N$ (на практике чаще всего используется $N \le 2$). Это позволяет существенно улучшить исходную ситуацию с переобучением дискриминатора. Также мы могли бы улучшить функцию потерь для генератора, сделав так, чтобы она сглаживала выходы дискриминатора около нуля. В качестве такой функции изначально был предложен логарифм. Нетрудно видеть, что оптимум улучшенной функции потерь («несатурирующий лосс») совпадает с исходной, что позволяет сохранить все описанные выше теоретические гарантии:

$$
\begin{aligned}
    \theta^* =\ & \arg \min_\theta \mathcal{L}_{\ \theta,\phi} \\
    =\ & \arg \min_\theta \mathbb{E}_{z \sim p(z)} \log \big[ 1 - D_\phi \big( G_\theta(z) \big) \big] \\
    =\ & \arg \min_\theta \mathbb{E}_{z \sim p(z)} - \log D_\phi \big( G_\theta(z) \big).
\end{aligned}
$$

Точка минимума у новой функции потерь та же, что у исходной, а градиенты оказываются ненулевыми на всех сгенерированных сэмплах.

![Artboard_34_1831bed686.svg](https://yastatic.net/s3/education-portal/media/Artboard_34_1831bed686_107e48ea9e.svg)

Помимо этого, на практике вместо обычного метода стохастического градиентного спуска используются его модификации, которые учитывают и первые, и вторые моменты градиентов например, Adam. Вообще, GAN-ы – довольно капризные модели, и настоятельно рекомендуется использовать готовые реализации с GitHub, оставляя большую часть гиперпараметров без изменений. Наиболее критичными среди них являются learning rate и расписание (то есть количество обновлений дискриминатора на одно обновление генератора).

## Метрики качества

После успешного обучения генератора хотелось бы также понять, насколько хорошо он работает. Для этого рассмотрим на примере задачи генерации изображений типовые ошибки, которые может совершать GAN. Наиболее частая проблема – плохое качество или наличие артефактов – вызвана ограничениями, связанными с capacity генератора и несовершенством самих методов обучения. Здесь всё просто: наша генеративная модель плохо работает, и мы это видим на сгенерированных сэмплах. Более скрытым видом ошибок является так называемый **mode collapse**: обученный генератор выдаёт реалистично выглядящие картинки, но они не покрывают всё разнообразие распределения $p(x)$. Например, если наша модель учится генерировать изображения с животными, то она может проигнорировать более редкие виды, а научиться генерировать только наиболее часто встречающиеся. Более экстремальная форма подобного поведения – это когда модель вообще выдаёт вариацию одной картинки. Иногда в литературе общее качество результатов работы нейросети, по аналогии с задачей классификации, измеряется точностью метода (precision), а отсутствие mode collapse измеряется полнотой (recall).

Самый простой и действенный способ измерить как precision, так и recall – сгенерировать данные и посмотреть на них, дав экспертную оценку уровня их реализма. Не стоит им пренебрегать! Формализовать этот подход в метрику можно в виде эксперимента, который в литературе называется user study. Например, мы можем сделать опрос экспертов, которым будем показывать два примера, настоящий и сгенерированный, и попросить их угадать, где фейк. Тогда процент неправильных ответов будет являться метрикой качества для нашего метода. Такой опрос в основном показывает степень реализма полученных результатов: есть ли в них какие-то заметные артефакты, соответствуют ли они реальным примерам по своей структуре, и так далее. Отчасти он также замеряет разнообразие примеров: то, насколько они хорошо покрывают носитель распределения $p(x)$. Если обученная модель генерирует очень похожие друг на друга примеры (то есть имеет место существенный mode collapse), то эксперт через несколько примеров научится определять ненастоящие. С другой стороны, если сэмплы в целом разнообразные, но всё равно не полностью покрывают основу целевого распределения, то user study не позволит обнаружить эту проблему.

### Frechet Inception Distance

Есть метрики, с помощью которых можно автоматически проводить тестирование, похожее на user study. Для изображений наиболее используемой является Frechet Inception Distance (FID). Чтобы её посчитать, нам в идеале понадобится нейросеть, предобученная на датасете, который мы генерируем, но на практике во всех случаях используется модель Inception v3, предобученная на датасете ImageNet (отсюда слово Inception в названии метрики).

Для того, чтобы понять идею этой метрики, рассмотрим следующий пример: если выходом нейросети является класс (число), то его вероятность можно смоделировать мультиномиальным распределением. Гипотетически, чтобы сравнить два распределения картинок $p$ и $q$, нам достаточно измерить расстояние между двумя мультиномиальными распределениями, построенными на выходах предобученного классификатора после прогона датасетов реальных и сгенерированных данных. Если в распределении $q$ примеров из каких-то классов будет меньше или больше, чем в $p$, то такая метрика будет отличная от нуля.

Понятно, что это слишком грубое приближение расстояния между двумя распределениями, т.к. оно практически никак не учитывает реализм получаемых картинок. Поэтому вместо выходов нейросети в FID было предложено использовать признаки с её глубоких слоёв. Они кодируют высокоуровневую семантику изображений, потому что по этим признакам модель предсказывает вероятность принадлежности картинки к тому или иному классу. При этом в них остаётся довольно много информации об исходном изображении и свойств локальных признаков (текстур), которые могут помочь распознать артефакты. Метрика FID работает таким образом, что сравнивает два распределения высокоуровневых признаков для реальных и сгенерированных картинок, используя в качестве их приближения многомерные гауссианы (каждая размерность соответствует одному каналу). Для измерения расстояния между этими двумя распределениями используется метрика Вассерштейна:

$$
\begin{equation}
    \text{FID} = \| \mu - \hat\mu \|^2 + \text{Tr}\big( \Sigma + \hat\Sigma - 2(\Sigma\hat\Sigma)^{1/2} \big),
\end{equation}
$$

где $\mu \in \mathbb{R}^C$ и $\Sigma \in \mathbb{R}^{C \times C}$ – это среднее и матрица ковариаций глубоких признаков $\\{F_i \in \mathbb{R}^{C\times H\times W}\\}_{i=1}^N$, которые считаются по выборке из $N$ реальных картинок. При этом как средние, так и матрицы ковариаций считаются по объединению всех признаков со всех картинок без учёта пространственной размерности, т.е. по второй размерности матрицы $F \in \mathbb{R}^{C \times NHW}$. То же самое делается для сгенерированных картинок, для них средние и ковариации обозначены как $\hat\mu$ и $\hat\Sigma$. Минимум этой метрики равен нулю, и достигается в случае, когда статистики, посчитанные по двум распределениям, совпадают. На практике эта метрика используется как для измерения реализма изображений, так и для детектирования mode collapse.

### Интерполяции в скрытом пространстве

Ещё один способ измерения качества, который мы рассмотрим, напрямую связан с тем, что генеративно-состязательные модели эффективно занимаются кодированием потенциально высокоразмерных данных в низкоразмерное представление. Но как для нейросети с большим числом параметров проверить, занимается ли она реальным кодированием или простым запоминанием выборки? 

Рассмотрим следующий пример. Пусть наша генеративная модель к случайным векторам $z$ применяет их функцию распределения и отображает векторы в равномерно распределённые на отрезке $[0, 1]$ числа $u$. Упорядочим наш датасет. В качестве случайного сэмпла пусть наша модель выдаёт ту картинку, чей индекс, поделённый на размер датасета, ближе всего к $u$. Другими словами, наша генеративная модель будет выдавать случайные картинки из датасета вместо генерации новых картинок. Методы оценки качества, которые мы описали выше, пропустят эту проблему: ведь «сгенерированные» картинки будут в точности совпадать с настоящими.  Поэтому для полной проверки качества работы генеративной модели важно понимать, действительно ли она производит сжатие выборки в низкоразмерное представление или просто запоминает обучающие примеры.

Одним из тестов на подобное поведение является интерполяция между сгенерированными примерами. Возьмём два случайных вектора $z_1$ и $z_2$ из $p(z)$. Рассмотрим все векторы, которые лежат между ними $z = \alpha z_1 + (1 - \alpha) z_2,\ \alpha \in [0, 1]\$. К каждому такому вектору $z$ применим наш генератор и получим $\hat{x}$ для промежуточных векторов и $\hat{x}_1, \hat{x}_2$ для $z_1$ и $z_2$. Для правильно обученного GANа мы должны увидеть следующую картинку: при изменении коэффициента $\alpha$ изображение $\hat{x}$ должно плавно меняться и перетекать из $\hat{x}_1$ в $\hat{x}_2$. При этом каждая промежуточная картинка должна быть так же реалистичным сэмплом.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/interpolation_example_f6ce073e6a_dcaef51a6c.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://arxiv.org/abs/1707.05776">ссылка на источник картинки</a>
  </figcaption>
</figure>


Качество такой интерполяции сложно измерить численно, но если мы видим, что промежуточные результаты меняются случайно без какой-либо связи с семантикой интерполируемых примеров, то это говорит о плохом качестве генератора. Стоит упомянуть, что для сэмплов из нормального распределения, которое обычно имеют векторы $z$, намного лучше работает [интерполяция по сфере (Slerp)](https://en.wikipedia.org/wiki/Slerp), потому что в многомерном пространстве векторы $z$ практически всегда будут лежать в объёме вокруг сферы диаметра $\sqrt{d}$, где $d$ – размерность вектора $z$.

Интерполяция в скрытом пространстве с недавних пор стала использоваться для генерации анимаций и видео. Ведь анимация — это последовательность кадров, плавно переходящих друг в друга. И если у нас есть обученный GAN для генерации картинок, то нам нужно лишь найти путь в скрытом пространстве таким образом, чтобы набор сгенерированных картинок складывался в анимацию. Более того, в скрытом пространстве можно находить различные интерпретируемые пути. Например, путь, при движении по которому размывается задний фон или меняется причёска. Почитать подробнее про это можно [тут](http://proceedings.mlr.press/v119/voynov20a/voynov20a.pdf).

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/latent_directions_d708e8af59_56a407218b.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="http://proceedings.mlr.press/v119/voynov20a/voynov20a.pdf">ссылка на источник картинки</a>
  </figcaption>
</figure>


### Ближайшие соседи

Ещё одним способом проверить, не запомнил ли генератор датасет, является поиск ближайших соседей по датасету. Для этого следует сгенерировать несколько изображений. Для каждого изображения нужно найти несколько ближайших соседей из датасета. В качестве признаков для картинок можно взять признаки с последних слоёв сети Inception. На соседей стоит посмотреть глазами. Если мы увидим, что ближайшие соседи из датасета визуально совпадают со сгенерированными сэмплами, то это значит, что генератор запомнил сэмплы из датасета.

## Базовые модели

Чтобы лучше понимать современные модели, давайте сначала рассмотрим более базовые модели. Хотя они редко используются напрямую, многие идеи из них легли в основу современных моделей.

### DCGAN

Наиболее простая версия генеративной модели для изображений — это DCGAN ([Deep Convolutional GAN](https://arxiv.org/pdf/1511.06434.pdf), 2015 год). Её до сих пор можно иногда встретить как в литературе, так и на практике.

В основе DCGAN лежит простая идея: нейросети, основанные на свёртках, отлично подходят для распознавания изображений, а значит вполне могут подойти и для их генерации. Единственное отличие, которое требуется – это постепенно увеличивать внутри нейросети пространственный размер признаков, а не уменьшать. Для этого в современных нейросетях делается операция **nearest upsampling**, очень похожая на max pooling. В nearest upsampling пространственное разрешение карты признаков увеличивается за счёт того, что каждый вектор повторяется $K$ раз по горизонтали и по вертикали. К примеру, после увеличения таким образом карты признаков, состоящей из одной единицы, мы получим квадрат размера $K \times K$ из единиц. На практике увеличение размерности происходит по аналогии с размерами пулинга в свёрточных дискриминативных сетях и почти всегда равно $K=2$.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/dcgan_d8eaa0ce24_237f87ff39.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <p>Модель DCGAN. <a href="https://arxiv.org/pdf/1511.06434.pdf">Ссылка на источник картинки.</a></p>
  </figcaption>
</figure>


Таким образом, генератор в случае DCGAN является последовательностью свёрток, слоёв батч нормализации, нелинейностей и слоёв upsampling, а дискриминатор – обычной классификационной нейросетью. При этом первым слоем в генераторе является линейный слой, который отображает вектор шума $z$ в карту признаков с начальным разрешением (как правило, размера $4 \times 4$).

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/dcgan_samples_6b57300506_43b4abf043.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <p>Результат работы DCGAN. <a href="https://arxiv.org/pdf/1511.06434.pdf">Ссылка на источник картинки.</a></p>
  </figcaption>
</figure>


Хотя результаты работы DCGAN довольно смазанные, эта модель показала большие перспективы генеративных нейросетей для изображений.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/dcgan_interpolation_879fe2b998_0971bf6eda.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <p>Так выглядит интерполяция в скрытом пространстве для модели DCGAN. <a href="https://arxiv.org/pdf/1511.06434.pdf">Ссылка на источник картинки.</a></p>
  </figcaption>
</figure>


### Условная генерация
Допустим, что в нашем датасете есть изображения, относящиеся к разным классам, и мы хотели бы уметь генерировать изображение заданного класса. В этом случае речь идёт об **условной генерации**. В качестве условия может выступать не только метка класса, но и объект любой природы. Например, когда вы можете захотеть сгенерировать изображение по текстовому описанию.  

Далее будем обозначать условие как $y$. Наша задача — построить генератор, который бы моделировал $ p(x \mid y) $. 

#### Conditional GAN

Самый основной метод условной генерации — конкатенация условия с вектором шума, который генератор принимает на вход. В [статье Conditional GAN](https://arxiv.org/pdf/1411.1784.pdf) 2014 года, где предложили этот метод, рекомендовалось подавать условие не только в генератор, но и в дискриминатор. 

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/cgan_1b1ac03e48_4cf45b5693.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <p>Модель Conditional GAN. <a href="https://arxiv.org/pdf/1411.1784.pdf">Ссылка на источник картинки.</a></p>
  </figcaption>
</figure>


Если мы генерируем векторные данные, то вектор на вход дискриминатора подаётся конкатенированным с $y$. При этом если если $ y $ — это метка класса, то стоит её закодировать с помощью one-hot encoding. Если же мы работаем с изображениями, то нам из вектора условия следует сделать изображение. Например, если картинки из датасета имеют размер $H \times W$, то следует размножить вектор $y$, создав из него тензор размера $H \times W \times d_y$, где $d_y$ — размерность вектора $y$. Далее полученное «изображение» конкатенируется с входным изображением.

## Современные модели

Теперь на примере наиболее успешных моделей мы расскажем об улучшениях, которые во многом отходят от оригинального подхода к обучению GAN-ов и при этом значительно улучшают практические результаты, а значит расширяют практическую применимость.

В этом разделе мы рассмотрим state-of-the-art систему генерации изображений StyleGAN, методы её *обращения* (т.е., поиска векторов шума, соответствующих произвольной картинке), а также методы манипуляции семантикой изображений. После этого мы рассмотрим несколько примеров условных генеративных моделей, которые вместо шума принимают на вход изображения. Такие модели используются как для задачи повышения разрешения (super resolution), так и стилизации (например, превращение пейзажей в картины Моне). Мы сфокусируемся на изображениях, так как в этой области сконцентрирован как основной прогресс, так и наиболее впечатляющие применения генеративных моделей.

### StyleGAN

Самой известной генеративно-состязательной моделью, работающей с изображениями, по праву считается StyleGAN, который до сих пор активно развивается и имеет большое количество расширений (например, существуют разнообразные методы его обращения).

#### Progressive Growing

Архитектура StyleGAN переняла progressive growing из модели [Progressive Growing of GANs](https://arxiv.org/abs/1710.10196). Суть данной техники заключается в том, чтобы не сразу генерировать изображение высокого разрешения, а постепенно. Давайте рассмотрим это подробнее. 

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/pggan_1_9d2b69beca_515c4fedba.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <p>Иллюстрация работы генератора и дискриминатора в модели Progressive Growing GANs. <a href="https://arxiv.org/pdf/1710.10196.pdf">Ссылка на источник картинки.</a></p>
  </figcaption>
</figure>


Мы хотим получить генератор, который генерирует изображения размера 1024x1024. Обучить такой генератор очень сложно. Поэтому мы начинаем с разрешения 4x4. У генератора мы оставляем только первый блок слоёв, который позволяет из шума получить изображение размера 4x4. У дискриминатора мы оставим, наоборот, только последний, который принимает на вход изображение размером 4x4. Такой GAN мы обучаем на изображениях из датасета (предварительно уменьшив их в размере). 

Спустя сколько-то итераций мы понимаем, что сеть уже умеет генерировать маленькие изображения. В этот момент мы добавляем к генератору один блок, чтобы на выходе у неё получалось изображение размера 8x8. Так же мы добавляем один блок в начало дискриминатора, чтобы он на вход принимал изображения размера 8x8. Теперь генератор и дискриминатор состоят из двух блоков, которые мы и обучаем.

Такой процесс мы повторяем несколько раз, пока в итоге не дойдём до нужного нам разрешения 1024x1024. Эта схема в итоге показала себя действенным способом генерации реалистичных изображений высокого разрешения.

#### Подача шума в нейросеть

Ключевой частью StyleGAN является используемый в нём способ подачи шума $z$ в нейросеть, и именно из-за него метод и получил своё название. Для того чтобы понять, что конкретно в нём особенного, давайте подробнее посмотрим на архитектуру сети (рисунок из предыдущего раздела, модель StyleGAN справа).

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/stylegan_1_e8931c250a_1a842b0e64.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <p>Слева: традиционный генератор. Справа: генератор модели StyleGAN. <a href="https://arxiv.org/pdf/1812.04948.pdf">Ссылка на источник картинки.</a></p>
  </figcaption>
</figure>


Во-первых, вместо того, чтобы подавать вектор шума $z$ только в самом начале генератора, нейросеть обуславливают на него много раз на разных разрешениях признаков. Исторически, впервые похожим методом решалась задача переноса стиля одной картинки на другую, отсюда и название: a style-based generator. 

В качестве метода обуславливания используются так называемые адаптивные слои. Это модификация обычных слоёв нейросетей, в которых часть параметров предсказывается другой нейросетью. Вообще говоря, адаптивным можно сделать любой вид нормализации, включая батч нормализацию, но наиболее известным примером такого слоя является *адаптивная инстанс нормализация* (*adaptive instance normalization*), и именно она использовалась в первой версии StyleGAN. 

Вспомним, как именно работает неадаптивная версия этого слоя. Пусть у нас есть батч $F^\text{in} \in \mathbb{R}^{B \times C \times H \times W}$, элементы которого будем обозначать как $f_{bchw}^\text{in} \in \mathbb{R}$. Здесь $B$ обозначает размер мини-батча, $C$ – количество признаков, а $H$ и $W$ – высоту и ширину. Тогда внутри слоя инстанс нормализации выполняется следующая операция:

$$
    F^\text{out} = \frac{F^\text{in} - \mu}{\sigma} \cdot \gamma + \beta.\quad \mu, \sigma \in \mathbb{R}^{B \times C},\quad \gamma, \beta \in \mathbb{R}^C.
$$

Здесь $\mu$ и $\sigma$ обозначают матрицы средних и стандартных отклонений, которые считаются отдельно для каждого элемента мини-батча и для каждого признака:

$$
    \mu_{bc} = \frac{1}{HW} \sum_{h=1}^H \sum_{w=1}^W f_{bchw},\quad \sigma_{bc} = \sqrt{\frac{1}{HW} \sum_{h=1}^H \sum_{w=1}^W (f_{bchw} - \mu_{bc})^2}.
$$

При этом $\gamma$ и $\beta$ являются параметрами слоя, которые настраиваются в процессе обучения. Особенностью этого слоя является то, что, в отличие от батч нормализации, он применяется одинаковым образом как при обучении, так и во время инференса. То есть вместо того, что приближать средние и стандартные отклонения по батчу при помощи скользящих средних, как это делается в батч нормализации, мы честно каждый раз считаем эти статистики для каждой новой картинки отдельно от всех остальных. Это делает инстанс нормализацию очень популярной в области обработки и генерации изображений, где зачастую бывает невозможным обучение с большим размером мини-батча, а значит и использование батч нормализации.

Адаптивной инстанс нормализацией (AdaIN) называется слой, где $\gamma$ и $\beta$ являются не обучаемыми параметрами, а нейросетями, которые предсказывают эти векторы из какого-то общего для всех слоёв адаптивной инстанс нормализации входа (обозначим его через $w$):

$$
    F^\text{out} = \frac{F^\text{in} - \mu}{\sigma} \cdot \gamma(w) + \beta(w).
$$

Это означает, что вместо оптимизации по векторам $\gamma$ и $\beta$ будет происходить оптимизация по параметрам этих двух нейросетей. Также это означает, что у адаптивной инстанс нормализации добавляется ещё один вход помимо набора признаков $F^\text{in}$, который определяет её поведение: некоторый вектор $w$, который также называют вектором стиля.

Как правило, в качестве $\gamma(w)$ и $\beta(w)$ используется нейросеть с одним линейным слоем или неглубокий персептрон.

Нетрудно видеть, что если в качестве вектора $w$ подавать сгенерированный шум $z$, то это будет хорошим способом многократного обуславливания нашего генератора на вектор шума. Это позволило бы глубоким слоям нейросети выучивать лишь часть той информации о выходном изображении, которая содержится в векторе $w$, например, глобальные признаки картинки. А информация о локальных признаках выходного изображения (текстурах) может появляться уже ближе к последним слоям на более высоком разрешении промежуточных признаков. Таким образом, у генератора нет необходимости хранить во всех своих картах признаков всю информацию о сгенерированной картинке, как это происходит в случае DCGAN: он может декодировать её напрямую из вектора шума по мере необходимости, что существенно облегчает обучение таких моделей и улучшает качество результатов.

Авторы StyleGAN пошли даже дальше: в качестве дополнительной регуляризации они специально заставляли нейросеть использовать информацию из вектора шума частями. А именно, во время обучения все слои адаптивной нормализации случайным образом делятся на две последовательно идущие группы: первая группа обуславливается при помощи одной части вектора шума $z_1$, а вторая – при помощи другой части $z_2$. На практике это приводит к следующему эффекту: нейросеть учиться декодировать часть признаков изображения, используя вектор $z_1$, а часть – используя $z_2$. Это позволяет после обучения напрямую манипулировать выходами нейросети, смешивая разные векторы стилей $z$. 

#### Обучение нового латентного пространства

Второе ключевое открытие авторов StyleGAN связано с задачей поиска семантически значимого редактирования векторов из выученного низкоразмерного многообразия $p(z)$. Зачем это нужно на практике мы уже упоминали ранее: на этом низкоразмером многообразии значительно проще семантически редактировать изображения, чем на уровне пикселей. Например, для задачи генерации лиц на многообразии $z$ за изменение возраста или гендера может отвечать простой аддитивный сдвиг вектора $z$ на $\Delta z$. Если же мы попытаемся приблизить такую операцию в пространстве пикселей, то для этого уже понадобится большая нейросеть с сотнями тысяч или даже миллионами параметров.

При этом, как правило, мы хотим использовать наиболее простые операции редактирования. В идеале, мы бы хотели ограничить класс преобразований редактирования (а) сдвигами на какой-то вектор и (б) линейной (или сферически-линейной) интерполяцией двух векторов. С одной стороны, кажется, что так задача редактирования векторов существенно усложняется: этот класс преобразований даже менее выразителен, чем линейные операции. Но, с другой стороны, для таких простых преобразований легче гарантировать, что они не выведут нас за пределы многообразия, в котором у распределения $p(z)$ большая «масса», т.е. того множества векторов $z$, которые генератор чаще всего видел во время обучения. Для нормального распределения, как было сказано ранее, это многообразие можно приблизить сферой радиуса $\sqrt{d}$.

Но будет ли легко найти хорошо работающие преобразования на многообразии случайных векторов $z$, взятых из нормального распределения? Авторы StyleGAN обнаружили, что если сначала пропустить векторы $z$ через многослойный персептрон $f$, и подавать на вход свёрточного генератора его выходы $w = f(z)$, то редактировать латентные векторы на выученном многообразии $w \in \mathcal{W}$ станет намного проще. Это объясняется тем, что функция $f$ имеет возможность выучить достаточно сложное распределение для переменной $w$, которое упростило бы задачу генерации картинки для свёрточной части генератора. И на практике оказывается, что такое выученное представление $\mathcal{W}$ улучшает не только качество генерируемых картинок, но и качество результатов для семантического редактирования векторов.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/stylegan_examples_f18ca2f78e_4336874805.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <p>Примеры генерации StyleGAN. <a href="https://arxiv.org/pdf/1812.04948.pdf">Ссылка на источник картинки.</a></p>
  </figcaption>
</figure>


#### Truncation trick

Последняя важная деталь, которая тем не менее очень сильно помогла авторам StyleGAN получить настолько хорошие результаты – это так называемый truncation trick. Он был впервые предложен в более ранних работах и продолжает оказывает огромное влияние на качество результатов. Его суть состоит в том, чтобы после обучения сэмплировать те примеры из латентного пространства, которые чаще всего видел генератор во время обучения. Например, если мы во время обучения брали вектор $z$ из нормального распределения, то при использовании truncation trick после обучения мы бы его сэмплировали из нормального распределения с обрезанными хвостами. Тем самым, интуитивно, мы убираем из сгенерированной выборки те примеры входных векторов, которые генератор реже видел во время обучения. Однако, нетрудно заметить что такая процедура приводит к потере разнообразия в выходных картинках. Например, если мы обрежем нормальное распределение вплоть до его среднего значения, то тогда нейросеть сможет выдавать лишь один пример. При всём при этом потеря разнообразия выходов – не такая большая проблема, т.к. обученный генератор всё ещё может часто ошибаться и выдавать маргинальные примеры. Фильтрация таких плохих примеров по какому-то выставленному порогу – в этом и есть суть применения truncation trick.

В случае StyleGAN, авторам хотелось бы применять этот трюк непосредственно на распределении в выученном латентном пространстве $\mathcal{W}$. Для этого они применяют простой трюк: сначала считают центр масс $\mathcal{W}$, усредняя векторы $w$ для большой выборки сэмплов $z$:

$$
    \bar{w} = \mathbb{E}_{z \sim p(z)} [ f(z) ],
$$

а затем сдвигают каждый сгенерированный вектор $w$ по направлению к этому центру:

$$
    w' = \bar{w} + \psi (w - \bar{w}),
$$

где $\psi < 1$ – это параметр, который задаёт trade-off между качеством результатов и их разнообразием.

### StyleGAN-2

Хотя работа StyleGAN показала довольно хорошие результаты, авторы статьи про StyleGAN-2 [Analyzing and Improving the Image Quality of StyleGAN](https://arxiv.org/abs/1912.04958) заметили, что в некоторых случаях она может выдавать некачественные изображения. В частности, StyleGAN в некоторых случая может выдавать изображения с артефактами. 

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/stylegan_artifacts_e8c61c9be6_92dbafc75f.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <p>Примеры артефактов StyleGAN. <a href="https://arxiv.org/pdf/1912.04958.pdf">Ссылка на источник картинки.</a></p>
  </figcaption>
</figure>


Основной причиной этих артефактов оказалась адаптивная нормализация. Изначально, адаптивная нормализация состояла из двух частей: нормализация (на рисунке обозначена как Norm) и модуляция (на рисунке обозначена как Mod). В нормализации мы вычитали среднее и делили на стандартное отклонение. В модуляции мы умножали на новое выученное стандартное отклонение и прибавляли новое выученное среднее. 

Авторы StyleGAN2 предложили несколько модификаций для этапа нормализации. Каждое изменение в статье добавляли последовательно, следя за изменением общего качества генерации. 

1. Как из нормализации, так и из модуляции убрали вычитание/прибавление среднего. Нормализация и модуляция теперь выполняются независимо друг от друга и были перемещены в начало/конец стилевых блоков (см рисунок ниже, (c) Revised architecture).
2. Нормализацию из предыдущего пункта заменили на демодуляцию весов. По сути это та же нормализация, только теперь нормализуются веса свёрток, а не входные данные (см рисунок ниже, (d) Weight demodulation. Обратите внимание на $w_1, w_2, w_3$). 
    <figure>
    <img src="https://yastatic.net/s3/education-portal/media/stylegan2_605de023ae_b010b216d2.webp" loading="lazy" decoding="async" alt="">
    <figcaption>
    <p>Изменения, которые добавили в StyleGAN. <a href="https://arxiv.org/pdf/1912.04958.pdf">Ссылка на источник картинки.</a></p>
    </figcaption>
    </figure>

3. В StyleGAN используется техника [progressive growing](https://arxiv.org/pdf/1710.10196.pdf) (см раздел про StyleGAN). Из-за этого StyleGAN появляются артефакты, возникающие при исследовании латентного пространства с помощью интерполяций. Некоторые объекты лиц (глаза, зубы), которые должны вращаться при вращении головы, оставались на месте. Чтобы побороть эти артефакты, вместо progressive growing в StyleGAN2 стали использовать residual connections.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/stylegan2_progressive_d01545db47_9ec9aa0376.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <p>Артефакты из-за прогрессивной генерации в StyleGAN. <a href="https://arxiv.org/pdf/1912.04958.pdf">Ссылка на источник картинки.</a></p>
  </figcaption>
</figure>


Эти изменения позволили улучшить качество генерируемых изображений и избавиться от артефактов StyleGAN. Вот, например, некоторые примеры сгенерированных изображений модели StyleGAN2:

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/stylegan2_results_cb58a8cc0b_f15cc11383.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <p>Примеры изображений, сгенерированных с помощью StyleGAN2. <a href="https://arxiv.org/pdf/1912.04958.pdf">Ссылка на источник картинки.</a></p>
  </figcaption>
</figure>


### StyleGAN-ADA

Следующий шаг в развитии архитектуры StyleGAN — это статья [StyleGAN-ADA](https://arxiv.org/abs/2006.06676). ADA расшифровывается как **A**daptive **D**iscriminator **A**ugmentation. В данной статье авторы предложили механизм аугментации данных, который позволяет стабилизировать обучение и избежать переобучения дискриминатора.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/stylegan_ada_augmentations_019b3516fe_9a8260a37a.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <p>На левом рисунке (b) изображено, куда добавляется аугментация (синие блоки). На правом рисунке (c) изображена степень аугментации в зависимости от контролирующего её параметра p. <a href="https://arxiv.org/pdf/2006.06676.pdf">Ссылка на источник картинки.</a></p>
  </figcaption>
</figure>


Всего в статье использовали 18 разных аугментаций. В статье также предложили некоторую эвристику того, как понимать, насколько переобучился дискриминатор. Эвристика нужна для того, чтобы адаптивно контролировать параметр аугментации $p$ в зависимости от степени переобучения.

Основная идея алгоритма контроля $p$ в процессе обучения следующая. Изначально этот параметр равен нулю. Его значение изменяется на фиксированную величину каждые четыре мини-батча (авторы пишут, что частота обновлений не влияет на результат). Если наблюдается, что дискриминатор слишком переобучился, то параметр $p$ увеличивается. И наоборот, при низкой степени переобучении дискриминатора значение $p$ уменьшается.

Аугментация, как показали авторы, действительно помогает стабилизировать обучение при маленьком количестве данных. Однако, большой набор реальных данных всегда будет выигрывать у аугментации. 

### StyleGAN-T

Большинство современных моделей, которые показали впечатляющие результаты для генерации изображений, работают по схеме text to image. То есть текст является входом для нейросети, изображение — выходом. Обычно текст на входе называют prompt. По такой схеме работают модели Stable Diffusion, DALLE 2, Midjourney. Все эти модели являются диффузионными. Однако, пока что списывать GANы со счетов не стоит. Хотя качество у GANов не такое высокое, как у диффузионных моделей, а обучать их сложнее, у них есть неоспоримое преимущество — быстрая генерация изображений.

На момент написания этого параграфа самая свежая статья про генерацию изображений с помощью GANов — [StyleGAN-T](https://arxiv.org/abs/2301.09515). Её авторы решили на основе StyleGAN сделать модель для генерации изображений из текста.

Архитектура модели StyleGAN-T очень похожа на архитектуру модели StyleGAN (за основу авторы взяли [StyleGAN-XL](https://arxiv.org/abs/2202.00273) — версию StyleGAN для больших обучающих выборок). В качестве кодировщика текста была использована предобученная модель [CLIP](https://arxiv.org/abs/2103.00020). 

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/stylegan_t_architecture_c881b0cf15_989f0c2419.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <p>Архитектура модели StyleGAN-T. <a href="https://arxiv.org/abs/2301.09515">Ссылка на источник картинки.</a></p>
  </figcaption>
</figure>

На что стоит обратить внимание в данной архитектуре:
- Текст, закодированный CLIP text encoder, подаётся на вход как генератору, так и дискриминатору. Дискриминатор в данном случае классифицирует не отдельное изображение, а пару текст/изображение.
- Сгенерированное изображение пропускается через фиксированный кодировщик изображений, также взятый из модели CLIP (CLIP image encoder на рисунке архитектуры). Полученное представление изображения должно быть близко с представлением текста, полученным с помощью CLIP text encoder. Это достигается за счёт добавление CLIP guidance loss в общую функцию потерь. Для разрешения выше 64x64 авторы берут случайные кропы размера 64x64 на изображении, чтобы посчитать CLIP guidance loss.

Основное новшество модели StyleGAN-T — это лучшая GAN-модель для генерации изображений из текста. До этого большинство хорошо работающих моделей позволяли генерировать изображения для заданного класса или вообще без условий. Связать текст с изображением — гораздо более сложная задача. 

#### Результаты генерации

Поскольку на вход модель принимает не только шум, но и закодированный текст, она позволяет делать интерполяции по пространству текста. Примеры сгенерированных изображений и интерполяций по текстовому пространству вы можете видеть на рисунке ниже. 

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/stylegan_t_results_f7fd7e01d0_8cade2358c.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <p>Результаты генерации моделью StyleGAN-T. <a href="https://arxiv.org/abs/2301.09515">Ссылка на источник картинки.</a></p>
  </figcaption>
</figure>


Качество изображений StyleGAN-T отстаёт от диффузионных моделей, таких как Stable Diffusion или DALLE 2, о чём пишут сами авторы. Однако данная модель сильно выигрывает по скорости: на одной и той же видеокарте Stable Diffusion генерирует изображение за 3.7 секунды, в то время как StyleGAN-T за 0.02 секунды.

#### Disclaimer

Выше были перечислены лишь основные особенности данного класса генеративных моделей, на которые стоит обратить внимание. Эти соображения нашли применение в других задачах помимо генерации картинок из шума. На самом деле, список трюков и нюансов, необходимых для успешного обучения такой модели, намного обширнее. На практике для генеративных моделей настоятельно рекомендуется отталкиваться от готовых кодовых баз, внося минимальные и контролируемые изменения в процесс обучения. Особенно чувствительны генеративные модели бывают к архитектуре генератора и дискриминатора, к параметрам оптимизации (learning rate, количество обновлений весов дискриминатора на одно обновление генератора, и т.д.), а также к значениям весов лоссов (например, к весу R1 регуляризации, которую мы тут не обсуждали).

## Применения генеративных состязательных нейросетей

До этого мы рассмотрели основные особенности генеративных состязательных нейросетей, а также их применение в задаче генерации изображений. В этом разделе мы рассмотрим, какие ещё задачи можно решать с помощью таких моделей.

Отметим, что задачи, которые мы рассмотрим ниже, можно решать и другими способами без ГАНов. Зачастую диффузионные модели (MidJourney, Stable Diffusion) показывают лучшие результаты в этих задачах. Тем не менее в данном же разделе мы рассмотрим именно методы на основе генеративных состязательных нейросетей.

### Inpainting

Представьте, что вы хотите удалить с фотографии людей на заднем плане. Встаёт вопрос, чем их заменить? Для этого существует задача инпеинтинга (inpainting). Она заключается в том, чтобы восстановить часть изображения, которая была выделена маской. Если выделить людей или объекты на фотографии маской, то нейросеть для инпеинтинга будет способна зарисовать эти участки чем-то подходящим для конкретной фотографии. 
 
<figure>
  <img src="https://yastatic.net/s3/education-portal/media/inpainting_example_bd2b238b82_ee4a87ad29.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <p>Пример работы модели инпеинтинга. <a href="https://arxiv.org/abs/2109.07161">Ссылка на источник картинки.</a></p>
  </figcaption>
</figure>


Обычно генератор модели GANs для инпеинтинга представляют собой image-to-image модели. То есть изображение подаётся как на вход, так и на выход. То, что происходит внутри генератора, зависит от архитектуры модели. Как правило, используются U-Net-подобные архитектуры с какими-то дополнениями.  Так, например, в одной из последних работ по инпеинтигу на основе GANs [Resolution-robust Large Mask Inpainting with Fourier Convolutions](https://arxiv.org/abs/2109.07161) используются Fast Fourier Convolutions.

Чтобы обучить модель инпеинтинга, нужно подготовить данные в формате пар <изображение с маской, изображение без маски>. Сделать это не сложно. Достаточно на существующем наборе изображений случайным образом выделить участки для удаления, после чего обучать нейросеть их восстанавливать.


#### Outpainting

Задачу inpainting можно так же превратить в задачу outpainting, то есть дорисовки изображения по краям. Для этого нужно в качестве маски подать пиксели, которые находятся за рамками изображения. При этом само исходное изображение можно уменьшить, если того требуют размерности нейросети. 

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/outpainting_example_5ca5da2a68_d418dcd028.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <p>Пример outpainting, сделанный моделью In&Out. <a href="https://arxiv.org/abs/2104.00675">Ссылка на источник картинки.</a></p>
  </figcaption>
</figure>


Задача outpainting может быть полезна, когда хочется расширить изображение, например, чтобы увеличить его разрешение.

#### Редактирование изображений

До этого мы рассматривали, как можно редактировать латентное пространство обученной состязательной модели, чтобы это отражалось на сгенерированных изображениях. В 2023 году вышла работа [Drag Your GAN](https://vcai.mpi-inf.mpg.de/projects/DragGAN/), которая основана на этом принципе, и позволяет редактировать изображения перетаскиванием одной точки в другую.

<iframe width="100%" height="500" src="https://frontend.vh.yandex.ru/player/48072e442ea77083824bc3af91595450?from=partner&mute=1&autoplay=1&tv=0&loop=true&play_on_visible=false" allow="autoplay; fullscreen; accelerometer; gyroscope; picture-in-picture; encrypted-media" frameborder="0" scrolling="no" allowfullscreen></iframe>

<p style="color:darkslategrey;font-size:14px">Пример работы Drag Your GAN.<a href="https://vcai.mpi-inf.mpg.de/projects/DragGAN/">cсылка на источник изображения</a></p>

Метод Drag Your GAN основан на модели StyleGAN2. Ему на вход подаётся набор изначальных точек и набор конечных точек. Внутри метода поочерёдно выполняются следующие два шага:
1. Обновление латентного пространства и обновления изображения с помощью оптимизации;
2. Обновление координат точек (трекинг точек).

Изначально метод работает только со сгенерированным изображениями. Однако, нет проблем в том, чтобы добавить кодировщик, который бы переводил реальные изображения в латентное пространство модели. В таком случае можно будет редактировать и реальные изображения.

Демо Drag Your GAN доступно по [ссылке](https://huggingface.co/spaces/DragGan/DragGan).

  ## handbook

  Учебник по машинному обучению

  ## title

  Генеративно-состязательные сети (GAN)

  ## description

  Генеративно-состязательные сети (GAN)

- 
  ## path

  /handbook/ml/article/normalizuyushie-potoki

  ## content

  ## Введение

В главе [Генеративный подход к классификации](https://education.yandex.ru/handbook/ml/article/generativnyj-podhod-k-klassifikacii) мы уже познакомились с типом моделей, которые оценивают совместное распределение $p(X,Y)$. Такие модели называют **генеративными**. Для простоты предположим, что мы имеем всего один класс, тогда задача моделирования $P(X,Y)$ сводится к задаче моделирования $p(X)$. Научившись моделировать это распределение, мы сможем:

- генерировать объекты $x \sim p_\theta^\mathcal{data}$, где $\theta$ – параметры модели;
- оценивать вероятность встретить данный объект $\mathbf{x}$ среди набора наблюдаемых данных $\mathcal{D}$;
- выучивать скрытые представления для объекта $\mathbf{x}$.

Известными примерами генеративных моделей являются:

- Авторегрессионные модели:

$$p_\theta(\mathbf{x}) = \prod_{i=1}^n p_\theta(x_i \vert \mathbf{x}_{<i}),
$$

$$x \in \mathbb{R}^n,
$$

- Вариационные автокодировщики:

$$p_\theta(\mathbf{x}) = \int p_\theta(\mathbf{x}, \mathbf{z}) d\mathbf{z},
$$

$$\mathbf{z} \in \mathbb{R}^m.
$$

Но оба эти метода не позволяют одновременно:

- получать скрытые представления для объектов
- точно вычислять функцию правдоподобия

**Нормализующие потоки** способны решить обе эти задачи.

## Мотивация

![flow](https://yastatic.net/s3/education-portal/media/flow_2ce610b050_f34b9b2f31.webp)

Пусть $\mathbb{x} \sim p_x(\mathbb{x})$, где $p_x(\mathbb{x})$ неизвестно, а $\mathbb{z} \sim p_z(\mathbb{z}) = \mathcal{N}(0, I)$. Мы хотим найти отображение $f_{\theta} : \mathbb{R}^n \rightarrow \mathbb{R}^n$, для которого $\mathbb{x} = f_{\theta}(\mathbb{z})$ и $\mathbb{z} = f_{\theta}^{-1}(\mathbb{x})$.

Отображение $f$ преобразует базовую функцию плотности $p_z$ к более сложной $p_x$. С его помощью мы можем генерировать сложный объект путем сэмплинга простого объекта $\mathbb{z}$ (скрытой переменной) из распределения $p_z$ и применения «генератора» $f(\mathbb{z}) = \mathbb{x}$. Обратное отображение $f^{-1}$ «нормализует» сложное распределение $p_x$, приводя его к простому $p_z$.

Найдя такое отображение $f$, мы сможем генерировать новые объекты $\mathbb{x}$, а оценить плотность $p_x(\mathbb{x})$ поможет формула преобразования плотности случайной величины. Давайте её вспомним.

### Формула замены переменной

Пусть $\mathbb{x} \sim p_x(\mathbb{x})$, $\mathbb{z} \sim p_z(\mathbb{z})$, при этом отображение $f : \mathbb{R}^n \rightarrow \mathbb{R}^n$ дифференцируемо, обратимо и $\mathbb{x} = f(\mathbb{z})$. Тогда:

$$p_x(\mathbb{x}) = p_z(f^{-1}(\mathbb{x})) \cdot \bigg| \text{det} \left( J_{f^{-1}} \right) \bigg|,
$$

где $\text{det} \left( J_{f^{-1}} \right)$ – якобиан отображения $f^{-1}$.

## Определение

Итак, **нормализующий поток** – это обратимое дифференцируемое отображение $f_{\theta} : \mathbb{R}^n \rightarrow \mathbb{R}^n$, которое переводит исходные представления объектов в скрытые: $\mathbb{x} = f_{\theta}(\mathbb{z})$ и $\mathbb{z} = f_{\theta}^{-1}(\mathbb{x})$.

При этом функция правдоподобия $p_x(\mathbb{x})$ вычисляется по формуле:

$$p_x(\mathbb{x}) = p_z(f^{-1}(\mathbb{x})) \cdot \bigg| \text{det} \left( J_{f^{-1}} \right) \bigg|
$$

Умея вычислять функцию правдоподобия, мы можем обучать нашу модель $f_\theta$ методом максимума правдоподобия (ММП):

$$\log \space p_x(\mathcal{D}; \theta) = \sum_{i=1}^M \text{log} \space p_z(f^{-1}_\theta(\mathbb{x})) + \log \space \bigg| \text{det} \left( J_{f^{-1}} \right) \bigg|.
$$

где $\mathcal{D} = \{ x^{(i)}\}_{i=1}^M$ – выборка наблюдаемых данных из распределения $p_x$.

Обычно модель нормализующего потока составляет композицию из $K$ не очень сложных отображений, чтобы она была, с одной стороны, достаточно контролируемой, а с другой – достаточно выразительной:

$$f = f_1 \circ f_2 \circ \dots \circ f_K
$$

Тогда якобиан вычисляется по формуле:

$$\det \left( J_{f^{-1}} \right) = \prod_{i=1}^{K} \det \left( J_{f^{-1}_i} \right)
$$

Но вычисление якобиана является очень затратной операцией. Для того, чтобы мы могли обучать модели эффективно на высокоразмерных данных (аудио, изображения), необходимо использовать такое отображение $f$, подсчет якобиана которого был бы эффективен!

<p-important>

В отличе от VAE и GANов, нормализующие потоки требуют вычисления функции правдоподобия, воэтому важно уметь **эффективно** вычислять функцию правдоподобия. Метод максимального правдоподобия позволяет обучать нормализующие потоки стабильнее в сравнении с GAN-ами, а возможность быстро и точно вычислять значение функции правдоподобия выделяет нормализующие потоки на фоне VAE и диффузионных моделей.
</p-important>

Примером такого отображения является **планарный поток** (**Planar Flow**), где отображение $f$ принадлежит следующему семейству функций:

$$\mathbb{x} = f_\theta(\mathbb{z}) = \mathbb{z} + \mathbb{u}_\theta h (\mathbb{w}_\theta ^ \top \mathbb{z} + b_\theta),
$$

где $\mathbb{u}_\theta, \mathbb{w}_\theta, b_\theta$ – обучаемые параметры, а $h$ – гладкая нелинейная функция, например, $\text{tanh}$.

Якобиан такого отображения можно будет посчитать за $O(n)$. Обозначим

$$\psi(\mathbb{z}) = h'(\mathbb{w}_\theta^\top \mathbb{z} + b_\theta) \mathbb{w}_\theta.
$$

Тогда

$$\bigg| \det \left( J_{f} \right) \bigg| = \big| \det( \mathbb{I} + \mathbb{u}_\theta \psi(\mathbb{z})^\top) \big| = \big| (1 + \mathbb{u}_\theta^\top \psi(\mathbb{z})) \big|.
$$

![planar](https://yastatic.net/s3/education-portal/media/planar_d4c78adc79_bc33898cce.webp)
Действие планарного нормализующего потока на нормальное и равномерное распределение ([ссылка на статью](https://arxiv.org/abs/1505.05770)).

## Развитие идеи

В планарных потоках нам удалось быстро посчитать якобиан, потому что матрица имела специальный вид (сумма единичной и низкоранговой). Но мы знаем и другие случаи, когда определитель можно посчитать быстрее – треугольные матрицы. Их определитель равен произведению элементов на диагонали.

Следующие модели активно использовали этот трюк.

### NICE: Non-linear Independent Component Estimation и RealNVP

Авторы модели [NICE](https://arxiv.org/abs/1410.8516) предложили использовать в качестве $f_\theta$ следующее семейство преобразований:

$$\mathbb{x} = f_\theta(\mathbb{z}) =
\begin{cases}
    \mathbb{x}_{1:d} = \mathbb{z}_{1:d} \\
    \mathbb{x}_{d+1:n} = \mathbb{z}_{d+1:n} + m_\theta(\mathbb{z}_{1:d})
\end{cases},
$$

где $1<d<n$, а $m_\theta$ – произвольная нейросеть с $d$ входами и $n - d$ выходами. Такое преобразование называют **аддитивным связыванием** (**additive coupling**).

Обратное преобразование вычисляется с такой же легкостью, а якобиан равен $1$. То есть, $p_x(\mathbb{x}) = p_z(f^{-1}(\mathbb{x}))$, что является довольно сильным ограничением модели.

Далее, из-за того, что

$$\mathbb{x}_{1:d} = \mathbb{z}_{1:d},
$$

первые $d$ каналов вектора $x$ совпадают с координатами нормального шума $z$, то есть моделирования этих каналов $x$ не происходит. Из-за этого выразительная способность модели NICE была относительно невысокой.

Позже авторы NICE позже предложили использовать между слоями нормализующих потоков зафиксированные перестановки признаков/каналов $\mathbb{x}$, что стало основой работы [RealNVP](https://arxiv.org/abs/1605.08803). Использование перестановок позволяет добиться того того, чтобы все выходные каналы оказались затронуты преобразованием $f_\theta(z)$; при этом градиент перестановки вычисляется легко.

$$\mathbb{x} = f_\theta(\mathbb{z}) =
\begin{cases}
    \mathbb{x}_{1:d} = \mathbb{z}_{1:d} \\
    \mathbb{x}_{d+1:n} = \exp(s_\theta(\mathbb{z}_{1:d})) \odot \mathbb{z}_{d+1:n} + m_\theta(\mathbb{z}_{1:d})
\end{cases},
$$

где $\odot$ – поэлементное умножение, а $s_\theta$ – нейросеть, которая может быть произвольной, но, как правило, выбирается такой же архитектуры, как и $m_\theta$. Такое преобразование называют **аффинным связыванием** (**affine coupling**).

Получившееся отображение тоже легко обращается, а его якобиан равен:

$$\det \left( J_{f^{-1}} \right) = \exp \sum_{i=d+1}^n ( s_\theta(\mathbb{z}_{1:d}) )_i
$$

Заметим, что, как и в случае аддитивного связывания, значительная часть каналов остается неизменной при использовании аффинного связывания. Для того, чтобы преобразование $f_\theta(x)$ моделировало распределение $x$ во всех каналах, на разных слоях неизменными оставляют разные подмножества из $d$ каналов.

Чтобы улучшить сходимость глубоких ($K > 1$) нормализующих потоков, авторы предложили использовать Batch Normalization. Данное преобразование тоже является обратимым, а его якобиан вычисляется крайне просто.

В результате, выразительная способность модели сильно повысилась, и она стала способна выучивать сложные распределения:

![realnvp](https://yastatic.net/s3/education-portal/media/realnvp_992fd9db24_90691e3b3e.webp)

### Masked Autoregressive Flows

[Ссылка на статью](https://arxiv.org/abs/1705.07057)

Данный вид нормализующих потоков также обладает нижнетреугольным якобианом, но он использует другое семейство функций:

$$x_i = z_i \exp( f_{\alpha_i} (\mathbb{x}_{1:i-1}) ) + f_{\mu_i} (\mathbb{x}_{1:i-1}),
$$

где $\mu_i(x_{1:i-1})$ и $\alpha_i(x_{1:i-1})$ – нейросети произвольной архитектуры.

Как видно из формулы, $x_i$ напрямую зависит от $x_{1:i-1}$. Таким образом, элементы генерируются **авторегрессивно**, что и дало название архитектуре.

Якобиан такого преобразования вычисляется по следующей формуле:

$$\det \left( J_{f^{-1}} \right) = \exp\bigg( -\sum_{i=1}^n f_{\alpha_i} (\mathbb{x}_{1:i-1}) \bigg)
$$

Таким образом, шаг генерации выглядит следующим образом:

1. $z \sim  \mathcal{N}(0, I)$
2. $x_1 = z_1  \exp(\alpha_1) + \mu_1$$
3. $x_2 = z_2  \exp(f_{\alpha_2}(x_1)) + f_{\mu_2}(x_1)$
   ...

Однако вычисление скрытых переменных $z$ не является авторегрессивным:

$$z_i = (x_i - f_{\mu_i}(x_{1:i-1})) \exp(-f_{\alpha_i} (x_{1:i-1}))
$$

Несмотря на то, что данная разновидность нормализующих потоков кажется более мощной моделью, её трудно применить на практике к данным высокой размерности. Это происходит из-за того, что генерация нового объекта осуществляется авторегрессивно по координатам, что становится слишком затратным при обучении на высокоразмерных данных, например, на изображениях.

### Inverse Autoregressive Flows

[Ссылка на статью](https://arxiv.org/abs/1606.04934)

Чтобы быстро генерировать объекты из сложного распределения, мы можем избавиться от авторегрессивности на шаге генерации, поставив в авторегрессивную зависимость не наблюдаемые, а латентные переменные:

$$z_i = (x_i - f_{\mu_i}(z_{1:i-1})) \exp(-f_{\alpha_i} ({z}_{1:i-1}))
$$

Можем заметить, что проблема долгого вычисления авторегрессивных выражений никуда не уходит. Мы лишь изменяем построение модели таким образом, чтобы генерировать объекты $\mathbb{x}$ быстрее:

$$x_i = z_i \cdot \exp(f_{\alpha_i}(z_{1:i-1})) + f_{\mu_i}(z_{1:i-1})
$$

Но вычисление $\mathbb{z}$, а значит и правдоподобия, становится долгим, и обучение занимает больше времени.

### Звук

Нормализующие потоки стали наиболее актуальны в задаче генерации звука, поскольку они обладают достаточно высокой выразительностью и эффективностью, чтобы быстро генерировать аудиозаписи высокого качества. В этом контексте, модель нормализующего потока должна генерировать аудио, получая на вход описание того, что ей необходимо сгенерировать. То есть модель обуславливается на дополнительные признаки.

Нормализующие потоки могут быть обусловлены на входные данные путем использования дополнительных входных данных в качестве переменной, от которой зависят преобразования, применяемые к данным. Обусловливающей переменной может быть любая дополнительная информация, имеющая отношение к задаче генерации, такая как текстовые описания, изображения или другие характеристики данных.

В контексте генерации аудио обуславливающей переменной обычно служит [mel-спектрограмма](https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53), которая позволяет отобразить интенсивность различных частот аудио-сигнала в разные моменты времени.

![melspec](https://yastatic.net/s3/education-portal/media/melspec_1dcc106402_25c40b6bfc.webp)

Нормализующий поток учится генерировать сигнал в виде waveform-а на основе спектрограммы путем обратного преобразования.

![waveform](https://yastatic.net/s3/education-portal/media/waveform_656aaac1a9_b981993d89.webp)

Чтобы генерировать более длинные фрагменты звука, модель генерирует короткие звуковые кадры (фреймы) за раз, которые затем объединяются для формирования полного waveform-а.

![Artboard](https://yastatic.net/s3/education-portal/media/Artboard_38_e7555b15c8_6d648a81ee.svg)

Теперь мы готовы узнать про применение нормализующих потоков в генерации аудио!

### Probability Density Distillation и Parallel WaveNet

[Ссылка на статью](https://arxiv.org/abs/1711.10433)

Архитектура Inverse Autoregressive Flow (IAF) была изначально предложена для задачи генерации аудио. Она позволяет генерировать объекты крайне эффективно, но обучение методом максимального правдоподобия занимает много времени из-за авторегрессивности вычислений. Метод Probability Density Estimation позволяет решить эту проблему с помощью использования второй предобученной авторегрессивной модели в качестве учителя. IAF обучается в качестве модели-студента, минимизируя KL дивергенцию $\text{KL}(p_S \vert\vert p_T),$ где $p_S$ и $p_T$ – распределения студента и учителя соответственно. Ключевым достижением данного подхода является то, что вычисление функции потерь требует вычисления кросс-энтропии между учителем и студентом, а не правдоподобия, что позволяет максимально распараллелить все вычисления ввиду отсутствия авторегрессивности в вычислениях.

Вместе с тем в данной работе в качестве учителя выбирается не случайная модель, а оригинальная авторегрессивная модель [WaveNet](https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio), которая в 2016 году позволила достичь state-of-the-art качества генерации аудио. Эта модель является не нормализующим потоком, а обыкновенной авторегрессивной моделью, которая обучается предсказывать следующий кусочек аудио (фрейм) длиной в несколько миллисекунд.

![wavenet](https://yastatic.net/s3/education-portal/media/wavenet_4a32f3d218_5baa882564.gif)

Таким образом, с помощью IAF и Probability Density Distillation авторам удалось ускорить генерацию более чем в 1000 раз без потери качества!

![Artboard](https://yastatic.net/s3/education-portal/media/Artboard_3_96419b9795_00e114ab5b.svg)

На картинке выше мы видим, что модель использует лингвистические признаки для генерации аудио. Эта задача является примером **задачи условной генерации**, где на вход модели подается [спектрограмма](https://en.wikipedia.org/wiki/Mel_scale), сгенерированная отдельной моделью по тексту, а на выход ожидается речь в аудио-формате (waveform). О том, как модель использует дополнительную информацию для обуславливания, поговорим в главе про [Waveglow](#waveglow)

### Glow

Исследователи из OpenAI в 2018 году опубликовали работу [Glow: Generative Flow with Invertible 1×1 Convolutions](https://arxiv.org/pdf/1807.03039.pdf), которая значительно улучшает результаты модели RealNVP. Опишем два главых улучшения.

Во-первых, для перемешивания каналов Glow использует **обратимые свертки с ядром 1x1** вместо фиксированной матрицы перестановок каналов в RealNVP;

Это нововведение является по-настоящему красивым, так как в нем предлагается способ вычисления якобиана 2D-свертки за $O(n)$. А именно, логарифм якобиана такой 1x1-свертки с числом каналов $n$ для тензора размера $h \times w \times n$ равен $hw \cdot \log \vert \det(W) \vert$, где $W$ – матрица свёртки 1х1.

Авторы предлагают использовать следующий вариант LU-разложения для матрицы $W$:

$$W = PL(U + \text{diag}(s)),
$$

где $P$ – фиксированная матрица перестановок, $L$ – нижнетреугольная матрица с единицами на диагонали, $U$ – верхнетреугольная матрица с нулями на диагонали, а $s$ – обучаемый вектор.

Нетрудно показать, что

$$\log | \det(W) | = \text{sum}(\log(s))
$$

Благодаря этому авторам удалось снизить сложность вычислений якобиана с $O(n^3)$ до $O(n)$

Кроме того, для улучшения сходимости использовали собственно разработанный **actnorm-слой** (**activation normalization**). Поскольку нормализующие потоки требуют много вычислительных ресурсов, для обучения используются мини-батчи маленького размера, из-за чего батч-нормализация работает не очень хорошо. Авторы предлагают использовать следующий тип нормализации – actnorm:

$$x'_{i,j} = s \odot x_{i,j} + b
$$

* Нормализуем входной тензор (промежуточное изображение) по размерности каналов;
* Инициализируем параметры смещения $b$ и разброса $s$ статистиками с **первого батча**;
* Далее обучаем их в качестве обычных параметров.

Таким образом, один блок нормализующего потока выглядит так:

![Artboard](https://yastatic.net/s3/education-portal/media/Artboard_4_130a9ea4b3_0424b0240e.svg)

### WaveGlow

[Ссылка на статью](https://arxiv.org/pdf/1811.00002.pdf)

Вторым важным с практической точки зрения применением нормализующих потоков стала модель **WaveGlow**. Она представляет собой версию модели Glow, адаптированную для генерации речи по тексту.

Как мы помним, эта задача также является примером **задачи условной генерации**:

$$\log \space p_{x|c}(\mathcal{D}; \theta) = \sum_{i=1}^M \text{log} \space p_{z|c}(f^{-1}_\theta(\mathbb{x}, \mathbb{c}) | \mathbb{c}) + \log \space \bigg| \text{det} \left( J_{f^{-1}} \right) \bigg|.
$$

На практике это приводит к тому, что все распределения в нашей формуле становятся **условными**. Таким образом, при генерации мы также сэмплируем из условного распределения $z \sim p_{z|c}(z|c)$, а в слоях нормализующих потоков используем преобразования $x_i = f_i(x_{i-1}, c)$.

В качестве обуславливающего фактора $c$ для WaveGlow мы имеем сгенерированную по тексту [mel-спектрограмму](https://en.wikipedia.org/wiki/Mel_scale), а на выходе ожидаем получить соответствующую тексту и спектрограмме аудио-запись. Как мы видим на изображении и в формулах ниже, mel-спектрограмма используется как дополнительный признак для нейросети, генерирующей параметры афинного преобразования. В качестве модели, которая производит параметры афинного преобразования, используется похожая на WaveNet архитектура с dilated-свертками.

Правая часть схемы ниже более подробно показывает строение слоя affine coupling:

![Artboard](https://yastatic.net/s3/education-portal/media/Artboard_32_5cf5f0ee4c_4fd72398b5.svg)

$$(x_a, x_b) = \text{split}(x) \\
(\log s, t) = WN(x_a, \text{mel-spectrogram}) \\
x_b' = s \odot x_a + t \\
f_{coupling}^{-1} (x)w = \text{concat}(x_a, x_b')
$$

Операция $split$ разделяет тензор $x$ пополам на два тензора меньшей размерности $x_a$ и $x_b$ для их последующего участия в слое **аффинного связывания** (affine coupling).

Пример генерации:
[Источник](https://nv-adlr.github.io/WaveGlow)

<table><thead>
<tr><th>Ground truth</th><th>WaveNet</th><th>WaveGlow</th></tr>
</thead><tbody><tr>
<td><audio controls=""><source src="audio/real.wav" type="audio/wav"></audio></td>
<td><audio controls=""><source src="audio/wavenet.wav" type="audio/wav"></audio></td>
<td><audio controls=""><source src="audio/waveglow.wav" type="audio/wav"></audio></td>
</tr></tbody></table>

## Out-of-distribution detection

Может показаться, что способность точно и эффективно вычислять функцию правдоподобия может позволить без труда обнаруживать аномалии в данных, что может пригодиться во многих приложениях. Однако в работе [Kirichenko et al.](https://arxiv.org/abs/2006.08545) на примере задачи генерации изображений было показано, что нормализующие потоки выучивают отображение картинок в латентное пространство, основываясь на локальных корреляциях пикселей и графических деталях, а не на семантическом контенте. Из-за этого правдоподобие OOD-объектов может быть выше, чем правдоподобие in-distribution сэмплов.

![flows](https://yastatic.net/s3/education-portal/media/flows_ood_8620c90463_5420c22f23.webp)

Однако позже было предложено использовать ряд эвристик для того, чтобы улучшить способность к детекции аномалий за счет подсчета значения функции правдоподобия:

1. Использовать значение правдоподобия второй модели потока, обученного на отличном от исходного датасете (например, ImageNet при исходном CelebA). А затем вычислять отношенение этих двух значений для вынесения вердикта об аномальности объекта. [Schirrmeister et al.](https://arxiv.org/abs/2006.10848)
2. В работе [Serrà et al.](https://arxiv.org/abs/1909.11480) показали, что проблема качества нормализующих потоков в задаче детекции аномалий связана с чрезмерным влиянием сложности входных данных на значение функции правдоподобия. Поэтому авторы предложили использовать в качестве поправки размер сжатого изображения с помощью одного из алгоритмов компрессии (JPEG2000/PNG).

![flows](https://yastatic.net/s3/education-portal/media/flows_ood_compression_f6c5a5e125_623f4182c0.webp)

## Сравнение с другими типами генеративных моделей

Обратимся к статье [Bond-Taylor et al.](https://arxiv.org/abs/2103.04922), в которой приводится количественный анализ всех существующих семейств генеративных моделей в задаче генерации изображений из датасета CIFAR-10.

![nf](https://yastatic.net/s3/education-portal/media/nf_comparison_8444b65d4c_71a5389edf.webp)

В таблице выше указано, насколько представители каждого из популярных семейств генеративных моделей эффективны в следующих аспектах решения задачи:

1. скорость обучения;
2. скорость генерации;
3. число обучаемых параметров;
4. разрешение генерируемого изображения;
5. ограничение на форму якобиана;
6. возможность вычислять правдоподобие объекта;
7. FID ([Fréchet Inception Distance](https://arxiv.org/abs/1706.08500)) тестовой выборки;
8. Отрицательный логарифм правдоподобия тестовой выборки.

За расшифровкой обратимся к таблице ниже:

![nf](https://yastatic.net/s3/education-portal/media/nf_comparison_criteria_8ea27b2e47_ee256aab04.webp)

Подведя итог, можно сказать, что нормализующие потоки:

* требуют очень много времени на обучение, так как при обучении проводятся нетривиальные неоптимизированные вычисления;
* имеют скорость генерации, сравнимую с GAN-ами;
* менее эффективны по соотношению качество/число параметров, чем GAN-ы и диффузионные модели;
* позволяют **быстро** вычислять точное значение функции правдоподобия объекта;
* обладают сравнительно неплохим качеством генерации, проигрывающим GAN-ам и диффузионным моделям.

Итак, нормализующие потоки явно выделяются среди других семейств генеративных моделей своими свойствами – обратимостью и способностью вычислять правдоподобие объекта. Но если для решения задачи они не требуются, то имеет смысл попробовать другие модели – в первую очередь, GAN-ы и диффузионные модели.

  ## handbook

  Учебник по машинному обучению

  ## title

  Нормализующие потоки

  ## description

  *null*

- 
  ## path

  /handbook/ml/article/diffuzionnye-modeli

  ## content

  ## Введение

В этом параграфе мы снова попробуем решить задачу генерации, когда нам дана выборка объектов из распределения $x_0 \sim q(x)$, и хотим научиться генерировать новые объекты из распределения , которых нет в нашей выборке.

Вероятно, вы уже знакомы с другими генеративными моделями, например VAE или GAN-ы. Здесь же мы познакомим вас с еще одним видом генеративных моделей: **диффузионные модели**, которые стали крайне популярны в последнее время благодаря своему высокому качеству генерации объектов из заданного распределения. В общий чертах, они работают следующим образом: берем шум из $\mathcal{N}(0, I)$ и шаг за шагом удаляем компоненты шума до тех пор, пока не получим объект $x_0$ из распределения, см. иллюстрацию ниже.

![Screenshot_2022_08_10_at_7_40_50_PM_4f35128d6e.webp](https://yastatic.net/s3/education-portal/media/Screenshot_2022_08_10_at_7_40_50_PM_4f35128d6e_5e997fb3b8.webp)

## Более детально

Для детального понимания стоит объяснить, что такое **прямой и обратный диффузионные процессы**. **Прямой** процесс заключается в постепенном зашумлении картинки с помощью распределения $q$, а **обратный**, наоборот, в расшумлении с помощью распределения $p$. Их можно схематично изобразить следующим образом:

![Artboard_37_5c7b09c386.svg](https://yastatic.net/s3/education-portal/media/Artboard_37_5c7b09c386_d0cccfa898.svg)

**Прямой** диффузионный процесс определяется как апостериорное распределение $q(x_{1:T}|x_0)$. Это распределение также является Марковской цепочкой, которая постепенно добавляет гауссовский шум к объекту $x_0$. На каждом шаге шум добавляется с различной магнитудой, которая определяется расписанием дисперсий $\{\beta_1, ... , \beta_T\}$. При правильном выборе расписания в пределе по числу шагов $T$ мы должны сойтись к шуму из $\mathcal{N}(0, I)$. В качестве распределений $q$ берут нормальные распределения:

$$
q(x_t | x_{t - 1}) := \mathcal{N}(x_t; \sqrt{1 - \beta_t}x_{t - 1}, \beta_tI), \ \ \ \ \ \ \ q(x_{1:T}|x_0) = \prod_{t = 1}^T q(x_t | x_{t - 1})
$$

Теперь перейдем к **обратному** процессу и к самой **диффузионной модели**.

**Диффузионная модель** - это вероятностная модель с латентными переменными вида $p_\theta(x_0) := \int p_\theta(x_{0:T}) dx_{1:T}$, где промежуточные состояния $x_1, ..., x_T$ соответствуют зашумленным объектам, a $x_0$ - объект из распределения. Совместное распределение $p_\theta(x_{0:T})$ называет **обратным диффузионным процессом**, который представляет собой Марковскую цепочку из гауссовских распределений $p_\theta(x_{i-1}|x_{i})$: 

$$
p(x_{0:T}) = p(x_0) \prod_{t = 1}^Tp_{\theta}(x_{t-1}|x_t) \ \ \ \ \ \ \ \ \ p_\theta(x_{T})=\mathcal{N}(x_T | 0, I)
$$

$$
  p_{\theta}(x_{t - 1}|x_t):= \mathcal{N}(x_{t - 1}; \mu_{\theta}(x_t, t), \Sigma_{\theta}(x_t, t))
$$

Таким образом, обратный процесс параметризуется моделью $\theta$, которая по зашумленному объекту $x_t$ и шагу $t$ предсказывает среднее $\mu_{\theta}(x_t, t)$ и дисперсию $\Sigma_{\theta}(x_t, t)$.

## Обучение диффузионной модели

Диффузионный модели обучаются, максимизируя вариационную нижнюю оценку (ELBO) логарифма правдоподобия  $\log p_{\theta}(x_0)$. По тому же принципу обучаются VAE, с тем лишь отличием, что у диффузионных моделей другая форма модели с латентными переменными.  Итак, давайте выведем ELBO для диффузии:

$$
\begin{aligned}
- \log p_\theta(\mathbf{x}_0) 
&\leq - \log p_\theta(\mathbf{x}_0) + D_\text{KL}(q(\mathbf{x}_{1:T}\vert\mathbf{x}_0) \| p_\theta(\mathbf{x}_{1:T}\vert\mathbf{x}_0) ) \\
&= -\log p_\theta(\mathbf{x}_0) + \mathbb{E}_{\mathbf{x}_{1:T}\sim q(\mathbf{x}_{1:T} \vert \mathbf{x}_0)} \Big[ \log\frac{q(\mathbf{x}_{1:T}\vert\mathbf{x}_0) p_\theta(\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} \Big] \\
&= -\log p_\theta(\mathbf{x}_0) + \mathbb{E}_q \Big[ \log\frac{q(\mathbf{x}_{1:T}\vert\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} + \log p_\theta(\mathbf{x}_0) \Big] \\
&= \mathbb{E}_q \Big[ \log \frac{q(\mathbf{x}_{1:T}\vert\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} \Big] \\
\text{Let }L_\text{VLB} 
&= \mathbb{E}_{q(\mathbf{x}_{0:T})} \Big[ \log \frac{q(\mathbf{x}_{1:T}\vert\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} \Big] \geq - \mathbb{E}_{q(\mathbf{x}_0)} \log p_\theta(\mathbf{x}_0)
\end{aligned}
$$

- Комментарий
    
Если вы знакомы с VAE, то вывод $L_{VLB}$ должен быть вам понятен, однако ниже приведен вывод с помощью неравенства Йенсена

$$
\begin{aligned}
L_\text{CE}
&= - \mathbb{E}_{q(\mathbf{x}_0)} \log p_\theta(\mathbf{x}_0) \\
&= - \mathbb{E}_{q(\mathbf{x}_0)} \log \Big( \int p_\theta(\mathbf{x}_{0:T}) d\mathbf{x}_{1:T} \Big) \\
&= - \mathbb{E}_{q(\mathbf{x}_0)} \log \Big( \int q(\mathbf{x}_{1:T} \vert \mathbf{x}_0) \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T} \vert \mathbf{x}_{0})} d\mathbf{x}_{1:T} \Big) \\
&= - \mathbb{E}_{q(\mathbf{x}_0)} \log \Big( \mathbb{E}_{q(\mathbf{x}_{1:T} \vert \mathbf{x}_0)} \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T} \vert \mathbf{x}_{0})} \Big) \\
&\leq - \mathbb{E}_{q(\mathbf{x}_{0:T})} \log \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T} \vert \mathbf{x}_{0})} \\
&= \mathbb{E}_{q(\mathbf{x}_{0:T})}\Big[\log \frac{q(\mathbf{x}_{1:T} \vert \mathbf{x}_{0})}{p_\theta(\mathbf{x}_{0:T})} \Big] = L_\text{VLB}
\end{aligned}
$$

Теперь вернемся к распределению $q(x_t | x_{t - 1})$.  Для того чтобы получить $x_t$, придется итеративно получать $x_1, ..., x_{t - 1}$. Однако это можно сделать более эффективно благодаря нормальным распределениям. Для этого обозначим $\alpha_t := 1- \beta_t$ и $\bar{\alpha}_t:= \prod_{i = 1}^t\alpha_i$, тогда</p> 

$$
q(x_t | x_0) = \mathcal{N}(x_t;\sqrt{\bar{\alpha}_t} x_0, (1-\bar{\alpha}_t)I)
$$

- Формальный вывод этого факта

$$
\begin{aligned}
\mathbf{x}_t 
&= \sqrt{\alpha_t}\mathbf{x}_{t-1} + \sqrt{1 - \alpha_t}\mathbf{z}_{t-1}; \text{ где } \mathbf{z}_{t-1}, \mathbf{z}_{t-2}, \dots \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) \\
&= \sqrt{\alpha_t}(\sqrt{\alpha}_{t - 1}x_{t - 2} + \sqrt{1 - \alpha_{t - 1}}\mathbf{z}_{t - 2})  + \sqrt{1 - \alpha_t} \mathbf{z}_{t-1} \\
&= \sqrt{\alpha_t \alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{1 - \alpha_t \alpha_{t-1}} \bar{\mathbf{z}}_{t-2}; \text{ где } \bar{\mathbf{z}}_{t-2} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) \ \ {(*)} \\
&= \dots \\
&= \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\mathbf{z} \\
q(\mathbf{x}_t \vert \mathbf{x}_0) &= \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1 - \bar{\alpha}_t)\mathbf{I})
\end{aligned}
$$
    
(*) Пояснение ко второму переходу. У нас выходит

$$
\sqrt{\alpha_t(1 - \alpha_{t - 1})}z_{t - 2} + \sqrt{1 - \alpha_t}z_{t - 1} \\ = \sqrt{\alpha_t(1 - \alpha_{t - 1}) + (1 - \alpha_t)}\bar{z}_{t- 2} \\ = \sqrt{1 - \alpha_t\alpha_{t - 1}}\bar{z}_{t - 2}; \text{ где } z_{t - 1},z_{t - 2},\bar{z}_{t - 2} \sim \mathcal{N}(0, I)
$$

Тогда $L_{VLB}$ может быть переписано как

$$
L_{VLB} = \mathbb{E}_q [\underbrace{D_\text{KL}(q(\mathbf{x}_T \vert 
\mathbf{x}_0) \parallel p_\theta(\mathbf{x}_T))}_{L_T} + + \sum_{t=2}^T 
\underbrace{D_\text{KL}(q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, 
\mathbf{x}_0) \parallel p_\theta(\mathbf{x}_{t-1} 
\vert\mathbf{x}_t))}_{L_{t-1}} \underbrace{- \log p_\theta(\mathbf{x}_0 
\vert \mathbf{x}_1)}_{L_0}
$$

- Долгий вывод
    
Серым в скобках комментарий к последующему переходу.

$$
\begin{aligned}L_\text{VLB} &= \mathbb{E}_{q(\mathbf{x}_{0:T})} \Big[ \log\frac{q(\mathbf{x}_{1:T}\vert\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} \Big] \quad \textit{\color{gray}{(расписываем совместное распределение)}}\\&= \mathbb{E}_q \Big[ \log\frac{\prod_{t=1}^T q(\mathbf{x}_t\vert\mathbf{x}_{t-1})}{ p_\theta(\mathbf{x}_T) \prod_{t=1}^T p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t) } \Big] \ \ \ \textit{\color{gray}{(берем логарифм)}} \\&= \mathbb{E}_q \Big[ -\log p_\theta(\mathbf{x}_T) + \sum_{t=1}^T \log \frac{q(\mathbf{x}_t\vert\mathbf{x}_{t-1})}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)} \Big] \textit{\color{gray}{(отщепляем члены суммы)}} \\&= \mathbb{E}_q \Big[ -\log p_\theta(\mathbf{x}_T) + \sum_{t=2}^T \log \frac{q(\mathbf{x}_t\vert\mathbf{x}_{t-1})}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)} + \log\frac{q(\mathbf{x}_1 \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)} \Big] \textit{\color{gray}{(*)}}\\&= \mathbb{E}_q \Big[ -\log p_\theta(\mathbf{x}_T) + \sum_{t=2}^T \log \Big( \frac{q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)}\cdot \frac{q(\mathbf{x}_t \vert \mathbf{x}_0)}{q(\mathbf{x}_{t-1}\vert\mathbf{x}_0)} \Big) + \log \frac{q(\mathbf{x}_1 \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)} \Big] \textit{\color{gray}{(лог произведения раскрываем)}}\\&= \mathbb{E}_q \Big[ -\log p_\theta(\mathbf{x}_T) + \sum_{t=2}^T \log \frac{q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)} + \sum_{t=2}^T \log \frac{q(\mathbf{x}_t \vert \mathbf{x}_0)}{q(\mathbf{x}_{t-1} \vert \mathbf{x}_0)} + \log\frac{q(\mathbf{x}_1 \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)} \Big] \textit{\color{gray}{(от второй суммы останется только 1ый и последний член)}} \\&= \mathbb{E}_q \Big[ -\log p_\theta(\mathbf{x}_T) + \sum_{t=2}^T \log \frac{q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)} + \log\frac{q(\mathbf{x}_T \vert \mathbf{x}_0)}{q(\mathbf{x}_1 \vert \mathbf{x}_0)} + \log \frac{q(\mathbf{x}_1 \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)} \Big] \textit{\color{gray}{(комбинируем 1 и 3 член, 3 и 4 член)}}\\&= \mathbb{E}_q \Big[ \log\frac{q(\mathbf{x}_T \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_T)} + \sum_{t=2}^T \log \frac{q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)} - \log p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1) \Big] \\&= \mathbb{E}_q [\underbrace{D_\text{KL}(q(\mathbf{x}_T \vert \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_T))}_{L_T} + \sum_{t=2}^T \underbrace{D_\text{KL}(q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t))}_{L_{t-1}} \underbrace{- \log p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)}_{L_0} ]\end{aligned}
$$

Пояснение (*). Пользуемся тем, что у нас Марковский процесс, и теоремой Байеса: 

$$
q(x_t | x_{t - 1}) = q(x_t|x_{t - 1}, x_0) = \frac{q(x_{t - 1}| x_t, x_0)q(x_t | x_0)}{q(x_{t - 1}|x_0)}
$$

Таким образом во время обучения, на каждой итерации параллельно оптимизируются случайные член $L_t$ с помощью градиентного спуск (сэмлируем $t \sim U\{1,...,T\}$). Поскольку все распределения нормальные, то KL между ними можно выписать в явной форме (см. ниже).

- Формула KL между двумя нормальными
    
$$
\begin{split}        KL(\mathcal{N}_1 \ || \ \mathcal{N}_2) = \frac{1}{2}\bigg(Tr(\Sigma^{-1}_2 \Sigma_1) + (\mu_2 - \mu_1)^T \Sigma^{-1}_2 (\mu_2 - \mu_1) + \\[1.5ex]        + \ln \frac{det(\Sigma_2)}{det(\Sigma_1)} - d \bigg)    \end{split}
$$

Если $\Sigma_1 = \sigma_1I, \ \Sigma_2 = \sigma_2I$

$$
\begin{split}        KL(\mathcal{N}_1 \ || \ \mathcal{N}_2) = \frac{1}{2}\bigg(\frac{d\sigma_1}{\sigma_2} + \frac{1}{\sigma_2}\|\mu_2 - \mu_1\|^2 + \\[1.5ex]        + \ln \frac{\sigma_2}{\sigma_1}\bigg)    \end{split}
$$

Осталось только выписать $q(x_{t - 1}| x_t, x_0)$ . Мы знаем, поскольку у нас все распределения нормальные, то и $q(x_{t - 1}| x_t, x_0)$  будет нормальным.

Обозначим

$$
q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_{t-1}; \color{blue}{\tilde{\boldsymbol{\mu}}}(\mathbf{x}_t, \mathbf{x}_0), \color{red}{\tilde{\beta}_t} \mathbf{I}) 
$$

- Вывод $q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)$
    
Применим формулу Байеса и распишем. Тут мы просто пытаемся понять, как будут выглядеть среднее и дисперсия, выделяя квадратичную форму в показателе экспоненты

$$
q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) = q(\mathbf{x}_t \vert \mathbf{x}_{t-1}, \mathbf{x}_0) \frac{ q(\mathbf{x}_{t-1} \vert \mathbf{x}_0) }{ q(\mathbf{x}_t \vert \mathbf{x}_0) } \propto 
$$

$$
  \propto \exp \Big(-\frac{1}{2} \big(\frac{(\mathbf{x}_t - \sqrt{\alpha_t} \mathbf{x}_{t-1})^2}{\beta_t} + \frac{(\mathbf{x}_{t-1} - \sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_0)^2}{1-\bar{\alpha}_{t-1}} - \frac{(\mathbf{x}_t - \sqrt{\bar{\alpha}_t} \mathbf{x}_0)^2}{1-\bar{\alpha}_t} \big) \Big) 
$$

$$
  = \exp \Big(-\frac{1}{2} \big(\frac{\mathbf{x}_t^2 - 2\sqrt{\alpha_t} \mathbf{x}_t \color{blue}{\mathbf{x}_{t-1}} \color{black}{+ \alpha_t} \color{red}{\mathbf{x}_{t-1}^2} }{\beta_t} + \frac{ \color{red}{\mathbf{x}_{t-1}^2} \color{black}{- 2 \sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_0} \color{blue}{\mathbf{x}_{t-1}} \color{black}{+ \bar{\alpha}_{t-1} \mathbf{x}_0^2} }{1-\bar{\alpha}_{t-1}} - \frac{(\mathbf{x}_t - \sqrt{\bar{\alpha}_t} \mathbf{x}_0)^2}{1-\bar{\alpha}_t} \big) \Big)=  
$$

$$
  = \exp\Big( -\frac{1}{2} \big( \color{red}{(\frac{\alpha_t}{\beta_t} + \frac{1}{1 - \bar{\alpha}_{t-1}})} \mathbf{x}_{t-1}^2 - \color{blue}{(\frac{2\sqrt{\alpha_t}}{\beta_t} \mathbf{x}_t+\frac{2\sqrt{\bar{\alpha}_{t-1}}}{1 - \bar{\alpha}_{t-1}} \mathbf{x}_0)} \mathbf{x}_{t-1} \color{black}{ + C(\mathbf{x}_t, \mathbf{x}_0) \big) \Big)}
$$
    
Далее перепишем красные и синие выражения в более красивой форме

$$
\color{red}{\tilde{\beta}_t} = 1/(\frac{\alpha_t}{\beta_t} + \frac{1}{1 - \bar{\alpha}_{t-1}}) = 1/(\frac{\alpha_t - \bar{\alpha}_t + \beta_t}{\beta_t(1 - \bar{\alpha}_{t-1})}) = \boxed{\frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \cdot \beta_t} 
$$

$$
\color{blue}{\tilde{\boldsymbol{\mu}}_t (\mathbf{x}_t, \mathbf{x}_0)} = (\frac{\sqrt{\alpha_t}}{\beta_t} \mathbf{x}_t + \frac{\sqrt{\bar{\alpha}_{t-1} }}{1 - \bar{\alpha}_{t-1}} \mathbf{x}_0)/(\frac{\alpha_t}{\beta_t} + \frac{1}{1 - \bar{\alpha}_{t-1}}) 
$$

$$
  = (\frac{\sqrt{\alpha_t}}{\beta_t} \mathbf{x}_t + \frac{\sqrt{\bar{\alpha}_{t-1} }}{1 - \bar{\alpha}_{t-1}} \mathbf{x}_0)\frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \cdot \beta_t 
$$

$$
  = \boxed{\frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \mathbf{x}_t + \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1 - \bar{\alpha}_t} \mathbf{x}_0}
$$

### Другой лосс. Предсказываем шум

В прошлой подсекции наша модель предсказывала среднее и дисперсию нормального распределения. Давайте зафиксируем $\Sigma_{\theta}(x_t, t) = \sigma^2_tI$. Обычно берут $\sigma^2_t = \beta_t$ или $\sigma^2_t = \tilde{\beta}_t = \frac{1 - \bar{\alpha}_{t - 1}}{1 - \bar{\alpha}_t}\beta_t.$  Тогда $L_{t - 1}$ из предыдущей секции можно переписать как

$$
L_{t} = \mathbb{E}_q\bigg[\frac{1}{2\sigma^2_t}\|\mu_{\theta}(x_t, x_0) - \tilde{\mu}_t(x_t, x_0)\|^2\bigg] + const(\theta)
$$

Это первый момент, как меняется функционал, если мы не хотим предсказывать $\Sigma_{\theta}(x_t, t)$, а фиксируем её.

Теперь вспомним, что  $q(x_t | x_0) = \mathcal{N}(x_t;\sqrt{\bar{\alpha}_t} x_0, (1-\bar{\alpha}_t)I)$, но благодаря тому, что у нас гауссовское распределение, это можно переписать в виде

$$
x_t(x_0, \epsilon) = \sqrt{\bar{\alpha}_t} x_0 +  \sqrt{1-\bar{\alpha}_t}\epsilon, \ \ \ \epsilon \sim \mathcal{N}(0, I)
$$

Выразим отсюда $x_0$ и получим, что $x_0 = \frac{1}{\sqrt{\bar{\alpha}_t}}(x_t - \sqrt{1 - \bar{\alpha}_t}\epsilon)$, тогда подставим это выражение в  формулу для $\tilde{\boldsymbol{\mu}}(\mathbf{x}_t, \mathbf{x}_0)$ (из подсекции «Вывод $q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)$») и получим

$$
\tilde{\boldsymbol{\mu}}(\mathbf{x}_t, \mathbf{x}_0) = \frac{1}{\sqrt{\alpha_t}}(x_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}}\epsilon) 
$$

Теперь скажем, что наша модель будет предсказывать $\epsilon$. И просто будем «подставлять» его в выражение для $\tilde{\mu}$ выше. Обозначим предсказание модели как $\epsilon_{\theta}(x_t, t)$ — предсказанный шум $\epsilon$. Тогда лосс $L_t$ превратиться в

$$
L_t = \mathbb{E}_{x_0, z}\bigg[ \frac{\beta^2_t}{2\alpha_t(1 - \bar{\alpha}_t )\sigma^2_t}\|\epsilon - \epsilon_{\theta}(\sqrt{\bar{\alpha}_t} x_0 +  (1-\bar{\alpha}_t)\epsilon, t)\|^2\bigg]
$$

Тем не менее лосс можно еще больше упростить и просто обучать с помощью MSE на $\epsilon$.

$$
L^{simple}_t = \mathbb{E}_{x_0, \epsilon, t}\bigg[ \|\epsilon - \epsilon_{\theta}(\sqrt{\bar{\alpha}_t} x_0 +  (1-\bar{\alpha}_t)\epsilon, t)\|^2\bigg]
$$

Итак, алгоритмы обучения и сэмплирования выглядят вот так (на картинке  $z:= \epsilon$).

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/Untitled_cf1a3672e9_43c6fd4279.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <p>Алгоритм обучения и сэмплирования диффузионной модели (Изображение взято из: <a href="https://arxiv.org/abs/2006.11239">Ho et al. 2020)</a></p>
  </figcaption>
</figure>

Стоит отметить, что важным недостатком диффузионных моделей является низкая скорость сэмплирования. Согласно [Song et al. 2020](https://arxiv.org/abs/2010.02502): «Требуется 20 часов на генерацию 50 тысяч картинок размера 32х32, используя DDPM, и меньше минуты, используя GAN» (Nvidia 2080 Ti GPU). Тем не менее, в данном направлении был достигнут значительный прогресс и в целом проблема медленного сэмплирования была частично решена: [Jiaming Song et al. (2021)](https://arxiv.org/abs/2010.02502), [Kong & Ping (2021)](https://openreview.net/pdf?id=agj4cdOfrAP), [Bond-Taylor et al. (2021)](https://arxiv.org/abs/2111.12701)

Давайте зафиксируем, какие функции потерь можно использовать. Для всех них справедлив тот факт, что мы сэмплируем шаг равномерно во время обучение $t \sim U\{1,...,T\})$ и оптимизируем соответствующий $L_{t}$.

- Оптимизируя член из суммы $L_{VLB}$. Это KL дивергенция между двумя нормальными распределениями

$$
L_{VLB} = \mathbb{E}_q \underbrace{D_\text{KL}(q(\mathbf{x}_T \vert 
\mathbf{x}_0) \parallel p_\theta(\mathbf{x}_T))}_{L_T} + + \sum_{t=2}^T 
\underbrace{D_\text{KL}(q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, 
\mathbf{x}_0) \parallel p_\theta(\mathbf{x}_{t-1} 
\vert\mathbf{x}_t))}_{L_{t-1}} \underbrace{- \log p_\theta(\mathbf{x}_0 
\vert \mathbf{x}_1)}_{L_0}
$$

- При фиксированной дисперсии $\Sigma_{\theta}$ можно оптимизировать взвешенную MSE между средними нормальных распределений

$$
L_{t} = \mathbb{E}_q\bigg[\frac{1}{2\sigma^2_t}\|\mu_{\theta}(x_t, x_0) - \tilde{\mu}_t(x_t, x_0)\|^2\bigg] + const(\theta)
$$

- При фиксированной дисперсии и при предсказании шума с помощью взвешенной MSE. Или просто MSE. $L^{simple}_t$ является **самым популярным** вариантом, который на практике дает лучшие результаты.

$$
L_t = \mathbb{E}_{x_0, z}\bigg[ \frac{\beta^2_t}{2\alpha_t(1 - \bar{\alpha}_t )\sigma^2_t}\|\epsilon - \epsilon_{\theta}(\sqrt{\bar{\alpha}_t} x_0 +  (1-\bar{\alpha}_t)\epsilon, t)\|^2\bigg] \\
L^{simple}_t = \mathbb{E}_{x_0, z}\bigg[ \|\epsilon - \epsilon_{\theta}(\sqrt{\bar{\alpha}_t} x_0 +  (1-\bar{\alpha}_t)\epsilon, t)\|^2\bigg]
$$

### Выбор расписания $\beta_t$

Расписание является гиперпараметром, основными требованиями на который являются невозрастание $(\beta_1 \leq ... \leq \beta_T)$ и чтобы прямой процесс сходился к $\mathcal{N}(0, I)$ в пределе по $T$. Второе может гарантироваться тем, что $\bar{\alpha}_t \to 0$. Вспомним,

$$
q(x_t | x_0) = \mathcal{N}(x_t;\sqrt{\bar{\alpha}_t} x_0, (1-\bar{\alpha}_t)I)
$$

Однако на практике [оно](https://arxiv.org/abs/2006.11239) также проверяется, чтобы $D_{KL}(q(x_T | x_0) || \mathcal{N}(0, I))$ было близко к 0.

Также стоит упомянуть, что обычно берут $T = 1000$. Но также важно помнить про требования выше, ведь расписание шума непосредственно зависит от $T$.

Чаще всего используют линейное расписание, где $\beta_1 = 10^{-4}, \ \beta_T = 0.02$.  У данных констант нет никакой мотивации, кроме той, которая описана выше. Они были предложены в [Ho et al. (2020)](https://arxiv.org/abs/2006.11239).

В [Nichol & Dhariwal (2021)](https://arxiv.org/abs/2102.09672) было предложено косинусное расписание, которое помогло диффузионным моделям достичь лучшего NLL (negative loglikelihood): 

$$
\beta_t = \text{clip}(1-\frac{\bar{\alpha}_t}{\bar{\alpha}_{t-1}}, 0.999) \quad\bar{\alpha}_t = \frac{f(t)}{f(0)}\quad\text{where }f(t)=\cos\Big(\frac{t/T+s}{1+s}\cdot\frac{\pi}{2}\Big)
$$

Авторы обнаружили, что линейное расписание плохо работает на картинках 64х64 и меньше. А именно, последнии шаги прямого прохода были шумными и малоинформатиыными (просто зашумляем шум еще больше):

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/Untitled_1_ef776c9292_19c00be1f3.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    Пример зашумления картинки для линейного (сверху) и косинусного (снизу) расписания.
  </figcaption>
</figure>

Также они обнаружили, что если обучать модель с линейным расписанием только на 80% первых шагов, то модель не становится сильно хуже, что подтверждает неиформативность последних шагов. Далее, они подобрали расписание так, чтобы $\sqrt{\bar{\alpha}_t}$ убывало линейно на большей части отрезка (от 0 до $T$) и почти не менялось рядом с 0 и $T$. Разницу в $\sqrt{\bar{\alpha}}_t$ для разных расписаний можно увидеть на картинке ниже:

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/Artboard_21_b212322bde_2961396e17.svg" loading="lazy" decoding="async" alt="">
  <figcaption>
    <p>Изображение взято из <a href='https://arxiv.org/abs/2102.09672'>Nichol & Dhariwal, 2021</a></p>
  </figcaption>
</figure>

- Детали
    
Также они  ограничивают $\beta_t$ числом 0.999, чтобы в конце процесса не было проблем с численной устойчивостью. Коэффициент $s$ используется, чтобы $\beta_t$ не были слишком малы рядом с нулем. Он равен 0.008. Такое число было выбрано так, чтобы «$\sqrt{\beta_0}$ была немного меньше, чем размер бина одного пикселя, то есть $1/127.5$»

### Classifier guidance

В [Nichol & Dhariwal (2021)](https://arxiv.org/pdf/2105.05233.pdf) был предложен метод условной генерации, который повышает качество генерируемых картинок, при этом уменьшая их разнообразие. Для этого предобучается «шумный» классификатор на зашумленных картинках, то есть $p_{\phi}(y |x_t)$. Затем он используется во время сэмплирования, корректируя предсказанное среднее на $\nabla_x \log p_{\phi}(y|x_t)$. В [Nichol & Dhariwal (2021)](https://arxiv.org/pdf/2105.05233.pdf) (Секция 4.1) показывают, что данная добавка позволяет превратить распределение $p_\theta(x_{i-1}|x_{i})$ в $p_\theta(x_{i-1}|x_{i}, y)$. Важно, что исходная диффузионная модель никак не меняется, что делает трюк еще более привлекательным.  Алгоритм сэмплирования можно видеть на картинке ниже. Коэффициент $s$ отвечает за силу guidance.

- Мотивация
У генеративной модели GAN есть способ, который позволяет «балансировать» между разнообразием картинок и их качеством — *truncation trick.* Он заключается в сэмплировании латентного вектора [truncated normal distibution](https://en.wikipedia.org/wiki/Truncated_normal_distribution). Данный трюк был хорошо описан и исследован в статье про [BigGAN](https://arxiv.org/abs/1809.11096).  Поэтому в диффузионных моделях тоже хотелось бы иметь метод, который позволяет балансировать между качеством и разнообразием. Авторы предложили classifier guidance, сравнили его с *truncation trick* и показали, что их метод строго лучше.
    
<figure>
  <img src="https://yastatic.net/s3/education-portal/media/Untitled_3_217b177b1b_1a4dd17e55.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <p>Изображение взято из <a href='https://arxiv.org/pdf/2105.05233.pdf'>Nichol & Dhariwal, 2021</a></p>
  </figcaption>
</figure>

### Classifier-free guidance

[Ho & Salimans (2021)](https://openreview.net/pdf?id=qw8AKxfYbI) предложили метод, в котором guidance достигается без использования дополнительной модели, поскольку это достаточно затратно. Для этого они обучали условную модель $\epsilon_{\theta}(x_t | y)$, у которой во время обучения реальная метка $y$ заменялась с какой-то фиксированной вероятностью (10%) на пустую метку ($y=\emptyset$). Это по сути позволяет нам обучать безусловную модель $\epsilon_{\theta}(x_t)$ одновременно с условной $\epsilon_{\theta}(x_t | y)$Тогда во время сэмплирования делаем так, чтобы предсказание немного менялось в сторону $\epsilon_{\theta}(x_t | y)$, а именно:

$$
\hat{\epsilon}_{\theta}(x_t | y) = \epsilon_{\theta}(x_t | \emptyset) + s\cdot(\epsilon_{\theta}(x_t | y) - \epsilon_{\theta}(x_t | \emptyset))
$$

Мотивация этой формулы следовала из формулы Байеса:

$$
p(y | x_t) \propto \frac{p(x_t | y) }{p(x_t)} \\
\implies \log p(y | x_t) \propto \log p(x_t | y) - \log p(x_t) \\
\implies \nabla_{x_t} \log p(y | x_t) \propto \nabla_{x_t} \log p(x_t | y) - \nabla_{x_t} \log p(x_t) \\
\implies \nabla_{x_t} \log p(y | x_t) \propto \epsilon(x_t | y) - \epsilon(x_t)
$$

Тогда мы можем просто подставить $\nabla_{x_t} \log p(y | x_t)$ в формулу для classifier guidance из предыдущей подсекции и получить желаемое равенство с точностью до коэффициента $s$.

## Овервью ключевых работ на сегодняшний день

- Jonathan Ho et al. [«Denoising diffusion probabilistic models.»](https://arxiv.org/abs/2006.11239) arxiv Preprint arxiv:2006.11239 (2020)

Основная работа, в которой диффузионные модели (Denoising Diffusion Probabilistic Models, DDPMs) были применены для генерации картинок. Параграф в основном построен на ней. 

- Jiaming Song et al. [«Denoising diffusion implicit models.»](https://arxiv.org/abs/2010.02502) arxiv Preprint arxiv:2010.02502 (2020)

Одна из первых попыток ускорить генерацию объектов. Идея следущая: давайте изменим прямой диффузионный процесс так, чтобы используя предобученную DDPM, приближать новый обратный процесс за меньшее число шагов. 

Чтобы не обучать новую модель, нам нужен прямой диффузионный процесс, у которого будет такая же (суррогатная) функция потерь, а обратный процесс все еще останется Марковским. Оказалось, что существует целое семейство не-Марковских прямых процессов, удовлетворяющих этим требования. Это семейство имеет следующий вид: 

$$
q_\sigma (x_{1:T}|x_0):= q_\sigma (x_{T}|x_0)\prod_{t=2}^{T} q_\sigma (x_{t-1}|x_t,x_0), 
$$

где $q_\sigma (x_{T}|x_0)= \mathcal{N}(\sqrt{\alpha_t}x_0, (1 - \alpha_t)I)$ и для всех $t>1,$

$$
q_\sigma (x_{t-1}|x_t,x_0)= \mathcal{N}(\sqrt{\alpha_{t-1}}x_0+\sqrt{1-\alpha_{t-1}-\sigma_t^2 }\cdot\frac{x_t-\sqrt{\alpha_{t}}x_0}{\sqrt{1-\alpha_t}},\sigma_t^2 I)
$$

Среднее было выбрано так, чтобы $q_{\sigma}(x_t | x_0) = \mathcal{N}(\sqrt{\alpha_t}x_0, (1 - \alpha_t)I)$ для всех $t$. (см. Лемму 1 в Приложении B к статье). То есть важно лишь то, чтобы маргинальное распределение $q_{\sigma}(x_t | x_0)$ не менялось по сравнению с обычным Марковским случаем. Прямой процесс может быть получен с помощью теоремы Байеса:

$$
q_\sigma (x_t|x_{t-1},x_0)=\frac{q_\sigma (x_{t-1}|x_t,x_0)q_\sigma (x_t|x_0)}{q_\sigma (x_{t-1}|x_0)}
$$

Тут $\sigma$ контролирует степень стохастичности прямого процесса. Можно заметить, что в отличии от исходного диффузионного процесса, предложенный прямой процесс больше не является Марковским, так как каждый $x_t$ теперь зависит и от $x_{t-1}$ и от $x_0$. Схематично, это можно изобразить как на картинке справа. (Слева исходный диффузионный процесс для сравнения)

![Screen_Shot_2022_08_22_at_00_38_27_b822865ad2.webp](https://yastatic.net/s3/education-portal/media/Screen_Shot_2022_08_22_at_00_38_27_b822865ad2_d2336fed77.webp)

- Заметка  
Авторы обращают внимание, что функция потерь в DDPM зависит от $q(x_t|x_0)$, а не от $q(x_1{:}x_T | x_0)$ напрямую. Это означает, что нам нужно выбрать любой другой прямой диффузионный процесс, у которого $q(x_t|x_0)$ остались те же. 

Далее, мы можем переписать обратный процесс в данном виде:

$$
x_{t-1}=\sqrt{\alpha_{t-1}}\;\underbrace{\frac{x_t-\sqrt{1-\alpha_{t}}\epsilon_\theta^{(t)}(x_t)}{\sqrt{\alpha_t}}}_{"predicted\:x_0"}+\underbrace{\sqrt{1-\alpha_{t-1}-\sigma_t^2 }\cdot \epsilon_\theta^{(t)}(x_t)}_{"direction\;pointing\;to\;x_t"}+\underbrace{\sigma_t\epsilon_t}_{random\:noise}
$$

Заметим, что при $\sigma_t = \sqrt{(1 - \alpha_{t - 1})(1 - \alpha_t)}\sqrt{1 - \alpha_t / \alpha_{t - 1}}$ прямой процесс становится марковским, а обратный как у DDPM (обычное сэмплирование, описанное в основной секции). При $\sigma_t = 0$ процесс сэмплирования становится детерминистичным (данный способ и называется DDIM). Ускорение сэмплирования достигается засчет использования лишь какого-то подмножества шагов ($0 \leq \tau_1 \leq ... \leq \tau_S \leq T, \ \ \ S < T$). Также одним из плюсов детерминистичного сэмплирования является возможность делать семантическую интерполяцию в латентном пространстве (как у GANов). 

- Alex Nichol & Prafulla Dhariwal. [«Improved denoising diffusion probabilistic models»](https://arxiv.org/abs/2102.09672) arxiv Preprint arxiv:2102.09672 (2021)

Улучшение DDPM, в котором был предложен новое расписание шума, что улучшило NLL. Также был изучен вариант, в котором дисперсия $\Sigma_{\theta}(x_t, t)$ предсказывается моделью.

- Prafula Dhariwal & Alex Nichol. [«Diffusion Models Beat GANs on Image Synthesis.»](https://arxiv.org/abs/2105.05233) arxiv Preprint arxiv:2105.05233 (2021).

Статья, в которой показывается, что DDPM могут генерировать более качественные картинки по сравнению с GANами. Также был предложен метод conditional сэмплирования. Для этого предобучается классификатор на зашумленных сэмплах, а во время сэмплирования среднее нормального распределения «корректируется» на градиент классификатора.

- Jacob Austin et al. [«Structured Denoising Diffusion Models in Discrete State-Spaces»](https://arxiv.org/abs/2107.03006).arXiv:2107.03006 (2021)

Диффузионные модели на дискретных данных (например, текст). Вместо нормальных распределений используются категориальные. Также была обобщена [мультиномиальная диффузия](https://arxiv.org/abs/2102.05379) с помощью «матриц перехода», которые задают способ зашумления дискретных данных. 

Более подробно: у нас есть $x_t \in \{1, ..., K\}$ — дискретная величина на всех шагах диффузии, тогда для каждого шага $t$ определена **матрица прямого перехода $Q_t$** такая, что $[Q_t]_{ij} = q(x_t = j| x_{t - 1} = i)$. То есть строки матрицы суммируются в единицу. Тогда если обозначить через $\mathbf{x}_t \in \mathbb{R}^K$ one-hot-закодированную версию $x_t$, то прямой процесс можно описать через категориальные распределения:

$$
q(\mathbf{x}_t | \mathbf{x}_{t -1}) = Cat(\mathbf{x}_t; \mathbf{p} = \mathbf{x}_{t - 1}Q_t)
$$

Как и в нормальных распределениях, можем выписать 

$$
q(\mathbf{x}_t | \mathbf{x}_{0}) = Cat(\mathbf{x}_t; \mathbf{p} = \mathbf{x}_{0}\bar{Q}_t), \ \ \ где \ \ \ \bar{Q}_t=Q_1Q_2...Q_t  
$$

$$
q(\mathbf{x}_{t - 1}| \mathbf{x}_t, \mathbf{x}_0) = Cat(\mathbf{x}_{t - 1}; \mathbf{p} = \frac{\mathbf{x}_{t}{Q}^T_t \odot \mathbf{x}_0\bar{Q}_{t-1}}{\mathbf{x}_0\bar{Q}_t\mathbf{x}^T_t})
$$

Поскольку тут нет такой хорошей параметризации через $\epsilon$, как у нормальных распределений, то единственный способ обучать — с помощью KL дивергенции  (членами$L_{VLB}$). 

Остается только понять, как выбирать $Q_t$. Помимо того, чтобы сумма в каждой строчке была один, требуется, чтобы $\bar{Q}_t$ сходилось (при $t \to \infty$) к равномерному распределению в каждой строчке (аналог нормального шума). За конкретными примерами стоит обратиться к статье. 

- Серия работ про text-conditional diffusions: [GLIDE](https://arxiv.org/pdf/2112.10741.pdf), [ImaGen](https://arxiv.org/pdf/2205.11487.pdf), [DALLE-2](https://cdn.openai.com/papers/dall-e-2.pdf)

Опишем работу метода GLIDE. Стоит задача генерировать картинки по заданному текстовому описанию. Для этого используется classifier-free guided diffusion model или [CLIP](https://openai.com/blog/clip/). Это два разных варианта модели, которые авторы сравнивают. В первом случае модель обуславливается на эмбеддинги текста, которые были получены из обучаемого трансформера. Во втором случае guidance осуществляется за счет $\nabla_{x_t} \langle f(x_t), g(c) \rangle$ (это по сути градиент лосса метода CLIP) . Тут $f$ — это картиночный энкодер (на зашумленных картинках), а $g$ — это  энкодер текстового входа. В целом, авторы получили, что classifier-free guidance генерирует более качественные картинки.

- Song et al. [«Score-Based Generative Modeling through Stochastic Differential Equations»](https://openreview.net/forum?id=PxTIG12RRHS)

Способ описать диффузионные модели через стохастические дифференциальные уравнения.

- [What are Diffusion Models?](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#reverse-diffusion-process). Прекрасный блог от Lilian Weng (OpenAI).

  ## handbook

  Учебник по машинному обучению

  ## title

  Диффузионные модели

  ## description

  *null*

- 
  ## path

  /handbook/ml/article/yazykovye-modeli

  ## content

  В 2023 году ChatGPT стал самой узнаваемой языковой моделью машинного обучения во всём мире — причём как среди специалистов, так и среди обычных людей.

Способность вести осмысленный диалог, отвечать на практически любые вопросы и быть применимыми без дообучения в большом спектре задач с высоким качеством — вот залог их  популярности.

В этом параграфе мы расскажем, что такое языковые модели, как они устроены, как развивались, а также как изменились за последнее время.

## Что такое языковые модели?

Говоря простым языком, языковые модели — это алгоритмы, способные продолжать тексты. Если чуть усложнить, то это [вероятностные алгоритмы](https://ru.wikipedia.org/wiki/%D0%92%D0%B5%D1%80%D0%BE%D1%8F%D1%82%D0%BD%D0%BE%D1%81%D1%82%D0%BD%D1%8B%D0%B9_%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC), и к ним сразу можно задать эмпирический критерий качества: хорошая модель даёт разумные продолжения данных ей текстов.

![Artboard](https://yastatic.net/s3/education-portal/media/Artboard_28_f05133b534_16ab8022ff.svg)

Давайте разберём пример выше.

Модель высчитывает вероятность возможных продолжений текста и предлагает их нам. Слово «фрукт» — наименее разумное продолжение нашей фразы, в то время как слово «наука» — наиболее разумное. И действительно, это часть определения машинного обучения, которое мы давали в начале этого учебника.

Таким образом, нам осталось лишь научить алгоритм моделировать эти вероятности и максимизировать их для разумных предложений. Но как это сделать? По ходу развития языковых моделей подходы менялись, мы расскажем о каждом из них в хронологическом порядке.

Начнём с краткого экскурса в историю — поговорим о статистических моделях, рекуррентных нейронных сетях и трансформерах. А затем перейдём к современным — GPT-1, GPT-2, GPT-3, InstructGPT, ChatGPT и LLaMa.

## Развитие языковых моделей

### Статистические модели

Идея модели лежит на поверхности, много где применяется в самых разных вариациях даже в ХХ веке, поэтому сложно назвать авторов или точную дату создания. Однако этот метод популярен до сих пор — используется в клавиатурах смартфонов для исправления опечаток и быстрого набора текстов через Т9.

Теперь подробнее о методе.

Напомним вероятностную формулировку цепей Маркова в общем виде:

$$P(w_1,w_2, ..., w_N) = P(w_N | w_1,w_2, ..., w_{N - 1}) \times P(w_1,w_2, ..., w_{N - 1}) = P(w_N | w_1,w_2, ..., w_{N - 1}) \times P(w_{N-1} | w_1,w_2, ..., w_{N - 2}) \times ... \times P(w_2| w_1) \times P(w_1)
$$

Если представить, что $w_i$  — это слово, а набор этих омега — это предложение, то по формуле становится возможным посчитать вероятность предложения $w_1, w_2, ..., w_N$ С практической точки зрения всё чуть сложнее, ведь распределение слов в реальном языке (какое, с какими и как часто встречается), вообще говоря, неизвестно.

Его принято аппроксимировать на основе **корпуса текстов** (например, всего интернета) — в этом случае считаются совстречаемости слов друг с другом, и по ним считаются вероятности.

В условной вероятности число переменных, от которых зависит распределение следующего слова, называется контекстом. Например, в выражении $P(w_N | w_1,w_2, ..., w_{N - 1})$ длина контекста равна $N - 1$. На практике же редко считают вероятности с контекстом больше трёх, на это есть несколько причин:

1. Сложность в подсчёте и хранении каждого возможного уникального контекста длины $K$. Если корпус текстов состоит из $N$  различных слов, то стоимость хранения счётчиков встречаемости для выбранной длины контекста равна $N^K$, что очень много при больших $K$.
2. Большой контекст реже встречается. То есть слова «яблоку», «негде» и «упасть» поодиночке встречаются чаще, чем их комбинация «яблоку негде упасть». Отсюда достаточность статистик падает с ростом длины контекста.

В учебном примере предлагается ограничиться шириной контекста размера 1:

$$P(w_1,w_2, ..., w_N) = P(w_N | w_{N - 1}) \times P(w_{N-1} | w_{N - 2}) \times ... \times P(w_2| w_1) \times P(w_1)
$$

![Artboard](https://yastatic.net/s3/education-portal/media/Artboard_26_ee99363b0b_e7ae9384f8.svg)

Интересно, что такой подход достаточно популярен до сих пор. Например, он используется в умных клавиатурах, чтобы подсказать следующее слово.

**Достоинства статистических моделей:**

- Простота имплементации.
- Высокая скорость работы алгоритма.
- Низкая вычислительная стоимость обучения и инференса.

**Недостатки статистических моделей:**

- Не сможет сгенерировать слова, которые не шли подряд в обучающем корпусе.
- Очень маленький контекст.
- Длинные последовательности равновероятны ≈ нулю (в цепях Маркова для длинных последовательностей много множителей меньше нуля, поэтому их произведение уже практически равно нулю для любых множителей). Отсюда алгоритм не может выдавать разумные продолжения большой длины.

### Токенизация

Языковые модели, да и вообще все модели, которые оперируют текстом, используют понятие токена. Токен — это единица текста, которую понимают алгоритмы. В примере выше токен&nbsp;— это отдельное слово $w_i$ (этот подход называется [мешком слов](https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%88%D0%BE%D0%BA_%D1%81%D0%BB%D0%BE%D0%B2)), однако текст можно разбивать на токены и иначе.

![Artboard](https://yastatic.net/s3/education-portal/media/Artboard_29_6f0ec76380_52cf81559a.svg)

Раньше предложение разбивалось на слова по пробелам, знакам препинания, исключались стоп-слова и так далее (назовем это [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)). Но у этого подхода возникали две проблемы с разными словоформами. Они:

- Либо обозначались разными токенами,&nbsp;что не совсем верно, ведь слово-то одно и то же. И получалось, что похожим смыслом обладало сразу несколько токенов.
- Либо приводились к начальной форме — и в итоге терялся падеж, время, число.

Современные токенизаторы построены на алгоритме BPE (Byte Pair Encoding; об устройстве BPE более подробно можно прочитать в&nbsp;[учебнике Лены Войта](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html#bpe)). Решение требует фиксации определённого числа токенов. Как только это сделано, в словарь добавляются все символы из текста, ищутся самые частые их сочетания и снова добавляются. Этот процесс продолжается до тех пор, пока число токенов не станет равно заданному значению.

Токенизатор SentencePiece в определённом смысле совершеннее, чем BPE, — он наследует логику Unigram- и BPE-токенизаторов, иначе работает с пробелами (добавляет `_`  перед соответствующим токеном) и не построен на логике разбиения слов по разделителям.

Поэтому, в отличие от BPE, он способен работать с такими языками, как японский или китайский. Подробнее о его устройстве можно прочитать [здесь](https://arxiv.org/pdf/1808.06226.pdf).

<aside>
💡 Токенизаторы не разделяют входной поток по значимости. Например, число 12345 BPE могут разбить на два токена — 1 и 2345, что явно не соответствует логике написанного выражения. Также они будут неправильно выделять всё число в отдельный токен, так как чисел бесконечное количество. Сейчас используется идея о разбиении всех чисел на цифры, чтобы множеством из десяти токенов представить всё многообразие чисел.

</aside>

### Рекуррентные нейронные сети (RNN)

Появились после статистических моделей, подробнее о хронологии [здесь](https://ru.wikipedia.org/wiki/%D0%A0%D0%B5%D0%BA%D1%83%D1%80%D1%80%D0%B5%D0%BD%D1%82%D0%BD%D0%B0%D1%8F_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B0%D1%8F_%D1%81%D0%B5%D1%82%D1%8C). Рекуррентные нейронные сети концептуально можно описать формулой, где:

$А$ — некоторая модель;

$h_t$  — внутреннее состояние модели на момент времени $t$;

$x_t$ — токен, который сейчас обрабатывается.

Тогда следующий токен $x_{t+1}$ получается так:

$$x_{t+1} = g(h_{t}); \space h_{t} = A(h_{t-1}, x_t)
$$

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/Artboard_8_5b50b2cae3_8b1196ece9.svg" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">ссылка на источник картинки</a>
  </figcaption>
</figure>

Подробно об устройстве RNN мы рассказываем в параграфе [Нейросети для работы с последовательностями](https://academy.yandex.ru/handbook/ml/article/nejroseti-dlya-raboty-s-posledovatelnostyami). Здесь же коротко отметим, что существуют различные модификации рекуррентных сетей, которые усложняют структуру алгоритма $А$, даже добавляют механизм внимания Attention. Если коротко, то он позволяет лучше оценивать взаимосвязи токенов в тексте. Все они в разной степени помогают модели усваивать более длинные и сложные последовательности токенов.

**Достоинства RNN:**

- Высокая скорость инференса и сравнительно низкая стоимость.
- Более качественный текст, чем у моделей на статистиках.
- Теоретически понимает контекст в сотни слов (а с Attention ещё больше).
- Точно учитывает весь контекст документа.

**Недостатки RNN:**

- Невозможность параллельного обучения на многих устройствах, отсюда не получится просто так обучить большую RNN.
- Модель «хорошо помнит» лишь несколько последних токенов контекста (без Attention).
- Проблемы с обучением (exploading/vanishing gradients).

## Трансформеры

Более подробно трансформеры и их устройство описаны в параграфе [Трансформеры](https://academy.yandex.ru/handbook/ml/article/transformery). Последней и наиболее успешной с точки зрения качества оказалась архитектура трансформеров. Она состоит из двух частей: encoder (на изображении слева) и decoder (на изображении справа).

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/Untitled_4_7ec4e62ad5_01f8318233.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://arxiv.org/abs/1706.03762">ссылка на источник картинки</a>
  </figcaption>
</figure>

Изначально был популярен подход обучать части отдельно. Так на базе encoder-блоков были построены [BERT-модели](https://en.wikipedia.org/wiki/BERT_\(language_model\)). Идея обучения звучит несложно: давайте из входного текста замаскируем токеном [MASK](https://academy.yandex.ru/handbook/ml/article/transformery) 15% имеющихся токенов и обучим модель угадывать, какие именно токены были скрыты. Тогда, если модель обучится это делать, она сможет очень хорошо понимать текст.

Таким образом, энкодеры обладают следующими особенностями:

- Анализируют входной текст и связи между токенами.
- Выделяют важные токены для определённой задачи.
- Ничего не генерируют.

На базе декодеров сделаны GPT-модели. Они обучаются предсказывать следующий токен на основе предыдущих. На инференсе, когда очередной токен сгенерирован, он добавляется в контекст, и уже на основе него выбирается новый токен. Таким образом модель:

- генерирует токен за токеном.
- смотрит на весь контекст, архитектурно, нет забывания токенов.
- имеет возможность (как и BERT-модели) обучаться параллельно.
- обладает достаточно высокой вычислительной стоимостью инференса.

<aside>
💡 Контекст в случае трансформеров определяется числом токенов, которые они могут обработать за раз. Архитектурно за понимание контекста отвечает блок Attention, и размеры матриц в нём как раз определяют размер контекста. 

Размер матриц конечен: чем они больше, тем сложнее вычислять блок внимания, поэтому контекст существенно ограничен. На момент написания параграфа разработаны различные модификации Attention, позволяющие растить понимаемый контекст, однако они имеют ряд проблем, с которыми предлагаем ознакомиться читателю [самостоятельно](https://arxiv.org/abs/2303.09752).

</aside>

## Современные подходы

### GPT-1 & GPT-2

Начнём немного издалека, с моделей GPT-1 и GPT-2.

Первая была обучена в 2018 году на 7000 книг и имела размер контекста в 512 токенов. И она сразу получилась довольно сильной: после дообучения на специализированные задачи (бенчмарки) показывала на них лучшее на то время качество.

Так, в задачах CoLA (бенчмарк классификационный, в нём надо определить грамматическую корректность предложения) результат вырос до 45,4 против прежнего результата в 35,0 у RNN. А в [GLUE](https://gluebenchmark.com) — с 72,8 до 68,9.

Вторая модель была обучена в 2019 году. Она состояла из рекордных для того времени 1,5 млрд параметров (то есть была в \~10 раз больше первой), имела контекст в 1024 токена и была обучена на 40 ГБ текстовых данных. GPT-2 снова побеждала предыдущие подходы, включая GPT-1, на многих [бенчмарках](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf).

По сравнению с первой версией модели у второй произошел качественный рост: теперь она могла генерировать разумные тексты — а не только предложения. Правда, не всегда и не с первой попытки.

### GPT-3

GPT-3 стала революцией с точки зрения качества и размеров. В 2020 году была получена модель размером в 175 млрд параметров, она обучалась на 570 ГБ текстовых данных с контекстом в 2048 токенов. Модель [могла](https://arxiv.org/abs/2005.14165) решать целый спектр задач, включая перевод, суммаризацию и ответы на вопросы, с качеством, близким к человеческому уровню, а также отличалась высокой способностью генерировать креативный контент. Демонстрацию работы модели лучше посмотреть в [этой статье](https://arxiv.org/pdf/2005.14165.pdf) на 28 странице и далее.

#|
||

Модель

|

Число обучающих данных

|

Контекст

|

Число параметров

|

Decoder-слои

|

Hidden-size (размерность тензоров внутри модели)

|

Train batchsize (размер батча при обучении)

||
||

GPT

|

7000 книг

|

512

|

117 млн

|

12

|

768

|

64

||
||

GPT-2

|

40 ГБ текстовых данных

|

1024

|

1,5 млрд

|

48

|

1600

|

512

||
||

GPT-3

|

570 ГБ текстовых данных

|

2048

|

175 млрд

|

96

|

12 288

|

3 200 000

||
|#

Модель демонстрировала действительно впечатляющие результаты: собрав обучающие данные, можно было с высоким качеством решить практически любую текстовую задачу.

Однако для применения таких решений остаётся проблема со стоимостью их обучения. Для обучения GPT-2 авторы использовали 16 GPU (иначе говоря —&nbsp;графических процессоров, видеокарт), а для GPT-3 уже 3200. Для дообучения модели под определенную задачу, конечно, понадобится меньше ресурсов, но всё равно достаточно много.

Что с этим делать? Использовать подводки.

## Подводки

### Few-shot обучение

Оказывается, что обучать большие языковые модели решать определённые задачи не всегда нужно (как мы говорили ранее, это ресурсоёмко): можно составить `few-shot` подводку. Подводка — словесное описание поставленной задачи, составленное определенным образом.

Представим, что мы хотим осуществить перевод с английского на французский. Для обучения нам необходимо было бы составить пары $(X,y)$, где $X$ — слово на английском, а $y$ — на французском. Сделаем иначе — опишем задание на естественном языке:

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/Untitled_5_5954076cc1_9266fdd720.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://arxiv.org/pdf/2005.14165.pdf">ссылка на источник картинки</a>
  </figcaption>
</figure>

Здесь на английском языке сформулировано задание и предлагается слово «cheese» перевести на французский. Назовем такую конструкцию `zero-shot`-примером. Такой запрос GPT-3, возможно, поймёт, но работать будет плохо.

Давайте увеличим количество примеров в подводке и назовем эту конструкцию `one-shot`:

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/Untitled_6_c4d7ff5bbe_7d9177e227.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://arxiv.org/pdf/2005.14165.pdf">ссылка на источник картинки</a>
  </figcaption>
</figure>

Или больше, и это будет `few-shot`:

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/Untitled_7_f46df68173_5889d09b18.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://arxiv.org/pdf/2005.14165.pdf">ссылка на источник картинки</a>
  </figcaption>
</figure>

При этом приёме не тратятся ресурсы на обучение модели, она лишь смотрит на контекст и генерирует продолжение. Оказывается, этого достаточно, чтобы сравняться с downstream-обучением. Продемонстрируем преимущество такого подхода на двух бенчмарках.

- TriviaQA — вопросно-ответный бенчмарк, составленный на основе Википедии. Он помогает оценивать знания модели и ее ответы на вопросы.
- Lambada — оценивает меморизацию длинного контекста модели. Чем выше скор, тем лучше модель на обоих бенчмарках.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/Artboard_15_d87b0ec3d3_709e99da30.svg" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://yastatic.net/s3/education-portal/media/Artboard_16_5f093e4e0e_89accafbce.svg">ссылка на источник картинки</a>
  </figcaption>
</figure>

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/Untitled_9_775c9a32b8_86cc9fc37b.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://arxiv.org/pdf/2005.14165.pdf">ссылка на источник картинки</a>
  </figcaption>
</figure>

Графики выше демонстрируют несколько особенностей:

- `Few-shot` позволяет получать качество, сравнимое с дообучением на определённом датасете, и стремится к человеческому качеству.
- С ростом числа обучаемых параметров модели растет её качество.
- На правом графике `few-shot`-примеры начинают работать лучше `zero-shot`-примеров лишь с некоторого размера модели. Это говорит о том, что модель начинает демонстрировать «умные» свойства лишь начиная с некоторого размера.

<aside>
💡 На самом деле последний пункт достаточно часто встречается в языковых моделях. Случается так, что определённые приёмы не работают с маленькими моделями, но показывают себя лишь на больших. 

Это можно назвать фазовым переходом, когда языковая модель вместе с увеличением размера и числа пройденных текстов на обучении обретает б**о**льшую обобщающую способность.

</aside>

### Формулировка имеет значение

`Few-shot` действительно полезен и помогает получать от модели нужный результат без обучения, но всё же недостаточно хорошо.

Предположим, мы хотим узнать у модели, как приготовить любимое блюдо. Пусть это будет лазанья:

![Artboard](https://yastatic.net/s3/education-portal/media/Artboard_25_e3e34eb28f_affbc5f625.svg)

Можно заметить, что запрос к модели можно задать по-разному, но ответ ожидается обычно какой-то конкретный. Авторы [этой статьи](https://arxiv.org/pdf/2102.09690.pdf) заметили, что сама по себе конструкция `few-shot`-примера не приводит к стабильному результату. Качество решения задачи очень зависит от:

1. Текстового описания задачи.
2. Числа примеров в подводке.
3. Порядка, в котором примеры следуют друг за другом в подводке.
4. Формате составления `few-shot`.

Чтобы улучшить качество решения задачи, авторы предлагают осуществлять калибровку подводок. В статье они заметили, что модели смещены относительно подводок, то есть переформулировка запроса ведёт к смещению в ответе модели, а также к росту разброса ответов.

Например, модели задают вопрос и её задача — ответить «да» или «нет». Если `few-shot` состоит из четырёх примеров и они идут в порядке «да», «да», «нет»,  «нет», то, вероятнее всего, дальше модель ответит «нет» на любой вход, просто потому что слово «нет» встречалось последним.

Калибровать модель предлагается с помощью выученного линейного преобразования:

$$\hat{q} = softmax(W\hat{p} + b)
$$

В этом преобразовании:

$W$ и $b$ — обучаемые;

$\hat{p}$ — вероятности на выходе модели;

$\hat{q}$ — откалиброванные вероятности;

Обучающие данные собираются так:

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/Untitled_11_baded546e5_7f0910407f.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://arxiv.org/pdf/2102.09690.pdf">ссылка на источник картинки</a>
  </figcaption>
</figure>

Для различных задач собираем подводки и добавляем нейтральное слово N/A. В этом примере несмещённая модель должна давать с вероятностью 50% ответ «positive» или «negative».

Чтобы добиться такого распределения ответов у смещённой модели, представим:

$$W = diag(\hat{p})^{-1}, \space b = 0
$$

Также все `few-shot`-примеры стандартизуются в специальный формат вопрос — ответ, как на картинке выше.

Этот метод (синий график) по сравнению со стандартными `few-shot`-примерами (красный график) помог повысить качество и уменьшить разброс результата. Таким образом, оптимизировав всего 4 параметра, авторы существенно улучшили итоговый результат.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/Artboard_17_9383aa3031_8186a854cf.svg" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://arxiv.org/pdf/2102.09690.pdf">ссылка на источник картинки</a>
  </figcaption>
</figure>

### Promt-tuning

Качество работы модели зависит от подводки, и `few-shot` просто один из способов её построения. Эксперименты показывают, что грамотный подбор промта позволяет экономить на обучении и решать задачи с высоким качеством. Проблема в обучении больших моделей — нехватка оперативной памяти на GPU, поэтому не будем оптимизировать все параметры модели.

Пусть необходимо решить задачу $А$, к ней имеется обучающее множество вида $(X,y)$. Введём дополнительные токены, которых не было в словаре: $<P_1>, <P_2>, … , <P_k>$ — и будем добавлять в каждый текст из X согласно какому-то правилу.

Правило может быть таким: имеем 20 спецтокенов, добавим токены 1–10 в начало строки, а 11–20 в конец.

Тогда, можно «заморозить» все параметры в модели, кроме этих токенов, и сэкономить на обучении. Если токенов 100 и каждый из них имеет размерность в 1024, то необходимо [оптимизировать](https://aclanthology.org/2021.emnlp-main.243.pdf) лишь 100 тысяч параметров вместо 175 млрд в случае обучения всей модели.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/Artboard_20_510dffa69e_dc695b3dc6.svg" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://habr.com/ru/companies/sberdevices/articles/596103/">ссылка на источник картинки</a>
  </figcaption>
</figure>

<aside>
💡 Эффект от такого трюка достаточно многогранен:

1. Меньше обучаемых параметров — меньше памяти занимает модель.
2. Меньше обучаемых параметров — быстрее происходит обучение.
3. Обычно нужно сильно меньше обучающих данных, чем при традиционном обучении всей модели для достижения высокого качества.
4. Высокое качество результата.

</aside>

Получается, что можно оптимизировать подводку, или, другими словами, находить наиболее оптимальный промт, который лучше прочих решает поставленную задачу.

### Как повысить качество решения задач из разных категорий

Языковые модели призваны решать самый широкий спектр текстовых задач — вопросно-ответные, суммаризацию, диалоговость, перевод и многие другие.

Получается, что модель должна после некого обучения (подбора подводки или оптимизации вообще всех параметров под каждую задачу) решать каждую из них на высоком уровне. Однако модель обычно учится на текстах из интернета, книгах и других доступных ресурcах. И формат задачи, который обычно требуется от модели, не соответсвует тому, что алгоритм привык видеть на обучении. К этому стоит добавить, что среди веб-документов просьба что-то сократить или определить тональность документа встречается не очень часто.

Исправить этот недостаток призваны подходы по генерализации языковых моделей: [FLAN](https://arxiv.org/abs/2109.01652) и [T0](https://arxiv.org/pdf/2110.08207.pdf). Инструкции даются на естественном языке и для подготовки качественного обучающего множества предлагается произвести следующие действия:

- Каждой отдельной задаче (будь то перевод, написание отзывов или суммаризация) пишется по несколько различных подводок, отражающих смысл задания.
- Итоговый датасет составляется из отдельных задач, все строчки датасета перемешиваются случайным образом.
- Авторы стараются собрать как можно более разнообразные задачи в обучающее множество.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/image_c59be1810f_9c218e10c8.svg" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://arxiv.org/pdf/2109.01652.pdf">ссылка на источник картинки</a>
  </figcaption>
</figure>

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/Untitled_15_cf6d62ddc7_86774d99a6.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://arxiv.org/pdf/2110.08207.pdf">ссылка на источник картинки</a>
  </figcaption>
</figure>

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/Artboard_14_a891eaf1bb_7e62044341.svg" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://arxiv.org/pdf/2109.01652.pdf">ссылка на источник картинки</a>
  </figcaption>
</figure>

Две картинки сверху демонстрируют FLAN- и T0- подходы по созданию датасета, а картинка снизу — рост усреднённого качества модели после обучения на смеси. Таким образом с некоторого размера модели наблюдается повышение метрик качества при дальнейших дообучениях генерализованной модели на отложенных задачах.

### Chain-of-Thought

Предыдущий подход со смесью датасетов помогает решать многие задачи в среднем заметно лучше. Однако есть задачи, где качество результатов модели всё ещё низкое. Например, предложить эффективный код, решающий некую алгоритмическую задачу, найти минимум некоторой аналитической функции потерь, посчитать производную фукнции в точке и так далее.

Такие вопросы требуют рассуждения, которое модель не может просто так провести из-за своей архитектуры. Выход — составить подводки в стиле `Chain-of-Thought (CoT)`**:**

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/Untitled_17_6d70a2cc66_1d6ba81a5b.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://arxiv.org/pdf/2201.11903.pdf">ссылка на источник картинки</a>
  </figcaption>
</figure>

`CoT`-подводка состоит из трёх обязательных элементов:

- Формулировки задачи на естественном языке.
- Подробного пошагового решения.
- Ответа на задачу.

Формирование такого промта, особенно на `few-shot`, заставляет модель рассуждать, как можно правильно решить задачу. Авторы [этой статьи](https://arxiv.org/pdf/2201.11903.pdf) сравнили на двух математических бенчмарках способность модели решать сложные задачи.

- MultiArith — проверяет умение решать простые арифметически задачки.
- GSM8K — более сложные.

Результаты демонстрируют, что наличие `CoT` в подводке увеличивает способность решать математические задачки у больших языковых моделей.

![Artboard](https://yastatic.net/s3/education-portal/media/Artboard_27_cc8f312f50_1540104611.svg)

## InstructGPT

Наконец, обсудив, как готовить обучающие данные, перейдем к прародителю ChatGPT. Инструкционная модель — это та, которая обучена отвечать на пользовательские запросы в режиме `zero-shot` (а вообще, и `few-shot`, и любой человекочитаемый формат) с высоким качеством.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/Untitled_19_a92859782e_e98ea7dd76.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://arxiv.org/abs/2203.02155">ссылка на источник картинки</a>
  </figcaption>
</figure>

InstructGPT — это модель, и она интересна с точки зрения выработки концепции обучения всех инструкционных моделей (InstructGPT, ChatGPT, GPT-4 и других). С некоторыми нюансами обучение состоит из четырех этапов:

1. Подготовка качественного претрейна. Языковая модель должна содержать в себе как можно больше знаний о мире, чтобы иметь возможность в последующем решать произвольные задачи с высоким качеством. На этом этапе необходимо озаботиться наибольшим разнообразием, чистотой и полнотой обучающих данных. Подробнее об этом мы поговорим в последнем разделе этого параграфа.
2. SFT (supervised finetuning) — обучение модели следовать инструкциям. Этот пункт мы подробно обсудили в предыдущей части параграфа (T0, FLAN, CoT). На этом этапе важно составить грамотный инструкционный датасет, где инструкция содержит произвольные запросы к модели, а ответ на неё — подробный текст, которым будущий пользователь будет доволен. Грамотный сбор таких данных довольно дорогостоящий процесс, но от него напрямую зависит, каким образом модель будет взаимодействовать с пользователем.
3. Обучение reward-модели. Каждый ответ алгоритма можно оценить с точки зрения вежливости, подробности или персонажности. Персонажность позволяет модели считать себя, например, капитаном Джеком Воробьем и общаться на пиратском говоре. Также есть менее формализуемые критерии качества ответов, их даже сложно описать словами. Например, что в основном людям ответ 1 нравится больше чем ответ 2.
   Reward-модель агрегирует эти кртитерии в число — меру качества. Чем оно выше, тем качественнее ответ модели. Для выравнивания поведения модели обычно важно уметь оценивать тысячи текстов, а вручную это делать дорого и долго, поэтому обучается специальная модель-оценщик. Про то, как обучать reward-модель, будет рассказано далее.
4. Этап Reinforcement Learning (RL). На нём языковая модель обучается генерировать такие ответы, которые имели бы наивысшую оценку относительно reward-модели. Про то, как делать RL, будет рассказано далее.

## ChatGPT

Одна из самых нашумевших языковых моделей в мире наследует логику обучения Instruct GPT. Основные отличия от последней заключаются в:

- Диалоговости. Модель обучена работать с диалогами, держать их в контексте и помнить историю того, что требовал пользователь. Обучение производится посредством сбора/написания диалоговых данных.
- Размере и качестве инструкционного датасета.
- Том, что больше внимания уделено разметке и обучению reward-модели и этапу с RL.

К сожалению, OpenAI не предоставили детали обучения ChatGPT, а предложили лишь общий ход действий. Также неизвестны архитектурные параметры модели.

## Как обучить свою LLM?

Обсудим детально на примере доступных в open-source моделей семейства LLaMA.

### LLaMa

В качестве примера возьмём самую свежую архитектуру трансформеров на первую половину 2023 года — [LLaMa](https://arxiv.org/pdf/2302.13971.pdf), а также способы превращать её в чатовую модель, проводить Alignment на примере [LLaMa-2](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/). Вторая модель архитектурно не отличается от первой (кроме увеличенного контекста до 4096 токенов), поэтому содержание статей можно объединить в один рассказ.

### Претрейн

Для обучения с нуля качественной языковой модели необходимы:

1. мощный кластер на сотни видеокарт, на котором можно производить параллельное обучение модели. Больше GPU — больше модель можно обучить и быстрее по времени обучения;
2. терабайты текстовых данных для тренировки на них;
3. архитектура, которая лучшим образом может моделировать язык.

Поговорим подробнее о двух последних пунктах.

**Текстовые данные**

Текстовые данные можно брать из открытых источников, таких как CommonCrawl, C4, Taiga и прочее. Важно обеспечить:

- чистоту данных — например, убрать html-тэги, устранить дублирование текстов;
- полноту — чтобы модель одинаково хорошо решала математические задачи, писала код или сочиняла стихотворения, текстов соответствующих доменов должно быть в достатке в обучающем корпусе;
- разнообразие данных.

Существуют эмпирические законы обученности модели, но здесь остановимся на числе пройденных за обучение токенов. В LLaMa-моделях это значение варьируется от 1T до 2Т. Ниже приведены основные параметры по числу размерности внутренних эмбедингов, числу голов Attention, слоёв и параметров обучения разных моделей:

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/Artboard_24_50eed180aa_353d8fabf6.svg" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://arxiv.org/pdf/2302.13971.pdf">ссылка на источник картинки</a>
  </figcaption>
</figure>

**Архитектура**

У LLaMa-моделей предлагается целый ряд архитектурных изменений. Так как в учебнике рассматривался лишь базовая архитектура трансформеров, то опишем, что в ней необходимо изменить, чтобы получить LLaMa-модель.

1. **Pre-нормализация**.

    <figure>
 <img src="https://yastatic.net/s3/education-portal/media/Artboard_7_c57422a1c6_83b3d48f85.svg" loading="lazy" decoding="async" alt="">
 <figcaption>
 <p>Обычно используется <a href="https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html">LayerNorm</a>, а в LLaMa — RMSNorm.</p>
 </figcaption>
 </figure>

   Пусть $x \in \R^m, y \in \R^n.$  Тогда нелинейное преобразование в общем виде выглядит так:

   $$a_i = \sum_{j=1}^{m} w_{ij}x_{j}, y_i = f(a_i + b_i)
   $$
   
   И LayerNorm можно описать следующими формулами:

   $$\mu = \frac{1}{n}\sum_{i=1}^n a_i, \space \sigma = \sqrt{\frac{1}{n}\sum_{i=1}^n (a_i - \mu) ^ 2}
   \\
   y_i = f(\frac{a_i - \mu}{\sigma}g_i + b_i)
   $$
   
   В свою очередь экспериментально RMSNorm демонстрирует лучшие результаты в сравнении с  LayerNorm и высчитывается так:

   $$y_i = f(\frac{a_i}{\sqrt{\frac{1}{n}\sum^{n}_{i=1} a_i^2}}g_i + b_i)
   $$
   
   

2. SwiGLU-активация используется вместо ReLU. $⊗$ — значок поэлементного умножения матриц.

   $$ReLU(x) = max(0, x)
   \\
   SwiGLU(x, W, V, b, c, β) = Swish_β(xW + b) ⊗ (xV + c)
   \\
   Swish_β(x) = x \sigma(βx)
   $$
   
   

3. Роторные эмбединги. Информацию о том, в каком порядке следуют токены внутри модели, хранят в себе позиционные эмбединги. Они могут быть абсолютными (кодирование синусами и косинусами, как описано в параграфе о [трансформерах](https://academy.yandex.ru/handbook/ml/article/transformery)) или относительными (кодируется расстояние между каждой парой токенов).

   Роторные эмбединги позволяют вычислять относительную связь между парой токенов на этапе вычисления Attention, также они выигрывают по сравнению с относительными в совместимости kernel-ов. То есть, одно из понятных не технических отличий их от других — вычисление позиционной информации на каждом слое модели при подсчёте Attention, а не только перед первым слоем. Это позволяет на каждом слое явно обрабатывать информацию об относительном расположении токенов. Роторные эмбединги показывают лучшее качество на многих задачах и являются стандартом для обучения языковых моделей. Подробнее о них можно почитать в [этой статье](https://arxiv.org/pdf/2104.09864v4.pdf).

Существуют также техники ускорения обучения моделей и оптимизации использования памяти, но с этим предлагаем читателям ознакомиться самостоятельно.

### SFT (supervised finetuning)

Второй этап обучения инструкционных языковых моделей требует множество инструкций. Рецепт как их готовить был подробно описан в [середине этого параграфа](https://academy.yandex.ru/handbook/ml/article/yazykovye-modeli#podvodki). Снова проговорим, что для написания инструкций или сбора датасета необходимо, чтобы инструкции были:

- разнообразными;
- качественными;
- имели одинаковый формат, чтобы чатовая модель могла обучиться диалоговости (где вопрос пользователя, где ее ответ);
- информативными;
- подробными;
- `Chain-of-Thought (CoT)`, `few-shot` и так далее.

### Reward-модель

Третий этап в создании инструкционных моделей. Есть несколько способов собрать датасет для обучения reward-модели. Он должен содержать тексты и метки к ним. Если меток много (например, в случае балльной оценки), можно использовать разновидности [ранжирующих лоссов.](https://gombru.github.io/2019/04/03/ranking_loss/) Разберем способ обучения модели на бинарную оценку.

Пусть модели подается на вход инструкция $x$. Поменяв температуру, способ сэмплирования или использовав разные чек-пойнты модели, возможно получить два разнообразных ответа $y_1$ и $y_2$. Не ограничивая общность, предположим, что, согласно некоторым предпочтениям, асессоры или пользователи установили, что первый ответ лучше второго.

Проделаем эту операцию много раз и получим обучающее множество, состоящее из $\{x^i, y_1^i, y_2^i\}_{i = 1} ^ N$. Тогда reward-модель можно обучать минимизацией следующей функции потерь:

$$\mathcal{L}_{ranking} = -log(\sigma(r_{\theta}(x, y_1) - r_{\theta}(x, y_2) - m(r)))
$$

Где:

$r_{\theta}$  — reward-модель с обучаемыми параметрами тета;

$m(r)$ — некий margin, который определяет, насколько сильно модель должна отделять хороший и плохой ответы друг от друга.

### **RL (Reinforcement Learning**)

На четвёртом этапе, этапе выравнивания модели, можно воспользоваться разными алгоритмами. LLaMa-2 Chat была обучена последовательно сначала на Rejection Sampling fine-tuning (RL «для бедных») и Proximal Policy Optimization (PPO).

**Rejection Sampling fine-tuning**. Этот подход основан на довольно простой стратегии. Пусть имеется инструкция $x$. Сгенерируем для неё $N$ ответов и выберем тот, который получает наивысшую оценку у reward-модели. График ниже демонстрирует, что чем больше $N$, тем больше reward-score у лучшего ответа. Собрав пары `инструкция — лучший ответ`, можно обучить на них языковую модель и провести таким образом выравнивание поведения модели.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/Artboard_18_4698578595_cac97c6825.svg" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/">ссылка на источник картинки</a>
  </figcaption>
</figure>

**Proximal Policy Optimization**. Для лучшего понимания происходящего советуем прочесть параграф, посвященный [RL](https://academy.yandex.ru/handbook/ml/article/obuchenie-s-podkrepleniem).

Основная задача, как обычно, следовать некой политике, которая лучшим образом отражает human feedback. Политика — наша итоговая модель, value-функция оценивает средний reward в текущем состоянии (обычно это та же самая модель с линейным слоем поверх).

Формализуем термины из RL для задачи выравнивания языковой модели:

- Политика $\pi$ — обучаемая языковая модель.
- Value-функция $V_{\pi}(s_t)$ — обычно та же самая модель с линейным слоем поверх, оценивает средний reward, если действовать из состояния $s_t$ согласно политике $\pi$.
- $s_t$ — состояние в момент времени $t$. Это весь контекст $t$-токенов, которые модель успела сгенерировать к текущему моменту.
- $a_t$ — действие из текущего состояния в момент времени $t$. Обозначает следующий токен, который будет сгенерирован.
- $\tau$ — траектория, т. е. тройки $\{s_i, a_i, r_i\}_{i = 0}^{\infty}$, — это состояния генерируемого токена и награды за него

Сразу можно сделать вывод, что в языковых моделях $s_{t+1} = concat([s_t, a_t])$, $Q(s_t, a_t) = V(s_{t+1}).$

Также, в RL символом $\infty$ обозначается вся последовательность токенов, то есть на практике сюда можно подставлять количество сгенерированных токенов.

Инициализируем $\theta_0, \psi_0$ — начальные веса политики и value-функции

Для $n = 0, 1, 2, …, N:$

1. Соберем коллекцию траекторий $D_n = \{\tau_i\}$ , следуя политике $\pi(\theta_n)$.

2. Посчитаем $A(s_t, a_t) = Q(s_t, a_t) - V(s_t)$. Эта формула отражает разницу между финальной наградой за выбранное действие $a_t$ в текущем состоянии $s_t$ и средней финальной наградой, которую можно было бы получить в этом состоянии. Вообще говоря, с помощью метода Generalized Advantage Estimation ([GAE](https://arxiv.org/abs/1907.00456)) её можно аппроксимировать следующим выражением:

   $$\hat{A_t} = r_t + \gamma V_{\theta_n}(s_{t+1}) - V_{\theta_n}(s_{t}) 
   $$
   
   

3. Обновляем веса политики согласно одному из лоссов PPO. Например, используем такой:

   $$\theta_{n+1} = \underset{\theta}{argmax} \space \mathbb{\hat{E}}_t \left[ \frac{\pi_{\theta_n}(a_t | s_t)}{\pi_{\theta_{n-1}}(a_t | s_t)} \hat{A}_t \right] - \beta KL(\pi_{\theta_{n-1}}(\cdot | s_t), \pi_{\theta_{n}}(\cdot | s_t))
   $$
   
   

4. С помощью MSE лосса оптимизируем значение value-функции:

   $$\mathcal{L}(\psi) = \hat{\mathbb{E_t}} \left[ || V_{\psi_n}(s_t) - \hat{R_t}|| ^ 2 \right]
   \\
   \hat{R_t} = \sum_{l=0}^{\infty} \gamma^l r_{t+l}
   $$
   
   

## Итог

Мы с вами обсудили, как развивались языковые модели, какие приёмы и техники необходимы для успешного обучения инструкционных моделей. Также на примере архитектуры LLaMa разобрали, как самостоятельно обучить языковые модели с нуля.

  ## handbook

  Учебник по машинному обучению

  ## title

  Языковые модели

  ## description

  *null*

- 
  ## path

  /handbook/ml/article/intro-recsys

  ## content

  
## Где можно встретить рекомендательные системы?

С рекомендательными системами можно столкнуться там, где есть большое множество товаров и пользователей, которые хотят найти нужные для себя товары. Рекомендательные системы помогают отобрать наиболее релевантные для пользователя объекты, тем самым экономя его время. Приведём несколько примеров:

- YouTube рекомендует пользователям видео;
- На сайтах интернет-магазинов можно встретить блоки с рекомендациями товаров;
- Музыкальные сервисы наподобие Spotify или Яндекс.Музыки рекомендуют музыкальные треки.

![recsys_example_c3d1b1a019.webp](https://yastatic.net/s3/education-portal/media/recsys_example_c3d1b1a019_7aee860313.webp)

Что такое «релевантные для пользователя товары» – это нетривиальный вопрос, который решается отдельно для каждой задачи исходя из бизнес-логики.

#### Поиск vs рекомендации

Отметим, что, хотя задачи поиска и рекомендаций кажутся похожими и, как мы увидим, могут использовать схожие методы, у них есть одно важное отличие: в задаче поиска есть сформулированный запрос от пользователя, а в задаче рекомендаций явного запроса нет, есть только история взаимодействий пользователя с объектами и наша надежда на то, что мы верно распознали его скрытые желания. Это различие объясняет некоторые особенности дизайна рекомендательных систем, которые мы подробнее обсудим в конце этого параграфа, при разборе классического пайплайна рекомендательной системы.

## Формализация задачи

### Explicit и Implicit feedback

Введём ряд обозначений. Пусть у нас есть множество пользователей $U$ и множество объектов $I$. Для каждого пользователя $u \in U$ есть множество объектов $I_u \subset I$, с которыми он взаимодействовал и которым поставил рейтинги $R_u = \left(r_{ui}\right)_{i \in I_u}$. Рейтинг (его также называют фидбеком) – это некоторая характеристика взаимодействия пользователя с объектом; про него можно думать, как про некоторый таргет, который мы выбрали для оптимизации рекомендательной системы.

Таким образом, задачу рекомендательных систем можно переформулировать в следующем виде: для каждого пользователя $u \in U$ необходимо оценить значение $r_{ui}$ для $i \in I \setminus I_{u}$ и выбрать несколько товаров с наибольшим $\hat{r}_{ui}$. Иными словами, надо научиться среди непоказанных пользователю товаров находить те, которые заинтересовали бы его больше всего.

Приведем несколько примеров фидбека:
- Для товара – факт добавления в корзину;
- Для музыки – дослушали ли трек до конца;
- Для статьи – лайк/дизлайк;
- Для видео – время его просмотра или факт просмотра, например, наполовину.

Как правило, фидбек разделяют на два типа – explicit и implicit. Из-за различия для каждого фидбека есть разные техники обработки и использования, которые будут обсуждаться в параграфе про [матричные факторизации](https://academy.yandex.ru/handbook/ml/article/matrichnaya-faktorizaciya).

**Explicit, или явный фидбек** – это такие действия пользователя, по которым точно можно понять, понравился ли ему объект. Это может быть оценка, поставленная, фильму, лайк/дизлайк к видео или рецензия на купленный товар. Такого фидбека очень мало, но он наиболее точно характеризует отношение пользователя к товару.

**Implicit, или неявный фидбек** – это любая другая информация о действиях пользователя на сайте. Он выступает в качестве прокси к явному фидбеку. Например, факт того, что пользователь досмотрел видео до конца, не говорит о том, понравилось ли оно ему, однако можно сделать предположение, что большинству досмотревших видео до конца оно понравилось. Приведем основные примеры неявного фидбека: клик на статью, время просмотра видео, покупка товара. Обычно такого сигнала в разы больше, чем явного, однако он более шумный, и не стоит доверять ему так же, как явному. Например, при оптимизации кликов на статью может получиться так, что рекомендательная система научится находить кликбейт, а не интересные пользователю статьи – это может плохо отразиться на сервисе в долгосрочной перспективе.

### Ранжирующая модель

Задачу построения рекомендательной системы можно формулировать в качестве задачи классификации (клик/не клик) или регрессию (сколько звёзд пользователь поставит объекту), но это не самые распространённые стратегии.

Обратим внимание, что нам на самом деле не обязательно уметь точно оценивать рейтинги $r_{ui}$. Достаточно уметь для пользователя и набора объектов генерировать перестановку этих объектов в порядке убывания рейтинга. Модель, решающую данную задачу, называют **ранжирующей**. 

Опишем классический пайплайн применения ранжирующей модели для одного пользователя. На вход подаются признаки пользователя и объекта, и для пары пользователь-объект на основе этих признаков выдается некоторое число, ответ модели. Далее мы сортируем объекты в порядке его убывания. Из полученной перестановки обычно берут несколько первых объектов для показа пользователю.

Более подробно о том, как решается задача ранжирования, вы можете прочитать в соответствующем параграфе.

## Коллаборативная фильтрация

![Artboard_22_590bae5dfb.svg](https://yastatic.net/s3/education-portal/media/Artboard_22_590bae5dfb_483ab4a53a.svg)

Рассмотрим матрицу взамодействий пользователя, приведённую выше. Что можно порекомендовать Кате, исходя из исторических данных? Можно заметить, что взаимодействия Кати похожи на взамодействия Пети (так как они оба лайкали объекты 1 и 8). Иными словами, их интересы в чём-то похожи, поэтому Кате можно порекомендовать, например, объект 3 (так как он понравился Пете). Можно проделать аналогичное упражнение с Петей и сделать вывод, что ему не стоит рекомендовать объект 10.

Можно решать и транспонированную задачу: для лайкнутого пользователем объекта искать похожие, то есть те, которые пользователи достаточно часто лайкали вместе с ним. Например, объекты 1 и 8 похожи друг на друга, так как их лайкали одни и те же пользователи, и точно так же похожи 1 и 3.

Проиллюстрированный выше подход называют **коллаборативной фильтрацией**. Он объединяет семейство методов рекомендаций, использующих сходство по истории взаимодействия между пользователем и товаром. Рассмотрим конкретные простые методы коллаборативной фильтрации.

### User2User рекомендации

Введём меру похожести двух пользователей $s(u, v)$, которая тем больше, чем выше сходство между $u$ и $v$. Для пользователя $u$ рассмотрим множество похожих на него пользователей $N(u) = \{ v \in U \setminus \{u\} \vert  s(u, v) > \alpha  \},$ где $\alpha$ – настраиваемый гиперпараметр, в чём-то аналогичный порогу бинарного классификатора.

Допустим, мы хотим теперь оценить рейтинг $r_{ui}$, который пользователь $u$ поставил бы объекту $i$. Сделаем это, опираясь на рейтинги, которые ставили похожие на $u$ пользователи. Например, можно взять взвешенное среднее:

$$
\hat{r}_{ui} = \frac{\sum_{v \in N(u)} s(u, v) r_{vi}}{\sum_{v \in N(u)} \lvert s(u, v) \rvert}
$$

Модуль добавляется для того, чтобы корректно обработать непохожих пользователей, то есть с пары с отрицательной похожестью, которая может возникнуть, если при построении $N(u)$ взять достаточно маленькое $\alpha$.

Можно пойти дальше и усовершенствовать метод оценивания. У пользователей могут быть разные диапазоны оценок: кто-то ставит почти всегда в диапазоне 1-3, а кто-то предпочитает ставить 4-5. Иными словами, для разных пользователей оценка «нормально» (и соответственно, оценки «хорошо» и «плохо») могут соответствовать разным значениям рейтинга. Для устранения этой проблемы, можно брать не сырой рейтинг пользователя $r_{vi}$, а его отклонение от среднего всех оценок пользователя: $r_{vi} - \overline{r}_{v}$. Таким образом, мы учитываем только разброс вокруг среднего и итоговая оценка будет выглядеть так:

$$
\hat{r}_{ui} = \overline{r}_u + \frac{\sum_{v \in N(u)} s(u, v) \left(r_{vi} - \overline{r}_v\right)}{\sum_{v \in N(u)} \lvert s(u, v) \rvert}
$$

Можно пойти еще дальше и учесть дисперсию оценок пользователей:

$$
\hat{r}_{ui} = \overline{r}_u + \sigma_{u} \frac{\sum_{v \in N(u)} s(u, v) \left(r_{vi} - \overline{r}_v\right) / \sigma_v}{\sum_{v \in N(u)} \lvert s(u, v) \rvert}, \, \text{где} \, \sigma_{u} = \sqrt{ \frac{1}{I_u} \sum_{i \in I_{u} } \left(r_{ui} - \overline{r}_u\right) ^ 2},
$$

где $I_{u}$ – множество объектов, с которыми взаимодействовал пользователь $u$.

В заключение приведём несколько вариантов оценки схожести пользователей:

- Мера Жаккара: $s(u, v) = \frac{\lvert P_u \cap P_v \rvert}{\lvert P_u \cup P_v \rvert},$ где $P_u$ – множество понравившихся $u$ айтемов;
- Скалярное произведение общих рейтингов: $s(u, v) = \sum_{i \in I_u \cap I_v} r_{ui} r_{vi}$;
- Корреляция Пирсона:

$$
s(u, v) = \frac{\sum_{i \in I_u \cap I_v} (r_{ui} - \overline{r}_u)(r_{vi} - \overline{r}_v)}{\sqrt{\sum_{i \in I_u \cap I_v} (r_{ui} - \overline{r}_u)^2} \sqrt{\sum_{i \in I_u \cap I_v} (r_{vi} - \overline{r}_v)^2}}
$$

- Дисконтированная корреляция Пирсона. Так как айтемов в пересечении $I_u \cap I_v$ в действительности не всегда может быть достаточно много, можно дисконтировать похожести, посчитанные по небольшому множеству айтемов, домножая корреляцию на $\min(\frac{\lvert I_u \cap I_v \rvert}{50}, 1)$.

### Item2Item рекомендации

Теперь попробуем решать транспонированную задачу. Введем меру похожести объектов $s(i, j)$. Если нам нужно оценить рейтинг, который пользователь $u$ поставил бы ещё не виденному им объекту $i$, то мы можем рассмотреть множество $N(i)$ близких к $i$ объектов и оценить $\hat{r}_{ui}$ аналогично user2user подходу:

$$
\hat{r}_{ui} = \frac{\sum_{j \in N(i)} s(i, j) r_{uj}}{\sum_{j \in N(i)} \lvert s(i, j) \rvert}
$$

Меру схожести объектов можно задать как adjusted cosine:

$$
s(i, j) = \frac{\sum_{u \in U_i \cap U_j} (r_{ui} - \overline{r}_u)(r_{uj} - \overline{r}_u)}{\sqrt{\sum_{u \in U_i \cap U_j} (r_{ui} - \overline{r}_u)^2} \sqrt{\sum_{u \in U_i \cap U_j} (r_{uj} - \overline{r}_u)^2}}
$$


где $U_i$ – множество пользователей, оценивших товар $i$. Обратите внимание, что $\hat{r}_u$ – это средняя оценка пользователя, а не объекта, то есть это не корреляция Пирсона – на практике данный подход обычно работает лучше.

### Особенности коллаборативной фильтрации

Выделим ключевые особенности методов, основанных на коллаборативной фильтрации, о которых следует помнить при разработке рекомендательных систем:
- Они не опираются ни на какую дополнительную информацию кроме матрицы оценок $R_{ui}$, предполагая, что этого должно быть достаточно для улавливания качественного сигнала о схожести пользователей и товаров;
- Предложенные методы не применимы для новых объектов и пользователей – для них просто нет истории или она недостаточно информативна для того, чтобы методы могли давать более-менее точные оценки;
- Так как методы коллаборативной фильтрации основаны только на истории прошлых взаимодействий, рекомендательная система, построенная исключительно на их основе будет постепенно вгонять пользователя в информационный пузырь: эти методы не предполагают открытия новых интересов у пользователя, они способны только эксплуатировать уже имеющиеся.

## Content-based рекомендации

Также помимо коллаборативной фильтрации существует content-based подход для построения рекомендаций: измерение похожести между объектами на основе их содержания. Например, две статьи про то, как заменить колесо на велосипеде можно считать похожими с точки зрения содержания. Иными словами, входом для content-based модели являются разные контентные признаки и характеристики товара (например, текст статьи, время публикации, картинки), а выходом является некоторое числовое представление объекта (эмбеддинг). Отметим, что никакую коллаборативную информацию такие модели не используют, они ничего не знают про других пользователей и про их взаимодействие с объектами. Например, Bert является чисто контентной моделью – он переводит текст в эмбеддинг.

Пусть у нас есть некоторый контентные эмбеддинги $e_i \in \mathbb{R}^n$ для каждого товара – например, мы применили обученный Bert для получения векторных представлений статей. Тогда мы можем посчитать скалярное произведение (или косинусное расстояние) до оценённых пользователем объектов и оценить рейтинги, как:

$$
\hat{r}_{ui} = \max_{j \in I_{u}, r_{uj} > \alpha} \rho(e_i, e_j) r_{uj},
$$

где $\rho$ – скалярное произведение или косинусное расстояние между двумя векторами, $I_{u}$ – множество оценённых пользователем объектов, а $\alpha$ – гиперпараметр. Таким образом, высокие рейтинги получат объекты, похожие на те, что понравились пользователю – мы получили простую ранжирующую модель.

Плюс контентного подхода в том, что, в отличие от чисто коллаборативного подхода, он одинаково хорошо работает на новых и старых айтемах, так как контентные модели основаны только на статичной контентной информации, которая всегда доступна. Из минусов можно отметить, что похожесть по контенту может ещё больше загонять пользователя в информационный пузырь: например, контентная модель вряд ли сможет к кофемашине порекомендовать кофейные зерна, в то время как коллаборативный подход получит сигнал о том, что товары являются дополняющими напрямую из действий других пользователей.

Отметим, что существуют гибридные модели, совмещающие в себе коллаборативный и контентный сигналы. Например, такой моделью является DSSM.

Подробнее о контентных моделях вы узнаете в соответствующем параграфе.

## Классический пайплайн рекомендательной системы

![Artboard_22_copy_b853d2a0c5.svg](https://yastatic.net/s3/education-portal/media/Artboard_22_copy_b853d2a0c5_18adbac397.svg)

Мы разобрали несколько классических подходов к построению рекомендаций, теперь нужно обсудить, как это скомпоновать в единую рекомендательную систему.

Для начала сформулируем ряд свойств, которыми должна обладать хорошая рекомендательная система:
1. При ранжировании товаров в порядке убывания $\hat{r}_{ui}$ нам хотелось бы учитывать как можно больше сигналов/фичей (как пользователя, так и объекта);
2. Рекомендательная система должна работать достаточно быстро;
3. Должен быть несложный механизм, позволяющий понятно учитывать «бизнес-логику» (например, если при прочих равных мы больше хотим показывать свежие статьи).

Для соблюдения первого пункта, очевидно, нужна ранжирующая модель. В качестве самой модели часто применяют бустинг – на табличных данных он, как правило, cправляется лучше, плюс он быстрее нейронных сетей с точки зрения времени применения. 

Здесь не будет лишним упомянуть про feedback loop. Для обучения ранжирующей модели мы обычно берем прошлую историю взаимодействия пользователей с показанными ему объектами, считаем $r_{ui}$ и составляем на основе этих оценок обучающий датасет. Таким образом, обучая новую модель, мы с некоторыми оговорками будем учиться предсказывать старую модель. Поэтому есть риск, что она застрянет в локальном оптимуме, из которого сложно выбраться. В качестве решения этой проблемы можно, например, подмешивать в выдачу случайные объекты и давать им больший вес в функции потерь. Таким образом, у нас появляется некоторое подмножество объектов, которые не были смоделированы нашей моделью. В качестве дополнительного плюса такого подхода мы в какой-то степени будем выбивать пользователя из его информационного пузыря, показывая объекты из категорий, которыми он еще не интересовался.

В реальной рекомендательной системе обычно от нескольких миллионов товаров и хотя бы несколько сотен тысяч пользователей в день (а чаще несколько миллионов). Обученная CatBoost модель на 5000 объектов отрабатывает где-то за 100-125ms на CPU. Фичи пользователей и объектов постоянно меняются, поэтому на каждый запрос пользователя мы должны заново скорить все объекты. Но тогда только на скоринг мы будем тратить порядка 25 секунд, а если это не CatBoost, а, например, нейронная сеть, то, скорее всего, ещё больше. Это очень существенные и необоснованные затраты.

В действительности, пользователю наверняка интересна лишь небольшая часть имеющихся у нас товаров. Можно попытаться сузить множество до потенциально интересных пользователю объектов и уже для них применить «тяжёлую» ранжирующую модель, которая определит финальную выдачу. Этот подход называется **отбором кандидатов**. К отбору кандидатов предъявляют два требования:

- он должен быть быстрым;
- он должен иметь хорошую полноту поиска подходящих пользователю объектов, то есть в полученной после отбора кандидатов подмножестве должны в избытке находиться интересные пользователю статьи/фильмы/продукты;

Приведем несколько подходов к отбору кандидатов:

- Эвристики: самые популярные товары, популярные за $X$ последних дней, популярные среди жителей этого города, недавно опубликованные;
- Коллаборативные: item2item или user2user рекомендации. Мы можем в оффлайне предподсчитывать все необходимые статистики и строить таблички из пользователя в множество подходящих айтемов или из айтема в айтемы. Также есть более сложные подходы на основе матричных разложений, о которых будет рассказано в соответствующем параграфе;
- Контентные методы: берём content-based эмбеддинги объектов и строим быстрый индекс для поиска ближайших объектов (например, HNSW). Подробнее о быстром поиске ближайших соседей вы можете почитать в параграфе про метрические методы. Далее, можем взять понравившиеся пользователю товары и найти похожие на них.

Обычно отбор кандидатов состоит из набора разных источников кандидатов, где каждый источник по смыслу пытается покрыть какой-то пользовательский аспект.

Двухступенчатая рекомендательная система уже обладает двумя хорошими свойствами, осталось предложить механизм, который позволит учитывать бизнес-логику. Под бизнес-логикой здесь понимается некоторое качество рекомендательной системы, которое хотелось бы иметь, но которое достаточно нетривиально, чтобы мы не стали зашивать его в саму ранжирующую модель. Приведем примеры возможных пожеланий:

- Реже показывать старые видео в ленте;
- Реже показывать слишком длинные видео или видео, снятые в плохом качестве;
- Обеспечить разнообразную для пользователя выдачу. Например, если пользователь интересуется кошками и машинами, А ранжирующая модель всем видео про кошек дала большую оценку, чем любому видео про машины, то получится, что лента пользователя будет состоять только из кошек, хотя ему также интересны и машины.

Все эти свойства подразумевают под собой небольшое переупорядочивание объектов после применения ранжирующей формулы. Этот механизм называется **переранжированием** (**реранкингом**).

  ## handbook

  Учебник по машинному обучению

  ## title

  Введение в рекомендательные системы

  ## description

  *null*

- 
  ## path

  /handbook/ml/article/rekomendacii-na-osnove-matrichnyh-razlozhenij

  ## content

  ## Введение

Допустим, мы работаем в сервисе рекомендаций фильмов и перед нами стоит задача подобрать для каждого пользователя набор наиболее релевантных фильмов. Пользователь может разными способами провзаимодействовать с фильмом: посмотреть его, оставить отзыв, поставить оценку (например, от 1 до 5).

В этом параграфе мы будем строить рекомендации на основе матрицы оценок user-item. Её строки соответствуют объектам, а столбцы – пользователям. На $(i, j)$-й позиции матрицы мы ставим либо пропуск, либо оценку, выставленную $i$-м объекту $j$-му пользователем. Разумеется, не все оценки нам известны: вряд ли каждый пользователь имел возможность ознакомиться с каждым объектом. В процессе решения задачи мы будем пытаться восстановить оценки на местах пропусков. Сделав это, мы сможем, например, порекомендовать пользователю те объекты, которые он ещё не смотрел, но предсказанная оценка которых для этого пользователя максимальна.

![user_item_matrix_b796b9180a_31e9322dd5.svg](https://yastatic.net/s3/education-portal/media/user_item_matrix_b796b9180a_31e9322dd5_f968f1cb28.svg)

Все типы взаимодействия пользователей с объектами мы можем рассматривать как пользовательский фидбек. Обычно различают **явный** (**explicit**) и **неявный** (**implicit**) виды фидбека. Фидбек называется явным, если он отражает степень интереса пользователя к объекту. Например, к этому типу относят рейтинги, лайки и дизлайки. Такого фидбека обычно мало, он поступает только от тех пользователей, которые соглашаются нам его дать.

Обычно гораздо больше информации имеется о неявных предпочтениях – просмотры, клики, добавление в закладки. Но если пользователь, например, посмотрел фильм, мы ещё не можем сделать вывод, что он ему понравился. Мы можем лишь утверждать, что до просмотра этот фильм казался ему достаточно интересным. Поэтому обычно неявная обратная связь более шумная, чем явная.

Для начала научимся работать с явным фидбеком.

## Связь с задачей матричной факторизации

Вернёмся к задаче восстановления матрицы оценок и предположим, что каждый пользователь и объект можно закодировать набором из $S$ скрытых признаков, а оценка $i$-го объекта $u$-м пользователем равна скалярному произведению соответствующих векторов скрытых представлений $x_u$ и $y_i$. Тогда если бы наша матрица оценок была заполнена полностью, её можно было бы представить в виде произведений двух матриц $X$ и $Y$, составленных по столбцам из скрытых представлений пользователей и объектов:

$$
U = X^T \cdot Y
$$

![Decomp31_30b64943e4.webp](https://yastatic.net/s3/education-portal/media/Decomp31_30b64943e4_4c8c18a7b4.webp)

Правда, в таком случае нам бы и не требовалось ничего решать: мы могли бы просто рекомендовать пользователю объекты с самыми высокими оценками в соответствующей строке. Но суровая реальность такова, что зачастую матрица оценок сильно разрежена. Мы можем поступить следующим образом: восстановить латентные векторы для пользователей и объектов по имеющемуся набору оценок, после чего предсказать оценки для всех отсутствующих позиций. В параграфе, посвящённом [матричной факторизации](https://academy.yandex.ru/handbook/ml/article/matrichnaya-faktorizaciya#ispolzovanie-svd-razdelyonnye-predstavleniya-i-rekomendatelnaya-sistema-dlya-bednyh), мы уже обсуждали способы решения данной задачи с помощью SVD и стохастического градиентного спуска. У SVD есть существенные недостатки: из-за большого количества пропусков в матрице полученное решение будет слишком шумным, а кроме того, его придется каждый раз рассчитывать заново при добавлении новых пользователей или объектов. Градиентный спуск не имеет данных проблем, но тоже не очень практичен. В этом параграфе мы рассмотрим более эффективный алгоритм, называемый **Alternating Least Squares** (**ALS**).

## Постановка задачи

Пусть, как и раньше, $x_u, y_i$ – скрытые представления пользователей и объектов соответственно размерности $T$. Запишем эти векторы по строкам в матрицы $X$ и $Y$ размера $S\times N$ и $S\times D$ соответственно, где $N$ – количество пользователей, а $D$ – количество объектов.

Обозначим через $R$ множество таких пар $(u, i)$ пользователей и объектов, для которых имеются явно проставленные оценки.

Предсказывать рейтинги мы будем как скалярное произведение скрытых представлений:

$$\hat{r}_{ui} = x_u^Ty_i$$

В результате мы приходим к следующей задаче оптимизации. Мы хотим научиться как можно лучше приближать известные рейтинги:

$$\min_{x_u, y_i} \sum\limits_{(u,i) \in R} (r_{ui} - x_u^Ty_i)^2  $$

Добавив регуляризацию получаем следующую функцию потерь:

$$\min_{x_u, y_i} \sum\limits_{(u,i) \in R}(r_{ui}-x_{u}^{T}y_i)^2+\lambda\sum\limits_{\forall u}||x_u||^2C_u+\lambda\sum\limits_{\forall i}||y_i||^2C_i$$

## Alternating Least Squares (ALS)

Оптимальные параметры можно найти с помощью хорошо знакомого нам градиентного спуска, но есть более быстрые и надёжные способы. Если мысленно заморозить параметры, соответствующие латентным факторам пользователей, задача оптимизации латентных представлений объектов сведётся к задаче наименьших квадратов, для которой мы знаем точное решение.

Итоговый процесс оптимизации функции потерь будет иметь следующий вид. 

В цикле до сходимости:

- Фиксируем матрицу $X$ (скрытые представления пользователей);
- Решаем задачу L2-регуляризованной регрессии для каждого товара и находим оптимальную матрицу $Y$;
- Фиксируем матрицу $Y$ (скрытые представления объектов);
- Решаем задачу L2-регуляризованной регрессии для каждого пользователя и находим оптимальную матрицу $X$;

Решение, получаемое путём попеременного вычисления точных аналитических решений, обычно точнее тех, что получаются с помощью наивного градиентного спуска. Более того, данное решение имеет эффективную реализацию, позволяющую использовать преимущества параллельных вычислений.

Для лучшего понимания распишем каждый шаг данного алгоритма оптимизации:

### ALS - шаг по (одному) $x_u$:

$$
\underset{x_u}{\mathrm{argmin}} \sum\limits_{(u,i) \in R}^{}(r_{ui}-x_{u}^{T}y_i)^2+\lambda\sum\limits_{\forall u}||x_u||^2C_u + \lambda\sum\limits_{\forall i}||y_i||^2C_i
$$

Раскроем квадратичный член:

$$
\underset{x_u}{\mathrm{argmin}} \sum\limits_{(u, i) \in R}^{} r_{ui}^2 - 2 \sum\limits_{(u,i) \in R}^{} r_{ui}x_{u}^{T}y_i + \sum\limits_{(u,i) \in R}^{} (x_{u}^{T}y_i)^2
+\lambda\sum\limits_{\forall u}||x_u||^2C_u + \lambda\sum\limits_{\forall i}||y_i||^2C_i
$$

В первой сумме константы, они уходят. Из второй и третьей возьмём только те слагаемые, в которых участвует $x_u$. Из четвёртой остается только член с $x_u$, так как все $x_v$ независимы. Последняя сумма пропадает, так как $x_v$ и $y_j$ независимы:

$$
\underset{x_u}{\mathrm{argmin}}  - 2 \sum\limits_{i:\,(u,i) \in R}^{} r_{ui}x_{u}^{T}y_i + \sum\limits_{i:\,(u,i) \in R}^{} (x_{u}^{T}y_i)^2 + \lambda C_u x_u^T x_u = 
$$

В первой сумме индекс $u$ фиксирован, поэтому $x_u$ можно вынести за знак суммы:

$$
\underset{x_u}{\mathrm{argmin}} -2x_{u}^{T} \sum\limits_{(u, i) \in R}^{} r_{ui}y_i + \sum\limits_{(u, i) \in R}^{} x_{u}^{T}y_i \cdot x_{u}^{T}y_i + \lambda C_u (x_u, x_u)  = 
$$

Объединим второй и третий члены формулы, вынесем умножение на $x_u$ за скобки:

$$
\underset{x_u}{\mathrm{argmin}} -2x_{u}^{T} \Bigl( \sum\limits_{(u,i) \in R}^{} r_{ui}y_i \Bigr) + x_u^{T} \Bigl( \sum\limits_{(u,i) \in R}^{} y_i y_i^T + \lambda C_u \Bigr) x_u = 
$$

Теперь воспользуемся тем, что

$$
\underset{x_u}{\mathrm{argmin}} -2x_u^TB_u + x_u^TA_ux_u = A_u^{-1}B_u
$$

и выпишем ответ:

$$
x_u^* = 
\Bigl( \sum\limits_{i: (u, i) \in R}^{} y_jy_j^T + \lambda C_i I \Bigr)^{-1} \Bigl( \sum\limits_{j: (i,j) \in R}^{} r_{ij}y_j \Bigr)
$$

Таким образом, мы получили аналитическое выражение для вычисления каждого $x_u$ на шаге алгоритма. Отметим, что каждый вектор $x_u$ мы можем вычислить независимо от других $x_v$. Данное наблюдение позволяет нам использовать всю мощь параллельных вычислений для эффективного решения оптимизационной задачи. Распределив данные так, что на каждой вычислительной машине хранятся все $y_i$ для некоторого подмножества $x_u$, на одной итерации алгоритма мы можем параллельно вычислить все $x_u$. На следующей итерации аналогичным образом вычисляем все $y_i$.

## IALS (Implicit ALS)

[Оригинальная статья](http://yifanhu.net/PUB/cf.pdf)

Раньше мы работали с матрицей $R$ как с матрицей рейтингов, явно проставленных пользователем. Как мы говорили выше, такого фидбека обычно довольно мало, а куда больше неявного фидбека. При этом количество данных может быть критичным при работе с такими разреженными структурами, как матрицы рейтингов, поэтому хочется научиться работать и с неявным фидбеком тоже.

Неявным фидбеком является в том числе и факт взаимодействия, поэтому мы можем заполнить всю матрицу user-item целиком: на тех позициях, где пользователь положительно взаимодействовал с объектом, поставим $1$, а на тех, где взаимодействие было негативным или его вообще не произошло, поставим $0$. Эта компонента фидбека называется предпочтением (preference):

$$
\begin{equation}
    p_{ui} = 
    \begin{cases}
        1, & r_{ui} > 0 \\
        0, & r_{ui} \le 0 \text{ или } r_{ui} \text{ не определено}
    \end{cases}
\end{equation}
$$

Тем самым мы избавились от пропусков в матрице, но использовали не всю информацию. Согласитесь, если один пользователь посмотрел часовое видео польностью, а другой выключил после 5 минут, несправедливо считать, что это видео им понравилось в одинаковой степени. Введём ещё степень уверенности (confidence), отражающую уверенность в оценке пользователя:

$$
c_{ui} = 1 + \alpha |r_{ui}|\ (\text{ степень уверенности в } p_{ui}),
$$

где $\alpha$ – некоторая константа.

На местах пропусков мы явно проставляем $p_{ij} = 0$. На остальных позициях мы можем сами регулировать степень уверенности в зависимости от фидбека пользователя. 

Рассмотрим следующую функцию потерь:

$$
\sum\limits_{\forall u,i}^{} c_{ui} (p_{ui} - x_{u}^{T}y_i) ^ 2 + \lambda \sum\limits_{\forall u}^{} ||x_u||^2C_u + \lambda \sum\limits_{\forall i}^{} ||y_i||^2C_i, \\
$$

Она позволяет:

- Учитывать неявный фидбек, которого обычно на порядок больше, чем явного,
- Регулировать степень уверенности в действиях пользователей.

### IALS: оптимизация

Распишем нашу функцию потерь по аналогии с ALS и приведем к форме $-2x_u^TB_u + x_u^TA_ux_u$:

$$
\underset{x_u}{\mathrm{argmin}} \sum\limits_{u,i}^{} c_{ui}(p_{ui} - x_u^{T}y_i)^2 + \lambda \sum\limits_{u}^{} ||x_u||^2C_u + \lambda \sum\limits_{i}^{} ||y_i||^2C_i = \\
\underset{x_u}{\mathrm{argmin}} \sum\limits_{i}^{} c_{ui} p_{ui}^2 - 
2 \sum\limits_{i}^{} c_{ui} p_{ui} x_u^{T} y_i +
\sum\limits_{i}^{} c_{ui} (x_u^{T}y_i)^2 + \lambda C_u x_u^Tx_u = \\
\underset{x_u}{\mathrm{argmin}} - 
2 x_u^{T} \sum\limits_{\forall i}^{} c_{ui} p_{ui} y_i +
\sum\limits_{\forall i}^{} c_{ui} x_u^{T}y_i \cdot x_u^{T}y_i + \lambda C_u x_u^{T}x_u = \\
\underset{x_i}{\mathrm{argmin}} - 
2 x_u^{T} 
    \Bigl(
        \sum\limits_{\forall i}^{} c_{ui} p_{ui} y_i
    \Bigr)  +
x_u^{T} 
    \Bigl(
        \sum\limits_{\forall i}^{} c_{ui} y_i y_i ^{T} + \lambda C_u
    \Bigr) 
x_u = \\
\Bigl( 
    \sum\limits_{\forall i}^{} c_{ui}y_{i}y_{i}^{T} + \lambda C_u I 
\Bigr)^{-1}
\Bigl(
    \sum\limits_{\forall i}^{} c_{ui}p_{ui}y_{i}
\Bigr) = 
$$

Разобьём сумму на 2 части. В первой будет сумма по тем элементам, с которыми у пользователя не было положительного взаимодействия. Во второй – сумма по всем остальным элементам. Также заметим, что во втором множителе суммирование имеет смысл только по ненулевым элементам:

$$
\Bigl(
    \sum\limits_{\forall i: p_{ui}=0}^{}c_{ui}y_i \cdot y_{i}^{T} + \sum\limits_{\forall i: p_{ui}\neq 0}^{} c_{ui}y_i \cdot y_{i}^{T} + \lambda C_u I
\Bigr)^{-1} 
\Bigl(
    \sum\limits_{\forall i: p_{ui}\neq 0}^{} c_{ui}p_{ui}y_i
\Bigr) = 
$$

Заметим, что в первой сумме все $c_{ui}$ будут равны 1 (так как везде $p_{ui} = 0$). Прибавим и вычтем единицу к $c_{ui}$ во второй сумме и разобьем её на две компоненты. Вторый из них будет сумма по всем $y_iy_i^T$, где $p_{ui} \neq 0$. Объединив её с первой суммой, получим<br>просто $Y^{T}Y$:

$$
\Bigl(
    Y^{T}Y + \lambda C_u I + \sum\limits_{\forall i: p_{ui}\neq 0}^{} (c_{ui} - 1)y_{i}y_{i}^{T}
\Bigr)^{-1}
\Bigl(
    \sum\limits_{\forall i: p_{ui}\neq 0}^{} c_{ui}p_{ui}y_{i}
\Bigr)
$$

Заметим, что произведение $Y^{T}Y$ никак не зависит от $u$. Мы можем посчитать его один раз для всех $x_u$ перед очередной итерацией. В остальном же мы точно так же, как и в случае с обычным ALS, можем распределить данные так, чтобы на одной машине содержались все $y_j$, необходимые для обновления $x_v$, хранящихся на этой машине, и сделать следующий шаг оптимизации нашей функции потерь.

## Обобщения ALS и IALS

- Обе модели: и ALS, и Imlicit ALS – можно несколько усложнить, вместо $r_{ui} \approx x_{u}y_{i}$ рассмотрев $r_{ui} \approx x_{u}y_{i} + b_{u} + b_{i} + \mu$. В таком случае $b_i$ и $b_j$ играют роль некоторых априорных усреднённых оценок пользователя и объекта соответственно, а $\mu$ является глобальной априорной константой.
- В модели IALS мы обычно полагаем элементы $p_{ui}$ равными $1$ во всех случаях, когда имело место взаимодействие, но можем использовать и другие значения, в том числе зависящие от того, что ещё нам известно о пользователях и объектах.
- Для уверенности $c_{uv} = 1 + \alpha \|r_{ui}\|$ для IALS необязательно использовать $1$ в качестве значения по умолчанию. Например, события «пользователь не посмотрел популярный фильм» и «пользователь не посмотрел редкий фильм» могут иметь для нас разный вес.


## FunkSVD

Этот подход получил широкую известность после конкурса Netflix Prize в 2006 году. [Пост Саймона Фанка про участие в Netflize Prize](https://sifter.org/simon/journal/20061211.html)

Фанк предложил моделировать рейтинг как $\hat{r}_{ui} = \mu + b_u + b_i + x_{u}y_{i}$. Однако, в отличие от ALS, оптимизация производилась с помощью стохастического градиентного спуска. Правила обновления весов выглядели следующим образом:

$$
\begin{cases}
    e_{ui} = \hat{r}_{ui} - r_{ui} \\ 
    x_u \xleftarrow{} x_u + \eta (e_{ui}y_{i} - \lambda x_{u}) \\
    y_i \xleftarrow{} y_i + \eta (e_{ui}x_{u} - \lambda y_{i}) \\
    b_u \xleftarrow{} b_u + \eta (e_{ui} - \lambda b_{u}) \\
    b_i \xleftarrow{} b_i + \eta (e_{ui} - \lambda b_{i}) \\
\end{cases}
$$

Этот подход не получил большой популярности, так как градиентный спуск, в отличие от ALS, намного сложнее распараллелить.

## Singular Value Decomposition with implicit feedback (SVD++)

[Оригинальная статья](https://people.engr.tamu.edu/huangrh/Spring16/papers_course/matrix_factorization.pdf)

Ранее мы отдельно рассматривали факторизации для явного и неявного фидбека. Но, ограничиваясь только одним типом фидбека, мы теряем много информации. Если мы работаем над стриминговым сервисом, то в качестве неявного фидбека мы можем взять, например, историю фильмов, взятых в прокат. Такие данные не предоставляют нам явных оценок пользователей, но позволяют выявить неявные предпочтения. Учесть неявный фидбек в модели можно следующим образом:

$$
r_{ui} \approx 
\Bigl(
    x_{u} + \frac{1}{\sqrt{|\{j|p_{uj} \neq 0\}|}} 
    \sum\limits_{\forall j: p_{uj}\neq 0}^{} \widehat{y}_j
\Bigr)^T
y_{i} + b_{u} + b_{i} + \mu
$$

В данной модели пользователь представлен скрытым представлением $x_u$, а также слагаемым, отражающим историю неявных взаимодей с айтемами: $\frac{1}{\sqrt{|\{j|p_{uj} \neq 0\}|}} \sum\limits_{\forall j: p_{uj}\neq 0}^{} \widehat{y}_j$.  

Важно отметить, что вектора $\widehat{y}_j$ не совпадают с векторами $y_{i}$. Это своего рода «неявные» вектора айтемов.

## Collaborative Filtering with Temporal Dynamics (timeSVD++)

[Оригинальная статья](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.379.1951&rep=rep1&type=pdf)

Особенностью всех рассмотренных на данный момент разложений является отсутствие учёта порядка просмотра объектов.

Однако, как показывает практика, со временем пользователь может менять своё мнение о тех или иных айтемах. Тогда, отсортировав взаимодействия по времени, мы можем разбить события на бакеты и модифицировать приведённую выше функцию потерь, в которой таргет выражается следующим образом:

$$
r_{ui}(t) \approx 
\Bigl(
    x_{u}(t) + \frac{1}{\sqrt{|\{j|p_{uj} \neq 0\}|}} 
    \sum\limits_{\forall j: p_{uj}\neq 0}^{} \widehat{y}_j
\Bigr)
y_{i} + b_{u}(t) + b_{i}(t) + \mu
$$

## SLIM (Sparse Linear Methods)

[Оригинальная статья](http://glaros.dtc.umn.edu/gkhome/node/774)

Описанные выше методы демонстрируют хорошее качество, однако требуют больших усилий для эффективной работы в онлайн сервисах. Возникает потребность в лёгких моделях, эффективность которых значительно выше, но качество которых не сильно хуже. Для этого была предложена линейная разреженная модель.

Итак, пусть $A$ – бинарная матрица $N \times D$ user-item взаимодействий, например, матрица кликов/показов. Будем определять ответ алгоритма $a_{ui}$ как взвешивание событий из истории пользователя:

$$\hat{a}_{ui} = \sum\limits_j w_{ij}a_{uj}$$

При этом наложим ограничение $w_{ij} \ge 0$. В такой постановке мы будем учить модель находить «похожие» объекты. Добавим ещё условие $w_{ii} = 0$, которое позволит нам избежать элементарного решения – единичной матрицы $W = I$.В результате вес $w_{ij}$ выступает в качестве некоторой меры схожести $i$-го и $j$-го объектов. Осталось определиться с методом оптимизации данных параметров.

Для оптимизации используется функция потерь MSE с $L_{1}$- и $L_{2}$-регуляризаторами:

$$
\frac{1}{2}\sum\limits_{u, i}^{}(a_{ui} - \sum\limits_{j}^{}w_{ij}a_{uj})^{2} + 
\lambda \sum\limits_{i,j}^{}|w_{ij}| +
\frac{\beta}{2}\sum\limits_{i, j}^{}(w_{ij})^2 \xrightarrow{} \min_{W}
$$

Можно заметить, что задачу можно разбить на $D$ независимых по строкам матрицы $W$:

$$
\frac{1}{2}\sum\limits_{u}^{}(a_{ui} - \sum\limits_{j}^{}w_{ij}a_{uj})^{2} + 
\lambda \sum\limits_{j}^{}|w_{ij}| +
\frac{\beta}{2}\sum\limits_{j}^{}(w_{ij})^2 \xrightarrow{} \min_{w_{i1}, ..., w_{iD}} (\forall i)
$$

Данную задачу можно решать покоординатным спуском:
1. Фиксируем все строки $W$, кроме одной координаты $w_{ij}$;
2. переходим в оптимум по $w_{ij}$;
3. переходим к следующей координате;
4. повторять до сходимости.

Применение данной модели выглядит следующим образом:
1. Рассчитываем вектор взаимодействий пользователя $(u_{ui})_{i=1}^D$;
2. Считаем $at{a}_{ui}$ для всех непросмотренных объектов;
3. Отбираем топ $k$ непросмотренных объектов по $\hat{a}_{ui}$.

Так как в задаче оптимизации мы пользуемся $L_{1}$-регуляризацией, матрица $W$ получается разреженной. Матрица просмотров $A$ тоже разреженная (по определению). Эти обстоятельства позволяют заметно улучшить эффективность применения модели.

## Итоги

В этом параграфе мы рассмотрели некоторые рекомендательные модели на основе матричных факторизаций. Такие модели редко используется в чистом виде для формирования рекомендательной выдачи. Обычно результаты матричной факторизации используются для генерации кандидатов в рекомендации, когда из сотен тысяч и миллионов объектов необходимо отобрать небольшое количество (например, сотни) самых релевантных. Для генерации кандидатов требуется перемножить вектор пользователя с вектором каждого из сотен тысяч объектов и отобрать топ самых релевантных. 

В онлайн-сервисах, когда время формирования рекомендаций составляет несколько сотен миллисекунд, нет возможности при каждом запросе рассчитывать релевантность каждого объекта для данного пользователя. Оптимизировать поиск можно с помощью инструментов для поиска ближайших соседей. Для любой функции близости, в том числе и для скалярного произведения, можно построить индекс – структуру данных, с помощью которой для любого пользователя мы сможем быстро приближённо, но зато быстро искать «ближайшие» объекты. В результате, принцип работы выглядит следующим образом: 

- обучаются эмбеддинги объектов и пользователей;
- для представлений эмбеддингов строится индекс; 
- в рантайме по вектору пользователя происходит приближённый поиск $n$ самых релевантных объектов; таким образом генерируется список кандидатов в рекомендации;
- дальше список кандидатов обрабатывается с помощью более хитрых методов машинного обучения.

Подробнее о том, как быстро искать ближайших соседей, вы можете узнать в параграфе посвященном [метрическим методам](https://education.yandex.ru/handbook/ml/article/metricheskiye-metody)

Помимо генерации кандидатов, полученные представления можно использовать в качестве признаков в более сложных моделях.

Основной недостаток методов, основанных на матричной факторизации, состоит в том, что они используют лишь информацию о взаимодействии пользователей и объектов, но не о них самих. В следующем параграфе мы рассмотрим [контентные методы](https://education.yandex.ru/handbook/ml/article/kontentnye-rekomendacii), которые используют атрибуты объектов и пользователей.

## Список литературы

- [Статья](http://yifanhu.net/PUB/cf.pdf) про Implicit ALS
- [Статья](https://people.engr.tamu.edu/huangrh/Spring16/papers_course/matrix_factorization.pdf) про SVD++
- [Статья](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.379.1951&rep=rep1&type=pdf) про TimeSVD++
- [Статья](http://glaros.dtc.umn.edu/gkhome/node/774) про SLIM
- [Пост](https://sifter.org/simon/journal/20061211.html) Саймона Фанка про участие в конкурсе Netflix Prize

  ## handbook

  Учебник по машинному обучению

  ## title

  Рекомендации на основе матричных разложений

  ## description

  *null*

- 
  ## path

  /handbook/ml/article/kontentnye-rekomendacii

  ## content

  ## Введение

Все рекомендательные системы можно поделить на три типа в зависимости от того, какую информацию они используют для построения рекомендаций:

- Контентные;
- Коллаборативые;
- Гибридные.

В данном разделе мы подробнее рассмотрим основные алгоритмы построения контентных рекомендаций.

Основная идея контентных рекомендаций состоит в том, что для их построения будут использоваться атрибуты объектов и пользователей. На основе данных атрибутов мы можем найти релевантные данному пользователю объекты и рекомендовать их.

Представим, например, что мы работаем в музыкальном онлайн-сервисе и хотим подбирать наиболее релевантную музыку нашим пользователям. Допустим у нас есть пользователь Иван, который интересуется русским роком. Тогда наша система может рекомендовать Ивану музыку этого или подобных жанров. 

Можно придумать много различных атрибутов трека: жанр, автор, год выхода, продолжительность и так далее. Также можно использовать дополнительную информацию о пользователе: возраст, уровень дохода и тому подобные.

### Какими бывают контентные признаки

Допустим, мы работаем в музыкальном сервисе. Тогда в качестве признаков объектов можно использовать: 

1. Стандартные статистики объекта: количество лайков, кликов, полных прослушиваний;
2. Признаки автора: количество слушателей, жанр;
3. Неструктурированные данные: названия треков, обложки альбомов или даже предобученные эмбеддинги треков целиком.

В качестве признаков пользователей можно использовать:

1. Информацию про пользователя, если она нам доступна: возраст, пол, язык, насколько долго пользуется сервисом;
2. Информацию про контекст запроса: с какого устройства был сделан, в какое время.
3. Информацию про друзей пользователя и их взаимодействия. Например, усреднённый эмбеддинг всех треков, которые слушал каждый из друзей. Или же можно обучить RNN или Transformer на истории и результат конкатенировать к остальным признакам.

## Факторизационные машины

Начнём с постановки задачи. Пусть I – множество объектов (айтемов), U - множество пользователей. Для каждой пары объект-пользователь построим вектор размерности $\vert U\vert + \vert I\vert$ взаимодействия этой пары, в котором единицы стоят на месте соответствующих пользователя и объекта:

![one_hot_user_item_b8c1678dc9_013706689e.svg](https://yastatic.net/s3/education-portal/media/one_hot_user_item_b8c1678dc9_013706689e_51661596dd.svg)

Предсказывать будем пользовательские рейтинги объектов $a(x)$.

Можно рассмотреть простейшую регрессионную модель:

$$ a(x) = w_0 + \sum_{t=1}^{\vert U\vert + \vert I\vert} w_t x_t $$

Заметим, что к этой модели легко добавить любые фичи объектов, пользователей или пар объект-пользователь:

![factorization_machines_with_content_0dcffc3780_20df998b08.svg](https://yastatic.net/s3/education-portal/media/factorization_machines_with_content_0dcffc3780_20df998b08_6965e8beab.svg)

Дальше будем обозначать через $n$ общее число фичей. Модель можно обогатить признаками, отвечающими за взаимодействия второго порядка:

$$ a(x) = w_0 + \sum_{t=1}^n w_t x_t + \sum_{r=1}^n \sum_{s=r+1}^n w_{rs} x_r x_s $$

Матрицу $W = (w_{rs})$ можно считать симметричной: в любом случае, мы используем только её верхний треугольник.

Из-за использования попарных взаимодействий пользователей и объектов в полученной модели будет $\frac{n(n+1)}{2} +n + 1$ параметр, и так как $n\geqslant\vert U\vert + \vert I\vert$ может быть очень большим, работать с такой моделью может оказаться непросто.

Для решения этой проблемы можно использовать следующий трюк. Сопоставим каждому признаку $x_t$ вектор $v_t \in \mathbb{R}^k$ для некоторого не очень большого $k$ и представим модель в виде:

$$ a(x) = w_0 + \sum_{t=1}^n w_t x_t  + \sum_{r=1}^n\sum_{s=r+1}^m <v_r, v_s> x_r x_s $$

Таким образом, мы заменяем симметричную матрицу коэффициентов $W$ на её низкоранговое приближение $V^T V$, где $V$ – матрица $n \times k$ с векторами $v_i$ по столбцам. Число параметров модели при этом можно снизить до $nk + n + 1$. На практике матрица $W$ разреженная, и, как правило, даже при небольшом $k$ получается её неплохо приблизить. В то же время, при небольших $k$ модель обладает лучшей обобщающей способностью.

Вычислить $\sum_{r=1}^n \sum_{s=1}^m w_{rs} x_r x_s$ по можно за $O(nk)$:

$$ \sum_{r=1}^n \sum_{s=r+1}^n <v_r, v_s> x_r x_s = $$

$$ = \frac12 \sum_{r=1}^n \sum_{s=1}^n <v_r, v_s> x_r x_s - \frac12 \sum_{r=1}^n <v_i, v_i> x_i x_i = $$ 

$$ = \frac12 \sum_{r=1}^n \sum_{s=1}^n \sum_{f=1}^k v_{rf} v_{sf} x_r x_s - \frac12 \sum_{r=1}^n \sum_{f=1}^k v_{rf} v_{sf} x_r x_s = $$

$$ = \frac12 \sum_{f=1}^k \left(\sum_{r=1}^n v_{rf} x_r) \cdot (\sum_{s=1}^n v_{sf} x_s) - \sum_{r=1}^n v_{rf}^2 x_r^2 \right) = $$

$$ = \frac12 \sum_{f=1}^k \left((\sum_{r=1}^n v_{rf} x_r)^2 - \sum_{r=1}^n v_{rf}^2 x_r^2 \right)$$ 

Итоговая модель имеет вид

$$
a(x) = w_0 + \sum_{r=1}^n w_r x_r + \frac12 \left|\left| \sum_{r=1}^n v_r x_r \right|\right|_2^2 - \frac12 \sum_{r=1}^n ||v_r||_2^2x_r^2
$$

Данная модель и называется **факторизационной машиной**.

Первоначально факторизационные машины использовали только коллаборативный сигнал, но, как мы уже видели, в такую модель можно естественным образом добавить и контентную информацию.

Факторизацонную машину можно обучать для решения разных задач. Например:

- Предсказание рейтинга. Ответ модели $a(x)$ можно интерпретировать, как вещественный рейтинг, и решать задачу регрессии.
- Бинарную классификацию рекомендовать/не рекомендовать. Тогда $a(x)$ имеет смысл логита, и мы можем оптимизировать оптимизировать log loss или hinge loss.
- Ранжирование объектов. Тогда $a(x)$ – это ранжирующая функция.

Модель обычно обучается градиентным спуском.

### FFM – Field-aware Factorization Machines 

[Оригинальная статья](https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf)

[Статья про практическое применение](https://arxiv.org/pdf/1701.04099.pdf)

Как следующий этап развития факториационных машин, появилась идея иметь несколько различных латентных представлений для каждой из фичей.

Пример: есть три разных по своей природе признака: год выпуска, цвет и марка автомобиля. В факторизационной машине для учёта взаимодействия год-цвет и год-марка используется один и тот же вектор для года. Но так как эти признаки разные по смыслу, то и характер их взаимодействия может отличаться.

Идея: использовать 2 разных вектора для признака «год выпуска» при учёте взаимодействий год-цвет и год-марка. Таким образом, модель принимает вид:

$$a(x) = w_0 + \sum_{t=1}^n w_t x_t  + \sum_{r=1}^n\sum_{s=r+1}^m <v_{r,s}, v_{s,r}> x_r x_s$$

![FFM_659f00db4d.webp](https://yastatic.net/s3/education-portal/media/FFM_659f00db4d_8c9f5b0e2a.webp)

Авторы статьи выложили [исходный код своей библиотеки libffm](https://github.com/ycjuan/libffm), с помощью которой они смогли войти в топ-3 сразу в трёх соревнованиях на kaggle (Criteo, Avazu, Outbrain). Подробнее об этом можно почитать [вот тут](https://www.csie.ntu.edu.tw/~r01922136/libffm/).

## DSSM (deep sematic similiarity model)

Теперь рассмотрим ещё одну популярную модель, которая использует контентную информацию для построения рекомендаций – **DSSM**.

[Оригинальная статья](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/cikm2013_DSSM_fullversion.pdf)

В оригинальной статье DSSM была использована для нахождения «схожести» между поисковым запросом и документом. Для этого она использовала текст запроса и текст документа. 

DSSM представляет из себя «двуногую» (two-tower) нейронную сеть. В исходной постановке на первый вход подаётся текст запроса, а на второй – текст документа. Далее, независимо для текста запроса и текста документа строятся эмбеддинги. Итоговая «схожесть» вычисляется, как косинусная мера близости между ними.

На схеме ниже Q – это запрос (query), а D – документ (document).

![DSSM_full_product_softmax_27645fd2c8.webp](https://yastatic.net/s3/education-portal/media/DSSM_full_product_softmax_27645fd2c8_4756f6f976.webp)

Некоторые авторы пытались в качестве меры близости рассматривать вместо косинусной меры обучаемый MLP, но это оказалось гиблой идеей.

Эта архитектура оказалась крайне удобной при использовании на практике, так как эмбеддинги пользователя и объекта можно предподсчитать независимо и дальше хранить сразу готовые представления для них, а при запросе к рекомендациям просто пересчитывать меру близости, что ускоряет применение модели.

Данная идея хорошо обобщается на построение рекомендаций. Поиск релевантных объектов можно представить, как задачу ранжирования, где вместо текстов запроса и документа мы будем иметь некоторую контентную информацию о пользователе и объекте.

### Обучение DSSM

Давайте считать, что мы для каждого запроса $q$ предсказываем один релевантный документ.

Обозначим через $y_q$ и $y_d$ построенные моделью эмбеддинги запроса $q$ и документа $d$ соответственно. Будем вычислять условную вероятность клика по документу $d$ при условии запроса $q$ следующим образом:

$$P(d\vert q)  = \frac{\exp(b_0 R(q, d))}{\sum_{i=1}^D \exp(b_0 R(q, d_i))}$$

где 

$$ R(q, d) = \cos(y_q, y_d) = \frac{y_q^T y_d}{||y_q|| \cdot ||y_d||}$$

Здесь $b_0$ – коэффициент сглаживания, который подбирается эмпирически, а $D$ – число всех документов.

Если в качестве функции потерь мы выбираем кросс-энтропию, то на паре запрос-кликнутый документ $(q, d^+)$ она принимает вид

$$ \mathcal{L}(q, d^+) = -log(P(d^+\vert q)).$$

Но вычислять градиент такого функционала для каждого примера дорого, ведь для этого придётся для каждого запроса находить вероятность клика по всем документам. Что же делать? На помощь приходит **negative sampling**. Заметим, что среди документов $d$ в знаменателе $P(d\vert q)$ есть лишь один кликнутый, а остальные тысячи и миллионы являются отрицательными примерами. Есть смысл на каждом шаге оптимизации рассматривать не все из них, а только небольшую выборку, вместо полной суммы

$$\sum_{i=1}^D \exp(b_0 R(q, d_i))$$

беря

$$\exp(b_0 R(q, d^+)) + \sum_{i=1}^k \exp(b_0 R(q, d^-_i)),$$

где $d^-_1,\ldots,d^-_k$ – подобранные для запроса $q$ негативные примеры. Генерировать их можно по-разному; на практике чаще всего используют одну из следующих стратегий:

1. Равновероятно выбирать подмножество документов из некликнутых. В оригинальной статье предлагают брать позитивные и негативные в соотношении $4:1$. 
2. С большей вероятностью выбирать те из некликнутых документов, популярность которых выше.
3. На каждой эпохе обучения выбирать некликнутые документы, получившие максимальный скор для этого запроса на предыдущей эпохе.

### Другие функции потерь

#### Pairwise loss 

Задачу построения рекомендаций можно решать, как задачу ранжирования. Например, это можно делать с помощью попарного лосса. А именно, рассмотрим пару объектов, в которой $i_1$ – релевантный, а $i_2$ не релевантный для пользователя $u$. Тогда мы можем использовать один из двух вариантов функции потерь:

- $\mathcal{L}(R(u, i_1), R(u, i_2)) = \text{CrossEntropy}(1.0, \sigma(R(u, i_1) - R(u, i_2)))$. Тем самым модель будет учиться ранжировать положительные примеры выше отрицательных.

- $\mathcal{L}(R(u, i_1), R(u, i_2)) = \max(0, \alpha - R(u, i_1) + R(u, i_2))$ (triplet loss). При этом модель обучается так, чтобы положительный и отрицательный примеры как можно больше отличались. Эта функция потерь довольно популярна не только в DSSM сетках, но и в целом в задачах, где нужно обучить парные представления $(y_q, y_d)$ объектов $(q, d)$ из разных доменов так, чтобы для релевантных друг другу $q$ и $d$ эмбеддинги оказывались близкими, а для не релевантных далёкими.

#### Full Product Softmax loss

Рассмотрим батч $(u_1, i_1, r_1),\ldots, (u_M, i_M, r_M)$ размера $M$, где $u_t$ – пользователь, $i_t$ – пользователю, а $r_t$ – таргет, степень релевантности объекта пользователю. Построим по ним:

- матрицу эмбеддингов пользователей $U \in \mathbb{R}^{M \times D}$;
- матрицу эмбеддингов объектов $W \in \mathbb{R}^{M \times D}$;
- вектор таргетов $r \in \mathbb{R}^M$.

Рассмотрим матрицу 

$$\text{softmax}(\alpha U W^T + \beta), U W^T \in \mathbb{R}^{M \times M},$$

где softmax берётся по строкам

![DSSM_full_product_softmax_27645fd2c8.webp](https://yastatic.net/s3/education-portal/media/DSSM_full_product_softmax_27645fd2c8_5e53be04fe.webp)

Рассмотрим функцию потерь вид

$$ L = -\mathbb{I}\{r > 0\}^T \cdot \log(\text{diag}(\text{softmax}(\alpha U W^T + \beta)))$$

Эта функция потерь старается сделать так, чтобы для релевантных друг другу (с $r > 0$) пар $(u, i)$ скалярное произведение эмбеддингов $\langle w_u, w_i\rangle$ было максимальным.

## Трансформеры для рекомендаций

В 2018 году появилась архитектура трансформеров на основе механизма внимания. Модели на основе трансформеров показали state-of-the-art результаты на большом числе NLP задач, а впоследствии оказалось, что они отлично подходят и для задач компьютерного зрения. С их помощью можно решать и задачи рекомендаций. Аналогия заключается в следующем: если в NLP трансформеры работают с последовательностями токенов, то в рекомендациях в качестве последовательности можно взять историю событий пользователя. Каждый элемент последовательности – это взаимодействие пользователя с объектом, например, клик на объект.

Классические модели рекомендаций часто игнорируют тот факт, что история пользователя – это направленная последовательность, в которой порядок событий имеет значение. Трансформеры позволяют учитывать как порядок событий, так и сложные паттерны в поведении и интересах пользователя. Например, исследователи из Alibaba представили модель, которую назвали Behaviour Sequence Transformer. Авторы заявляют, что модель используется в продакшене. Модель решает задачу Click Through Rate (CTR) prediction – предсказание вероятности клика по объекту. 

![transformer_ea88a1cd77.webp](https://yastatic.net/s3/education-portal/media/transformer_ea88a1cd77_b6f60a80db.webp)

На вход модели подается история кликов пользователя, на основе которой нужно предсказать вероятность клика по заданному объекту. Роль архитектуры трансформера здесь в том, чтобы качественно закодировать представление пользователя, после чего применяется обычный multi layer perceptron (MLP) для предсказания вероятности.

Помимо архитектур, которые специально разрабатываются под задачи рекомендаций, трансформеры можно использовать и как обособленные предобученные модели для построения векторых представлений текстов или изображений, которые затем подаются как признаки для решения downstream задач в домене рекомендаций. Несмотря на очевидные преимущества трансформеров с точки зрения качества, их использование в продакшене часто ограничивается имеющимися вычислительными ресурсами. Это особенно актуально для рекомендаций, где модели важно применять непосредственно в момент запроса пользователя.

  ## handbook

  Учебник по машинному обучению

  ## title

  Контентные рекомендации

  ## description

  *null*

- 
  ## path

  /handbook/ml/article/horoshie-svojstva-rekomendatelnyh-sistem

  ## content

  ## Введение

Предположим, выдача нашей рекомендательной системы имеет высокие значения метрик ранжирования. Значит ли это, что система действительно хорошая? Не всегда просто ответить на этот вопрос. Оптимизируя определенные метрики, можно выкрутить кликбейт, и пользователи будут охотно кликать в моменте, но больше не станут пользоваться таким сервисом. Соответственно, нужно как-то измерять «счастье пользователей», попытаться формализовать свойства, которыми должна обладать хорошая рекомендательная система. Однозначного ответа на этот вопрос нет, всё зависит от контекста применения рекомендательной системы. В этом разделе мы поговорим о наиболее распространённых критерях, которые довольно часто оказываются важными.

## Полнота (Coverage)

Под полнотой в данном контексте понимается доля рекомендованных объектов $I_{recommended}$ среди всех объектов $I$. 

$$
Coverage = \frac{|I_{recommended}|}{|I|}
$$

Эта метрика была предложенна в статье Ge, M., Delgado-Battenfeld, C., Jannach, D. (2010, September). Beyond accuracy: evaluating recommender systems by coverage and serendipity. In Proceedings of the fourth ACM conference on Recommender systems (pp. 257-260).

Данную метрику имеет смысл оценивать в разных временных интервалах, при этом принимая во внимание возможные ограничения, связанные с объемом данных. Например, нас может интересовать значение полноты за первый день работы рекомендательной системы, а может – за неделю. Целевое поведение полноты будет различаться в зависимости от доменных областей и бизнес деталей конкретного случая. Например, в рекомендациях музыки может быть полезно периодически повторно рекомендовать треки, которые пользователю в наибольшей степени нравятся, так как пользователь может захотеть послушать их еще раз. В то же время в рекомендациях фильмов это реже оказывается осмысленным: обычно проходит много времени, прежде чем пользователь захочет пересмотреть фильм. Таким образом, во втором случае полнота будет расти быстрее за счет отсутствия повторов. 

Еще одним фактором, влияющим на полноту, является алгоритм холодного старта, который может использоваться для того чтобы найти подходящие объекты для нового пользователя или подходящих пользователей для нового объекта. Часто пользователям на этапе холодного старта показывают самые популярные объекты. Из-за этого свежедобавленные объекты (например, музыкальные треки) могут неявно пессмимизироваться алгоритмом. Один из способов решения проблемы – бустить свежие объекты в течение определённого времени, чтобы они показывались чаще. Настроки логики холодного старта могут сильно повлиять на метрику полноты.

Среди других актуальных вопросов, которыми стоит задаваться: 

- Cколько нужно дней, чтобы полнота достигала заданного значения $p$? 
- Возможно ли достичь такого значения в принципе, используя текущий алгоритм? 

Чтобы ответить на эти вопросы, нужно принимать во внимание ряд факторов: 

- Какой объём трафика у системы рекомендаций? 
- Есть ли у бизнеса ограничения, влияющие на конечный список рекомендаций? 
- Имеет ли алгоритм рекомендаций достаточную степень персонализации? 
- Можно ли регулировать режимы exploration и exploitation во время работы рекомендательной системы?

Каждый из этих факторов может по-разному влиять на динамику полноты. Бизнес ограничения и слабая степень персонализации могут сдерживать рост полноты. Напротив, если модель высокоперсонализированная и учитывает много пользовательских факторов, то она способна рекомендовать больше уникальных объектов из хвоста распределения, которые тоже могут ему понравиться, тем самым обеспечивая рост полноты.

## Новизна (Novelty)

Один из способов оценить новизну рекомендательной системы – использовать статистическую меру собственной информации объекта (self information), которая используется в теории информации и тесно связана с понятием энтропии. Значение собственной информации для события $X$ равняется логарифму вероятности наступления данного события. Согласно теории, чем меньше вероятность наступления события, тем больше потенциальной информации принесет это событие при его наступлении. Единицей информации при использовании логарифма по основании $2$ является бит.

Теперь если переносить идею собственной информации в парадигму рекомендательных систем, то получается, что чем менее популярен объект, тем более вероятно, что он будет новым для пользователя. А значит мера информации у такого объекта будет выше. Для каждого рекомендованного объекта $i$ считаем вероятность, с которой его порекомендуют случайному пользователю: $ P_{i} = \frac{m_{i}}{N} $, где $ m_{i} $ – количество пользователей, которым был показан $i$-й объект, а $N$ – общее число пользователей. Для заданного пользователя усредняем значение собственной информации по списку его рекомендаций $R$ и получаем итоговое значение метрики:

$$
Novelty_{user} = \frac{1}{|R|}\sum_{i\in R}^{}-log(P(i))
$$

## Разнообразие (Diversity) 

Разнообразие – это способность модели рекомендовать разные по содержанию объекты. Такое свойство очень важно для долгосрочного успеха сервисов, основанных на рекомендательных системах. Действительно, если модель постоянно рекомендует похожие друг на друга объекты, то рано или поздно пользователю наскучат такие рекомендации. 

![diversity_4b72cbf150.webp](https://yastatic.net/s3/education-portal/media/diversity_4b72cbf150_c6708d3208.webp)

Разнообразие можно рассчитывать на основе комбинаций метрик полноты и новизны. Также мерой разнообразия может быть дисперсия рекомендаций за заданный промежуток времени. Помимо этого популярны подходы, использующие эмбединги объектов для оценки попарной похожести объектов и расчёта на основе неё значения разнообразия. Одна из таких метрик – Intra List Similarity (ILS). Чтобы ее посчитать, нужно иметь эмбединги объектов рекомендаций, находящиеся в едином векторном пространстве. Для расчёта разнообразия для одного пользователя нужно усреднить попарную схожесть $\text{sim}$ между рекомендованными объектами:

$$
ILS_{user} = \frac{1}{R}\sum_{i\epsilon R}^{}\sum_{j\epsilon R}^{}\text{sim}(i,j),
$$

где $R$ – это набор рекомендованных пользователю объектов.

Для того чтобы добиться большего разнообразия, метрику нужно минимизировать. Мера схожести должна быть *больше* для более похожих объектов. Чаще всего используется косинусная близость (cosine similarity).

## Serendipity

Одно из самых желанных свойств для любой рекомендательной системы. У слова serendipity нет четкого перевода, в 2008 году оно даже [попало в список самых неподдающихся переводу слов в мире](http://www.todaytranslations.com/blog/most-untranslatable-word/). На русский иногда оно переводится как «интуитивная прозорливость». 

Serendipity – это способность рекомендовать такие объекты, которые не только релевантны для пользователя, но ещё и существенно отличаются от того, с какими объектами пользователь взаимодействовал в прошлом.

![serendipity_52a0a5da8c_8e0517ac0b.svg](https://yastatic.net/s3/education-portal/media/serendipity_52a0a5da8c_8e0517ac0b_633c54ae13.svg)

Serendipity – довольно субъективное свойство и его сложно формализовать. Более того рекомендации, удовлетворяющие этому свойству, встречаются редко, что усложняет интерпретацию и измерение serendipity. Нет консенсуса о том, какой метрикой можно оценить его. Мы расскажем о способе, предложенном в статье T. Murakami, K. Mori, R. Orihara, Metrics for evaluating the serendipity of recommendation lists, in: New Frontiers in Artificial Intelligence, Vol. 4914, Springer Berlin Heidelberg, Berlin, Heidelberg, 2008, pp. 40–46.

Пусть $R_{u}$ – список рекомендаций для пользователя, $\text{Pr}_{u}(i)$ – предсказание модели, для каждого объекта из списка, а $\text{Prim}_{u}(i)$ – предсказание примитивной модели (в качестве примитивной можно брать модель на основе эвристик без машинного обучения или простую неперсональную модель), а $\text{rel}$ – известная релевантность объекта для пользователя. Тогда Serendipity рассчитывается следующим образом:

$$
Serendipity_{user} = \sum_{i\epsilon R}^{}max(\text{Pr}_{u}(i)-\text{Prim}_{u}(i), 0) \cdot {\text{rel}_{u}(i)}
$$

Значение метрики можно усреднить по всем пользователям тестовой выборки. Чем больше значение, тем больше модель удовлетворяет свойству Serendipity.

Ключевая идея формулы такова: если уверенность персонализированной модели в том, что пользователю понравится $i$-ый айтем, больше, чем уверенность неперсональной модели (примитивной), это значит, что данному пользователю может особенно понравиться $i$-й айтем.

Отдельный вопрос – как оптимизировать Serendipity. Нужно улучшать способность модели к персонализации: 

- добавлять больше фичей для пар (пользователь, объект); 
- взвешивать таргеты, чтобы более тонко учитывать необычные клики/просмотры;
- писать кастомные функции потерь, которые будут поощрять модель за буст неожиданныйх объектов (которые в большей степени удовлетворяют свойству serendipity).

Кроме того, имеет смысл оптимизировать модель по метрике serendipity на офлайн тестовой выборке.

## Заключение

В этом разделе мы рассмотрели ключевые свойства рекомендательных систем и метрики для их оценки. Рекомендательные системы – сложная область, где нет готовых рецептов оценки качества. Ключевые метрики всегда идут от продуктовых деталей применения рекомендательной системы. Полезно смотреть на несколько метрик одновременно, чтобы оценить разные свойства моделей.

В какой момент нужно начинать следить за метриками из данного раздела? Несмотря на их ценность, на начальном этапе стоит концентрироваться на более простых и интуитивно понятных с точки зрения бизнеса метриках: конверсии, среднем времени визита и так далее. А вот как только базовые метрики будут на удовлетворительном уровне, стоит начинать мониторить и оптимизировать метрики, разобранные в этом разделе.

  ## handbook

  Учебник по машинному обучению

  ## title

  Хорошие свойства рекомендательных систем

  ## description

  *null*

- 
  ## path

  /handbook/ml/article/klasterizaciya

  ## content

  ## Задача кластеризации

В задаче классификации мы имели дело с восстановлением отображения из множества объектов в конечный набор меток классов. При этом классы были зафиксированы заранее, то есть мы с самого начала примерно понимали, какого рода объекты должны относиться к каждому из них, и мы располагали обучающей выборкой с примерами объектов и классов, к которым они относятся. В задаче кластеризации мы тоже разбиваем объекты на конечное множество классов, но у нас нет ни обучающей выборки, ни понимания, какой будет природа этих классов. То, что модель кластеризации какие-то объекты сочла «похожими», отнеся к одному классу, будет новой информацией, «открытием», сделанным этой моделью. Обучающей выборки у нас также не будет: ведь мы не знаем заранее, что за классы получатся (а иногда и сколько их будет). Таким образом, **кластеризация** — это задача обучения без учителя. Из-за общего сходства постановок задач в литературе кластеризацию иногда называют **unsupervised classification**. 

Методы кластеризации часто применяют, когда фактически нужно решить задачу классификации, но обучающую выборку собрать затруднительно (дорого или долго). При этом валидационную выборку для оценки результатов кластеризации собрать значительно проще, так как для неё требуется меньше примеров. При этом стоит помнить, что точность работы supervised-методов значительно выше. Поэтому, если обучающую выборку всё-таки можно собрать, лучше решать задачу классификации, чем задачу кластеризации.

## Примеры задач кластеризации

Хороший пример применения методов кластеризации — анализ геоданных. В мобильных приложениях, собирающих геоданные пользователей, часто требуется понять, где именно пользователь находился. GPS-координаты известны с некоторой погрешностью, пользователь тоже обычно двигается, поэтому вместо точного положения часто приходится иметь дело с роем точек. Положение усугубляется, когда мы пытаемся анализировать поведение сразу тысяч людей в какой-то локации — например, определить, в каких точках люди чаще всего садятся в такси у аэропорта. Может показаться, что достаточно посмотреть на данные — и мы увидим в точности нужные нам кластеры. Изображение ниже показывает, как может выглядеть ситуация всего для нескольких пользователей: согласно данным GPS, такси подбирают пассажиров и внутри здания аэропорта, и на взлётной полосе, и там, где это происходит на самом деле:

![19_1_54408bffaa.webp](https://yastatic.net/s3/education-portal/media/19_1_54408bffaa_7f0ab021e1.webp)

Подобная задача решалась в Яндекс.Такси при разработке пикап-пойнтов (наиболее удобных точек вызова такси, подсвечиваемых в приложении). Координаты точек заказа кластеризовались таким образом, чтобы кластер соответствовал какому-то одному, удобному для пользователя месту, и центры кластеров использовались как кандидаты в пикап-пойнты. Те кандидаты, которые удовлетворяли простым фильтрам (например, не попадали в здание или в воду), использовались в приложении. При этом не обходилось и без вручную проставленных пикап-пойнтов: например, такое решение использовалось в окрестностях аэропортов.

Другой пример кластеризации геоданных, который всегда рядом с нами, — это интерфейсы для просмотра фотографий в вашем смартфоне. Почти наверняка вы можете просмотреть их в привязке к местам, где они были сделаны, и по мере масштабирования карты вы будете видеть разное количество кластеров фотографий. Кстати, если говорить об интерфейсах, то есть и другой интересный пример: если нужно подстроить цветовую схему вашего интерфейса под выбираемое пользователем изображение (например, фоновую картинку), достаточно кластеризовать цвета из пользовательского изображения, используя RGB-представление (или любое другое) как признаки цвета, и воспользоваться для оформления цветами, соответствующими центрам кластеров.

## Простейшие методы кластеризации с помощью графов

Можно приводить примеры не только про геоаналитику, однако тема геоданных поможет нам придумать пару наиболее простых и наглядных методов кластеризации. Представим, что перед нами рой геокоординат и нам нужно предложить по этим данным пикап-пойнты для такси. Разберём пару очевидных методов.

### Выделение компонент связности

Логично попробовать объединить точки, которые находятся друг от друга на расстоянии двух-трёх метров, а потом просто выбрать наиболее популярные места. Для этого давайте построим на известных нам точках граф: точки, расстояние между которыми в пределах трёх метров, мы соединим рёбрами. Выделим в этом графе компоненты связности, они и будут нашими кластерами.

У этого способа есть пара очевидных недостатков. Во-первых, может найтись сколько угодно длинная цепочка точек, в которой соседние отстоят друг от друга на пару метров, — и вся она попадёт в одну компоненту связности. В итоге наша отсечка по трём метрам имеет очень опосредованное отношение к диаметру кластеров, а сами кластеры будут получаться значительно больше, чем нам хотелось бы. Во-вторых (и с первой проблемой это тоже связано), непонятно, как мы выбираем максимальное расстояние, при котором соединяем точки ребром. В данной задаче ещё можно предъявить хоть какую-то логику, а вот если бы мы кластеризовали не геометки, а что-то многомерное, например электронные письма по их тематике, придумать отсечку было бы уже сложнее. Если наша цель — не только решить практическую задачу, но и придумать достаточно общий метод кластеризации, понятно, что нам хочется понимать, как подбирать параметры этого метода (в данном случае условие добавления рёбер в граф). Эти соображения могут привести нас к другому решению.

### Минимальное остовное дерево

Вместо того чтобы проводить рёбра в графе, давайте их удалять. Построим [минимальное остовное дерево](https://ru.wikipedia.org/wiki/Минимальное_остовное_дерево), считая расстояния между точками весами рёбер. Тогда, удалив $N$ рёбер с наибольшим весом, мы получим $N+1$ компоненту связности, которые, как и в прошлом подходе, будем считать кластерами. Различие в том, что теперь нам нужно задавать не расстояние, при котором проводится ребро, а количество кластеров. С одной стороны, если мы решаем задачу расчёта пикап-пойнтов в какой-то конкретной локации (аэропорт, торговый центр, жилой дом), нам может быть понятно, сколько примерно пикап-пойнтов мы хотим получить. С другой стороны, даже без локального рассмотрения можно просто сделать достаточно много кластеров, чтобы было из чего выбирать, но при этом достаточно мало, чтобы в каждый кластер попадало репрезентативное количество точек. Аналогичная логика будет справедлива и во многих других задачах кластеризации: количество кластеров — достаточно общий и достаточно хорошо интерпретируемый параметр, чтобы настраивать его вручную, поэтому во многих методах кластеризации количество кластеров выступает как гиперпараметр.

Далее будем рассматривать некоторую обобщённую задачу кластеризации без привязки к нашему примеру с анализом геоданных. Мы приведём три наиболее популярных метода кластеризации — k-средних, иерархическую кластеризацию и DBSCAN, а затем рассмотрим вопросы оценки качества кластеризации.

## Метод K средних

Пожалуй, один из наиболее популярных методов кластеризации — это метод K-средних (K-means). Основная идея метода — итеративное повторение двух шагов:

1) распределение объектов выборки по кластерам;
2) пересчёт центров кластеров.

В начале работы алгоритма выбираются $K$ случайных центров в пространстве признаков. Каждый объект выборки относят к тому кластеру, к центру которого объект оказался ближе. Далее центры кластеров пересчитывают как среднее арифметическое векторов признаков всех вошедших в этот кластер объектов (то есть центр масс кластера). Как только мы обновили центры кластеров, объекты заново перераспределяются по ним, а затем можно снова уточнить положение центров. Процесс продолжается до тех пор, пока центры кластеров не перестанут меняться.

![kmeans_4a27aaf200.gif](https://yastatic.net/s3/education-portal/media/kmeans_4a27aaf200_0c3fe72855.gif)

### Выбор начального приближения

Первый вопрос при выборе начального положения центров — как, выбирая центры из некоторого случайного распределения, не попасть в область пространства признаков, где нет точек выборки. Базовое решение — просто выбрать в качестве центров какие-то из объектов выборки.

Вторая потенциальная проблема — кучное размещение центров. В этом случае их начальное положение с большой вероятностью окажется далёким от итогового положения центров кластеров. Например, для таких изначальных положений центров

![19_2_55eb1fbe5f.webp](https://yastatic.net/s3/education-portal/media/19_2_55eb1fbe5f_1bc4d07ce0.webp)

мы получим неправильную кластеризацию.

![19_3_a081b9c158.webp](https://yastatic.net/s3/education-portal/media/19_3_a081b9c158_06b66f9a43.webp)

Чтобы бороться с этим явлением, выгодно брать максимально удаленные друг от друга центры. 

На практике работает следующая эвристика: 

1) первый центр выбираем случайно из равномерного распределения на точках выборки;
2) каждый следующий центр выбираем из случайного распределения на объектах выборки, в котором вероятность выбрать объект пропорциональна квадрату расстояния от него до ближайшего к нему центра кластера.
Модификация K-means, использующая эту эвристику для выбора начальных приближений, называется K-means++.

### Выбор метрик

Так как работа метода K-средних состоит из последовательного повторения до сходимости двух шагов, обоснованность применения различных метрик (расстояний между точками, а не метрик качества :) или функций близости связана с тем, «ломают» они какой-либо из этих шагов или нет.

Первый шаг с отнесением объектов к ближайшим центрам не зависит от вида метрики. Второй шаг предполагает пересчёт центров как среднего арифметического входящих в кластер точек, и вот здесь будет подвох: к оптимальности выбора центров в среднем арифметическом приводит именно евклидова метрика (подробнее в разделе «Что оптимизирует K-means»).

Однако на практике никто не мешает использовать метод и без должного обоснования, поэтому можно экспериментировать с любыми расстояниями, с той лишь оговоркой, что не будет никаких теоретических гарантий, что метод сработает. Наиболее распространённая альтернатива евклидовой метрике — это косинусная мера близости векторов (она особенно популярна в задачах анализа текстов):

$$CosineSimilarity(\mu_k, x_i)=\frac{<\mu_k, x_i>}{\|\mu_k\|_2 \cdot \|x_i\|_2}$$

При её использовании стоит не забывать, что косинусная мера — это функция близости, а не расстояние, так что чем больше её значения, тем ближе друг к другу векторы.

### Mini-batch K-means

Несложно заметить, что, если считать $K$ и размерность пространства признаков константами, оба шага алгоритма работают за $O(n)$, где n — количество объектов обучающей выборки. Отсюда возникает идея ускорения работы алгоритма. В mini-batch K-means мы не считаем шаги сразу на всей выборке, а на каждой итерации выбираем случайную подвыборку (мини-батч) и работаем на ней. В случае когда исходная выборка очень велика, переход к пакетной обработке не приводит к большой потере качества, зато значительно ускоряет работу алгоритма.

### Понижение размерности

С другой стороны, вычисление расстояний и средних делается за $O(d)$, где $d$ — размерность пространства признаков, так что другая идея ускорения K-means — это предварительно понизить размерность пространства признаков (с помощью PCA или эмбеддингов). Особенно удачно эта идея работает в задачах кластеризации текстов, когда K-means применяют на эмбеддингах слов: получается выиграть не только в скорости работы, но и в интерпретируемости результатов кластеризации.

Кстати, сам алгоритм кластеризации тоже можно использовать как метод понижения размерности. Если вы решаете задачу обучения с учителем и пространство признаков очень разнообразно (то есть обучающая выборка не даёт вам достаточно статистики при столь большом числе признаков), можно выполнить кластеризацию объектов выборки на 500 или 1000 кластеров и оперировать попаданием объектов в какой-то кластер как признаком. Такой подход называется **квантизацией пространства признаков** (**feature space quantization**) и часто помогает на практике, когда нужно огрубить признаки, добавить им интерпретируемости или же, наоборот, обезличить.

Хрестоматийный пример такого использования кластеризации — метод bag of visual words, расширяющий bag of words из анализа текстов на работу с изображениями. Идея метода в том, чтобы строить признаковое описание изображений на основе входящих в него фрагментов: так, изображения с лицами будут содержать фрагменты с носом, глазами, ртом, а изображения с машинами — колёса, зеркала, двери. Но проблема здесь в том, что нарезать такие фрагменты из обучающей выборки и искать точные совпадения в новых примерах изображений, которые нужно классифицировать, — безнадёжная затея. В жизни фрагменты изображений не повторяются в других изображениях с попиксельной точностью. Решение этой проблемы оказалось возможным при помощи алгоритмов кластеризации (исторически использовался именно K-means): фрагменты изображений из обучающей выборки кластеризовали на 100–1000 кластеров («визуальных слов»), а проходясь по новым изображениям, также нарезали их на фрагменты и относили к одному из этих кластеров. В итоге как новые изображения, так и изображения из обучающей выборки можно было описать количеством вхождений в них фрагментов из различных кластеров («визуальных слов»), так же как в анализе текстов описывают текст количеством вхождений в него слов из словаря. В таком признаковом пространстве уже можно было успешно обучать модели машинного обучения. 

Сейчас выделение «визуальных слов» в задаче классификации изображений происходит автоматически: с одной стороны, задачи компьютерного зрения теперь решаются нейросетями, но с другой стороны — если мы визуализируем отдельные слои этих нейросетей, станет понятно, что их логика работы во многом похожа на описанную выше. При этом идея квантизации признаков не утратила своей актуальности. Вот лишь несколько современных примеров её применения:

1. Если вам необходимо дать возможность заказчику (например, внешней компании) анализировать используемые вами признаки — отсутствие провалов в данных и какие-то другие общие показатели, но нельзя отдавать признаки как есть (например, из-за законодательства, регулирующего передачу пользовательских данных), возможное решение — это агрегировать признаки по кластерам.
2. Та же цель может быть отчасти достигнута, если перейти к самим кластерам как к признакам, чтобы скрыть исходные признаки.
3. Переход к кластерам может быть сделан не с целью что-то скрыть, а наоборот, с целью повысить интерпретируемость: исходные сырые данные часто не вполне понятны бизнесу, но позволяют построить маркетинговые сегменты по различным коммерческим интересам пользователей, из-за чего становится удобно показывать принадлежность к этим сегментам как исходные признаки, не вдаваясь в детали о том, на каких данных эти сегменты построены.
4. Для ускорения поиска похожих объектов в пространстве признаков вы также можете проводить поиск внутри того же кластера и соседних кластеров, так что за счёт «огрублённого» вида признаков какие-то процессы можно ещё и ускорить.

### Что оптимизирует K-means

Проговорим на интуитивном уровне, какую оптимизационную задачу решает K-means.

Оба шага алгоритма работают на уменьшение среднего квадрата евклидова расстояния от объектов до центров их кластеров:

$$\Phi_0 = \frac{1}{nK} \sum\limits_{k=1}^{K} \sum\limits_{i=1}^{n} (\mu_k - x_i)^2 \mathbb{I}[a(x_i)=k]$$

На шаге отнесения объектов к одному из кластеров мы выбираем кластер с ближайшим центроидом, то есть минимизируем каждое слагаемое в $\Phi_0$: все потенциально большие слагаемые мы зануляем, а оставляем ненулевыми только наименьшие из возможных (при условии фиксирования центров кластеров).

На шаге пересчёта центров кластеров мы выбираем центр таким образом, чтобы при фиксированном наборе объектов, относящихся к кластеру, для всех $k$ минимизировать выражение, стоящее под суммой по $k$:

$$\sum\limits_{i=1}^{n}  (\mu_k - x_i)^2 \mathbb{I}[a(x_i)=k]$$

Здесь уже становится принципиально, что мы определяем квадрат расстояния как квадрат разности векторов, так как именно отсюда при дифференцировании по $\mu_k$ и записи необходимого условия экстремума получается, что центры кластеров нужно пересчитывать как средние арифметические $x_i$, принадлежащих кластеру.

Этих соображений, конечно, недостаточно, чтобы утверждать, что мы найдём минимум $\Phi_0$. Более того, гарантии того, что мы найдём глобальный минимум, вообще говоря, нет. Однако, потратив чуть больше усилий, можно доказать, что процесс сойдётся в один из локальных минимумов.

Также можно справедливо заметить, что, так как любой центр кластера — это среднее арифметическое входящих в кластер объектов $x_i$, на выборке фиксированного размера есть лишь конечное множество потенциальных центров кластеров. Если предположить, что в ходе работы K-means не зацикливается, отсюда следует, что рано или поздно центры кластеров не изменятся на следующем шаге и алгоритм сойдётся. При этом фактическая сходимость, конечно же, происходит задолго до полного перебора всех возможных центров кластеров.

## Иерархическая агломеративная кластеризация

Другой классический метод кластеризации — это **иерархическая кластеризация**. Иногда дополнительно уточняют: **иерархическая агломеративная кластеризация**. Название указывает сразу на два обстоятельства.

Во-первых, есть деление алгоритмов кластеризации на **агломеративные** (**agglomerative**) и **дивизивные**, или **дивизионные** (**divisive**). Агломеративные алгоритмы начинают с небольших кластеров (обычно с кластеров, состоящих из одного объекта) и постепенно объединяют их в кластеры побольше. Дивизивные начинают с больших кластеров (обычно – с одного единственного кластера) и постепенно делят на кластеры поменьше.

![19_4_7ed88aaa93.webp](https://yastatic.net/s3/education-portal/media/19_4_7ed88aaa93_a97e85a875.webp)

Во-вторых, кластеризация бывает, по аналогии с оргструктурой в организациях, плоской (когда все кластеры равноправны и находятся на одном уровне кластеризации) и иерархической (когда кластеры бывают вложены друг в друга и образуют древовидную структуру).

В случае иерархической агломеративной кластеризации мы действительно будем начинать с кластеров из одного объекта, постепенно объединяя их, а уже последовательность этих объединений даст структуру вложенности кластеров. Даже если в итоге мы будем использовать кластеры с одного уровня, не углубляясь ни в какую вложенность, кластеризация всё равно называется иерархической, так как иерархия естественным образом возникает в процессе работы алгоритма.

Сам алгоритм выглядит предельно просто:

1. Создаём столько кластеров, сколько у нас объектов в выборке, каждый объект — в своём отдельном кластере.
2. Повторяем итеративно слияние двух ближайших кластеров, пока не выполнится критерий останова.

### Расстояния в иерархической кластеризации

Как измерить расстояние между кластерами из одного объекта? Нужно просто взять расстояние между этими объектами. Остаётся вопрос, как обобщить расстояние между объектами до расстояния между кластерами (если в них более одного объекта). Традиционные решения — брать среднее расстояние между объектами кластеров, минимальное расстояние или максимальное. Если обозначить кластеры $U$ и $V$, расстояние между ними в этом случае можем вычислять по одной из формул:

$$d_{avg}(U, V) = \frac{1}{|U| \cdot |V|} \sum\limits_{u \in U} \sum\limits_{v \in V} \rho(u,v)$$

$$d_{min}(U, V) = \min\limits_{(u,v) \in U \times V} \rho(u,v)$$

$$d_{max}(U, V) = \max\limits_{(u,v) \in U \times V} \rho(u,v)$$

Используемая формула расстояния между кластерами — один из гиперпараметров алгоритма. Кроме приведённых стандартных вариантов бывают и более экзотичные, например расстояние Уорда (Ward distance). В наиболее общем виде способы задания расстояния между кластерами даются формулой Ланса — Уильямса (Lance — Williams; более подробно вы можете почитать в [этой статье](https://arxiv.org/pdf/1105.0121.pdf)). Сами же расстояния между объектами можно задавать любой метрикой, как евклидовой, так и манхэттенским расстоянием или, например, косинусной мерой (с той лишь поправкой, что это мера близости, а не расстояние).

### Условия окончания работы алгоритма (критерии останова)

В качестве условия для завершения работы алгоритма можем выбрать либо получение нужного количества кластеров (количество кластеров может быть гиперпараметром алгоритма), либо выполнение эвристик на основе расстояния между объединяемыми кластерами (например, если расстояние сливаемых кластеров значительно выросло по сравнению с прошлой итерацией). На практике же обычно кластеризацию проводят вплоть до одного кластера, включающего в себя всю выборку, а затем анализируют получившуюся иерархическую структуру с помощью дендрограммы.

### Дендрограмма

По мере объединения кластеров, каждой итерации алгоритма соответствует пара объединяемых на этой итерации кластеров, а также расстояние между кластерами в момент слияния. Расстояния с ростом итерации будут только увеличиваться, поэтому возникает возможность построить следующую схему, называемую **дендрограммой**:

![19_5_64e913b1bf.webp](https://yastatic.net/s3/education-portal/media/19_5_64e913b1bf_85496f57cf.webp)

Здесь по горизонтали внизу отмечены объекты кластеризуемой выборки, под горизонтальной осью подписаны номера объектов, а их расположение вдоль оси продиктовано только эстетическими соображениями: нам удобно строить дендрограмму так, чтобы никакие дуги в ней не пересекались. По вертикали отложены расстояния между кластерами в момент слияния. Когда происходит объединение кластеров, состоящих из нескольких объектов, соответствующая этой итерации алгоритма дуга идёт не до конкретных объектов выборки, а до дуги другого кластера.

Таким образом мы получаем наглядную визуализацию древовидной структуры процесса кластеризации. В частности, на дендрограмме мы можем визуально заметить, в какой момент происходит скачок расстояний между кластерами, и попытаться определить «естественное» количество кластеров в нашей задаче. На практике же это соображение, как правило, остаётся лишь красивой теорией, так как любую кластеризацию можно делать в разной степени «мелкой» и «естественного» количества кластеров в практических задачах часто не существует. В случае же если данные были получены таким образом, что в них действительно есть какое-то естественное количество кластеров, иерархическая кластеризация обычно справляется с определением числа кластеров по дендрограмме заметно хуже, чем DBSCAN. Именно алгоритму DBSCAN мы и посвятим следующий раздел.

## DBSCAN

Алгоритм **DBSCAN** (**Density-based spatial clustering of applications with noise**) развивает идею кластеризации с помощью выделения связных компонент. 

Прежде чем перейти к построению графа, введём понятие плотности объектов выборки в пространстве признаков. Плотность в DBSCAN определяется в окрестности каждого объекта выборки $x_i$ как количество других точек выборки в шаре $B(\varepsilon, x_i)$. Кроме радиуса $\varepsilon$ окрестности в качестве гиперпараметра алгоритма задается порог $N_0$ по количеству точек в окрестности.

Далее все объекты выборки делятся на три типа: **внутренние / основные точки** (**core points**), **граничные** (**border points**) и **шумовые точки** (**noise points**). К основным относятся точки, в окрестности которых больше $N_0$ объектов выборки. К граничным — точки, в окрестности которых есть основные, но общее количество точек в окрестности меньше $N_0$. Шумовыми называют точки, в окрестности которых нет основных точек и в целом содержится менее $N_0$ объектов выборки.

Алгоритм кластеризации выглядит следующим образом:

1. Шумовые точки убираются из рассмотрения и не приписываются ни к какому кластеру.
2. Основные точки, у которых есть общая окрестность, соединяются ребром. 
3. В полученном графе выделяются компоненты связности.
4. Каждая граничная точка относится к тому кластеру, в который попала ближайшая к ней основная точка.

Удобство DBSCAN заключается в том, что он сам определяет количество кластеров (по модулю задания других гиперпараметров — $\varepsilon$ и $N_0$), а также в том, что метод успешно справляется даже с достаточно сложными формами кластеров. Кластеры могут иметь вид протяжённых лент или быть вложенными друг в друга как концентрические гиперсферы. На изображении ниже показан пример выделения кластеров достаточно сложной формы с помощью DBSCAN:

![19_6_9cae9e34d3.webp](https://yastatic.net/s3/education-portal/media/19_6_9cae9e34d3_76ab832434.webp)

DBSCAN — один из самых сильных алгоритмов кластеризации, но работает он, как правило, заметно дольше, чем mini-batch K-means, к тому же весьма чувствителен к размерности пространства признаков, поэтому используется на практике DBSCAN только тогда, когда успевает отрабатывать за приемлемое время. 

## Какой метод кластеризации выбирать?

Если сравнивать частоту использования K-means, иерархической кластеризации и DBSCAN, то на первом месте, бесспорно, будет K-means, а второе место будут делить иерархический подход и DBSCAN. Иерархическая кластеризация — более известный и простой в понимании метод, чем DBSCAN, но довольно редко отрабатывающий качественно. Частая проблема иерархической кластеризации — раннее образование одного гигантского кластера и ряда очень небольших, что приводит к сильной несбалансированности количества объектов в итоговых кластерах. В то же время DBSCAN — менее широко известный подход, но, когда его можно применить, качество, как правило, получается выше, чем в K-means или иерархической кластеризации.

## Оценка качества кластеризации

Далее приведём список основных метрик качества кластеризации и обсудим некоторые особенности их применения.

### Среднее внутрикластерное расстояние

Смысл среднего внутрикластерного расстояния максимально соответствует названию:

$$F_0 = \frac{\sum\limits_{i=1}^{n} \sum\limits_{j=i}^{n} \rho(x_i, x_j) \mathbb{I}[a(x_i)=a(x_j)]}{\sum\limits_{i=1}^{n} \sum\limits_{j=i}^{n} \mathbb{I}[a(x_i)=a(x_j)]}$$

Сумма расстояний между точками из одного и того же кластера делится на количество пар точек, принадлежащих к одному кластеру. В приведённой выше формуле пары вида $(x_i, x_i)$ включены в рассмотрение, чтобы избежать неопределённости $\frac{0}{0}$ в случае, когда в каждом кластере ровно по одному объекту. Однако иногда записывают суммы по $i < j$, просто доопределяя $F_0$ в описанном случае нулём.

Решая задачу кластеризации, мы хотим по возможности получать как можно более кучные кластеры, то есть минимизировать $F_0$.

В случае если у кластеров есть центры $\mu_k$, часто рассматривается аналогичная метрика — средний квадрат внутрикластерного расстояния:

$$\Phi_0 = \frac{1}{nK} \sum\limits_{k=1}^{K} \sum\limits_{i=1}^{n} \rho(\mu_k, x_i)^2 \mathbb{I}[a(x_i)=k]$$

### Среднее межкластерное расстояние

Аналогично среднему внутрикластерному расстоянию вводится среднее межкластерное:

$$F_1 = \frac{\sum\limits_{i=1}^{n} \sum\limits_{j=i}^{n} \rho(x_i, x_j) \mathbb{I}[a(x_i) \neq a(x_j)]}{\sum\limits_{i=1}^{n} \sum\limits_{j=i}^{n} \mathbb{I}[a(x_i) \neq a(x_j)]}$$

Среднее межкластерное расстояние, напротив, нужно максимизировать, то есть имеет смысл выделять в разные кластеры наиболее удалённые друг от друга объекты.

### Гомогенность

Для измерения следующих метрик (гомогенности, полноты и V-меры) нам уже потребуется разметка выборки. Будем обозначать кластеры, к которым наш алгоритм кластеризации относит каждый объект, буквами $k \in \lbrace 1, ..., K \rbrace$, а классы, к которым объекты отнесены разметкой, — буквами $с \in \lbrace 1, ..., С \rbrace$. Разумный вопрос при наличии разметки — зачем нам решать задачу кластеризации, если с разметкой можно поставить задачу как задачу классификации. Это и правда хороший вопрос в том случае, если размеченных данных достаточно много для обучения классификатора. На практике же часто встречаются ситуации, когда данных достаточно для оценки качества кластеризации, но всё ещё не хватает для использования методов обучения с учителем.

Пусть $n$ — общее количество объектов в выборке, $n_k$ — количество объектов в кластере номер $k$, $m_c$ — количество объектов в классе номер $с$, а $n_{ck}$ — количество объектов из класса $c$ в кластере $k$. Рассмотрим следующие величины:

$$H_{class} = - \sum\limits_{c = 1}^{C} \frac{m_c}{n} \log {\frac{m_c}{n}}$$

$$H_{clust} = - \sum\limits_{k = 1}^{K} \frac{n_k}{n} \log {\frac{n_k}{n}}$$

$$H_{class | clust} = - \sum\limits_{c = 1}^{C}\sum\limits_{k = 1}^{K} \frac{n_{ck}}{n} \log {\frac{n_{ck}}{n_k}}$$

Несложно заметить, что эти величины соответствуют формуле энтропии и условной энтропии для мультиномиальных распределений $\frac{m_c}{n}$, $\frac{n_k}{n}$ и $\frac{n_{ck}}{n_k}$ соответственно.

Гомогенность кластеризации определяется следующим выражением:

$$Homogeneity = 1 - \frac{H_{class | clust}}{H_{class}}$$

Отношение $\frac{H_{class \vert clust}}{H_{class}}$ показывает, во сколько раз энтропия изменяется за счёт того, что мы считаем известной принадлежность объектов к выделенным нашим алгоритмом кластерам. Худший случай — когда отношение оказывается равным единице (энтропия не изменилась, условная энтропия совпала с обычной), лучший — когда каждый кластер содержит элементы только одного класса и номер кластера, таким образом, точно определяет номер класса (в этом случае $h=1$).

Тривиальный способ максимизировать гомогенность кластеризации — выделить каждый объект выборки в отдельный кластер.

### Полнота

Полнота задаётся аналогично гомогенности, с той лишь разницей, что вводится величина $H_{clust \vert class}$, симметричная $H_{class \vert clust}$:

$$Completeness = 1 - \frac{H_{clust | class}}{H_{clust}}$$

Полнота равна единице, когда все объекты класса всегда оказываются в одном кластере.

Тривиальный способ максимизировать полноту кластеризации — объединить всю выборку в один кластер.


### V-мера

**Гомогенность** и **полнота** кластеризации – это в некотором смысле аналоги точности и полноты классификации. Аналог F-меры для задачи кластеризации тоже есть, он называется V-мерой и связан с гомогенностью и полнотой той же формулой, что и F-мера с точностью и полнотой:

$$V_{\beta} = \frac{(1 + \beta) \cdot Homogeneity \cdot Completeness}{\beta \cdot Homogeneity + Completeness}$$

В частности, $V_1$ по аналогии с $F_1$-мерой в классификации (не путать со средним межкластерным расстоянием, которое мы тоже обозначали $F_1$ выше) будет просто средним гармоническим гомогенности и полноты:

$$V_{1} = \frac{2 \cdot Homogeneity \cdot Completeness}{Homogeneity + Completeness} $$

V-мера комбинирует в себе гомогенность и полноту таким образом, чтобы максимизация итоговой метрики не приводила к тривиальным решениям.

### Коэффициент силуэта

Ещё одна метрика кластеризации, на этот раз уже не требующая разметки, это **коэффициент силуэта** (**silhouette coefficient**). Изначально коэффициент определяется для каждого объекта выборки, а метрика для результатов кластеризации всей выборки вводится как средний коэффициент силуэта для всех объектов выборки.

Чтобы ввести коэффициент силуэта $S(x_i)$, нам потребуются две вспомогательные величины. Первая, $A(x_i)$, — это среднее расстояние между $x_i$ и объектами того же кластера. Вторая, $B(x_i)$, — это среднее расстояние между $x_i$ и объектами следующего ближайшего кластера. Коэффициент силуэта вводится следующим образом:

$$S(x_i) = \frac{B(x_i) - A(x_i)}{\max(B(x_i), A(x_i))}$$

В идеальном случае объекты «родного» кластера $x_i$ должны быть ближе к $x_i$, чем объекты соседнего кластера, то есть $A(x_i) < B(x_i)$. Однако это неравенство выполняется далеко не всегда. Если «родной» кластер $x_i$, например, имеет форму очень протяжённой ленты или просто большой размер, а недалеко от $x_i$ есть кластер поменьше, может оказаться, что $A(x_i) > B(x_i)$. Таким образом, если мы посмотрим на разность $B(x_i) - A(x_i)$, она может оказаться как положительной, так и отрицательной, но в идеальном сценарии всё же следует ожидать положительное значение. Сам же коэффициент $S(x_i)$ принимает значения от –1 до +1 и максимизируется, когда кластеры кучные и хорошо отделены друг от друга.

Коэффициент силуэта особенно полезен (по сравнению с другими приведёнными метриками) тем, что одновременно и не требует разметки, и позволяет подбирать количество кластеров. См. подробнее [в примере из документации scikit-learn](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html).

### Различия и выбор метрик качества кластеризации

Подводя итог в теме метрик качества в задаче кластеризации, отметим, что есть несколько разных сценариев использования этих метрик. Если вы уже определились с количеством кластеров, можно оптимизировать среднее внутрикластерное или среднее межкластерное расстояние. Если у вас ещё и есть разметка — гомогенность и полноту. V-мера за счёт сочетания гомогенности и полноты в целом позволяет выполнять и подбор количества кластеров.

Однако разметка, с одной стороны, есть далеко не всегда, а с другой стороны, в задаче кластеризации часто очень субъективна. Сложность кластеризации в том, что на одной и той же выборке нас вполне могут устроить сразу несколько различных вариантов кластеризации, то есть задача поставлена некорректно и имеет более одного решения. Формализовать, какие решения нас устроят, на практике довольно сложно, поэтому сама по себе задача кластеризации решается не слишком хорошо.

Если разметки нет и число кластеров не фиксировано, лучшая метрика на практике — коэффициент силуэта. Исключение — ситуация, когда результат кластеризации используется далее для решения некоторой задачи обучения с учителем (как было в примере классификации изображений с помощью visual bag of words). В этом случае можно абстрагироваться от качества кластеризации и выбирать такой алгоритм кластеризации и такие его гиперпараметры, которые позволят лучше всего решить итоговую задачу.

  ## handbook

  Учебник по машинному обучению

  ## title

  Кластеризация

  ## description

  Методы кластеризации: K-Means, агломеративная кластеризация, DBSCAN. Оценка качества кластеризации

- 
  ## path

  /handbook/ml/article/vremennye-ryady

  ## content

  ## Введение

**Временной ряд** — значения меняющихся во времени признаков, полученные в некоторые моменты времени.

![ts](https://yastatic.net/s3/education-portal/media/ts_4586b0902c_3f22fd0ec7.webp)

**Задача прогнозирования**

Пусть $(y_t, t \in \mathbb{N})$ – временной ряд, для которого известны значения $y_1, \ldots, y_T$. Требуется построить <b>прогноз</b> – функцию $f$, такую что величина $\widehat{y}_{T+h} = f(y_1, \ldots, y_T, h)$ как можно лучше приближает значение $y_{T+h}$, где $h \in \{1, \ldots, H\}$ – количество шагов, на которое нужно построить прогноз, а величина $H$ – горизонт прогнозирования, то есть максимальное количество шагов для построения прогноза. Иными словами, прогноз значения ряда в момент времени $T+h$ строится на основе известных значений ряда до момента времени $T$. Кроме этого имеет смысл строить <b>предсказательный интервал</b>, то есть интервал $(d_{T+h}, u_{T+h})$, т.ч. $\mathbb{P}(d_{T+h} \leqslant y_{T+h} \leqslant u_{T+h}) \geqslant \alpha$.

Например, пусть $y_t$ – значение какого-то признака в момент времени $t$, и у нас есть значения ряда за месяц, то есть $y_1, \ldots, y_{30}$. Пусть также требуется предсказать значения ряда на неделю вперед. Тогда прогноз на первые сутки вперед будет равен $\widehat{y}_{31} = f(y_1, \ldots, y_{30}, 1)$, а прогноз на пятые сутки $\widehat{y}_{35} = f(y_1, \ldots, y_{30}, 5)$.

Спустя некоторое время прогноз можно перестроить. Например, пусть прогноз перестраивается один раз в трое суток. Тогда оценку значения $y_{35}$ мы уточним как $\widehat{y}_{35} = f(y_1, \ldots, y_{33}, 2)$.  При этом может оказаться, что функция $f$ умеет принимать на вход лишь фиксированное количество предыдущих значений ряда. Например, если она умеем строить прогноз по последним 30 значениям ряда, то запись будет иметь вид $\widehat{y}_{35} = f(y_4, \ldots, y_{33}, 2)$. Иногда для уточнения того, в какой момент построен прогноз значения $y_{35}$, указывают момент времени построения прогноза. Например, запись $\widehat{y}_{35\:\vert\:30}$ означает, что прогноз на 35-й день построен в день 30, а $\widehat{y}_{35\:\vert\:33}$ – в день 33.

Если признаков несколько, не обязательно прогнозировать каждый признак. Часто выделяется один или несколько целевых признаков, а остальные признаки являются вспомогательными и могут улучшить прогноз.

Практические примеры:

1. Прогноз погоды на 10 дней вперед.
2. Прогноз осадков на 2 часа вперед.

## Примеры временных рядов

**Ежемесячные продажи антидиабетических лекарств в Австралии с июля 1991 по июнь 2008.** На этом графике мы можем видеть возрастающий тренд, возможно, даже нелинейный, и кроме этого есть сезонность (периодичность) значений по годам.

![ex](https://yastatic.net/s3/education-portal/media/ex_5_2ee6327df8_7ac9055a6a.webp)

**Максимальный спрос на электричество в штате Виктория (Австралия) за 30-минутные интервалы с 10 января 2000 в течении 115 дней.** Здесь мы можем наблюдать сразу две сезонности – суточную и недельную. Первая сезонность вызвана тем, что люди обычно больше потребляют электричество днем, чем ночью. Недельная сезонность вызвана более высоким потреблением электричества по будням. Если бы мы посмотрели данные за несколько лет, то увидели бы еще одну сезонность – годовую, например, вызванную тем, что в теплое время года работает больше кондиционеров.

![ex](https://yastatic.net/s3/education-portal/media/ex_2_edd214b6d4_e624752725.webp)

Почему бы нам не построить прогноз простыми методами, например, линейной регрессией по времени? Общий тренд так можно уловить. Но в остатках (то есть в разности между истинными значениями и прогнозом) этой модели будет достаточно много информации, которую хотелось бы как-то учесть.

![ex](https://yastatic.net/s3/education-portal/media/ex_5_regr_86f4ec08db_d8869c1677.webp)

Можно также добавить квадрат значения времени. Тогда можно уловить квадратичный тренд, но не более.

![ex](https://yastatic.net/s3/education-portal/media/ex_5_regr_sq_abc4a91d9d_7f51695156.webp)

## Прогнозирование с помощью сведения к задаче регрессии

Давайте для начала поймем, что мы вообще хотим сделать. Посмотрим на этот график, на котором показаны продажи одного из товаров в магазине за разные года.

![ts](https://yastatic.net/s3/education-portal/media/ts_predict_df49cf88a0_2a78d25310.webp)

Мы знаем значения ряда (<font color="green"><b>зеленые</b></font>) до момента времени $t$, в данном случае за 4 года с 2013 по 2016 включительно. Предположим также, что в данный момент мы отмечаем Новый год 2017. В этот момент мы хотели бы предсказать (<font color="blue"><b>синее</b></font>) будущие значения ряда (<font color="orange"><b>оранжевое</b></font>) за весь 2017 год на основе четырехлетней истории продаж.

Основная идея – подадим известные (<font color="green"><b>зеленые</b></font>) значения ряда в какую-то регрессионную функцию, получив тем самым предсказания. При этом можем брать не все известные значения ряда, а только $p$ последних значений. Иначе говоря, модель имеет вид

$$y_t = f(y_{t-1}, \ldots, y_{t-p}),
$$

где $f$ – произвольная функция. Ее можно построить некоторым известным методом машинного обучения, например, линейной регрессией, решающими деревьями, бустингами, нейронными сетями (как сверточными, так и рекуррентными). Разберёмся, какие признаки мы подадим на вход регрессии.

### Признаки

#### Общий принцип

На практике при генерации идей о том, какие признаки можно создавать для построения модели, рекомендуется строить следующий график. На нем нужно отметить момент времени $t$ и мысленно поставить себя в этот момент времени. Затем нужно подумать, какие данные нам при этом доступны. В модель можно брать любые признаки, которые доступны к моменту времени $t$. Если все данные поступают сразу, то можно брать все признаки, которые зависят только от значений до момента времени $t$. В реальности часть данных может поступать с задержкой. Например, если данные загружаются в базу данных раз в сутки в полночь, то в полдень нам не доступны данные за последние 12 часов.

![1](https://yastatic.net/s3/education-portal/media/1_rgb_split_rooster_902d9298d4_8020007396.svg)

Также нужно помнить о том, на сколько времени вперед нужно сделать прогноз. Например, пусть у нас задача состоит в том, чтобы построить прогнозы продаж в магазине с целью планирования новых поставок. После того, как на основе прогноза мы примем решение о составе товаров в новой поставке, необходимо сначала собрать данные товары на складе, потом отправить машину в магазин, и затем еще разложить товар на полки в магазине. На эту процедуру может уходить от нескольких часов до нескольких дней. Тем самым еще до момента начала формирования новой поставки модель прогнозирования продаж должна построить прогноз спроса на товар к тому моменту, когда его выложат на полки.

#### Даты

Посмотрим на то, какие признаки можно извлечь

Пусть дана какая-то дата: **13.04.2021 09:00**.

Отсюда можно получить следующие признаки:

* день недели: \[2\];
* месяц:       \[4\];
* год: \[2021\];
* сезон: \[весна\];
* праздник: \[0\];
* выходной: \[0\];
* час: \[9\].

#### Предыдущие значения ряда.

Например, если мы хотим построить признаки в момент времени $t$ для прогнозирования $y_t$, то можно рассмотреть в качестве признаков $p$ предыдущих значений ряда $y_{t-1}, \ldots, y_{t-p}$.

#|
||

Время

|

Таргет

|

Признаки

||
||

$t$

|

$y_t$

|

$y_{t-1}, \ldots, y_{t-p}$

||
||

$t-1$

|

$y_{t-1}$

|

$y_{t-2}, \ldots, y_{t-p-1}$

||
||

$t-2$

|

$y_{t-2}$

|

$y_{t-3}, \ldots, y_{t-p-2}$

||
|#


Для реализации таких признаков можно выполнить сдвиги **вперед** временного ряда на $1, 2, \ldots, p$ шагов. Например, в таблице для прогнозирования значений ряда мы рассматриваем два предыдущих значений ряда, выполняя тем самым два сдвига вперед. Таким образом, для прогнозирования значения 5 января, которое равно 235, мы берем признаки 230 и 215, которые являются значением ряда за 4 и 3 января соответственно.

![1](https://yastatic.net/s3/education-portal/media/1_rgb_split_rooster_copy_127d9b7e45_0e438ecc93.svg)

#### Скользящее окно

Не всегда имеет смысл в качестве признаков в чистом виде брать все предыдущие значения ряда. Например, если данные приходят раз в секунду, то для того чтобы учесть изменения ряда за последний час, пришлось бы создавать 3600 признаков. Вместо этого по предыдущим значениям ряда $y_{t-1}, \ldots, y_{t-p}$ можно посчитать:

* среднее;
* взвешенное среднее;
* экспоненциальное сглаживание;
* медиана;
* минимум/максимум;
* стандартное отклонение;
* любую другую статистику.

Подобное скользящее окно можно рассматривать и по другим временн*ы*м факторам, которые мы не прогнозируем.

Примеры:

* Средняя *температура* на прошлой неделе для предсказания температуры на завтра.
* Средняя *влажность* на прошлой неделе для предсказания температуры на завтра.

Если в задаче данные хорошие и удаётся использовать более-менее стандартные признаки, то можно воспользоваться готовыми инструментами. Если данные не очень приятные, стоит подумать над тем, какие признаки использовать и как реализовать их получение.

#### Сезонность

Если во временном ряду наблюдается сезонность, то стоит использовать сезонные признаки, например, следующие.

* Значение переменной сутки/неделю/месяц/год назад. Такие факторы также можно усреднять.
* Сезонность, полученная методами декомпозирования ряда (об этом расскажем ниже).

Примеры:

* Значение температуры год назад.
* Среднее значение температуры 23 ноября за 5 последних лет.
* Среднее значение температуры за 5 последних лет на неделе, в которую входит 23 ноября.

#### Счётчики

Идея состоит в том, чтобы группировать данные не только по временным факторам, но и по любым категориальным. Например, пусть сегодня нет ветра. Тогда в качестве признака можно рассмотреть среднюю температуру в безветренные дни по историческим данным.

Можно также использовать сразу несколько факторов. Например, мы строим прогноз в апреле. Тогда можно рассматривать среднюю температуру в безветренные дни в апреле по историческим данным.

#### Резюме

Подведем итог о том, какие признаки можем использовать для построения нашей модели.

* Используются **только** данные из прошлого, никакие данные из будущего нельзя использовать при прогнозировании. Нужно также учитывать возможные задержки в поступлении данных.
* Большое количество признаков может привести к вычислительным затратам.
* Можно генерировать и другие признаки с учетом знаний о предметной области.

### Построение прогноза

Мы определились с тем, какие брать признаки. Теперь разберемся с тем, как прогнозировать. Пусть требуется построить прогноз на $H$ шагов вперед. Выделяют три основных способа построить прогнозы:

* Рекурсивная стратегия;
* Прямая стратегия;
* Гибридная стратегия.

#### Рекурсивная стратегия

Для каждого момента времени $t_0\leqslant t \leqslant T$ создается объект обучающей выборки:

* *Признаковое описание* – история ряда до момента времени $t-1$.
* *Целевая метка* – значение $y_t$.

По этим данным мы обучаем какую-либо регрессионную модель строить прогнозы на один шаг вперед. При построении прогноза на несколько шагов вперед мы сначала построим прогноз на один шаг. Затем – на второй шаг, используя полученный прогноз на первый шаг в качестве признаков, и далее аналогично.

Иначе говоря, если для прогнозирования $y_t$ признаковое описание имеет вид $(y_{t-p}, \ldots, y_{t-1})$, то для построения прогноза $y_{t+1}$ рассматривается признаковое описание $(y_{t-p+1}, \ldots, y_{t-1}, \widehat{y}_t)$.

На картинке считаем, что M-2, M-1 и M это названия признаков у построенной модели.

![recursive](https://yastatic.net/s3/education-portal/media/recursive_8427cc778b_923a246e65.webp)

#### Прямая стратегия

В прямой стратегии предполагается, что построением каждого прогноза в рамках горизонта прогнозирования должна заниматься своя модель. Тем самым создается $H$ моделей прогнозирования для каждого момента времени $t_0\leqslant t \leqslant t_0+H-1$.

* *Признаковое описание* – история ряда до момента времени $t_0-1$, причем признаки одни и те же для каждой модели.
* *Целевая метка* – значение $y_t$.

![direct](https://yastatic.net/s3/education-portal/media/direct_7ff22dabcb_ba7c98a644.webp)

#### Гибридная стратегия

Гибридная стратегия объединяет в себе преимущества рекурсивной и прямой стратегий. Как и в прямой стратегии, создается $H$ моделей прогнозирования, но при этом каждая следующая модель использует прогнозы предыдущей подобно тому, как это делает рекурсивная стратегия.

Итак, мы должны построить

* модель для прогноза на 1 шаг вперед;
* модель для прогноза на 2 шага вперед, используя прогноз уже обученной модели на 1 шаг вперед в качестве признака;
* модель для прогноза на 3 шага вперед, используя прогноз уже обученных моделей на 1 и 2 шага вперед в качестве признаков;
* и так далее обучается $H$ моделей.

Признаковое описание:

* история ряда до момента времени $t_0-1$;
* предсказание предыдущих моделей для $t_0, t_0-1, \ldots, t-1$.

На картинке показаны признаковые описания для моделей в такой стратегии.

![hybrid](https://yastatic.net/s3/education-portal/media/hybrid_8f9dd2444d_70893afe15.webp)

Можно задаться вопросом: что лучше брать при обучении моделей для прогноза на несколько шагов вперёд – истинные значения или же предсказания предыдущих моделей. Если брать истинные, то мы можем точнее построить модели прогнозирования, но, с другой стороны, на этапе применения вы будете использовать прогнозы, а они могут иметь другое распределение, чем истинные данные, в частности, могут иметь смещение и большую дисперсию. В таком случае мы получим плохие следующие прогнозы.

#### Модели для нескольких временных рядов

В реальности очень часто нужно прогнозировать сразу огромное количество временных рядов.

Примеры:

* Предсказание температуры для различных регионов/городов.
* Предсказания уровня продаж для различных типов товаров (молоко/яблоки/мясо).

Проблема:

* модель на каждый временной ряд – слишком много ресурсов и не масштабируемо;
* мало моделей – плохие предсказания для каждого ряда по отдельности.

Идея: создавать модели не для каждого временного ряда, а для группы временных рядов. Иначе говоря, разделить объекты на категории, и для каждой категории создавать отдельную модель.

![cluster](https://yastatic.net/s3/education-portal/media/cluster_model_444955a517_8bd1eedf81.webp)

### Оценка качества моделей

Для оценка качества моделей прогнозирования временного ряда в основном используются метрики качества регрессии.

* Средняя квадратичная ошибка

$$MSE = \frac{1}{T-R+1} \sum_{t=R}^T \left(\widehat{y}_t - y_t\right)^2.
$$

* Средняя абсолютная ошибка

$$MAE = \frac{1}{T-R+1} \sum_{t=R}^T \left\vert\widehat{y}_t - y_t\right\vert.
$$

Эти метрики показывают, например, на сколько рублей или на сколько единиц товара мы ошибемся.

Также могут использоваться:

* Средняя абсолютная ошибка в процентах

$$MAPE = \frac{100}{T-R+1} \sum_{t=R}^T \left\vert\frac{\widehat{y}_t - y_t}{y_t}\right\vert.
$$

* Взвешенная средняя ошибка в процентах.

$$WAPE = 100 \cdot \dfrac{\sum_{t=R}^T \vert\widehat{y}_t - y_t\vert}{\sum_{t=R}^T \vert y_t\vert}.
$$

Эти метрики достаточно популярны из-за того, что позволяют оценить качество в относительных величинах без зависимости от шкалы измерений.

#### Кросс-валидация для временных рядов.

Стандартные схемы кросс-валидации нельзя применять для временных рядов потому что значения во временных рядах нельзя перемешивать. Существует два способа построить кросс-валидацию на временных рядах.

**Схема 1.**

* Обучаемся на первых $t$ значениях временного ряда $y_1\:...\:y_{t}$, прогнозируем следующие $\Delta t$ значений ряда $\widehat{y}_{t+1}\:...\:\widehat{y}_{t+\Delta t}$.
* Обучаемся на $y_1\:...\:y_{t+\Delta t}$, прогнозируем $\widehat{y}_{t+\Delta t+1}\:...\:\widehat{y}_{t+2\Delta t}$.
* ...
* Обучаемся на $y_1\:...\:y_{t+(k-1)\Delta t}$, прогнозируем $\widehat{y}_{t+(k-1)\Delta t+1}\:...\:\widehat{y}_{t+k\Delta t}$.
* На каждой итерации считаем ошибки и усредняем.

![Group](https://yastatic.net/s3/education-portal/media/Group_1_df539253d3_78a875d6a0.svg)

**Схема 2.**

* Обучаемся на первых $t$ значениях временного ряда $y_1\:...\:y_{t}$, прогнозируем следующие $\Delta t$ значений ряда $\widehat{y}_{t+1}\:...\:\widehat{y}_{t+\Delta t}$.
* Обучаемся на $y_{1+\Delta t}\:...\:y_{t+\Delta t}$, прогнозируем $\widehat{y}_{t+\Delta t+1}\:...\:\widehat{y}_{t+2\Delta t}$.
* ...
* Обучаемся на $y_{1+(k-1)\Delta t}\:...\:y_{t+(k-1)\Delta t}$,~прогнозируем~$\widehat{y}_{t+(k-1)\Delta t+1}\:...\: \widehat{y}_{t+k\Delta t}$.
* Считаем ошибки и усредняем.

![cross](https://yastatic.net/s3/education-portal/media/cross_val2_1f5518a032_b791f27515_1e8335dd56.svg)

Эти две схемы отличаются только размером обучающего множества. В первом случае он постоянно растет, во втором – не меняется, а само обучающее множество при этом сдвигается. Ту или иную схему на практике стоит использовать в зависимости от того, какая решается задача. Например, если данных достаточно много и предполагается онлайн работа модели с периодическим дообучением, то обычно при каждом дообучении размер обучающего множества фиксируют. В таком случае имеет смысл воспользоваться второй схемой, чтобы оценить качество модели, обученной именно на таким количестве данных. Если же данных немного, то для обучения желательно использовать все доступные данные. В таком случае имеет смысл использовать первую схему.

Обратите внимание, что во всех случаях размер тестового интервала времени фиксирован. Это необходимое условие, потому как распределение значений метрики на разных размерах данных может отличаться. Вспомните, например, про зависимость дисперсии выборочного от размера выборки.

### Резюме: стандартные модели ML для временных рядов

Преимущества

* Свободно используют дополнительную информацию – экзогенные факторы или признаки.
* Много рядов – много моделей. Нейронная сеть может иметь несколько выходов, и это позволяет прогнозировать сразу несколько рядов одной моделью. Пример: прогнозирование продаж различных товаров.

Недостатки

* Предсказательные интервалы напрямую не строятся.
* Иногда работают хуже стандартных моделей.
* Обработка признаков может быть труднее, чем в других моделях, которые мы рассмотрим далее.
* Интерпретация моделей может вызывать трудности у заказчика.

## Декомпозиция временных рядов

Декомпозиция  – процедура разложения временного ряда на три временных компоненты:

* **Тренд** $T_t$ – плавное долгосрочное изменение временного ряда.
* **Сезонность** $S_t$ – циклические изменения временного ряда с постоянным периодом сезонности $s$.
* **Ошибка** $R_t$ – непрогнозируемая случайная компонента ряда.

Можно рассматривать аддитивную декомпозицию, в которой ряд представляется в виде $y_t = T_t + S_t + R_t$, а также мультипликативную в виде $y_t = T_t \cdot S_t \cdot R_t$. Нетрудно понять, что для построения мультипликативного разложения достаточно выполнить аддитивную декомпозицию для ряда $\log  y_t$.

### Декомпозиция на основе скользящего среднего

Пусть $s$ – известный заранее период сезонности. Компоненты разложения вычисляются последовательно по следующим правилам.

* **Тренд**

Вычисляется с помощью скользящего окна длины $s$: $T_t = \dfrac{1}{s}\sum\limits_{i=t-s/2}^{t+s/2}y_i$.

* **Сезонность**
  Усреднение значений по сезону после удаления тренда.

  * Вычитаем тренд $y_t: = y_t - T_t$;
  * Формируются $s$ подгрупп: $G_i = \{y_i, y_{i + s}, \ldots, y_{i + ks}\}$;
  * $i$-е значение сезонности вычисляется усреднением по $i$-й группе $S_i = \overline{G_i}$.

* **Ошибка**&tab;&tab;
  $R_t = y_t - T_t - S_{(t\ mod\ s)}$.

![ex](https://yastatic.net/s3/education-portal/media/ex_3_stl_c09e4ba22c_fec382cf63.webp)

### STL-декомпозиция

Название метода расшифровывается как **S**easonal-**T**rend decomposition using **L**OESS. Является более продвинутой моделю для декомпозиции временного ряда. Напоминание: LOESS – взвешенная линейная регрессия, где вес объекта пропорционален расстоянию от него до точек обучающей выборки.

Принцип работы:

* Инициализация тренда нулем: $T^0=0$;
* В цикле по $k$ до сходимости:
  * Вычитаем из ряда текущее значение тренда $y_t: = y_t - T_t^k$.
  * Формируются $s$ подгрупп: $G_i = \{y_i, y_{i + s}, \ldots, y_{i + ks}\}$.
  * С помощью LOESS в каждой группе в каждый момент времени предсказывается сезонность $S_t$.
  * Вычитаем из ряда полученную сезонность $y_t: = y_t - S_t$.
  * С помощью LOESS предсказывается новое значение тренда $T_t^{k + 1}$.

*Замечание.* Пропущен шаг работы с выбросами.

![stl](https://yastatic.net/s3/education-portal/media/stl_f3b3a92cd7_343b7feb4d.webp)

Преимущества STL-декомпозиции:

* Больше настраиваемых параметров, позволяющих подогнать модель под любые данные.
* Сезонная компонента может изменяться с течением времени, и это изменение контролируется пользователем.
* Модель может быть устойчива к выбросам.

  ## handbook

  Учебник по машинному обучению

  ## title

  Временные ряды

  ## description

  Временные ряды

- 
  ## path

  /handbook/ml/article/analitika-vremennyh-ryadov

  ## content

  ## Автокорреляционная функция
	
Временной ряд – зависимые между собой наблюдения. Например, температура воздуха сегодня достаточно сильно зависит от вчерашнего показателя температуры. Эту зависимость хотелось бы описать численно. Для этого часто используют разные виды коэффициентов корреляции, например, корреляции Пирсона, Спирмена и Кендалла. Каждый из этих коэффициентов корреляции вычисляется по двум выборкам, корреляцию между которыми требуется посчитать. В данном случае мы имеем один временной ряд, и наша задача – оценить корреляцию между разными наблюдениями ряда, считая, что она не меняется со временем.
	
В качестве оценки корреляции значений $y_t$ и $y_{t+\tau}$ для любых $t$ рассмотрим коэффициент корреляции Пирсона ряда с самим собой со сдвигом на $\tau$. Тем самым мы получим численную оценку степени влияния значения $y_t$ на значение $y_{t+\tau}$:

$$r_{\tau} = \widehat{corr}(y_t, y_{t+\tau})   = \frac{\sum\limits_{t=1}^{T-\tau} \left(y_t - \overline{y}\right)\left(y_{t+\tau} - \overline{y}\right)}{\sum\limits_{t=1}^T \left(y_t - \overline{y}\right)^2},$$

где $\tau$ – лаг автокорреляции, а среднее вычисляется по всему ряду $\overline{y} = \frac{1}{T}\sum\limits_{t=1}^T y_t$.  Например, если мы хотим оценить степень влияния сегодняшней температуры на завтрашнюю, то посчитаем коэффициент корреляции исходного ряда и им же, сдвинутым на 1 день.
	
*Замечание.* Формула содержит некоторые упрощения при оценке ковариации и дисперсий.
	
Свойства коэффициента корреляции:
* $\vert r_{\tau}\vert \leqslant 1$;
* $r_{\tau} = 0$ – отсутствие автокорреляции, при этом значения могут быть зависимыми (см. подробнее про разницу между независимостью и некоррелированностью);
* $r_{\tau} > 0$ – положительная корреляция, то есть чем *больше* было значение вчера, тем оно будет *больше* сегодня;
* $r_{\tau} < 0$ – отрицательная корреляция, то есть чем *больше* было значение вчера, тем оно будет *меньше* сегодня;
* $\vert r_{\tau} \vert = 1$ означает строгую линейную зависимость.


Посмотреть простые примеры и потренировать свою интуицию вы можете в игре [Guess The Correlation](http://guessthecorrelation.com/).
	
Пусть мы посчитали значение автокорреляции. А как понять, значение 0.1 – это много или мало? На этот вопрос может ответить *статистический критерий Льюнга-Бокса* (Ljung–Box), который проверяет значимость отклонения $r_{\tau}$ от нуля. Основное правило, которое нужно здесь понять – если значение p-value критерия не превосходит $0.05$ (или другого *заранее* фиксированного порога значимости), то автокорреляция с лагом $\tau$ значима. Это число вычисляется с помощью стандартных статистических пакетов (например, [statsmodels](https://www.statsmodels.org/dev/generated/statsmodels.stats.diagnostic.acorr_ljungbox.html) в Питоне).
	
Рассмотрим временной ряд дорожно-транспортных происшествий за 14 лет с дискретностью 1 месяц, то есть с 12 измерениями в год. На графике мы видим явную сезонность. На нижнем графике изображена *коррелограмма* – график, визуализирующий автокорреляционную функцию. Точками на графике показаны значения автокорреляционной функции. Ее значение в нуле всегда равно 1, так как это автокорреляция ряда с собой же. Также мы видим, что значение $r_{12}$ является локальным максимумом, что означает высокую положительную корреляцию значения ряда за текущий месяц с аналогичным значением год назад. Иными словами, ряд со сдвигом на год ведёт себя «похожим образом», и это подкрепляет наше наблюдение про наличие годичной сезонности. Наоборот, значение $r_{6}$ минимально в своей окрестност и, что означает высокую отрицательную корреляцию значения ряда со значением полгода назад. Закрашенная область визуализирует границу незначимой автокорреляции, то есть тех значений автокорреляции, для которых не выявлена статистически значимое отличие от 0 (иначе говоря, доверительный интервал пересекает 0). Так мы видим, что последняя значимая сезонная автокорреляция это – это $r_{24}$. Кроме нее также значима сезонная корреляция $r_{12}$ и корреляция за половину сезона $r_6$. Из несезонных автокорреляций значимы оказались $r_{1}$ и $r_{2}$. Отсюда можно сделать вывод, что для построения прогноза значения $y_t$ имеет смысл рассматривать признаки $y_{t-1}, y_{t-2}, y_{t-6}, y_{t-12}, y_{t-24}$.

![ex_1_6fd71c0161.webp](https://yastatic.net/s3/education-portal/media/ex_1_6fd71c0161_ef125825e0.webp)

![ex_1_autocorr_77d8a02254.webp](https://yastatic.net/s3/education-portal/media/ex_1_autocorr_77d8a02254_20f571b267.webp)

Рассмотрим временной ряд потребления электричества в Австралии с дискретностью 30 минут. По графику мы можем заметить две разных сезонности – суточную и недельную. Кроме того, при наличии большего количества данных мы смогли бы увидеть еще и годовую сезонность. На данном графике видно повышенное потребление электричества в январе-феврале, когда в Австралии жарко и работает много кондиционеров. По графику автокорреляций мы видим, что наибольшую корреляцию имеют соседние измерения, а также значения сутки назад, двое суток назад, и т.д. Наоборот, значения 12, 36, ... часов назад имеют отрицательную корреляцию. 


![ex_2_edd214b6d4.webp](https://yastatic.net/s3/education-portal/media/ex_2_edd214b6d4_4e74f3807d.webp)

![ex_2_autocorr_0728c487f9.webp](https://yastatic.net/s3/education-portal/media/ex_2_autocorr_0728c487f9_d664a5bf1f.webp)

## Стационарные временные ряды
	
Временной ряд $y_t$ называется **стационарным**   
* **в узком смысле**, если для любых $\ t_1, ..., t_n, \tau$ вектор $(y_{t_1+\tau}, ..., y_{t_n+\tau})$ совпадает по распределению с $(y_{t_1}, ..., y_{t_n})$, то есть при сдвиге всех моментов времени на одно и тоже число совместное распределение значений временного ряда в эти моменты времени не поменяется.   
* **в широком смысле**, если 
    * $\mathbb{E} y_t^2 < +\infty$ для любого $t$. 
    * $\mathbb{E} y_t$ не зависит от $t$, то есть в среднем значение временного ряда постоянно.   
    * $cov(y_{t+\tau}, y_{s+\tau}) = cov(y_{t}, y_{s})$ для любых $t, s, \tau$, то есть значение автокорреляции зависит только от длины отрезка времени между двумя значениями.  
* **для гауссовских распределений**, то есть для случая, когда все векторы вида $(y_{t_1}, ..., y_{t_n})$ имеют нормальное распределение, определения эквивалентны. Это следует из того, что распределение гауссовского случайного вектора полностью определяется математическим ожиданием и ковариациями.
		
*Пример.* Рассмотрим временной ряд $y_t = \xi_1 \cos t + \xi_2 \sin t$, где $\xi_1, \xi_2$ и независимы и одинаково распределены, причем $\mathsf{P}(\xi_1=1) = \mathsf{P}(\xi_1=-1) = 1/2$.
	
Можем заметить, что $\mathbb{E} y_1 = 0$ и $cov(y_t, y_s) = \cos t\cos s\ \mathsf{D} \xi_1 + \sin t\sin s\ \mathsf{D} \xi_2 = \cos (t-s)$. Тем самым имеем стационарность в широком смысле.
	
Но при $t=0$ получаем $y_0 = \xi_1 \in \{-1, 1\}$, а при $t = \pi/4$ получаем 

$$y_{\pi/4} = \frac{\xi_1 + \xi_2}{\sqrt{2}} \in \left\{-\sqrt{2}, 0, \sqrt{2}\right\}.$$

Мы получили разные распределения, поэтому нет стационарности в узком смысле.   

**Некоторые примеры нестационарных временных рядов:**
* *Случайное блуждание* – пример: $y_t = y_{t-1} + \varepsilon_t$, где $\varepsilon_t$ – белый шум, то есть независимые одинаково распределенные случайные величины. Математическое ожидание постоянно, но ряд не является стационарным, поскольку $\mathsf{D} y_t$ бесконечно растет. 
* *Временной ряд с трендом* – пример: $y_t = \alpha + \beta t + \varepsilon_t$, где $\varepsilon_t$ – белый шум. Ряд не стационарен, так как $\mathbb{E} y_t$ меняется с течением времени. 
* *Временной ряд с сезонностью* – пример: $y_t = \sin t + \varepsilon_t$, где $\varepsilon_t$ – белый шум. Не стационарен, так как 

$$\mathbb{E} y_t=\begin{cases}
-1, \text{ при }t = -\pi/2 + 2\pi k; \\ 
1, \text{ при }t = t = \pi/2 + 2\pi k.
\end{cases}$$

Стационарный ряд визуально не имеет предсказуемых закономерностей. Если посмотреть на график такого ряда издалека, то он будет горизонтален. 
	
Ряд можно проверить строго на станционарность с помощью различных статистических критериев. Наиболее популярны следующие критерии:
* критерий KPSS (Kwiatkowski–Phillips–Schmidt–Shin): если p-value $\leqslant 0.05$, то отвергаем стационарность; 
* критерий Дики-Фуллера: если p-value $\leqslant 0.05$, то отвергаем \textbf{НЕ}стационарность.

Рассмотрим несколько примеров временных рядов:
* ряды **а**, **c**, **d**, **e**, **f**, **i** не стационарны, поскольку они имеют тренд;
* ряд **b** скорее всего стационарный, имеется выброс;
* ряды **d**, **g**, **h**, **i** не стационарны, потому что имеют сезонность.

![stationary_1_d37b1e56c7.webp](https://yastatic.net/s3/education-portal/media/stationary_1_d37b1e56c7_6a2ecf5808.webp)

Когда вы определяете, чем вызвано изменение данных – трендом или шумом – стоит учитывать природу данных. Например, колебания значений ряда **f** теоретически можно было бы объяснить шумом в данных, но по временной оси мы видим, что данные представлены за 15 лет, соответственно, понимаем данные колебания как изменения тренда. Аналогично, для ряда **d** мы говорим о наличии меняющегося тренда помимо годовой сезонности.

### Приведение к стационарным: стабилизация дисперсии
	
**Зачем?**
	
Данные методы рекомендуется использовать, если задача требует некоторой аналитики временного ряда. Если же требуется только построить точечный прогноз на будущее без построения предсказательных интервалов, то стабилизация дисперсии не является необходимой процедурой. Если же нас интересует предсказательный интервал, то многие методы лучше обрабатывают именно стационарные ряды, поэтому имеет смысл стабилизировать дисперсию.
	
**Преобразования:**
* Класс преобразований Бокса-Кокса с параметром $\lambda$:

$$z_t = 
\begin{cases} \ln y_t, & \lambda = 0 \\
  (y_t^\lambda - 1) / \lambda, & \lambda \not=0 
  \end{cases}.$$  
   

* Если есть предположения о зависимости $\mathsf{D} y_t$ от $t$, то можно рассмотреть ряд $z_t = y_t \left/ \sqrt{\mathsf{D} y_t} \right.$.
	
После построения прогноза для преобразованного ряда нужно сделать обратное преобразование.  

### Приведение к стационарным: тренд и сезонность
	
**Преобразования:**

* Дифференцирование ряда, то есть переход к ряду $(y'_t, t \in \{2, ..., T\})$, где $y'_t = y_t - y_{t-1}$. Данное преобразование используется для снятие тренда.
* Сезонное дифференцирование ряда, то есть переход к ряду $(y'_t, t \in \{s+1, ..., T\})$, где $y'_t = y_t - y_{t-s}$, $s$ – длина сезона. 
	
Преобразования можно применять несколько раз. Обычно сначала применяют сезонное дифференцирование.  

Посмотрим на пример. В критерии KPSS для исходного ряда $pvalue < 0.01$, то есть ряд можно считать нестационарным. После логарифмирования ряда $pvalue < 0.01$, а после ещё и сезонного дифференцированная $pvalue > 0.1$, тем самым полученный ряд мы уже не можем отличить от стационарного.

![ex_5_log_diff_43309ae800.webp](https://yastatic.net/s3/education-portal/media/ex_5_log_diff_43309ae800_072a5d4f53.webp)

### Модели вида экспоненциального сглаживания
	
#### Простое экспоненциальное сглаживание

Не редко временной ряд выглядит довольно шумным, что может достаточно плохо сказаться на работе других моделей и подходов к анализу этого временного ряда. В таком случае можно попытаться сгладить значения ряда. Далее мы рассмотрим несколько моделей сглаживания ряда, в том числе при наличии тренда и сезонности ряда. Помимо сглаживания истории ряда, с помощью данных методов можно также осуществлять простое прогнозирование ряда.
	
Пусть имеется временной ряд $y_t$. В результате экспоненциального сглаживания получается новый временной ряд $\widehat{y}$ по правилу

$$\widehat{y}_{t+1\vert t} = \alpha y_t + (1 - \alpha) \widehat{y}_{t\vert t - 1},$$

где $\widehat{y}_{t+h\vert t}$ – прогноз значения $y_{t+h}$ в момент времени $t$, а $\alpha$ – *параметр сглаживания*.

Смысл преобразования следующий – сглаженное значение в момент времени $t+1$ есть взвешенная комбинация предыдущего значения ряда $y_t$ и предыдущего сглаженного значения ряда $\widehat{y}_{t\vert t - 1}$.
	
Свойства:
* при $\alpha \approx 1$ больший вес последнему значению ряда, поэтому получается слабое сглаживание $\widehat{y}_{T+1\vert T} \approx y_T$; 
* при $\alpha \approx 0$ больший вес отдается предыдущему сглаженному значению, и получается сильное сглаживание, что в пределе вырождается в среднее $\widehat{y}_{T+1\vert T} \approx \overline{y}$;
* Оптимальное значение $\alpha^{\ast}$ можно подобрать либо по графику, либо оптимизируя

$$\sum\limits_{t=t_0}^T \left(\widehat{y}_{t}(\alpha) - y_t\right)^2 \to \min_{\alpha}.$$

Существуют следующие эмпирические правила:
* если $\alpha^{\ast} \in (0, 0.3)$ то ряд стационарен, можно применять экспоненциальное сглаживание без риска большой потери информации; 
* если $\alpha^{\ast} \in (0.3, 1)$ то ряд нестационарен, применение экспоненциального сглаживания может привести к потере информации или смещению.

Примеры: на каждом из графиков изображен исходный ряд (синий) и сглаженный ряд (оранжевый) для разных значений параметра сглаживания. Если имеется тренд или сезонность, то при большом сглаживании полученный ряд начинает «запаздывать» за исходным рядом.

![es001_5ecc7e857a.webp](https://yastatic.net/s3/education-portal/media/es001_5ecc7e857a_061711ef44.webp)

![es01_9f507d59c5.webp](https://yastatic.net/s3/education-portal/media/es01_9f507d59c5_ec54dd8b9a.webp)
	
**Откуда взялась эта формула экспоненциального сглаживания?**
	
Покажем, что сглаженное значение соответствует прогнозу величины $x$ в момент времени $T+1$, подбираемому по правилу

$$\sum\limits_{t = 0}^T\beta^{T - t}(y_{t} - x)^2 \rightarrow \min\limits_x.$$ 
	
Иначе говоря, для прогнозирования мы берем взвешенный MSE с экспоненциально убывающими по времени весами.
	
Приравняем производную к нулю:

$$2\sum\limits_{t = 0}^T\beta^{T - t}(y_{t} - x) = 0$$ 
		
Отсюда выразим $x$ и воспользуемся разложением функции $\frac{1}{1-x}$ в ряд Тейлора  

$$x = \dfrac{\sum\limits_{t = 0}^{T}\beta^{T - t}y_t}{\sum\limits_{t = 0}^{T}\beta^t} 
	 \approx \dfrac{\sum\limits_{t = 0}^{T}\beta^{T - t}y_t}{1/(1 - \beta)} 
	  =(1 - \beta) \sum\limits_{t = 0}^T\beta^{T - t}y_t =$$ 

$$ =  (1 - \beta)  y_T + (1 - \beta) \beta \sum\limits_{t=0}^{T - 1}\beta^{T - 1 - t}y_t 
	 = (1 - \beta)y_T + \beta \widehat{y}_{T|T - 1}$$ 
	
Тем самым мы получили модель экспоненциального сглаживания для $\beta = 1 - \alpha$.
	
### Модель Хольта
	
Аддитивный линейный тренд:
	
Прогноз на $d$ шагов вперед выражается с помощью линейной функции от числа шагов, где коэффициенты меняются по формулам, аналогичным экспоненциальному сглаживанию

$$\widehat{y}_{t+d|t} = a_t + b_t \cdot d,$$

$$a_t = \alpha y_t + (1-\alpha) (a_{t-1} + b_{t-1})$$

$$b_t = \beta (a_t - a_{t-1}) + (1-\beta) b_{t-1}$$  
	
Модель для мультипликативного линейного тренда выглядит аналогично

$$\widehat{y}_{t+d|t} = a_t b_t^d,$$

$$a_t = \alpha y_t + (1-\alpha) (a_{t-1} b_{t-1})$$

$$b_t = \beta \frac{a_t}{a_{t-1}} + (1-\beta) b_{t-1}$$ 
	
**Модель Хольта: пояснение формул**
	
Поясним на примере аддитивного тренда, почему формулы для $a_t$ и $b_t$ получаются именно такими.
	
Заметим, что для $d=0$ и $d=1$ и момента времени $t+1$ получаем $\widehat{y}_{t + 1\vert t + 1} = a_{t + 1}$, $\widehat{y}_{t + 1\vert t} = a_{t} + b_{t}$. Хотелось бы, чтобы эти прогнозы примерно совпадали, то есть чтобы имело место $a_{t + 1} - a_{t} \approx b_{t}$.

* Рассмотрим ряд разностей $\Delta y_t = a_{t + 1} - a_{t}$ и задачу константного прогноза для него (то есть $\Delta \widehat{y}_{t \vert t - 1} = b$) методом простого экспоненциального сглаживания
$$\sum\limits_{i = 0}^{t}(1 - \beta)^{t - i}(\Delta y_i - b)^2 \rightarrow \min_b.$$

  Ее решение мы уже получили ранее:
  
  $$b_t = \beta \Delta y_{t - 1} + (1 - \beta)b_{t - 1} = \beta (a_t - a_{t-1}) + (1-\beta) b_{t-1}.$$ 
	
  Получилась формулу для $b_t$ в модели аддитивного тренда.
	
* Далее, мы хотим, чтобы имело место $a_{t + 1} \approx b_{t} + a_{t}$. Рассматривая для $y_t$ экспоненциальное сглаживание, в котором в качестве предыдущего значения сглаженного ряда берется $b_t+a_t$, а в качестве нового – $a_{t + 1}$, получаем

  $$a_t = \alpha y_t + (1-\alpha) (a_{t-1} + b_{t - 1}).$$
	
### Модель Хольта-Уинтерса
	
**Аддитивная сезонность с трендом:**

$$\widehat{y}_{t+d|t} = a_t + db_t + s_{t-m+(d \text{ mod } m)},$$

$$a_t = \alpha (y_t - s_{t-m}) + (1-\alpha) (a_{t-1} + b_{t-1})$$

$$b_t = \beta (a_t - a_{t-1}) + (1-\beta) b_{t-1}$$

$$s_t = \gamma(y_t - a_t) + (1-\gamma)s_{t-m}$$

где $m$ – длина сезона 

**Мультипликативная сезонность**
	
*Без тренда*

$$\widehat{y}_{t+d|t} = a_t \cdot s_{t-m+(d \text{ mod } m)},$$

$$a_t = \alpha (y_t / s_{t-m}) + (1-\alpha) a_{t-1}$$

$$s_t = \gamma(y_t / a_t) + (1-\gamma)s_{t-m}$$
	
*С линейным трендом*

$$\widehat{y}_{t+d|t} = (a_t + db_t) s_{t-m+(d \text{ mod } m)},$$

$$a_t = \alpha \frac{y_t}{s_{t-m}} + (1-\alpha) (a_{t-1} + b_{t-1})$$

$$b_t = \beta (a_t - a_{t-1}) + (1-\beta) b_{t-1}$$

$$s_t = \gamma\frac{y_t}{a_t} + (1-\gamma)s_{t-m}$$

**Разные модели с трендом и сезонностью**

![trends_variants_338d57ad2e.webp](https://yastatic.net/s3/education-portal/media/trends_variants_338d57ad2e_6cd82b851a.webp)

### Адаптивное сглаживание

В примерах выше мы видели, что при изменении локального тренда ряда экспоненциальное сглаживание запазывает за значениями ряда при использовании сильного сглаживания. Если же использовать слабое сглаживание, то существенного запаздывания не проиходит, но ряд остается шумным. Если для ряда предполагаются значительные структурные изменения, можно использовать модель адаптивного экспоненциального сглаживания, в которой параметр сглаживания может меняться для разных отрезков временного ряда.

Пусть $\widehat{y}_t$ – прогноз значения $y_t$ в момент времени $t-1$ обычным экспоненциальным сглаживанием, а $\widehat{\varepsilon}_t = y_t - \widehat{y}_t$ – ошибка прогноза, сделанного на шаге $t-1$.  
	
Определим следующие значения
* Среднее значение ошибки:

$$E_t = \gamma \widehat{\varepsilon}_t + (1-\gamma)E_{t-1}.$$

* Средний разброс ошибки: 

$$A_t = \gamma \left|\widehat{\varepsilon}_t\right| + (1-\gamma)A_{t-1}.$$

* $K_t = E_t/A_t$ – статистика, которая сигнализирует, насколько адекватно модель работает в момент времени $t$.  
    * $K_t \approx \pm 1$ – модель систематически ошибается в одну сторону. 
    * $K_t \approx 0$ – модель работает адекватно. 
	
Обычно берут значения $\gamma \in (0.05, 0.1)$. 
Чтобы экспоненциальное сглаживание быстро приспосабливалось к резким структурным изменениям берут $\alpha_t = \min \left(|K_t|, 1\right)$.

Однако, у данного подхода есть и недостатки, например, 
* плохо реагирует на одиночные выбросы;
* требует подбора $\gamma$. 

![adaptive_7840bb4683.webp](https://yastatic.net/s3/education-portal/media/adaptive_7840bb4683_0b340f664c.webp)

  ## handbook

  Учебник по машинному обучению

  ## title

  Аналитика временных рядов

  ## description

  *null*

- 
  ## path

  /handbook/ml/article/modeli-vida-arima

  ## content

  Прежде чем перейти к рассмотрению модели ARIMA, познакомимся сначала с двумя другими моделями: скользящего среднего и моделью авторегрессии.

## Модель скользящего среднего MA($q$)
	
Модель скользящего среднего порядка $q$ или просто MA($q$) предполагает следующую зависимость даннных:

$$y_t\ =\ \mu + \varepsilon_t + \theta_1 \varepsilon_{t-1} + ... + \theta_q \varepsilon_{t-q},$$

где $y_t$ &mdash; стационарный ряд со средним $\mu$, а $\varepsilon_t$ &mdash; гауссовский белый шум, то есть $\varepsilon_t \sim \mathcal{N}(0, \sigma^2)$ и независимы. 
	
По сути наш ряд $y_t$ выражается через сумму некоторого фиксированного среднего $\mu$, значения белого шума в текущий момент времени $\varepsilon_t$ и не более $q$ предыдущих значений белого шума, домноженных на некоторые коэффициенты, которые являются параметрами модели. 

Рассмотрим некоторые свойства модели MA($q$). Как уже было упомянуто выше, ряд $y_t$ будет являтьcя стационарным со средним $\mu$. Найдем также $\mathsf{D}y_t$. Воспользовавшись свойством независимости для $\varepsilon_t$, можем заключить, что 

$$\mathsf{D}y_t = \left(1 + \theta_1^2 + \dots + \theta_q^2\right)\sigma^2.$$

Посчитаем автоковариационную функцию для ряда $y_t$, то есть найдем значение $cov(y_t, y_{t + \tau})$. Легко понять, что если $\tau > q$, то $cov(y_t, y_{t+\tau})$ = 0, т.к. $\varepsilon_t$ независимы. Если же $\tau \leq q$, то тогда 

$$cov(y_t, y_{t+\tau}) = \left(\theta_{\tau} + \theta_1\theta_{\tau + 1} + \dots + \theta_{q-\tau}\theta_{q}\right)\sigma^2.$$

Записав более компактно, можем получить:

$$
cov(y_t, y_{t+\tau}) = \begin{cases}
\sigma^2\sum_{j=0}^{q-\tau}\theta_j\theta_{\tau + j}, & \tau \leq q; \\
0 & \tau > q;
\end{cases}
$$

где $\theta_0 = 1$. Из посчитанных значений для дисперсии и ковариационной функции, можете попробовать получить выражение и для автокорреляционной функции. Ее особенностью будет как раз равенство нулю на лаге, превосходящим $q$.

Посмотрим на визуализацию:

![ma2_modeling_a6f7777dc7.svg](https://yastatic.net/s3/education-portal/media/ma2_modeling_a6f7777dc7_1901e89036.svg)
	
## Модель авторегрессии AR($p$)
Модель авторегрессии для временного ряда можно записать следующим образом:

$$
y_t\ =\ \alpha + \varphi_1 y_{t-1} + ... + \varphi_p y_{t-p} + \varepsilon_t,
$$

где $y_t$ &mdash; стационарный ряд, а $\varepsilon_t$ &mdash; гауссовский белый шум, то есть $\varepsilon_t \sim \mathcal{N}(0, \sigma^2)$ и независимы. Отметим, что, вообще говоря,  для стационарности нужны некоторые условия на коэффициенты $\varphi_1, ..., \varphi_p$.
	
По сути наш ряд $y_t$ выражается через сумму некоторого фиксированного числа $\alpha$, значения белого шума в текущий момент времени $\varepsilon_t$ и не более $p$ предыдущих значений этого же ряда, домноженных на некоторые коэффициенты, которые являются параметрами модели.
	
Другими словами, модель AR($p$) &mdash; это модель $\textit{линейной регрессией}$ для которой 
* **Таргет:** $y_t$ &mdash; значение ряда в момент времени $t$ 
* **Признаки:** $y_{t-1}, ..., y_{t-p}$ &mdash; значения ряда в предыдущие моменты времени
		
Введем **$L$ &mdash; оператор сдвига**, обладающий следующими свойствами:
* применение $L$ к ряду дает предыдущее значение этого же ряда: $Ly_t = y_{t-1}.$  
* применение $L$ к белому шуму дает предыдущее значение шума: $L\varepsilon_t = \varepsilon_{t-1}.$  
* применение $L$ к константе &mdash; это константа: $Lc = c.$

Оператор $L$ иногда называют также **лаговым** оператором. Можно рассматривать функции от оператора сдвига, например, кратное применение оператора $L$: $L^2 y_t = L(L y_t) = L(y_{t-1}) = y_{t-2}$ или $L^{-1} y_t= y_{t+1}$. Для записей некоторых моделей временных рядов будет удобно использовать лаговый многочлен: 

$$\varphi(L) = \sum_{i=1}^p \varphi_i L^{i} $$

Обратным к оператору $\varphi(L)$ называют оператор $\varphi^{-1}(L)$ такой, что:

$$\varphi(L)\varphi^{-1}(L)y_t = \varphi^{-1}(L)\varphi(L)y_t=y_t$$

Так, например, для $\lvert \varphi \rvert < 1$ можно заключить, что:
$$\frac{1}{1 - \varphi L} = \left(1-\varphi L \right)^{-1} = \sum_{i=1}^{\infty}\varphi^{i}L^{i}$$

Рассмотрим модель AR($p$):

$$y_t\ =\ \alpha + \varphi_1 y_{t-1} + ... + \varphi_p y_{t-p} + \varepsilon_t$$ 
 
С помощью оператора сдвига ее можно представить в следующем виде:

$$a(L) y_t\ =\ \alpha + \varepsilon_t,$$

где $a(z) = 1 - \varphi_1 z - ... - \varphi_p z^p$ &mdash; характеристический полином.   
	
Сформулируем пару важных утверждений:
* Любой стационарный (в широком смысле) процесс представим в виде $MA(\infty)$, то есть в виде модели скользящего среднего с неограниченным количеством слагаемых (конечное или бесконечное число). Этот результат так же известен как **теорема Волда** о декомпозиции временного ряда.
* Модель $AR(p)$ задает стационарный временной ряд $\Longleftrightarrow$ все комплексные корни $a(z)=0$ лежат вне единичного круга. 
	
Приведем пояснение второго утверждения. В самом деле, пусть $z_1, ..., z_p$ &mdash; все его комплексные корни (их ровно $p$ с учетом кратности), тогда справедливо представление:

$$a(z) = (z-z_1)...(z-z_p) = z_1...z_p \left(1 - \frac{z}{z_1}\right) ... \left(1 - \frac{z}{z_p}\right)$$

Тогда при представлении временного ряда в виде

$$y_t\ =\ \frac{\alpha + \varepsilon_t}{a(L)}$$

и дальнейшего его разложения на простые дроби возникнут слагаемые вида

$$\frac{\varepsilon_t}{1 - \frac{L}{z_j}}.$$

Если при этом $z_j$ лежит внутри единичного круга или на его границе, то соответствующий ряд будет расходящимся. На самом деле, случай $z_j=1$ мы в дальнейшем учтем.

В качестве примера рассмотрим подробнее модель $AR(1)$.
Зависимость имеет вид $y_t = \alpha + \varphi y_{t - 1} + \varepsilon_t$, где $\varepsilon_t \sim \mathcal{N}(0, \sigma^2)$. Для данного ряда можно выписать следующие свойства:
* Уравнение $1 - \varphi z = 0$, имеет корень $\lambda = 1/\varphi$. 
* Тем самым, $AR(1)$ стационарен $\Longleftrightarrow$ $\lvert\varphi\rvert < 1$. Кроме того, чем меньше $\varphi$, тем предыдущее значение ряда вносит меньший вклад в текущее значение.
* Если ряд стационарен, то:
	* $\mathsf{E} y_t = \frac{\alpha}{1-\varphi}$
	* $\mathsf{D} y_t = \dfrac{\sigma^2}{1-\varphi^2}$
	* $cov (y_t, y_{t - h}) = \varphi^h \cdot \dfrac{\sigma^2}{1-\varphi^2}$.

Разберем первое равенство, остальные получаются аналогично. Возьмем математическое ожидание в уравнении ряда

$$\mathsf{E} y_t = \alpha + \varphi \mathsf{E} y_{t - 1} +\mathsf{E} \varepsilon_t$$

Поскольку ряд стационарен, то его математическое ожидание не меняется во времени, а для белого шума математическое ожидание равно нулю. Тем самым мы получаем уравнение на $m = \mathsf{E} y_t$, откуда следует доказываемая формула.

Таким образом, в зависимости от значения $\varphi$ мы можем получить следующие результаты:
* Если $\lvert\varphi\rvert < 1$, то $y_t = \mu + \sum\limits_{j = 0}^{\infty}\varphi^j\varepsilon_{t - j}$ &mdash; представление ряда в виде MA($\infty$). 
* Если $\lvert\varphi\rvert = 1$, то $AR(1)$ &mdash; это случайное блуждание. 
* Если $\lvert\varphi\rvert > 1$, то $AR(1)$ &mdash; экспоненциально растущий процесс. 

Посмотрим на визуализацию.

![ar1_modeling_2e6dc1d6f7.svg](https://yastatic.net/s3/education-portal/media/ar1_modeling_2e6dc1d6f7_82d4f46e94.svg)

* В первом случае мы имеем модель $y_t = - 0.5 y_{t - 1} + \varepsilon_t$, отрицательный коэффициент является следствием больших колебаний ряда. 

* Во втором случае модель $y_t = 0.9 y_{t - 1} + \varepsilon_t$, большой положительный коэффициент делает ряд менее шумным. 

* В третьем случае показано несколько рядов вида случайного блуждания $y_t = y_{t - 1} + \varepsilon_t$, что соответствует случаю $\varphi=1$. 

* В четвертом случае показан экспоненциальный процесс $y_t = 1.1 y_{t - 1} + \varepsilon_t$, на графике шум уже не заметен из-за масштаба.

На немного вернемся к модели MA($q$). Чуть выше мы выяснили, что при некоторых условиях на коэффиценты $\varphi$ временной ряд модели AR($p$) будет стационарным, а значит имеет представление в виде MA($\infty$). На самом деле, модель скользящего среднего порядка $q$ тоже можно представить с помощью оператора $L$ следующим образом:

$$y_t\ =\ \mu + \varepsilon_t + \theta_1 \varepsilon_{t-1} + ... + \theta_q \varepsilon_{t-q}\ =\  
		\mu + b(L)\varepsilon_t$$

где $b(z) = 1 + \theta_1 z + ... + \theta_q z^q$ &mdash; **характеристический многочлен**. Для простоты изложения пусть $\mu = 0$. Важным при такой записи оказывается понятие обратимости, то есть представления в виде

$$
\varepsilon_t = b^{-1}(L)y_t,
$$

которое означает, что ряд можно представить в виде бесконечной авторегрессионной модели.
Здесь, как и в рассуждениях выше, можно заключить, что временной ряд $y_t$ обратим, если все комплексные корни $b(z) = 0$ лежат вне единичного круга. 

## Модель ARMA($p, q$)
	
Модель ARMA($p, q$) по сути является суммой моделей $AR(p)$ и $MA(q)$, иначе говоря, модель есть сумма нескольких предыдущих значений ряда и нескольких предыдущих значений белого шума с некоторым коэффициентами.

$$y_t\ =\ \alpha + \varphi_1 y_{t-1} + ... + \varphi_p y_{t-p}
	 + \varepsilon_t + \theta_1 \varepsilon_{t-1} + ... + \theta_q \varepsilon_{t-q}$$
	
Эквивалентную запись ряда в терминах оператора сдвига можно получить, рассмотрев два многочлена

$$a(L) y_t = \alpha + b(L) \varepsilon_t$$

или

$$y_t = \mu + \frac{b(L)}{a(L)} \varepsilon_t,$$

где $a(z) = 1 - \varphi_1 z - ... - \varphi_p z^p,$ и $b(z) = 1 + \theta_1 z + ... + \theta_q z^q$.
Заметим, что во втором представлении константа $\alpha$ заменена на $\mu = \mathsf{E} y_t$. На самом деле, стационарность такого ряда будет определяться только его AR($p$) компонентой, то есть значениями коэффициентов $\varphi$, так ряд в модели MA($q$) всегда является стационарным.

## Модель ARIMA($p, d, q$)
	
Модель ARIMA($p, d, q$) &mdash; это расширение моделей типа ARMA на нестационарные временные ряды, которые однако могут стать стационарным после применениея процедуры дифференцирования ряда. Модель ARIMA($p, d, q$) для ряда $y_t$ определяется как модель ARMA($p, q$) для ряда разностей порядка $d$ ряда $y_t$.  
	
* Разность порядка 1: $y_t - y_{t-1} = (1 - L) y_t$.
* Разность порядка 2: $(1 - L)^2 y_t = (1 - L) (y_t - y_{t-1}) = (y_t - y_{t-1}) - (y_{t-1} - y_{t-2}) = y_t - 2y_{t-1} + y_{t-2}$.
	
Получаем формулу модели ARIMA:

$$a(L) (1 - L)^d y_t = \alpha + b(L) \varepsilon_t$$

или

$$(1 - L)^d y_t = \mu + \frac{b(L)}{a(L)} \varepsilon_t.$$ 

То есть многочлен $\widetilde{a}(z) = a(z) (1-z)^d$ имеет $d$ единичных корней. 
Тем самым такая модель позволяет учесть нестационарности, в частности, тренд. 

В качестве примера рассмотрим процесс случайного блуждания:

$$y_t = y_{t-1} + \varepsilon_t, $$

где $\varepsilon_t$ &mdash; белый шум. Как уже упомяналось ранее, такой ряд не является стационарным. Однако, если мы применим операцию дифференцирования, то можем перейти к новому, уже стационарном ряду $y'_t = y_t - y_{t-1}$, который можно записать в виде:

$$ y'_t = \varepsilon_t$$

## Частичная автокорреляция

Для модели скользящего среднего порядка $q$ мы выяснили, что значения автокорреляционной функции для такого ряда оказывается равной нулю после лага $q$. Эта особенность позволяет использовать автокорреляционную функцию для определения порядка модели скользящего среднего. Возникает разумный вопрос, как оценить порядок $p$ для модели AR($p$)? Здесь оказывается полезным понятие частичной (частной) автокорреляционной функции. 

**Частичная автокорреляция (PACF)** &mdash; корреляция ряда с собой после снятия линеной зависимости от промежуточных значений ряда. Иначе говоря, мы хотим как-то учесть опосредованного влияние промежуточных значений ряда и оценить непосредственное влияние $y_{t - \tau}$ на $y_t$. Чуть более формально частичную автокорреляцию можно записать следующим образом:

$$
\gamma_{\tau} = \begin{cases} corr(y_{t+1}, y_t), & \tau=1; 
	\\ corr\left(y_{t+\tau} - y_{t+\tau}^{h-1}, y_t - y_t^{\tau-1}\right), & \tau\geqslant2, \end{cases}
$$

где $y_t^{\tau-1}$ &mdash; линейная регрессия на $y_{t-1}, y_{t-2}, ..., y_{t - (\tau-1)}$: 
* $y_t^{\tau-1} = \varphi_1 y_{t-1} + \varphi_2 y_{t-2} + ... + \varphi_{\tau-1} y_{t - (\tau-1)}$ 
* $y_{t+\tau}^{\tau-1} = \varphi_1 y_{t+\tau-1} + \varphi_2 y_{t+\tau-2} + ... + \varphi_{\tau-1} y_{t+1}$

Пример для $\tau=2$:

$$\gamma_{2} = corr\left(y_{t+2} - \varphi_1 y_{t+1}, y_t - \varphi_1 y_{t-1}\right)$$
		
где $\varphi_1$ &mdash; МНК-оценка в модели $y_t = \varphi y_{t-1}$.

Можно показать, что значение частиной автокорреляции для модели авторегресии AR($p$) будет ненулевой для лагов $\tau \leq p$ и равняться нулю для лагов $\tau > p$. Имеет место быть полная аналогия с автокорреляционной функцией и моделью MA($q$). Таким образом, исследование поведения автокорреляционной и частичной автокорреляционной функции может быть использовано для определения порядка $q$ модели скользящего среднего и порядка $p$ модели авторегрессии соответсвтенно.

## Оценка коэффициентов в ARIMA
	
Пусть гиперпараметры $p, d, q$ фиксированы.	В предположении, что $\varepsilon_t$ &mdash; гауссовский белый шум, в нашей модели мы можем выписать функцию правдоподобия $L_y(\theta, \varphi, \alpha) = p_{\theta, \varphi, \alpha}(y_1, ..., y_T),$ где $ p_{\theta, \varphi, \alpha}(a_1, ..., a_T)$ &mdash; соместная плотность. Из-за того, что $\varepsilon_t$ имеют нормальное распределение, она будет иметь разумный вид. Соответственно, в качестве оценок параметров берется оценка максимального правдоподобия.   

Для поиска начальных приближение для параметров $p$ и $q$ воспользуемся автокорреляционной и частичной автокорреляционной функцией.
* Начальное приближение $p$: последний значимый пик у PACF.
* Начальное приближение $q$: последний значимый пик у ACF.

Далее обычно используется поиск по сетке вокруг подобранных значений, минимизируя информационный критерий:
* $AIC = -2\ell^* + 2(p+q+1)$ &mdash; критерий Акаике; 	
* $AIC_c = -2\ell^* + \frac{2(p+q+1)(p+q+2)}{T-p-q-2}$ &mdash; критерий Акаике (короткие ряды); 
* $BIC = -2\ell^* + (\log T - 2)(p+q+1)$ &mdash; Байесовский информационный критерий или критерий Шварца,
	
где $\ell^* = \ln L_y\left(\widehat{\theta}, \widehat{\varphi}, \widehat{\alpha}\right)$ &mdash; логарифм функции правдоподобия, $T$ &mdash; длина временного ряда.
	
Приведем некоторый план при применению модели ARIMA для прогнозирования временных рядов.
	
1. Анализ выбросов: замена нерелевантых выбросов на `NA` или усреднение по соседним элементам.  	
2. Стабилизация дисперсии (преобразования).    
3. Дифференцирование, если ряд не стационарен.    	
4. Выбор пилотных $p$ и $q$ по PACF и ACF.    	
5. Вокруг этих параметров подбираем оптим. модель по $AIC$/$AIC_c$.    
6. Пошаговое построение прогноза:               
	  — для $t \leqslant T$: $\varepsilon_t \Longrightarrow \widehat{\varepsilon}_t = y_t - \widehat{y}_t$;  
	  — для $t > T$: $\varepsilon_t \Longrightarrow 0$;  
	  — для $t > T$: $y_t \Longrightarrow \widehat{y}_t$. 
		
7. Построение предсказательного интервала:  
		
	— если остатки модели нормальны и гомоскедастичны (дисперсия постоянна), то строится теоретический предсказательный интервал

	$$\widehat \sigma^2(h) = \widehat \sigma^2 \left(1 + \sum\limits_{i = 1}^{h - 1} \widehat{\psi}_i^2\right)$$

	где $h$ &mdash; горизонт прогнозирования, $\widehat\sigma^2$ &mdash; оценка на дисперсию шума $\varepsilon_t$, $\widehat{\psi}_i$ &mdash; коэф. для ряда при его представлении в виде бесконечного процесса скользящего среднего. И $\widehat\sigma^2$, и $\widehat{\psi}_i$ могут быть выражены через оценки на параметры $\varphi$ и $\theta$.       
	— иначе интервалы строятся с помощью бутстрепа.

## Модели SARIMA и ARIMAX

Рассмотрим некоторые расширение модели ARIMA. Обобщение модели ARIMA на ряды с наличием сезонной составляющей назвается SARIMA. Пусть $s$ &mdash; известная сезонность ряда. Добавим в модель ARIMA($p, d, q$) компоненты, отвечающие за значения в предыдущие сезоны. Тогда модель SARIMA $(p, d, q)\times (P, D, Q)_s$ может быть записана следующим образом:

$$(1 - L)^d (1 - L^s)^D y_t = \mu + \frac{b(L) B(L^s)}{a(L) A(L^s)} \varepsilon_t,$$

где 

$$a(z) = 1 - \varphi_1 z - ... - \varphi_p z^p,$$ 

$$b(z) = 1 + \theta_1 z + ... + \theta_q z^q,$$

$$A(z) = 1 - \varphi_1^s z - \dots - \varphi_P z^P,$$

$$B(z) = 1 + \theta^s_1 z + \dots + \theta_Q^s z^Q.$$

Параметр сезонного дифференцирования $D$, а также параметры $P, Q$ подбираются из тех же соображений, что и для $p, d, q$, но только с поправкой, что делается это с учетом сезонности $s$. ARIMAX &mdash; обобщение модели ARIMA, которая учитывает некоторые экзогенные факторы. Пусть $x_t \in \mathbb{R}^n$ &mdash; ряд регрессоров, *известный до начала прогноза*.

Простой вариант:

$$(1 - L)^d y_t = \mu + \sum_{i=1}^n \frac{\beta_i}{a(L)} x_t^i + \frac{b(L)}{a(L)} \varepsilon_t$$

Общий случай:

$$(1 - L)^d y_t = \mu + \sum_{i=1}^n \frac{u_i(L)}{v_i(L)} x_t^i + \frac{b(L)}{a(L)} \varepsilon_t$$

*Пример:* $x_t = I \{\text{в момент времени t праздник}\}$

Вышеукзанные модели можно объединить и получить SARIMAX $(p, d, q)\times (P, D, Q)_s $:
	
$$(1 - L)^d (1 - L^s)^D y_t = \mu + \sum_{i=1}^n \frac{u_i(L)}{v_i(L)} x_t^i + \frac{b(L) B(L^s)}{a(L) A(L^s)} \varepsilon_t$$
	
Проведем аналогию с линейной регрессией. Это линейная по признакам модель, в которой
* **Отклик:** $y_t$ &mdash; значение ряда в моменты времени $t$
* **Признаки:**  
	* $y_{t-1}, ..., y_{t-p}$ &mdash; значения ряда в предыдущие моменты времени  
	* Значение ряда за предыдущие сезоны  
	* Значения признаков в предыдущие моменты времени  
	* Значения признаков в предыдущие сезоны  
* **Ошибка:** сумма шума за предыдущие моменты времени и предыдущие сезоны.

  ## handbook

  Учебник по машинному обучению

  ## title

  Модели вида ARIMA

  ## description

  *null*

- 
  ## path

  /handbook/ml/article/zadacha-ranzhirovaniya

  ## content

  Человек, который пользуется интернетом, часто начинает решение своих задач с поиска. Поисковая система по запросу помогает найти самую полезную для пользователя информацию, будь то поиск видео или научных статей. Для решения такой задачи необходимо отсортировать по полезности имеющуюся в базе информацию и выдать самую необходимую. Обычно такую сортировку называют *ранжированием*, полезность — *релевантностью*, а соответствующую задачу — *задачей ранжирования*.

Опишем задачу формально и введём обозначения.

$$D\ -\  \text{коллекция документов (или других объектов)}
$$

$$Q\ -\  \text{множество запросов}
$$

$$\forall q \in Q\ D_q \in D\ -\  \text{набор документов, потенциально релевантных запросу}
$$

**Задача:** Отсортировать документы внутри $D_q$ по убыванию релевантности запросу.

Уже по формулировке задачи видно, что построение решения разбивается на несколько стадий. Сначала нужно сформировать набор кандидатов $D_q$, а уже потом строить финальную сортировку. Подробнее об этом в разделе про методы ранжирования.

## Примеры

### WEB-поиск

Классический пример задачи ранжирования — поиск информации в интернете. Пользователь задаёт запрос, и под этот запрос формируется выдача, в которой сайты (документы) расположены по убыванию полезности. В наших обозначениях:

$$D\ -\  \text{база веб-страниц, проиндексированных поисковой системой}
$$

$$Q\ -\  \text{множество запросов пользователей}
$$

Чтобы решать задачу ранжирования с помощью машинного обучения, необходимо иметь датасет с оценками асессоров. Обычно по парам (запрос, документ) собирают данные о том, насколько документ релевантен запросу. Такие оценки называются метками релевантности.

### Поиск синонимов

Задача подбора синонимов по слову тоже может восприниматься как задача ранжирования, поскольку похожесть слов друг на друга — неоднозначное свойство, и некоторые пары слов больше похожи, чем другие. Поэтому можно сортировать слова по похожести на слово из запроса. Если $V$ — доступный словарь, то:

$$D = V\ -\  \text{список всех слов}
$$

$$Q = V\ -\  \text{cлова, для которых можно искать синонимы}
$$

Для построения моделей можно использовать датасет пар синонимичных слов. Обучив модель классификации, обычно мы получаем предсказатор вероятности положительного класса. Отсортировав слова по предсказанной вероятности синонимичности, мы решим задачу ранжирования.

### Рекомендательная система

Рекомендательные системы встроены во многие онлайн сервисы. Если вы заходите на сервис с фильмами, система порекомендует фильмы для вас, если в социальную сеть — посты или новые видео, которые вас заинтересуют. Это тоже отбор самых полезных объектов, но «запрос» в данном случае — это сам пользователь.

$$D = I\ -\  \text{список айтемов, доступных на сайте (объявления, фильмы)}
$$

$$Q = U\ -\  \text{пользователь вместе с его историей}
$$

Для решения задачи построения рекомендаций используются свои методы, они рассмотрены в отдельной главе.

## Метрики качества ранжирования

Предположим, мы решили задачу ранжирования. Обычно это делается обучением некоторой функции от запроса и документа $a_\theta(q, d)$ ($\theta$ — это параметры модели). Если такая функция готова, то выдача по запросу $q \in Q$ получается сортировкой множества $D_q$ по убыванию $a_\theta(q, d)$.

Здесь и далее, не нарушая общности, будем считать, что мы решаем задачу WEB-поиска. Выберем множество тестовых запросов $Q_{t}$, на которых оценим качество нашего решения.

Для формул метрик качества введём следующие обозначения:

$$T_K(q)\ -\  \text{первые } K \text{ элементов выдачи по запросу } q 
$$

$$d_q^{(k)}\ -\ k \text{ по порядку документ в выдаче}
$$

Соответственно, $T_K(q) = \{ d_q^{(1)}, \dots, d_q^{(k)} \}$

### Бинарная релевантность

Величина, которая обозначает, насколько документ подходит запросу, называется *релевантностью*. Способов измерить эту величину много. Обычно решение о том, релевантен документ запросу или нет, принимают асессоры — специальные люди, которые размечают данные для обучения ML-моделей. Собранные оценки называются *метками релевантности*, будем обозначать метку для запроса $q$ и документа $d$ за $y(q, d)$.

Для начала введём метрики для случая бинарной релевантности, когда $y(q, d) \in \{0, 1\}$.

#### Precision / Recall

Если у релевантности есть всего 2 класса, то можно вспомнить метрики классификации и обобщить их для задачи ранжирования.

Будем считать, что ранжирующая модель считает релевантными те документы, которые попали в первые $K$ элементов выдачи, то есть $T_K(q)$. Тогда можно вычислить метрики precision и recall в зависимости от $K$.

$$Precision@K = \frac{\text{число релевантных в топе}}{K} = \frac{\sum\limits_{d \in T_K(q)} I\{y(q, d) = 1\}}{K} 
$$

$$Recall@K = \frac{\text{число релевантных в топе}}{\text{всего релевантных}} = \frac{\sum\limits_{d \in T_K(q)} I\{y(q, d) = 1\}}{\min(K, \sum\limits_{d \in D} I\{y(q, d) = 1\})} 
$$

В случае с recall приходится брать минимум в знаменателе, поскольку в такой постановке модель не может выявить больше, чем $K$ релевантных документов.

#### Mean Average Precision

Заметим, что метрики precision и recall хоть и показывают качество нашей ранжирующей системы, но совсем не смотрят на порядок элементов в $T_K(q)$. Чтобы его учесть, посмотрим на Precision по тем позициям, где стоят релевантные документы, и усредним их. Такая величина называется Average Precision.

$$AP(q) = \frac{1}{K} \sum\limits_{k = 1}^K Precision@k \cdot y(q, d_q^{(k)})
$$

Теперь, чтобы получить труднопереводимую на русский язык метрику Mean Average Precision, нужно усреднить значения AP по всем запросам из набора.

$$MAP(q) = \frac{1}{|Q_t|} \sum\limits_{q \in Q_t} AP(q)
$$

#### Mean Reciprocal Rank

Название этой метрики переводится как средний обратный ранг. Ранжирование работает тем лучше, чем ближе к началу выдачи релевантный для пользователя документ. Для каждого запроса найдём позицию первого релевантного документа, возьмём обратное от этого числа и усредним по всем запросам.

$$MRR = \frac{1}{|Q|} \sum\limits_{q \in Q} \min\left(i\ |\ y(q, d_q^{(i)}) = 1\right)^{-1} 
$$

Обозначим релевантность $k$-го документа в выдаче как $R_k = y(q, d_q^{(k)})$.

Тогда формулу можно переписать в следующем виде:

$$MRR = \frac{1}{|Q|} \sum\limits_{q \in Q}\sum\limits_{i} \prod\limits_{k=1}^{i - 1}(1 - R_k) \cdot R_i \cdot \frac{1}{i}
$$

Произведение в формуле будет равно $1$ только в случае, когда $i$ документ релевантный, а все до него нет. В остальных случаях произведение равно $0$.

## Вещественная релевантность

Рассмотрим теперь метрики для случая, когда релевантность может принимать вещественные значения.

### Expected Reciprocal Rank

Пусть теперь $R_i = P(q, d_q^{(i)})$ — вероятность того, что документ $d_q^{(i)}$ релевантен. Например, если $y_i$ — метка релевантности, а $Y_{max}$ — максимальное её значение, то можно определить $R_i = \frac{2^{y_i}}{2^{Y_{max}}}$.

Будем считать, что пользователь листает выдачу документ за документом. В каждом он находит информацию, которая ему нужна, с вероятностью $R_i$. Если информацию он нашел, то он заканчивает поиск.

В такой модели хорошая система позволит пользователю найти информацию как можно быстрее. Но в отличие от бинарной релевантности позиция, где пользователь закончил поиск, теперь случайная величина, как и обратная позиция. Поэтому будем считать матожидание этой величины.

$$ERR(q) = \sum\limits_i P(\text{пользователь дошёл до док-та и остановился}) \cdot \frac{1}{i}
$$

Если пользователь остановился на позиции $i$, это значит, что на предыдущих документах задачу он не решил, а остановился именно на $i$-ом. В модели эти события предполагаем независимыми, поэтому вероятности можно перемножить.

$$ERR(q) = \sum\limits_{i} \prod\limits_{k=1}^{i - 1}(1 - R_k) \cdot R_i \cdot \frac{1}{i}
$$

Чтобы получить финальную метрику, усредняем эту величину по всем запросам.

$$ERR = \frac{1}{|Q|} \sum\limits_{q \in Q}\sum\limits_{i} \prod\limits_{k=1}^{i - 1}(1 - R_k) \cdot R_i \cdot \frac{1}{i}
$$

Заметим, что формула совпала с формулой для метрики MRR для бинарной релевантности.

### pFound

Рассмотрим ещё одну метрику, которая основывается на модели поведения пользователя. Метрика pFound была придумана в Яндексе и некоторое время была основной для ранжирования.

Пусть релевантность задаётся одним из классов $y(q, d) \in \mathbb{Y} = \{\text{Not Rel}, \text{Rel-}, \text{Rel+}, \text{Useful}, \text{Vital}\}$

По историческим данным считаются соответствующие вероятности найти нужное в документе в зависимости от класса $P_q(d) = \{0, 0.07, 0.14, 0.41, 0.61\}$

Отличием от предыдущей модели является введённая константа $P_{break} = 0.15$ — вероятность бросить искать информацию и листать выдачу.

Посчитаем вероятность $P_i$ того, что пользователь дошёл до позиции $i$. Для этого он должен дойти до документа на позиции $i - 1$, не устать искать, и при этом не найти нужного в предыдущем документе. Опять же перемножаем вероятности.

$$P_1 = 1;\ P_i = P_{i - 1} \cdot \left(1 - P_{break}\right) \cdot \left(1 - P_q(d_q^{(i - 1)})\right)
$$

Теперь $pFound$ — это вероятность найти нужное в выдаче

$$pFound(q) = \sum\limits_{i = 1}^n P_i \cdot P_q(d_q^{(i)}) 
$$

Чтобы теперь посчитать финальную метрику, усредняем $pFound$ по всем запросам в тестовом множестве $Q_t$.

### nDCG

Введём метрику DCG (Discounted Cumulative Gain).

Будем считать, что релевантный документ в топе приносит некоторую пользу (gain) в зависимости от своей релевантности. При этом до низкого документа в выдаче могут и не долистать, поэтому он приносит меньше пользы, то есть она уменьшается. Будем дисконтировать пользу в зависимости от позиции (discount).

$$DCG_n(q) = \sum\limits_{i = 1}^n G_q(d_q^{(i)}) \cdot D(i) 
$$

Здесь $G_q(d)$ — функция пользы, а $D$ — функция дисконтирования от позиции. Для этих функций возможны разные вариации, рассмотрим классический и упрощённый.

Классический вариант: $G_q(d) = 2^{y(q, d) - 1};\ D(i) = \frac{1}{\log_2(i + 1)}$

Упрощённый вариант: $G_q(d) = y(q, d);\ D(i) = \frac{1}{i + 1}$

Иногда при реализации поисковой системы может быть понимание, как падает внимание пользователя с ростом позиции в зависимости от типа запроса. В этом случае функция дисконтирования может стать запросозависимой.

Однако низкое значение метрики $DCG$ не всегда означает, что ранжирование отработало плохо. Могло быть так, что по запросу просто нет релевантных документов, или же их очень мало. Чтобы избавиться от этой проблемы, значение $DCG$ нормируют на эту метрику при идеальном ранжировании, когда документы отсортированы по истинным значениям релевантности.

$$nDCG(q) = \frac{DCG(q)}{\max DCG(q)}
$$

Как и всегда, для получения метрики по набору запросов, считают среднее значение $nDCG.$

## Дополнительные метрики

### Другие сигналы и экосистема

Описанные выше метрики были введены для агрегации релевантности документов в топе выдачи. Но те же рассуждения и формулы могут быть применены для других сигналов, сопоставляющих запрос и документ. Вы можете придумать любую инструкцию для асессоров и собрать разметку под вашу задачу. Одним из полезных сигналов может быть свежесть документа, которую можно понять и по времени создания документа.

Обычно в противовес релевантности смотрят на кликабельность элементов выдачи. Поисковым системам интересно получить больше кликов пользователей, тем не менее могут встречаться «кликбейтные» документы, которые побуждают кликать, но не решают на самом деле задачу пользователя. Кликабельность можно замерять как DCG предсказатора вероятности клика.

Также важно следить за чистотой выдачи. Нужно не допускать в топ мошеннические документы, шокирующие документы и документы 18\+ в поиске для детей. В качестве метрики можно замерять DCG или MAP «плохих» документов в топе.

### Разнообразие и метрики рекомендаций

Бывает полезно следить за тем, как документы в топе соотносятся друг с другом. В частности, нужно не допускать, чтобы все документы выдачи были с одного хоста и чтобы в них не было написано одно и то же. Для этого используются алгоритмы группировки и дедупликации.

Также, если рассматривать поиск как решение задачи рекомендации документов, можно измерять метрики новизны и serendipity, про которые подробнее рассказано в главе о рекомендательных системах.

## Методы обучения ранжированию

Рассмотрим некоторую модель $a_\theta(q, d)$, по предсказаниям которой мы будем сортировать документы $d$ по запросу $q$ (здесь $\theta$ — это параметры модели). Мы хотим обучить модель так, чтобы у неё было оптимальное значение одной из метрик ранжирования, например, NDCG. Заметим, что если совсем немного поменять $\theta$, то предсказания модели $a_\theta(q, d)$ изменятся тоже несильно. Но небольшие изменения в предсказаниях могут не привести к изменению порядка документов. Тогда не изменится и метрика NDCG. Получается, что NDCG как функция от параметров $\theta$ является кусочно постоянной, поэтому нельзя оптимизировать её напрямую.

Наша дальнейшая задача — представить методы, позволяющие получить модель, оптимальную по NDCG или другой аналогичной метрике ранжирования.

Методы обучения ранжированию обычно делят на 3 типа:

**Поточечный (pointwise) подход**

В этом подходе у нас известны некоторые оценки релевантности каждой пары запрос-документ, и модель учится предсказывать эти оценки. Взаимоотношения между разными документами внутри $D_q$ не рассматриваются.

**Попарный (pairwise) подход**

Здесь во время обучения используют тройки $(q, d_1, d_2)$, где $d_1, d_2$ — документы из $D_q$, причём $d_1$ релевантнее $d_2$ по запросу $q$. При этом модель всё равно может давать предсказания релевантности по паре $(q, d)$.

**Списочный (listwise) подход**

В данном подходе для обучения используются перестановки документов из $D_q$. Например, асессорские оценки дают наилучшую известную сортировку. Для её получения нужно сначала показать на выдаче докумены с самой высокой оценкой, затем со следующей по порядку и т.д.

Будем рассматривать каждый из подходов по очереди.

### Поточечный подход

#### Сведение к регрессии и классификации

Рассмотрим простейшую постановку задачи, в которой у нас есть набор запросов $Q$, для каждого запроса $q \in Q$ имеется набор документов $D_q$, который необходимо отсортировать, а в качестве обучающей выборки известны асессорские оценки релевантности для некоторых пар запрос-документ $(q, d)$. Будем обозначать множество возможных оценок $\mathbb{Y}$, конкретную оценку — $y(q, d)$.

Пусть $\mathbb{Y} = \mathbb{R}$ — множество действительных чисел. Тогда мы можем обучить любую модель регрессии на признаках пар $(q, d)$ для предсказания оценок асессоров. Это может быть и линейная модель, и градиентный бустинг, и нейронная сеть. Обучать модель можно, например, оптимизируя MSE:

$$\sum_{q \in Q} \sum_{d \in D_q} (\widehat{y}(q, d) - y(q, d)) ^ 2 \rightarrow \min 
$$

Аналогично можно сводить задачу к классификации, если метки релевантности $y$ бинарны или категориальны. Например, при шкале $\mathbb{Y} = \{0, 1, 2, 3, 4, 5\}$. Оптимизировать в данном случае можно кросс-энтропийную функцию потерь.

#### PRank

В случае классификации мы учим модель разделять классы, но никаким образом не задаём, что на метках имеется порядок. Мы не даём алгоритму никакой информации о том, что метка `4` находится между метками `5` и `3` и наоборот. Чтобы побороть эту проблему, была придумана модификация линейной модели, которая получила название PRank и была описана в статье  [Pranking with ranking](https://proceedings.neurips.cc/paper/2001/file/5531a5834816222280f20d1ef9e95f69-Paper.pdf).

Если предположить, что метки релевантности — это целые числа от $0$ до $K$, то есть $\mathbb{Y} = \{0, 1, 2, \dots, K\}$, то можно ввести пороги для значений ранжирующей функции, которые разделяют классы друг от друга.

В качестве ранжирущей функции возьмём обычную линейную, то есть $\langle\theta, x\rangle$, где $x$ — вектор признаков. Обозначим через $b_1, b_2, \dots, b_{K - 1}$ границы классов. Они будут изменяться в процессе обучения. Также фиктивно введём $b_{K} = \infty$.

Чтобы предсказать класс, будем искать первую границу, которая больше вычисленной линейной функции:

$$\widehat{y}(q, d) = \min \{r\ :\ x^\top\theta - b_r < 0\} 
$$

Коротко опишем процесс обучения. Представим, что мы получили очередной объект и вычислили линейную функцию $\langle\theta, x\rangle$. Отметим на оси её значение и границы классов:

![Artboard](https://yastatic.net/s3/education-portal/media/Artboard_10_8b1b6f1483_380be5fa8a.svg)

Если предсказан ранг `1`, а правильный класс для объекта `4`, то необходимо, во-первых, обновить вектор $\theta$, а во-вторых, сдвинуть границы других классов в сторону получившегося предсказания.

![Artboard](https://yastatic.net/s3/education-portal/media/Artboard_13_40ecabc386_a88d0d99de.svg)

После осуществления сдвигов, предсказание на точке из обучающей выборки становится ближе к правильному.

![Artboard](https://yastatic.net/s3/education-portal/media/Artboard_11_ba670b3f82_0b65fa0438.svg)

Мы не будем приводить конкретные формулы для обновления обучаемых параметров; вы можете посмотреть их в статье.

### Попарный подход

Начнём рассмотрение попарного подхода. Чтобы его применить, нужен датасет, состоящий из троек $(q, d_1, d_2)$, где $d_1, d_2\in D_q$, причём известно, что по запросу $q$ документ $d_1$ релевантнее, чем $d_2$. Будем обозначать такое соотношение $d_2 \prec d_1$.

**Замечание**. Такие данные можно собирать с использованием асессорской Side-by-Side разметки, в которой асессоры отмечают, какой из двух предложенных документов релевантнее по заданному запросу. Также можно собирать данные с помощью пользовательских логов. Например, если пользователь пролистал первые 3 документа по запросу $q$, а решил свою задачу только на 4-ом, можно считать, что первые 3 документа были хуже. Пользовательских логов достаточно много, с их помощью можно обучать разные модели для ранжирования на основе кликов. Но надо помнить, что совсем не всегда те документы, на которые первоначально хочется кликнуть, релевантны и содержат необходимую информацию. Чтобы учесть это в модели, можно рассматривать только клики, после которых пользователь надолго остался на странице. Дополнительно можно смешивать модели на кликах с моделями на оценках асессоров, что поможет избежать проблемы смещения обучающей выборки в текущее ранжирование. Поясним, из-за чего может возникнуть эта проблема. Логи строятся по результатам взаимодействия пользователя с нашей ранжирующей моделью, и может оказаться так, что обучающая выборка будет состоять только из документов, уже попавших в топ выдачи.

#### Классификатор на тройках

В качестве самого простого решения снова рассмотрим сведение задачи ранжирования к уже известным задачам машинного обучения. А именно, будем решать задачу классификации троек $(q, d_1, d_2)$. В качестве целевой переменной запишем $0$, если лучше документ $d_1$, и $1$, если лучше $d_2$. Собрав признаки для каждой тройки, можем обучить любой классификатор. Для этого можем взять и нейронную сеть, и линейную модель, и методы на основе деревьев.

Чтобы отсортировать документы внутри $D_q$ по запросу $q$, можем воспользоваться стандартным алгоритмом сортировки с компаратором, задаваемым предсказаниями нашего классификатора.

Проблема в том, что компаратор должен быть как минимум транзитивным. Гарантировать такое свойство для большинства моделей машинного обучения мы не можем.

#### RankingSVM

Но если ввести модель специальным образом, можно гарантировать транзитивность предсказаний. Снова воспользуемся линейной моделью.

**Задача**. Найти вектор весов $\theta$, для которого $\forall q \in Q \ \forall d_i, d_j\ :\ d_i \prec d_j$ было бы выполнено $\langle x_i, \theta\rangle < \langle x_j, \theta\rangle$. Здесь $x_i$, $x_j$ – векторы признаков для пар $(q, d_i)$ и $(q, d_j)$ соответственно.

Так как требуется выполнение скалярных произведений вида $\langle x_i - x_j, \theta \rangle$, где $x_i - x_j$ — вектор поэлементной разницы признаков, можно обучить линейную модель на парах, где признак пары – это как раз вектор $x_i - x_j$.

Если в качестве модели взять SVM, получится классический метод обучения ранжированию, который называется RankingSVM.

Чтобы сравнить 2 документа по запросу, надо сравнить $\langle x_i - x_j, \theta\rangle$ с нулём. Получается, что для двух одинаковых документов это выражение всегда $0$. Кроме того, выполнена требуемая от компаратора транзитивность.

#### RankNet

Пусть имеется обучающая выборка, в которой известны метки релевантности для пар запрос-документ.

$$\begin{align*}
X &= \{x_i\} \ \text{– признаковые описания $(q_i, d_i)$}, \\
Y &= \{y_i\} \ \text{– асессорские оценки}, \\
Q &= \{q_i\} \ \text{– запросы}. \\
\end{align*}
$$

Введём модель для ранжирующей функции $\widehat{y}_\theta(x)$, дифференцируемую по $\theta$. Например, это может быть линейная модель или нейронная сеть. Также возможно применение композиций деревьев. Ниже мы будем описывать оптимизационную процедуру для моделей, которые обучаются с помощью градиентного спуска; для деревьев потребуются другие методы.

Будем рассматривать события $x_i \succ x_j$, то есть события вида «документ $d_i$ релевантнее $d_j$ по запросу $q_i = q_j = q$».

Из разметки асессоров мы можем задать вероятности таких событий.

$$ Q_{ij} =
\begin{cases}
    1, \text{если } y_i > y_j \\
    0, \text{если } y_i < y_j \\
    \frac{1}{2}, \text{если } y_i = y_j
\end{cases}
$$

Если асессорских оценок на каждый документ несколько, можем по-разному агрегировать эти оценки в $Q_{ij}$. Если же доступна попарная разметка (какой из документов $d_i$ и $d_j$ релевантнее запросу $q$), то вероятностью можно назвать долю оценок, в которых $d_i$ признан лучше, чем $d_j$.

Далее введём оценку вероятности этого события, порождённую моделью.

$$P_{ij} = \frac{1}{1 + e^{-\sigma(s_i - s_j)}},\ \ s_i = \widehat{y}_\theta(x_i),\  s_j = \widehat{y}_\theta(x_j)
$$

Для пары $(x_i, x_j)$ рассмотрим случайную величину

$$\xi_{ij} = \mathbb{I}[x_i \succ x_j]
$$

Согласно асессорам, $\xi_{ij} \sim Bern(Q_{ij})$. Согласно модели, $\xi_{ij} \sim Bern(P_{ij})$. Задача обучения в том, чтобы уравнять эти два распределения между собой для всех пар документов. Поэтому в качестве функции потерь будем использовать KL-дивергенцию. Введём для каждой пары лосс

$$C_{ij} = \text{KL}(Bern(Q_{ij})\,\vert\vert\, Bern(P_{ij})) = Q_{ij}\log\frac{Q_{ij}}{P_{ij}} + (1 - Q_{ij}) \log\frac{1 - Q_{ij}}{1 - P_{ij}} = 
$$

$$= H(Q_{ij}) - Q_{ij}\log P_{ij} - (1 - Q_{ij})\log(1 - P_{ij}) 
$$

Тут $H(Q_{ij})$ – энтропия, не зависящая от $P_{ij}$, а значит и от параметров модели $\theta$.

Для определённых выше $Q_{ij}$ выполнены следующие свойства:

$$\begin{align*}
C_{ij} &= \log(1 + e^{-\sigma(s_i - s_j)}), \text{если $y_i > y_j$} \\
C_{ij} &= \log(1 + e^{-\sigma(s_j - s_i)}), \text{если $y_i < y_j$} \\
C_{ij} &= \log2, \text{если $y_i = y_j$ и $s_i \neq s_j$} \\
C_{ij} &= 0, \text{если $y_i = y_j$ и $s_i = s_j$}
\end{align*}
$$

Видно, что для двух документов с разными истинными метками релевантности такой метод обучения штрафует модель за одинаковые предсказания. Это свойство очень полезно, поскольку в случае одинаковых предсказаний непонятно, в каком порядке располагать документы.

Полная функция потерь выглядит следующим образом:

$$Q(\theta) = \sum_{q \in Q}\sum_{i, j: q_i = q_j = q} C_{ij} \ \rightarrow\  \min_{\theta} 
$$

Минимизировать её можно при помощи градиентного спуска или различных его модификаций.

$$\theta_k^{t + 1} = \theta_k^t - \eta \sum_{i, j: q_i = q_j = q} \frac{\partial C_{ij}}{\partial \theta_k} (\theta_k^t) = 
$$

$$= \theta_k^t - \eta  \sum_{i, j: q_i = q_j = q}\left(\frac{\partial C_{ij}}{\partial s_i}\frac{\partial s_i}{\partial \theta_k} + \frac{\partial C_{ij}}{\partial s_j}\frac{\partial s_j}{\partial \theta_k}\right) 
$$

Вычислим производные функции потерь по $s_i$.

Пусть $y_i > y_j$. Тогда:

$$\frac{\partial C_{ij}}{\partial s_i} = \frac{-\sigma e^{-\sigma(s_i - s_j)}}{1 + e^{-\sigma(s_i - s_j)}} = \frac{-\sigma}{1 + e^{\sigma(s_i - s_j)}}
$$

$$\frac{\partial C_{ij}}{\partial s_j} = \frac{\sigma e^{-\sigma(s_i - s_j)}}{1 + e^{-\sigma(s_i - s_j)}} = \frac{\sigma}{1 + e^{\sigma(s_i - s_j)}},
$$

то есть $\frac{\partial C_{ij}}{\partial s_i} = -\frac{\partial C_{ij}}{\partial s_j}$.

Подставим полученное выражение в производную функции потерь по весам.

$$\frac{\partial C_{ij}}{\partial \theta_k} = \frac{\partial C_{ij}}{\partial s_i}\frac{\partial s_i}{\partial \theta_k} + \frac{\partial C_{ij}}{\partial s_j}\frac{\partial s_j}{\partial \theta_k} = \underbrace{\frac{-\sigma}{1 + e^{\sigma(s_i - s_j)}}}_\text{обозначаем $\lambda_{ij}$} \left(\frac{\partial s_i}{\partial \theta_k} - \frac{\partial s_j}{\partial \theta_k}\right)
$$

Аналогично получаем выражения для остальных случаев соотношения между $y_i$ и $y_j$.

Определим теперь

$$\lambda_i = \sum
\limits_{j:\ x_i \succ x_j} \lambda_{ij} - \sum
\limits_{j:\ x_i \prec x_j} \lambda_{ij}$$

Тогда получаем

$$\frac{\partial Q}{\partial \theta_k} = \sum\limits_i \lambda_i \frac{\partial s_i}{\partial \theta_k}
$$

Итерацию градиентного спуска теперь можно записать в более простом виде:

$$\theta^{t + 1} = \theta^t - \eta \sum\limits_i \lambda_i \nabla_{\theta^t} s_i 
$$

Таким образом, мы можем делать SGD не по парам документов, а по отдельным документам. Это увеличивает скорость сходимости.

Получается, что $\lambda_{i}$ зависит от номера документа и от попарных разностей скоров модели на документах $s_i - s_j$, при этом не зависит от производных самих $s$ по параметру $\theta$. Введённые $\lambda_i$ можно представить в виде стрелок, которые прикреплены к каждому документу в поисковой выдаче. Направление стрелки означает, куда мы хотим перенести документ, чтобы выросла нужная метрика, а длина – насколько сильно.

![lambdas](https://yastatic.net/s3/education-portal/media/lambdas_f0c41a1edb_554aa553d9.webp)

#### LambdaRank

Задача этого метода в том, чтобы соединить RankNet и наше желание напрямую оптимизировать введённые ранее кусочно постоянные метрики качества, например, NDCG. Обозначим через $Z(q, \widehat{y}) = Z(q, s)$ значение этой метрики для запроса $q$ при ранжировании функцией $\widehat{y}$. Попытаемся придумать гладкую попарную функцию потерь

$$\overline{Q} = \sum_{i, j\, q_i = q_j = q}\overline{C}_{ij},
$$

где

$$\overline{C}_{ij} = \overline{C}_{ij}(s_i - s_j),
$$

оптимизация которой была бы эквивалентна оптимизации $Z$.

Заметим, впрочем, что для обучения сама $\overline{Q}$ нам не нужна, а нужны только производные, которые, как и в случае RankNet, можно записать в виде

$$\nabla_{\theta_k}\overline{C}_{ij} = \overline{\lambda}_{ij}(\nabla_{\theta_k}s_i - \nabla_{\theta_k}s_j)
$$

В данном случае мы хотели бы задать $\overline{\lambda}_{ij}$ специальным образом: так, чтобы сдвиг в направлении антиградиента вёл к уменьшению метрики $Z$. Ясно, что не любое выражение для $\lambda_i$ может задавать градиент. Чтобы проверить существование функции потерь $\overline{C}$, применим следующий частный случай леммы Пуанкаре:

**Лемма**. Пусть $f_1(\theta_1, \dots, \theta_n), f_2(\theta_1, \dots, \theta_n), \dots, f_n(\theta_1, \dots, \theta_n)$ – функции, такие что

$$\forall i, j\ \ \ \frac{\partial f_i}{\partial \theta_j} = \frac{\partial f_j}{\partial \theta_i}.
$$

Тогда существует функция $F$, такая что $\forall i\ \ \frac{\partial F}{\partial \theta_i} = f_i$.

Значит, нужно ввести лямбды таким образом, чтобы совпадали смешанные производные.

Определим $y_{ij}$ – функцию релевантности, в которой поменяли местами $x_i$ и $x_j$:

$$ y_{ij} =
\begin{cases}
    y(x_j), \text{если } x = x_i \\
    y(x_i), \text{если } x = x_j \\
    y(x)\  \text{иначе}
\end{cases}
$$

Обозначим также через $\Delta Z_{ij} = Z(q, \widehat{y}_{ij}) - Z(q, \widehat{y})$ приращение метрики при перестановке местами $x_i$ и $x_j$. В методе LambdaRank $\lambda_{ij}$ определяется следующим образом:

$$\lambda_{ij} = \frac{-\sigma}{1 + e^{\sigma(s_i - s_j)}} |\Delta Z_{ij}|,
$$

где $\sigma$ – некоторая константа. Множитель $\vert\Delta Z_{ij}\vert$ кусочно постоянен, так что не повлияет на градиент.

**Вопрос на подумать**. Проверьте, что для указанных $\lambda_{ij}$ действительно выполнено условие леммы Пуанкаре.

Авторы метода проверяли его для оптимизации NDCG. Они пытались случайными сдвигами параметра улучшить NDCG после оптимизации через LambdaRank. Доля успешных сдвигов оказалось очень мала, так что экспериментально подтверждается успешная оптимизация этим методом недифференцируемых метрик ранжирования.

#### LambdaMART

Этот метод является конкретной реализацией подхода LambdaRank. В нём для предсказания $\widehat{y}_{\theta}(x)$ строится модель градиентного бустинга на решающих деревьях.

Каждое дерево обучается на градиент функции потерь предыдущей итерации алгоритма (и градиент мы как раз умеем считать, хотя саму функцию потерь – нет). Структура дерева определяется жадными по MSE разделениями.

При этом размер шага (коэффициент, с которым берётся значение в следующем дереве) задаётся не один для всей модели, а подбирается во всех листах каждого дерева при помощи метода Ньютона.

Более подробно о последних трёх методах можно почитать в оригинальной статье от Microsoft: [From RankNet to LambdaRank to LambdaMART](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf).

### Списочный подход

#### SoftRank

Вспомним, что основной проблемой, из-за которой невозможно оптимизировать NDCG и похожие метрики напрямую, является их кусочная линейность. Идея метода SoftRank — сгладить метрику, чтобы при небольших изменениях параметров модели она тоже изменялась. Для этого оценку релевантности документа будем рассматривать не как константу, а как случайную величину.

Сглаживание метрики рассмотрим на примере. Пусть имеются документы $d_1$, $d_2$, $d_3$ для запроса $q$, а ранжирующая модель дала им соответственно оценки релевантности $s_1$, $s_2$, $s_3$. Тогда, если это случайные величины, то они константны и их распределение вырождено. В связи с этим порядок документов на выдаче тоже определён однозначно, и первое место занимает документ с наибольшей оценкой.

![Artboard](https://yastatic.net/s3/education-portal/media/Artboard_12_25ad9e8ab4_0e140919b1.svg)

Чтобы поменять метрику, будем считать, что оценка релевантности документа $d_i$ по нашей модели имеет распределение $\mathcal{N}(s_i, \sigma^2)$, где $\sigma^2$ — гиперпараметр метода. Чтобы отсортировать документы, будем генерировать число из этого распределения и ранжировать по нему. Тогда может случиться так, что документ с самым большим $s_i$ окажется на последнем месте. Но с наибольшей вероятностью он всё равно будет первым. И распределение позиций документов на выдаче будет выглядеть примерно так:

![Artboard](https://yastatic.net/s3/education-portal/media/Artboard_9_c873c9b6e0_a10dea05ea.svg)

Теперь, чтобы получить метрику, осталось только в формуле NDCG заменить дискаунт на его математическое ожидание.

$$G_{soft} = \frac{1}{G_{max}} \sum\limits_{j = 1}^N g(d_j) \mathbb{E} D(j) 
$$

Можно заметить, что теперь небольшие сдвиги параметров будут менять распределение позиций документов, а значит и $G_{soft}$ будет изменяться. Поэтому функция становится дифференцируемой, и её можно использовать как функцию потерь. При этом для того, чтобы вычислить распределение рангов, необходимо использовать отсортированный список документов. А значит, хоть напрямую оптимизируемый функционал и не зависит от перестановок, этот метод можно отнести к списочному подходу.

Подробнее об этом методе можно прочитать в [статье](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/SoftRankWsdm08Submitted.pdf).

#### ListNet

Следующие 2 метода, которые мы рассмотрим, работают с перестановками документов для одного запроса. Если заданы оценки релевантности $y(q, d_i)$, $i = 1, \dots, N$ для запроса $q$, то они формируют распределение на перестановках.

Пусть дана перестановка  $\pi = (q_1, \dots, q_N)$. Определим её вероятность следующим образом:

$$P(\pi \vert y) = \prod\limits_{j = 1}^N \frac{y(q, d_j)}{\sum\limits_{k=j}^N y(q, d_k)} 
$$

Такая вероятностная модель называется моделью Люcа-Плакетта.

Для примера рассмотрим 3 документа и оценки $y$. Тогда вероятность перестановки $ABC$ запишется так:

$$P(ABC \vert y) = P(\text{A на первом месте}) \cdot 
$$

$$P(\text{B на втором месте}\setminus\text{vert A на первом месте})\cdot
$$

$$P(\text{С на третьем месте}\setminus\text{vert A на первом месте и}\text{B на втором месте})=
$$

$$= \frac{y(A)}{y(A) + y(B) + y(C)} \cdot \frac{y(B)}{y(B) + y(C)} \cdot \frac{y(C)}{y(C)} 
$$

Аналогично методу RankNet, мы имеем распределение, которое можно сформировать из оценок асессоров $y$ и из оценок модели $\widehat{y}$. А значит можно в качестве функции потерь использовать KL-дивергенцию между этими распределениями.

$$\text{KL}(P(\pi | y)\,\vert\vert\, P(\pi | \widehat{y}_\theta)) \rightarrow \min\limits_{\theta},
$$

где $\theta$ — параметр модели.

Однако различных перестановок получается $N!$, а значит, даже чтобы просто вычислить дивергенцию Кульбака-Лейблера, потребуется немало времени, не говоря уже про оптимизацию этой функции потерь.

Поэтому вместо того, чтобы рассматривать вероятность полной перестановки, смотрят на распределение индекса первого документа на выдаче.

$$P_{\widehat{y}}(j) = \frac{\widehat{y}(q, d_j)}{\sum\limits_{d \in D_q} \widehat{y}(q, d)} 
$$

$$P_{y}(j) = \frac{y(q, d_j)}{\sum\limits_{d \in D_q} y(q, d)} 
$$

Оптимизируют KL-дивергенцию между этими распределениями:

$$\text{KL}(P_y\,\vert\vert\, P_{\widehat{y}}) \rightarrow \min\limits_{\theta} 
$$

#### ListMLE

В этом методе рассматривают вероятность одной «правильной» перестановки. В данном случае под правильной перестановкой $\pi_q$ понимается та, которая получается, если упорядочить документы из $D_q$ по убыванию асессорских оценок $y$. Логично, что хорошая модель должна давать большую вероятность такой перестановке, поэтому именно она и максимизируется.

$$P(\pi_q | \widehat{y}_\theta) \rightarrow \max\limits_\theta
$$

Всё сводится к поиску оценки максимального правдоподобия, поэтому метод и называется ListMLE.

## Практические советы

### Популярные признаки

Выше показано, как можно построить модель, если уже известны признаки для пар запрос-документ $(q, d)$, а также, как можно собрать целевые переменные для обучения. Но какие стоит взять признаки, чтобы получить хорошую модель?

Признаки для моделей ранжирования можно разделить на 3 типа: *запросные*, *документные* и *запросно-документные*. Первые зависят только от запросы, вторые только от документа, а третьи от всей пары, то есть для их вычисления необходимо сопоставить запрос и документ.

Конечно, наибольший интерес представляют запросно-документные факторы. Но и другие группы факторов могут быть полезны.

Запросными являются, например, следующие факторы:

* Количество слов в запросе
* Язык запроса
* Страна, из которой задали запрос
* Значение классификаторов
  * $P(\text{запрос про машинное обучение})$
  * $P(\text{запрос пиратский})$

Многие модели могут подстроиться под эти факторы и отдельно обучиться под различные значения запросных признаков. Например, так модель может по-разному реагировать на запросы из разных стран.

#### TF-IDF

Чтобы сопоставить запрос и документ, можно использовать TF-IDF слов запроса. Например, можно просуммировать его по всем словам из запроса и получить фактор для ранжирования. Подробно о том, как считать TF-IDF, можно прочитать в главе про NLP.

#### DSSM и другие нейросетевые факторы

Запросно-документные факторы можно получать, «соединяя» векторные представления запроса и документа. Классическим способом такого соединения является простой подсчёт косинуса угла между векторами.

Если обучить модель, которая для пар, где документ релевантен запросу, выдаёт вектора, похожие на сонаправленные, то скалярное произведение становится оценкой релевантности. Если оно близко к единице, векторы сонаправлены, а значит документ подходит запросу. При этом обычно нормируют векторы на выходе, чтобы косинус и скалярное произведение были одним и тем же.

В качестве модели эмбеддинга можно использовать полносвязную нейронную сеть. На вход такой сети можно подать BagOfWords вектор или же вектор TfIdf. Эти векторы большой размерности нейросеть преобразует в меньшие. Обычно размер выходного слоя выбирают, балансируя между качеством и ресурсами, необходимыми на расчёт и хранение выхода сети.

Этот подход был назван `Deep Structured Semantic Models` и описан в [статье](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/cikm2013_DSSM_fullversion.pdf) от Microsoft. Идею можно применять в целом в любой задаче ранжирования для своих типов «документов».

Посмотреть, как она была воплощена в Web поиске для поиска по смыслу, можно в [блоге](https://habr.com/ru/company/yandex/blog/314222/) Яндекса. Если вкратце, у модели следующие особенности:

* На входе у модели не все тексты, а только заголовок документа и запрос;
* Для уменьшения размера входа текст разбит на буквенные триграммы, вектор на входе - это Bag Of Trigrams;
* Архитектура обработки запроса и документа разная;
* Особый способ генерации негативных примеров.

Схематически архитектура модели показана на рисунке ниже:

![Artboard](https://yastatic.net/s3/education-portal/media/Artboard_6_6f7cf41862_1ea2758c5a.svg)

Конечно, чтобы заставить простую архитектуру давать хорошее качество, нужно экспериментировать с методами сбора данных и улучшения качества, которые описаны в других главах учебника. Однако при наличии соответствующих мощностей можно улучшать качество, изменяя архитектуру обработки текста. В частности, модели на основе трансформеров (например, BERT) улучшают качество. Это же касается и косинуса, то есть соединительной части. Вполне можно вместо него использовать полносвязную сеть или даже трансформерную архитектуру.

Трансформеры, которые по-отдельности обрабатывают сущности запроса и документа, в Яндексе названы split-моделями и более подробно описаны в том же [блоге](https://habr.com/ru/company/yandex/blog/529658/) на Хабре.

#### Метафичи

В предыдущем пункте мы уже ввели фактор для модели, который сам является значением модели. Мы улучшили качество с помощью стекинга DSSM и итоговой модели. Аналогичным образом можно использовать предсказания моделей, обученных на разные таргеты и разными способами: их можно добавить к другим фичам для итоговой модели.

К сожалению, если факторов-моделей (метафичей) много, такая модель не будет удовлетворять требованиям по времени работы. В этом случае можно прибегнуть к дистилляции знаний большой модели в более компактную.

#### Фичи, зависящие от времени

Известно, что модель машинного обучения работает хорошо (а точнее, ожидаемо) только в случае, когда при её применении распределения данных и факторов похожи на те, которые использовались при обучении. Но в продакшн-системах постоянное выполнение этого свойства невозможно обеспечить.

Поэтому главный совет: настраивайте мониторинги качества ваших ML-систем для того, чтобы не пропускать моменты поломки. Качество может снизиться как из-за изменений в логике других сервисов, на которые вы полагаетесь при вычислении факторов, так и из-за появления новых трендов. Например, это происходит при появлении новых тем, о которых раньше не было запросов. Показательный случай — пандемия вируса COVID-19, который стал резко появляться среди запросов пользователей.

Но бывают факторы, которые зависят от времени сами по себе. Так, для ранжирования новых документов может быть полезно знать возраст документа. Со временем распределение этого фактора сдвигается вправо, поскольку многие старые документы не удаляются из базы. Получается, фактор старого документа меняется, а релевантность нет. Появление фичей вне ожидаемых значений может привести к непредсказуемому поведению модели. Так что их нужно применять с осторожностью, лучше в тех моделях, которые можно быстро обучить и обновить в продакшне. Ещё лучшим решением будет преобразовать или отнормировать фичи таким образом, чтобы их распределение не менялось так кардинально. Наш фактор с возрастом документа можно преобразовать в индикатор того, что возраст документа меньше одного дня. Тогда только один раз за историю документа этот фактор будет изменён, при этом распределение этого фактора в документах изо дня в день будет похожим.

### Многостадийное ранжирование

Представьте, что вы смогли обучить сложную модель, применив все описанные выше подходы: добавили метафичи, которые сами по себе являются формулами, обучили DSSM, BERT, или даже более тяжёлую нейросеть.

Наконец, вам пришёл запрос от пользователя, и вы намерены отсортировать все документы в базе по оценке релевантности, которую даёт ваша модель. Если вы ранжируете 1000, 10000 или даже 100000 документов, это ещё может получиться, и пользователь дождётся ответа. Но что делать, если в вашей базе миллионы, а то и миллиарды документов?

Конечно же вам на помощь могут прийти распределённые системы, и разбив документы по разным инстансам сервиса, который рассчитывает прогноз, вы ускорите получение ответа. Но даже с учётом распределённых вычислений быстро вычислить BERT миллиард раз будет либо очень дорого, либо очень долго. Поэтому применяется подход многостадийного ранжирования.

Тяжёлые модели применяются не сразу. Сначала можно ограничиться применением самых простых оценок релевантности. Как пример, можно просто взять TF-IDF или BM25. Простой моделью отсекаются самые нерелевантные документы, а прошедшие дальше уже сортируются с помощью более ресурсоёмкой и продвинутой модели.

В зависимости от количества документов в базе вы можете соединить столько уровней, сколько нужно, для получения приемлемого времени ответа. Конечно, количество параметров такой системы возрастает в несколько раз, но это делает возможным быстрое взаимодействие с пользователем.

### Готовые решения

Если вы разрабатываете продукт, для которого требуется поиск по текстовым документам, для начала вы можете воспользоваться готовыми решениями.

Одним из самых популярных сервисов для поиска является [Sphinx](http://sphinxsearch.com/). Этот сервис позволяет индексировать текстовые документы и сохранять их в базу данных (как SQL, так и NoSQL). Через специальный SQL-подобный язык запросов он позволяет получать списки релевантных документов и сопутствующие им данные. Таким образом можно доставать только документы, подходящие по заданным фильтрам, отсортированные по релевантности. Это может быть полезно, например, для реализации поиска по интернет-магазину.

Более того, получив некоторый топ выдачи, вы можете переранжировать его, сразу используя сложную модель или учитывая другие потребности ваших пользователей.

Другие альтернативы для текстового поиска можно посмотреть в [статье](https://medevel.com/os-fulltext-search-solutions/).

  ## handbook

  Учебник по машинному обучению

  ## title

  Задача ранжирования

  ## description

  *null*

- 
  ## path

  /handbook/ml/article/obuchenie-s-podkrepleniem

  ## content

  До сих пор опыт, благодаря которому было возможно обучение в наших алгоритмах, был задан в виде обучающей выборки. Насколько такая модель обучения соотносится с тем, как учится, например, человек? Чтобы научиться кататься на велосипеде, печь тортики или играть в теннис, нам не нужны огромные датасеты с примерами того, что нужно делать в каждый момент; вместо этого мы способны обучаться **методом проб и ошибок** (**trial and error**), предпринимая попытки решить задачу, взаимодействуя с окружающим миром, и как-то улучшая своё поведение на основе полученного в ходе этого взаимодействия опыта.

В **обучении с подкреплением** (**reinforcement learning**, **RL**) мы хотим построить алгоритм, моделирующий обучение методом проб и ошибок. Вместо получения обучающей выборки на вход такой алгоритм будет взаимодействовать с некоторой **средой** (**environment**), окружающим миром, а в роли «разметки» будет выступать **награда** (**reward**) — скалярная величина, которая выдаётся после каждого шага взаимодействия со средой и показывает, насколько хорошо алгоритм справляется с поставленной ему задачей. Например, если вы печёте тортики, то за каждый испечённый тортик вы получаете \+1, а если вы пытаетесь кататься на велосипеде, то за каждое падение с велосипеда вам прилетает -1.

* Награда не подсказывает, как именно нужно решать задачу и что вообще нужно делать;
* Награда может быть отложенной во времени (вы нашли в пустыне сокровища, но чтобы получить заслуженные тортики, вам ещё понадобится куча времени, чтобы выбраться из пустыни; а награда приходит только за тортики) или сильно разреженной (большую часть времени давать агенту \+0). Всё это сильно отличает задачу от обучения с учителем;
* Награда предоставляет какой-то «сигнал» для обучения (хорошо/плохо), которого нет, например, в обучении без учителя.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/trial_and_error_1a25729c9c_3de10833a7.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <p>источник картинки — курс <a href="http://ai.berkeley.edu/lecture_slides.html">UC Berkeley AI</a></p>
  </figcaption>
</figure>

## Постановка задачи

Теперь попробуем формализовать всю эту концепцию и познакомиться с местной терминологией. Задача обучения с подкреплением задаётся **Марковским Процессом Принятия Решений** (**Markov Decision Process** или сокращённо **MDP**) это четвёрка $(\mathcal{S}, \mathcal{A}, \mathcal{P}, r)$, где:

* $\mathcal{S}$ — **пространство состояний** (state space), множество состояний, в которых в каждый момент времени может находиться среда.
* $\mathcal{A}$ — **пространство действий** (action space), множество вариантов, из которых нужно производить выбор на каждом шаге своего взаимодействия со средой.
* $\mathcal{P}$ — **функция переходов** (transition function), которая задаёт изменение среды после того, как в состоянии $s \in \mathcal{S}$ было выбрано действие $a \in \mathcal{A}$. В общем случае функция переходов может быть стохастична, и тогда такая функция переходов моделируется распределением $p(s' \mid s, a)$: с какой вероятностью в какое состояние перейдёт среда после выбора действия $a$ в состоянии $s$.
* $r \colon \mathcal{S} \times \mathcal{A} \to \mathbb{R}$ — **функция награды** (reward function), выдающая скалярную величину за выбор действия $a$ в состоянии $s$. Это наш «обучающий сигнал».

Традиционно субъект, взаимодействующий со средой и влияющий на неё, называется в обучении с подкреплением **агентом** (agent). Агент руководствуется некоторым правилом, возможно, тоже стохастичным, как выбирать действия в зависимости от текущего состояния среды, которое называется **стратегией** (**policy**; термин часто транслитерируют и говорят **политика**) и моделируется распределением $\pi(a \mid s)$. Стратегия и будет нашим объектом поиска, поэтому, как и в классическом машинном обучении, мы ищем какую-то функцию.

Взаимодействие со средой агента со стратегией $\pi(a \mid s)$ моделируется так. Изначально среда находится в некотором состоянии $s_0$. Агент сэмплирует действие из своей стратегии $a_0 \sim \pi(a_0 \mid s_0)$. Среда отвечает на это, сэмплируя своё следующее состояние $s_1 \sim p(s_1 \mid s_0, a_0)$ из функции переходов, а также выдаёт агенту награду в размере $r(s_0, a_0)$. Процесс повторяется: агент снова сэмплирует $a_1$, а среда отвечает генерацией $s_2$ и скалярной наградой $r(s_1, a_1)$. Так продолжается до бесконечности или пока среда не перейдёт в терминальное состояние, после попадания в которое взаимодействие прерывается, и сбор агентом награды заканчивается. Если в среде есть терминальные состояния, одна итерация взаимодействия от начального состояния до попадания в терминальное состояние называется **эпизодом** (**episode**). Цепочка генерируемых в ходе взаимодействия случайных величин $s_0, a_0, s_1, a_1, s_2, a_2, \dots$ называется **траекторией** (**trajectory**). *Примечание:* функция награды тоже может быть стохастичной, и тогда награды за шаг тоже будут случайными величинами и частью траекторий, но без ограничения общности мы будем рассматривать детерминированные функции награды.

![MDP](https://yastatic.net/s3/education-portal/media/MDP_8b61770354_32a9e2928e.webp)

Итак, фактически среда для нас — это управляемая марковская цепь: на каждом шаге мы выбором $a$ определяем то распределение, из которого будет генерироваться следующее состояние. Мы предполагаем, во-первых, марковское свойство: что переход в следующее состояние определяется лишь текущим состоянием и не зависит от всей предыдущей истории:

$$p(s_{t+1} \mid s_{t}, a_{t}, s_{t-1}, a_{t-1}, \dots, s_0, a_0) = p(s_{t+1} \mid s_{t}, a_{t})
$$

Во-вторых, мы предполагаем стационарность: функция переходов $p(s' \mid s, a)$ не зависит от времени, от того, сколько шагов прошло с начала взаимодействия. Это довольно реалистичные предположения: законы мира не изменяются со временем (стационарность), а состояние — описывает мир целиком (марковость). В этой модели взаимодействия есть только одно нереалистичное допущение: **полная наблюдаемость** (**full observability**), которая гласит, что агент в своей стратегии $\pi(a \mid s)$ наблюдает всё состояние $s$ полностью и может выбирать действия, зная об окружающем мире абсолютно всё; в реальности нам же доступны лишь какие-то частичные наблюдения состояния. Такая более реалистичная ситуация моделируется в **частично наблюдаемых MDP** (**Partially observable MDP**, **PoMDP**), но мы далее ограничимся полностью наблюдаемыми средами.

Итак, мы научились на математическом языке моделировать среду, агента и их взаимодействие между собой. Осталось понять, чего же мы хотим. Во время взаимодействия на каждом шаге агенту приходит награда $r_t = r(s_t, a_t)$, однако, состояния и действия $s_t, a_t$ в рамках такой постановки — случайные величины. Один и тот же агент может в силу стохастики как внутренней (в силу случайности выбора действий в его стратегии), так и внешней (в силу стохастики в функции переходов) набирать очень разную суммарную награду $\sum_{t \ge 0} r_t$ в зависимости от везения. Мы скажем, что хотим научиться выбирать действия так, чтобы собирать *в среднем* как можно больше награды.

Что значит в среднем, в среднем по чему? По всей стохастике, которая заложена в нашем процессе взаимодействия со средой. Каждая стратегия $\pi$ задаёт распределение в пространстве траекторий — с какой вероятностью нам может встретится траектория $\mathcal{T} = (s_0, a_0, s_1, a_1, \dots)$:

$$p(\mathcal{T} \mid \pi) = p(s_0, a_0, s_1, a_1, \dots \mid \pi) = \prod_{t \ge 0} p(s_{t + 1} \mid s_t, a_t)\pi(a_t \mid s_t)
$$

Вот по такому распределению мы и хотим взять среднее получаемой агентом награды. Записывают это обычно как-нибудь так:

$$\mathbb{E}_{\mathcal{T} \sim \pi} \sum_{t \ge 0} r_t \to \max_{\pi}
$$

Здесь мат.ожидание по траекториям — это бесконечная цепочка вложенных мат.ожиданий:

$$\mathbb{E}_{\mathcal{T} \sim \pi} ( \cdot ) = \mathbb{E}_{a_0 \sim \pi(a_0 \mid s_0)} \mathbb{E}_{s_1 \sim p(s_1 \mid s_0, a_0)} \mathbb{E}_{a_1 \sim \pi(a_1 \mid s_1)} \dots ( \cdot )
$$

Вот такую конструкцию мы и хотим оптимизировать выбором стратегии $\pi$. На практике, однако, вносят ещё одну маленькую корректировку. В средах, где взаимодействие может продолжаться бесконечно долго, агент может научиться набирать бесконечную награду, с чем могут быть связаны разные парадоксы (например, получать \+1 на каждом втором шаге становится также хорошо, как получать \+1 на каждом сотом шаге). Поэтому вводят **дисконтирование** (**discounting**) награды, которое гласит: тортик сейчас лучше, чем тот же самый тортик завтра. Награду, которую мы получим в будущем, агент будет дисконтировать на некоторое число $\gamma$, меньшее единицы. Тогда наш функционал примет такой вид:

$$\mathbb{E}_{\mathcal{T} \sim \pi} \sum_{t \ge 0} \gamma^t r_t \to \max_{\pi}
$$

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/discounting_603b057cbc_246b1f7745.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <p>источник картинки — курс <a href="http://ai.berkeley.edu/lecture_slides.html">UC Berkeley AI</a></p>
  </figcaption>
</figure>

Заметим, что обучение с подкреплением - это в первую очередь задача оптимизации, оптимизации функционалов определённого вида. Если в классическом машинном обучении подбор функции потерь можно считать элементом инженерной части решения, то здесь функция награды задана нам готовая и определяет тот функционал, который мы хотим оптимизировать.

{% cut "Примеры" %}

Формализм MDP очень общий, и под него попадает практически всё, что можно назвать «интеллектуальной задачей» (с той оговоркой, что не всегда очевидно, какая функция награды задаёт ту или иную задачу).

Самые простые примеры MDP можно нарисовать «на бумажке». Например, часто рассматривают «клетчатые миры» (GridWorlds): агент находится в некоторой позиции клетчатой доски и может в качестве действий выбирать одно из четырёх направлений. Такие миры могут по-разному реагировать агента за выбор действия «пойти в стену», с некоторой вероятностью перемещать агента не в том направлении, которое он выбрал, содержать предметы в некоторых клетках и так далее. Пространство состояний, в которых может оказаться агент, в таких примерах конечно, как и пространство действий. Такие MDP называют **табличными**: все состояния и действия можно перечислить.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/Tabular_MDP_96af6be96e_725a165d54.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <p>источник картинки — курс <a href="http://ai.berkeley.edu/lecture_slides.html">UC Berkeley AI</a></p>
  </figcaption>
</figure>

Огромное разнообразие MDP предоставляют видеоигры. Можно считать, что на вход агенту подаётся изображение экрана видеоигры, и несколько раз в секунду агент выбирает, какие кнопки на контроллере он хочет нажать. Тогда пространство состояний - множество всевозможных картинок, которые вам может показать видеоигра. Множество, в общем-то, конечное (конечное количество пикселей экрана с тремя цветовыми каналами, каждый из который показывает целочисленное значение от 0 до 255), но только очень большое; например, их уже нельзя перечислить или сохранить все возможные варианты в памяти. Но на каждом шаге нужно выбирать действие из конечного набора: какие кнопки нажать, поэтому это задачи **дискретного управления**.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/flappybird_4547e2f1a3_5e870c369a.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <p>источник картинки — курс <a href="http://ai.berkeley.edu/lecture_slides.html">UC Berkeley AI</a></p>
  </figcaption>
</figure>

Наконец, естественный способ создавать среды — использование физических симуляций. В качестве бенчмарка часто используют locomotion — задачу научить какое-нибудь «существо» ходить в рамках той или иной физической модели (примеры можно посмотреть, например, [здесь](https://deepmind.com/blog/article/producing-flexible-behaviours-simulated-environments)). Причём концептуально, в рамках задачи обучения с подкреплением, нам даже неважно, как именно устроена симуляция или как задана функция награды: мы хотим построить общий алгоритм оптимизации этой самой награды. Если награда поощряет перемещение центра масс «существа» вдоль некоторого направления, агент постепенно научится выбирать действия так, чтобы существо перемещалось и не падало, если последнее приводит к завершению эпизода и мешает дальнейшему получению награды.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/locomotion_9da7ae9aaf_87a611b13b.gif" loading="lazy" decoding="async" alt="">
  <figcaption>
    <p>источник картинки — статья DeepMind <a href="https://deepmind.com/blog/article/producing-flexible-behaviours-simulated-environments">Producing Flexible Behaviours in Simulated Environments</a></p>
  </figcaption>
</figure>

В таких задачах агент на каждом шаге выбирает несколько вещественных чисел в диапазоне $[-1, 1]$, где -1 — «максимально расслабить» сустав, а \+1 — «максимально напрячь». Такое пространство действий возникает во многих задачах робототехники, где нужно научиться поворачивать какой-нибудь руль, и у него есть крайнее правое и крайнее левое положение, но можно выбрать и любое промежуточное. Такие задачи называются задачами **непрерывного управления** (continuous control).

{% endcut %}

## Окей, и как такое решать?

Выглядит сложновато, но у человечества есть уже довольно много наработок, как подойти к этой на вид очень общей задаче, причём с основной идеей вы скорее всего уже сталкивались. Называется она **динамическим программированием**.

Дело в том, что мы оптимизируем не абы какой функционал, а среднюю дисконтированную кумулятивную награду. Чтобы придумать более эффективное решение, чем какой-нибудь подход, не использующий этот факт (например, эволюционные алгоритмы), нам нужно воспользоваться структурой поставленной задачи. Эта структура задана в формализме MDP и определении процесса взаимодействия агента со средой. Интуитивно она выражается так: вот мы сидим в некотором состоянии $s$ и хотим выбрать действие $a$ как можно оптимальнее. Мы знаем, что после выбора этого действия мы получим награду за этот шаг $r = r(s, a)$, среда перекинет нас в состояние $s'$ и, внимание, дальше нас ждёт *подзадача эквивалентной структуры*: в точности та же задача выбора оптимального действия, только в другом состоянии. Действительно: когда мы будем принимать решение на следующем шаге, на прошлое мы повлиять уже не способны; стационарность означает, что законы, по которым ведёт себя среда, не поменялись, а марковость говорит, что история не влияет на дальнейший процесс нашего взаимодействия. Это наводит на мысль, что задача максимизации награды из текущего состояния тесно связана с задачей максимизации награды из следующего состояния $s'$, каким бы оно ни было.

Чтобы сформулировать это на языке математики, вводятся «дополнительные переменные», вспомогательные величины, называемые **оценочными функциями**. Познакомимся с одной такой оценочной функцией - оптимальной Q-функцией, которую будем обозначать $Q^{*}(s,a)$. Скажем, что $Q^{*}(s,a)$ - это то, сколько максимально награды можно (в среднем) набрать после выбора действия $a$ из состояния $s$. Итак:

$$Q^{*}(s, a) = \max_{\pi} \mathbb{E}_{\mathcal{T} \sim \pi \mid s_0 = s, a_0 = a} \sum_{t \ge 0} \gamma^t r_t
$$

Запись $\mathcal{T} \sim \pi \mid s_0 = s, a_0 = a$ здесь означает, что мы садимся в состояние $s_0 = s$; выбираем действие $a_0 = a$, а затем продолжаем взаимодействие со средой при помощи стратегии $\pi$, порождая таким образом траекторию $\mathcal{T}$. По определению, чтобы посчитать $Q^{*}(s,a)$, нужно перебрать все стратегии, посмотреть, сколько каждая из них набирает награды после выбора $a$ из состояния $s$, и взять наилучшую стратегию. Поэтому эта оценочная функция называется оптимальной: она предполагает, что в будущем после выбора действия $a$ из состояния $s$ агент будет вести себя оптимально.

Определение неконструктивное, конечно, поскольку в реальности мы так сделать не можем, зато обладает интересным свойством. Если мы каким-то чудом узнали $Q^{*}(s,a)$, то мы знаем оптимальную стратегию. Действительно: представьте, что вы находитесь в состоянии $s$, вам нужно сделать выбор из трёх действий, и вы знаете значения $Q^{*}(s,a)$. Вы знаете, что если выберете первое действие $a = 0$, то в будущем сможете набрать не более чем, допустим, $Q^{*}(s, a = 0) = +3$ награды. При этом вы знаете, что существует какая-то стратегия $\pi$, на которой достигается максимум в определении оптимальной Q-функции, то есть которая действительно позволяет набрать эти \+3. Вы знаете, что если выберете второе действие, то в будущем сможете набрать, допустим, $Q^{*}(s, a = 1) = +10$, а для третьего действия $Q^{*}(s, a = 2) = -1$. Вопрос: так как нужно действовать? Интуиция подсказывает, что надо просто выбирать действие $a = 1$, что позволит набрать \+10, ведь по определению больше набрать никак не получится. Значит, выбор в этом состоянии действия $a = 1$ оптимален. Эта интуиция нас не обманывает, и принцип такого выбора называется **принципом оптимальности Беллмана**.

Выбор того действия, на котором достигается максимум по действиям Q-функции, называется **жадным** (greedy) по отношению к ней. Таким образом, принцип оптимальности Беллмана гласит:\
*жадный выбор по отношению к оптимальной Q-функции оптимален*:

$$\pi^*(s) = \operatorname{argmax}\limits_a Q^{*}(s, a)
$$

*Примечание:* если Q-функция достигает максимума на нескольких действиях, то можно выбирать любое из них.

Заметим, что эта оптимальная стратегия детерминирована. Этот интересный факт означает, что нам, в общем-то, необязательно искать стохастичную стратегию. Наше рассуждение пока даже показывает, что мы можем просто пытаться найти $Q^{*}(s,a)$, а дальше выводить из неё оптимальную стратегию, выбирая действие жадно.

Но как искать $Q^{*}(s,a)$? Тут на сцене и появляется наше наблюдение про структуру задачи. Оказывается, $Q^{*}(s,a)$ выражается через саму себя. Действительно: рассмотрим некоторую пару состояние-действие $s, a$. С одной стороны, по определению, мы в будущем сможем при условии оптимального поведения получить $Q^{*}(s,a)$ награды. С другой стороны, после того, как мы выберем действие $a$ в состоянии $s$, мы получим награду за один шаг $r(s, a)$, вся дальнейшая награда будет дисконтирована на $\gamma$, среда ответит нам сэмплированием $s' \sim p(s' \mid s, a)$ (на результат этого сэмплирования мы уже никак повлиять не можем и по этой стохастике нашу будущую награду надо будет усреднять), а затем в состоянии $s'$ мы, в предположении оптимальности поведения, выберем то действие $a'$, на котором достигается максимум $Q^{*}(s', a')$. Другими словами, в дальнейшем после попадания в $s'$ мы сможем получить $\max\limits_{a'} Q^{*}(s', a')$ награды. А значит, верно следующее рекурсивное соотношение, называемое **уравнением оптимальности Беллмана для Q-функции**:

$$Q^{*}(s, a) = r(s, a) + \gamma \mathbb{E}_{s' \sim p(s' \mid s, a)} \max_{a'} Q^{*}(s', a')
$$

Мы получили систему уравнений, связывающую значения $Q^{*}(s,a)$ с самой собой. Это нелинейная система уравнений, но оказывается, что она в некотором смысле «хорошая». У неё единственное решение - и, значит, решение этого уравнения можно считать эквивалентным определением $Q^{*}(s,a)$, - и его можно искать **методом простой итерации**. Метод простой итерации решения систем уравнений позволяет улучшать своё текущее приближение $x$ решения некоторого уравнения вида $x = f(x)$ его подстановкой в правую часть. То есть: инициализируем произвольную функцию $Q_0^*(s, a) \colon \mathcal{S} \times \mathcal{A} \to \mathbb{R}$, которая будет приближать $Q^{*}(s,a)$, затем итеративно будем подставлять её в правую часть уравнений оптимальности Беллмана и полученным значением обновлять наше приближение:

$$Q^{*}_{k+1}(s, a) \leftarrow r(s, a) + \gamma \mathbb{E}_{s' \sim p(s' \mid s, a)} \max_{a'} Q^{*}_k(s', a')
$$

Такая процедура в пределе приведёт нас к истинной $Q^{*}(s,a)$, а значит и оптимальной стратегии. Кстати, когда вы в прошлом встречались с динамическим программированием, вы скорее всего неявно использовали именно эту идею, разве что часто в задачах для решения уравнений оптимальности Беллмана можно просто последовательно исключать неизвестные переменные; но метод простой итерации даёт более общую схему, применимую всегда. А сейчас для нас принципиально следующее: если у нас есть какое-то приближение $Q^{*}$, то вычисление правой части уравнения оптимальности Беллмана позволит получить приближение лучше.

## А где же метод проб и ошибок?

Решать методом простой итерации уравнения оптимальности Беллмана и таким образом получать $Q^{*}(s,a)$ в реальности можно только при двух очень существенных ограничивающих условиях. Нужно, чтобы, во-первых, мы могли хранить как-то текущее приближение $Q^{*}_k(s, a)$ в памяти. Это возможно только если пространства состояний и действий конечные и не очень большие, то есть, например, в вашем MDP всего 10 состояний и 5 действий, тогда $Q^{*}(s,a)$ — это табличка 10x5. Но что, если вы хотите научиться играть в видеоигру, и состояние — это входное изображение? Тогда множество картинок, которые вам может показать видеоигра, сохранить в памяти уже не получится. Ну, допустим пока, что число состояний и число действий не очень большое, и мы всё-таки можем хранить таблицу в памяти, а позже мы снимем это ограничение, моделируя $Q^{*}(s,a)$ при помощи нейросети.

Во-вторых, нам необходимо уметь считать выражение, стоящее справа в уравнение оптимальности Беллмана:

$$r(s, a) + \gamma \mathbb{E}_{s' \sim p(s' \mid s, a)} \max_{a'} Q^{*}_k(s', a')
$$

Мало того, что в сложных средах взять мат.ожидание по функции переходов $\mathbb{E}_{s' \sim p(s' \mid s, a)}$ в реальности мы не сможем, так ещё и обычно мы эту функцию переходов на самом деле не знаем. Представьте, что вы катаетесь на велосипеде: можете ли вы по текущему состоянию окружающего мира, например, положению всех атомов во вселенной, рассказать, с какими вероятностями в каком состоянии мир окажется в следующий момент времени? Это соображение также подсказывает, что было бы здорово, если б мы смогли решать задачу, избегая даже попыток выучить эту сложную функцию переходов.

Что нам доступно? Мы можем взять *какую-нибудь* стратегию $\pi$ (важный момент: мы должны сами выбрать какую) и повзаимодействовать ею со средой. «Попробовать решить задачу». Мы можем сгенерировать при помощи $\pi$ целую траекторию или даже сделать всего один шаг в среде. Таким образом мы *соберём данные*: допустим, мы были в состоянии $s$ и сделали выбор действия $a$, тогда мы узнаем, какую награду $r = r(s, a)$ мы получаем за такой шаг и, самое главное, в какое состояние $s'$ нас перевела среда. Полученный $s'$ — сэмпл из функции переходов $s' \sim p(s' \mid s, a)$. Собранная так информация — четвёрка $(s, a, r, s')$ — называется **переходом** (transition), и может быть как-то использована для оптимизации нашей стратегии.

Можем ли мы, используя лишь переходы $(s, a, r, s')$, то есть имея на руках лишь сэмплы $s' \sim p(s' \mid s, a)$, как-то пользоваться схемой динамического программирования? Что, если мы будем заменять значение $Q_{k}^{*}(s, a)$ не на

$$r(s, a) + \gamma \mathbb{E}_{s' \sim p(s' \mid s, a)} \max_{a'} Q_{k}^{*}(s', a'),
$$

которое мы не можем посчитать, а на его Монте Карло оценку:

$$r(s, a) + \gamma \max_{a'} Q^{*}_k(s', a'),
$$

где $s'$ — сэмпл из функции переходов из собранного нами опыта? В среднем-то такая замена верная. Такая Монте-Карло оценка правой части для заданного переходика $(s, a, r, s')$ называется **Беллмановским таргетом**, то есть «целевой переменной». Почему такое название — мы увидим чуть позже.

Чтобы понять, как нам нужно действовать, рассмотрим какую-нибудь типичную ситуацию. Допустим, после выполнения действия $a$ из некоторого состояния $s$ среда награждает нас $r(s, a) = 0$ и перекидывает нас с равными вероятностями то в состояние $s'$, для которого $\max_{a'} Q^{*}_{k}(s', a') = +1$, то в состояние $s'$, для которого $\max_{a'} Q^{*}_{k}(s', a') = -1$. Метод простой итерации говорит, что на очередной итерации нужно заменить $Q^{*}_{k}(s, a)$ на $0.5 \gamma \cdot (+1) + 0.5 \gamma \cdot (-1) = 0$, но в реальности мы встретимся лишь с одним исходом, и таргет — Монте-Карло оценка правой части уравнения оптимальности Беллмана — будет с вероятностью 0.5 равен $+\gamma$, а с вероятностью 0.5 равен $-\gamma$. Ясно, что нельзя просто взять и жёстко заменять наше текущее приближение $Q^{*}_k(s, a)$ на посчитанный Беллмановский таргет по некоторому одному переходу, поскольку нам могло повезти (мы увидели $+\gamma$) или не повезти (мы увидели $-\gamma$). Давайте вместо этого поступать также, как учат среднее по выборке: не сдвигать «жёстко» наше текущее приближение в значение очередного сэмпла, а *смешивать* текущее приближение с очередным сэмплом. То есть: берём переходик $(s, a, r, s')$, и не заменяем $Q^{*}_k(s, a)$ на стохастичную оценку правой части уравнения оптимальности Беллмана, а только сдвигаемся в его сторону:

$$Q^{*}_{k+1}(s, a) \leftarrow (1 - \alpha)Q^{*}_k(s, a) + \alpha (r + \gamma \max_{a'} Q^{*}_k(s', a'))
$$

Таким образом, мы проводим <b>экспоненциальное сглаживание</b> старого приближения $Q^{*}_k(s, a)$ и новой оценки правой части уравнения оптимальности Беллмана со свежим сэмплом $s'$. Выбор $\alpha$ здесь определяет, насколько сильно мы обращаем внимание на последние сэмплы, и имеет тот же физический смысл, что и learning rate. В среднем по стохастике (а стохастика в этой формуле обновления заложена в случайном $s'$) мы будем сдвигаться в сторону

$$r(s, a) + \gamma \mathbb{E}_{s' \sim p(s' \mid s, a)} \max_{a'} Q_{k}^{*}(s', a'),
$$

и значит применять этакий «зашумлённый» метод простой итерации.

Итак, возникает следующая идея. Будем как-то взаимодействовать со средой и собирать переходики $(s, a, r, s')$. Для каждого перехода будем обновлять одну ячейку нашей Q-таблицы размера число состояний на число действий по вышеуказанной формуле. Таким образом мы получим как бы «зашумлённый» метод простой итерации, где мы на каждом шаге обновляем только одну ячейку таблицы, и не заменяем жёстко значение на правую часть уравнений оптимальности, а лишь сдвигаемся по некоторому в среднем верному стохастичному направлению.

Очень похоже на стохастическую оптимизацию вроде стохастического градиентного спуска, и поэтому гарантии сходимости выглядят схожим образом. Оказывается, такой алгоритм сходится к истинной $Q^{*}(s,a)$, если для любой пары $s, a$ мы в ходе всего процесса проводим бесконечное количество обновлений, а learning rate (гиперпараметр $\alpha$) в них ведёт себя как learning rate из условий сходимости стохастического градиентного спуска:

$$\sum_i \alpha_i = +\infty, \qquad \sum_i \alpha^2_i < +\infty
$$

{% cut "Пример" %}

Колобок Колабулька любит играть в лотереи. Допустим, в некотором состоянии $s$ он выполнил действие $a$ «купить билет» и предполагает, что в будущем сможет набрать $Q^{*}(s, a) = +100$ награды. Однако, за покупку билета он платит 10 долларов, и таким образом теряет 10 награды на данном шаге $r(s, a) = -10$, при этом попадая в состояние $s' = s$, где ему снова предлагается купить билет в лотерею (он может выбрать действие «купить» или действие «не купить»). Ну, допустим,  текущая оценка Колабульки будущей награды в случае отказа купить билет равна 0 \< \+100, поэтому колобок предполагает, что в будущем из состояния $s'$ он сможет получить $\max\limits_{a'} Q^{*}(s', a') = +100$. Для простоты допустим $\gamma = +1$. Тогда получается, что одношаговое приближение будущей награды $r(s, a) + \gamma \max\limits_{a'} Q^{*}(s', a') = -10 + 100 = 90$. Да, $s'$ здесь — случайная величина, колобку могло повезти или не повезти (и мы, увидев всего один сэмпл из функции переходов, не можем сказать наверняка, повезло ли нам сейчас или нет), но наша формула говорит сдвинуть аппроксимацию $Q^{*}(s,a)$ в сторону Беллмановского таргета.

![Qlearning](https://yastatic.net/s3/education-portal/media/Qlearning_ex1_bb18ea339b_1_e1a76edecb_fbb4604362.svg)

Допустим, learning rate $\alpha = 0.5$: тогда, сдвигая \+100 в сторону \+90, ожидания от будущей награды после покупки лотерейного билета опускаются до 95. Всё ещё $+95 > 0$, поэтому колобку кажется, что покупать билет выгоднее, чем не покупать, поэтому рассмотрим следующий переходик. Допустим, колобок снова купил билет, снова потерял 10 долларов и снова попал в то же самое $s' = s$. Наше обновление снова скажет уменьшать значение $Q^{*}(s,a)$:

![Qlearning](https://yastatic.net/s3/education-portal/media/Qlearning_ex1_bb18ea339b_2_6380b90eda_031106cae9.svg)

Видно, что если колобку продолжит так не везти, таргет будет всё время на 10 меньше, чем текущее приближение, и $Q^{*}(s,a)$ будет всё уменьшаться и уменьшаться, пока не свалится до нуля (а там будет выгоднее уже не покупать билет). Но если на очередной итерации Колабульке повезло, и среда перевела его в $s'$, соответствующее победе в лотерею (а это, видимо, происходит с какой-то маленькой вероятностью), таргет получится очень большим, и аппроксимацию $Q^{*}(s,a)$ наше обновление скажет сильно увеличить:

![Qlearning](https://yastatic.net/s3/education-portal/media/Qlearning_ex1_bb18ea339b_3_2219f47f64_e7a6afb54a.svg)

Куда будет сходиться такой алгоритм? Давайте предположим, что среда на покупку лотерейного билета отвечает с вероятностью $p$ возвращением в то же состояние $s' = s$, где колобку предлагается купить ещё один билет, а с вероятностью $1 - p$ билет оказывается выигрышным, и колобок попадает в такое состояние $s'$, в котором он может забрать приз и получить \+1000 (после этого взаимодействие со средой, скажем, заканчивается). Давайте запишем уравнение оптимальности Беллмана для действия $a$ «купить билет» в состоянии $s$:

$$Q^{*}(s, a) = r(s, a) + \gamma \left( p \max_{a'} Q^{*}(s' = s, a') + (1 - p) \cdot (+1000) \right)
$$

Здесь $\max_{a'} Q^{*}(s' = s, a') = \max (Q^{*}(s, a), 0)$, поскольку колобок может или покупать билет, или не покупать (это, допустим, принесёт ему 0 награды). Понятно, что если покупка билета не принесёт больше 0 награды, то не имеет смысла его покупать. Подставляя все числа из примера, получаем:

$$Q^{*}(s, a) = -10 + p \max (Q^{*}(s, a), 0) + 1000(1 - p) 
$$

Видно, что если вероятность проигрыша в лотерею $p = 0.99$, то решением уравнения является $Q^{*}(s, a) = 0$: Колабулька платит за билет 10 долларов и получает 1000 награды с вероятностью 0.01. В этом случае действие «купить билет» и «не покупать» равноценны, и оба в будущем принесут в среднем 0 награды. Если же $p > 0.99$, то покупать билет становится невыгодно, а если $p < 0.99$, то выгодно покупать билет до тех пор, пока не случится победа. Несмотря на то, что в таргете содержится собственная же текущая аппроксимация будущей награды и используется лишь один сэмпл $s'$ вместо честного усреднения по всевозможным исходам, формула обновления постепенно сойдётся к этому решению. Причём колобку истинное значение $p$ неизвестно, и в формуле обновления эта вероятность влияла лишь на появление того или иного $s'$ в очередном таргете.

{% endcut %}

Этот алгоритм, к которому мы уже практически пришли, называется **Q-learning**, «обучение оптимальной Q-функции». Нам, однако, осталось ответить на один вопрос: так как же нужно собирать данные, чтобы удовлетворить требованиям для сходимости? Как взаимодействовать со средой так, чтобы мы каждую ячейку $s, a$ не прекращали обновлять?

## Дилемма Exploration-exploitation

Мы уже встречали дилемму exploration-exploitation (букв. «исследования-использования») в параграфе про [тюнинг гиперпараметров](https://academy.yandex.ru/handbook/ml/article/podbor-giperparametrov). Задача многоруких бандитов, которая там встретилась, на самом деле является частным случаем задачи обучения с подкреплением, в котором после первого выбора действия эпизод гарантированно завершается, и этот частный случай задачи часто используется для изучения этой дилеммы. Рассмотрим эту дилемму в нашем контексте.

Допустим, на очередном шаге алгоритма у нас есть некоторое приближение $Q_k(s, a) \approx Q^{*}(s, a)$. Приближение это, конечно, неточное, поскольку алгоритм, если и сходится к истинной оптимальной Q-функции, то на бесконечности. Как нужно взаимодействовать со средой? Если вы хотите набрать максимальную награду, наверное, стоит воспользоваться нашей теорией и заниматься **exploitation**-ом, выбирая действие жадно:

$$\pi(s) = \operatorname{argmax}_{a} Q_k(s, a)
$$

Увы, такой выбор не факт что совпадёт с истинной оптимальной стратегией, а главное, он детерминирован. Это значит, что при взаимодействии этой стратегией со средой, многие пары $s, a$ никогда не будут встречаться просто потому, что мы никогда не выбираем действие $a$ в состоянии $s$. А тогда мы, получается, рискуем больше никогда не обновить ячейку $Q_k(s, a)$ для таких пар!

Такие ситуации запросто могут привести к застреванию алгоритма. Мы хотели научиться кататься на велосипеде и получали \+0.1 за каждый пройденный метр и -5 за каждое попадание в дерево. После первых проб и ошибок мы обнаружили, что катание на велосипеде приносит нам -5, поскольку мы очень скоро врезаемся в деревья и обновляли нашу аппроксимацию Q-функции сэмплами с негативной наградой; зато если мы не будем даже забираться на велосипед и просто займёмся ничего не деланьем, то мы сможем избежать деревьев и будем получать 0. Просто из-за того, что в нашей стратегии взаимодействия со средой никогда не встречались те $s, a$, которые приводят к положительной награде, и жадная стратегия по отношению к нашей текущей аппроксимации Q-функции никогда не выбирает их. Поэтому нам нужно экспериментировать и пробовать новые варианты.

Режим **exploration**-а предполагает, что мы взаимодействуем со средой при помощи какой-нибудь *стохастичной* стратегии $\forall s, a \colon \pi(a \mid s) > 0$. Например, такой стратегией является случайная стратегия, выбирающая рандомные действия. Как ни странно, сбор опыта при помощи случайной стратегии позволяет побывать с ненулевой вероятностью во всех областях пространства состояний, и теоретически даже наш алгоритм обучения Q-функции будет сходится. Означает ли это, что exploration-а хватит, и на exploitation можно забить?

В реальности мы понимаем, что добраться до самых интересных областей пространства состояний, где функция награда самая большая, не так-то просто, и случайная стратегия хоть и будет это делать с ненулевой вероятностью, но вероятность эта будет экспоненциально маленькая. А для сходимости нам нужно обновить ячейки $Q_k(s, a)$ для этих интересных состояний бесконечно много раз, то есть нам придётся дожидаться необычайно редкого везения далеко не один раз. Куда разумнее использовать уже имеющиеся знания и при помощи жадной стратегии, которая уже что-то умеет, идти к этим интересным состояниям. Поэтому для решения дилеммы exploration-exploitation обычно берут нашу текущую жадную стратегию и что-нибудь с ней делают такое, чтобы она стала чуть-чуть случайной. Например, с вероятностью $\varepsilon > 0$ выбирают случайное действие, а с вероятностью $1 - \varepsilon$ — жадное. Тогда мы чаще всё-таки и знаниями пользуемся, и любое действие с ненулевой вероятностью выбираем; такая стратегия называется $\varepsilon$-жадной, и она является самым простым способом как-то порешать эту дилемму.

Давайте закрепим, что у нас получилось, в виде табличного алгоритма обучения с подкреплением под названием Q-learning:

1. Проинициализировать $Q^{*}(s,a)$ произвольным образом.
2. Пронаблюдать $s_0$ из среды.
3. Для $k = 0, 1, 2, \dots$:

* с вероятностью $\varepsilon$ выбрать действие $a_k$ случайно, иначе жадно: ${a_k​}= \operatorname{argmax}_{a_k}Q^{∗}(s_k​,a_k​)$
* отправить действие $a_k$ в среду, получить награду за шаг $r_k$ и следующее состояние<br>$s_{k+1}$.
* обновить одну ячейку таблицы:

$$Q^{*}(s_k, a_k) \leftarrow (1 - \alpha)Q^{*}(s_k, a_k) + \alpha (r_k + \gamma \max\limits_{a'} Q^{*}(s_{k+1}, a'))
$$

## Добавим нейросеток

Наконец, чтобы перейти к алгоритмам, способным на обучение в сложных MDP со сложным пространством состояний, нужно объединять классическую теорию обучения с подкреплением с парадигмами глубокого обучения.

Допустим, мы не можем позволить себе хранить $Q^{*}(s,a)$ как таблицу в памяти, например, если мы играем в видеоигру и на вход нам подаются какие-нибудь изображения. Тогда мы можем обрабатывать любые имеющиеся у агента входные сигналы при помощи нейросетки $Q^{*}(s, a, \theta)$. Для тех же видеоигр мы легко обработаем изображение экрана небольшой свёрточной сеточкой и выдадим для каждого возможного действия $a$ вещественный скаляр $Q^{*}(s, a, \theta)$. Допустим также, что пространство действий всё ещё конечное и маленькое, чтобы мы могли для такой модели строить жадную стратегию, выбирать $\operatorname{argmax}_{a} Q^{*}(s, a, \theta)$. Но как обучать такую нейросетку?

Давайте ещё раз посмотрим на формулу обновления в Q-learning для одного переходика $(s, a, r, s')$:

$$\begin{align*}
Q^{*}_{k+1}(s, a) \leftarrow (1 - \alpha)Q^{*}_k(s, a) + \alpha (r + \gamma \max_{a'} Q^{*}_k(s', a')) = \\
= Q^{*}_k(s, a) + \alpha (r + \gamma \max_{a'} Q^{*}_k(s', a') - Q^{*}_k(s, a))
\end{align*}
$$

Теория Q-learning-а подсказывала, что у процесса такого обучения Q-функции много общего с обычным стохастическим градиентным спуском. В таком виде формула подсказывает, что, видимо,

$$r + \gamma \max_{a'} Q^{*}_{k}(s', a') - Q^{*}_{k}(s, a)
$$

— это стохастическая оценка какого-то градиента. Этот градиент сравнивает Беллмановский таргет

$$r + \gamma \max_{a'} Q^{*}_{k}(s', a')
$$

с нашим текущим приближением $Q^{*}_{k}(s, a)$ и чуть-чуть корректирует это значение, сдвигая в сторону таргета. Попробуем «заменить» в этой формуле Q-функцию с табличного представления на нейросетку.

Рассмотрим такую задачу регрессии. Чтобы построить один прецедент для обучающей выборки, возьмём один имеющийся у нас переходик $(s, a, r, s')$. Входом будет пара $s, a$. Целевой переменной, таргетом, будет Беллмановский таргет

$$y = r + \gamma \max_{a'} Q^{*}(s', a', \theta);
$$

его зависимость от параметров $\theta$ нашей нейронки мы далее будем игнорировать и будем «притворяться», что это и есть наш ground truth. Именно поэтому Монте-Карло оценка правой части уравнения оптимальности Беллмана и называют таргетом. Но важно помнить, что эта целевая переменная на самом деле «зашумлена»: в формуле используется взятый из перехода $s'$, который есть лишь сэмпл из функции переходов. На самом же деле мы хотели бы выучить среднее значение такой целевой переменной, и поэтому в качестве функции потерь мы возьмём MSE. Как будет выглядеть шаг стохастического градиентного спуска для решения этой задачи регрессии (для простоты — для одного прецедента)?

$$\begin{aligned}
\theta_{k+1} \leftarrow &\theta_{k} - \alpha \nabla_{\theta} (y - Q^{*}(s, a, \theta))^2 = \\
= &\theta_{k} + 2 \alpha (y - Q^{*}(s, a, \theta)) \nabla_{\theta} Q^{*}(s, a, \theta) = \\
= &\theta_{k} + 2 \alpha (r + \gamma \max_{a'} Q^{*}(s', a', \theta) - Q^{*}(s, a, \theta)) \nabla_{\theta} Q^{*}(s, a, \theta)
\end{aligned}
$$

Это практически в точности повторяет формулу Q-learning, которая гласит, что если таргет $r + \gamma \max_{a'} Q^{*}(s', a', \theta)$ больше $Q^{*}(s, a, \theta)$, то нужно подстроить веса нашей модели так, чтобы $Q^{*}(s, a, \theta)$ стало чуть побольше, и наоборот. В среднем при такой оптимизации мы будем двигаться в сторону

$$\mathbb{E}_{s' \sim p(s' \mid s, a)} y = \mathbb{E}_{s' \sim p(s' \mid s, a)} \left[ r + \gamma \max_{a'} Q^{*}(s', a', \theta) \right]
$$

— в сторону правой части уравнения оптимальности Беллмана, то есть моделировать метод простой итерации для решения системы нелинейных уравнений.

Единственное отличие такой задачи регрессии от тех, с которыми сталкивается традиционное глубокое обучение — то, что целевая переменная *зависит от нашей же собственной модели*. Раньше целевые переменные были напрямую источником обучающего сигнала. Теперь же, когда мы хотим выучить будущую награду при условии оптимального поведения, мы не знаем этого истинного значения или даже её стохастичных оценок. Поэтому мы применяем идею **бутстрапирования** (**bootstrapping**): берём награду за следующий шаг, и нечестно приближаем всю остальную награду нашей же текущей аппроксимацией $\max_{a'} Q^{*}(s', a', \theta)$. Да, за этим кроется идея метода простой итерации, но важно понимать, что такая целевая переменная лишь указывает направление для обучения, но не является истинным приближением будущих наград или даже их несмещённой оценкой. Поэтому говорят, что в этой задаче регрессии очень **смещённые** (**biased**) целевые переменные.

На практике из-за этого возникает беда. Наша задача регрессии в таком виде меняется после каждого же шага. Если вдруг после очередного шага оптимизации и обновления весов нейросети наша модель начала выдавать какие-то немного неадекватные значения, они рискуют попасть в целевую переменную на следующем шаге, мы сделаем шаг обучения под неадекватные целевые переменные, модель станет ещё хуже, и так далее, начнётся цепная реакция. Алгоритмы, в которых целевая переменная вот так напрямую зависит от текущей же модели, из-за этого страшно нестабильны.

Для стабилизации применяется трюк, называемый **таргет-сетью** (**target network**). Давайте сделаем так, чтобы у нас задача регрессии менялась не после каждого обновления весов нейросетки, а хотя бы раз, скажем, в 1000 шагов оптимизации. Для этого заведём полную копию нашей нейросети («таргет-сеть»), веса которой будем обозначать $\theta^{-}$. Каждые 1000 шагов будем копировать веса из нашей модели в таргет-сеть $\theta^{-} \leftarrow \theta$, больше никак менять $\theta^{-}$ не будем. Когда мы захотим для очередного перехода $(s, a, r, s')$ построить таргет, мы воспользуемся не нашей свежей моделью, а таргет-сетью:

$$y = r + \gamma \max_{a'} Q^{*}(s', a', \theta^{-})
$$

Тогда правило, по которому строится целевая переменная, будет меняться раз в 1000 шагов, и мы 1000 шагов будем решать одну и ту же задачу регрессии. Такой процесс будет намного стабильнее.

## Experience Replay

Чтобы окончательно собрать алгоритм **Deep Q-learning** (обычно называемый **DQN**, **Deep Q-network**), нам понадобится сделать последний шаг, связанный опять со сбором данных. Коли мы хотим обучать нейросетку, нам нужно для каждого обновления весов откуда-то взять целый мини-батч данных, то есть батч переходов $(s, a, r, s')$, чтобы по нему усреднить оценку градиента. Однако, если мы возьмём среду, сделаем в ней $N$ шагов, то встреченные нами $N$ переходов будут очень похожи друг на друга: они все придут из одной и той же области пространства состояний. Обучение нейросетки на скоррелированных данных — плохая идея, поскольку такая модель быстро забудет, что она учила на прошлых итерациях.

Бороться с этой проблемой можно двумя способами. Первый способ, доступный всегда, когда среда задана при помощи виртуального симулятора — запуск **параллельных агентов**. Запускается параллельно $N$ процессов взаимодействия агента со средой, и для того, чтобы собрать очередной мини-батч переходов для обучения, во всех экземплярах проводится по одному шагу взаимодействия, собирается по одному переходику. Такой мини-батч уже будет разнообразным.

Более интересный второй способ. Давайте после очередного шага взаимодействия со средой мы не будем тут же использовать переход $(s, a, r, s')$ для обновления модели, а запомним этот переход и положим его себе в коллекцию. Память со всеми встретившимися в ходе проб и ошибок переходами $(s, a, r, s')$ называется **реплей буфером** (**replay buffer** или **experience replay**). Теперь для того, чтобы обновить веса нашей сети, мы возьмём и случайно засэмплируем из равномерного распределения желаемое количество переходов из всей истории.

Однако, использование реплей буфера возможно далеко не во всех алгоритмах обучения с подкреплением. Дело в том, что некоторые алгоритмы обучения с подкреплением требуют, чтобы данные для очередного шага обновления весов были сгенерированы именно текущей, самой свежей версией стратегии. Такие алгоритмы относят к классу **on-policy**: они могут улучшать стратегию только по данным из неё же самой («on policy»). Примером on-policy алгоритмов выступают, например, эволюционные алгоритмы. Как они устроены: например, можно завести популяцию стратегий, поиграть каждой со средой, отобрать лучшие и как-то породить новую популяцию (подробнее про одну из самых успешных схем в рамках такой идеи можно посмотреть [здесь](https://openai.com/blog/evolution-strategies/)). Как бы ни была устроена эта схема, эволюционный алгоритм никак не может использовать данные из, например, старых, плохих стратегий, которые вели себя, скажем, не сильно лучше случайной стратегии. Поэтому неизбежно в эволюционном подходе нужно свежую популяцию отправлять в среду и собирать новые данные перед каждым следующим шагом.

И вот важный момент: Deep Q-learning, как и обычный Q-learning, относится к **off-policy** алгоритмам обучения с подкреплением. Совершенно неважно, какая стратегия, умная или не очень, старая или новая, породила переход $(s, a, r, s')$, нам всё равно нужно решать уравнение оптимальности Беллмана в том числе и для этой пары $s, a$ и нам достаточно при построении таргета лишь чтобы $s'$ был сэмплом из функции переходов (а она-то как раз одна вне зависимости от того, какая стратегия взаимодействует в среде). Поэтому обновлять модель $Q^{*}(s,a)$ мы можем по совершенно произвольному опыту, и, значит, мы в том числе можем использовать experience replay.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/off_policy_0f8a26897c_087a80257a.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <p>источник картинки — курс <a href="http://ai.berkeley.edu/lecture_slides.html">UC Berkeley AI</a></p>
  </figcaption>
</figure>

В любом случае, даже в сложных средах, при взаимодействии со средой мы всё равно должны как-то разрешить дилемму exploration-exploitation, и пользоваться, например, $\varepsilon$-жадной стратегией исследования. Итак, алгоритм DQN выглядит так:

1. Проинициализировать нейросеть $Q^{*}(s, a, \theta)$.
2. Проинициализировать таргет-сеть, положив $\theta^{-} = \theta$.
3. Пронаблюдать $s_0$ из среды.
4. Для $k = 0, 1, 2, \dots$:

* с вероятностью $\varepsilon$ выбрать действие $a_k$ случайно, иначе жадно:

$$a_k = \operatorname{argmax}\limits_{a_k} Q^{*}(s_k, a_k, \theta)
$$

* отправить действие $a_k$ в среду, получить награду за шаг $r_k$ и следующее состояние<br>$s_{k+1}$.
* добавить переход $(s_k, a_k, r_k, s_{k+1})$ в реплей буфер.
* если в реплей буфере скопилось достаточное число переходиков, провести шаг обучения. Для этого сэмплируем мини-батч переходиков $(s, a, r, s')$ из буфера.
* для каждого переходика считаем целевую переменную: $y = r + \gamma \max\limits_{a'} Q^{*}(s', a', \theta^{-})$
* сделать шаг градиентного спуска для обновления $\theta$, минимизируя

$$\sum (y - Q^{*}(s, a, \theta))^2
$$

* если $k$ делится на 1000, обновить таргет-сеть: $\theta^{-} \leftarrow \theta$.

Алгоритм DQN не требует никаких handcrafted признаков или специфических настроек под заданную игру. Один и тот же алгоритм, с одними и теми же гиперпараметрами, можно запустить на любой из 57 игр древней консоли Atari ([пример игры в Breakout](https://www.youtube.com/watch?v=TmPfTpjtdgg)) и получить какую-то стратегию. Для сравнения алгоритмов RL между собой результаты обычно усредняют по всем 57 играм Atari. Недавно алгоритм под названием Agent57, объединяющий довольно много модификаций и улучшений DQN и развивающий эту идею, [смог победить человека сразу во всех этих 57 играх](https://deepmind.com/blog/article/Agent57-Outperforming-the-human-Atari-benchmark).

## А если пространство действий непрерывно?

Всюду в DQN мы предполагали, что пространство действий дискретно и маленькое, чтобы мы могли считать жадную стратегию $\pi(s) = \operatorname{argmax}_a Q^{*}(s, a, \theta)$ и считать максимум в формуле целевой переменной $\max_a Q^{*}(s, a, \theta)$. Если пространство действий непрерывно, и на каждом шаге от агента ожидается выбор нескольких вещественных чисел, то как это делать непонятно. Такая ситуация повсюду возникает в робототехнике. Там каждое сочленение робота можно, например, поворачивать вправо / влево, и такие действия проще описывать набором чисел в диапазоне \[-1, 1\], где -1 — крайне левое положение, \+1 — крайне правое, и доступны любые промежуточные варианты. При этом дискретизация действий не вариант из-за экспоненциального взрыва числа вариантов и потери семантики действий. Нам, в общем-то, нужно в DQN только одну проблему решить: как-то научиться аргмаксимум по действиям брать.

А давайте, коли мы не знаем $\operatorname{argmax}_a Q^{*}(s, a)$, приблизим его другой нейросеткой. А то есть, заведём вторую нейросеть $\pi(s, \phi)$ с параметрами $\phi$, и будем учить её так, чтобы

$$\pi(s, \phi) \approx \operatorname{argmax}_a Q^{*}(s, a, \theta).
$$

Как это сделать? Ну, будем на каждой итерации алгоритма брать батч состояний $s$ из нашего реплей буфера и будем учить $\pi(s, \phi)$ выдавать такие действия, на которых наша Q-функция выдаёт большие скалярные значения:

$$\sum_{s} Q^{*}(s, \pi(s, \phi), \theta) \to \max_{\phi}
$$

Причём, поскольку действия непрерывные, всё слева дифференцируемо и мы можем напрямую применять самый обычный backpropagation!

![DDPG](https://yastatic.net/s3/education-portal/media/DDPG_b3d646163c_268137e15a.webp)

Теперь когда на руках есть приближение $\pi(s, \phi) \approx \operatorname{argmax}_a Q^{*}(s, a, \theta)$, можно просто использовать его всюду, где нам нужны аргмаксимумы и максимумы от нашей Q-функции. Мы получили **Actor-Critic** схему: у нас есть *актёр*, $\pi(s, \phi)$ — детерминированная стратегия, и *критик* $Q^{*}(s,a)$, который оценивает выбор действий актёром и предоставляет градиент для его улучшения. Актёр учится выбирать действия, которые больше всего нравятся критику, а критик учится регрессией с целевой переменной

$$y = r + \gamma \max_{a'} Q^{*}(s', a', \theta^{-}) \approx r + \gamma Q^{*}(s', \pi(s', \phi), \theta^{-})
$$

Эта прикольная рабочая эвристика позволяет придумать off-policy алгоритмы для непрерывных пространств действий; к такому подходу относятся такие алгоритмы, как DDPG, TD3 и SAC.

## Policy Gradient алгоритмы

В рассмотренных алгоритмах есть несколько приниципиальных ограничений, которые вытекают непосредственно из самой идеи подхода. Мы учимся с таргетов, заглядывающих всего на один шаг вперёд, использующих только $s'$; это чревато проблемой накапливающейся ошибки, поскольку если между выполнением действия и получением награды \+1 проходит 100 шагов, нам нужно на сто шагов «распространять» полученный сигнал. Мы должны учить $Q^{*}(s,a)$ вместо того, чтобы как-то напрямую («end-to-end») запомнить, какие действия в каких состояниях хорошие. Наконец, наша стратегия всегда детерминирована, когда для взаимодействия со средой во время сбора данных, например, нам позарез нужна была стохастичная, чтобы гарантированно обновлять Q-функцию для всех пар $s, a$, и эту проблему пришлось закрывать костылями.

Есть второй подход model-free алгоритмов RL, называемый **Policy Gradient**, который позволяет избежать вышеперечисленных недостатков за счёт on-policy режима работы. Идея выглядит так: давайте будем искать стратегию в классе стохастичных стратегий, то есть заведём нейросеть, моделирующую $\pi_{\theta}(a \mid s)$ напрямую. Тогда наш функционал, который мы оптимизируем,

$$J(\theta) = \mathbb{E}_{\mathcal{T} \sim \pi_{\theta}} \sum_{t \ge 0} \gamma^t r_t \to \max_{\theta},
$$

дифференцируем по параметрам $\theta$, и градиент равен:

$$\nabla_{\theta} J(\theta) = \mathbb{E}_{\mathcal{T} \sim \pi_{\theta}} \sum_{t \ge 0} \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) \gamma^t R_t,
$$

где $R_t$ - reward-to-go с шага $t$, то есть награда, собранная в сыгранном эпизоде после шага $t$:

$$R_t = \sum_{t' \ge t} \gamma^{t' - t} r_{t'}
$$

{% cut "Скетч доказательства" %}

Давайте распишем мат.ожидание по определению, как следующий интеграл:

$$\nabla_{\theta} J(\theta) = \nabla_{\theta} \mathbb{E}_{\mathcal{T} \sim \pi_{\theta}} \sum_{t \ge 0} \gamma^t r_t = \nabla_{\theta} \int_{\mathcal{T}} p(\mathcal{T} \mid \pi_{\theta}) R_0 d\mathcal{T},
$$

где интеграл берётся по пространству всевозможных траекторий, \$\$p(\\mathcal\{T\} \\mid \\pi_\{\\theta\})\$\$ — вероятность встретить траекторию $\mathcal{T}$ при взаимодействии со средой стратегии $\pi_{\theta}$, а $R_0$ — reward-to-go с шага $t = 0$, то есть суммарная дисконтированная награда за весь эпизод для рассматриваемой траектории $\mathcal{T}$. Награда от параметров $\theta$ не зависит; от выбора стратегии зависят лишь вероятности встретить ту или иную траекторию $\mathcal{T}$. Давайте пронесём градиент внутрь интеграла:

$$\nabla_{\theta} J(\theta) = \int_{\mathcal{T}} \nabla_{\theta} p(\mathcal{T} \mid \pi_{\theta}) R_0 d \mathcal{T},
$$

и теперь этот сложный интеграл перестал быть каким-то мат.ожиданием. Давайте исправим это при помощи трюка, который называется **log-derivative trick**: для этого домножим и поделим внутри на $p(\mathcal{T} \mid \pi_{\theta})$, получим:

$$\nabla_{\theta} J(\theta) = \int_{\mathcal{T}} p(\mathcal{T} \mid \pi_{\theta}) \frac{\nabla_{\theta} p(\mathcal{T} \mid \pi_{\theta})}{p(\mathcal{T} \mid \pi_{\theta})} R_0 d \mathcal{T} = \mathbb{E}_{\mathcal{T} \sim \pi_{\theta}} \frac{\nabla_{\theta} p(\mathcal{T} \mid \pi_{\theta})}{p(\mathcal{T} \mid \pi_{\theta})} R_0
$$

Итак, мы увидели, что градиент нашего функционала — тоже мат.ожидание по всевозможным траекториям. Осталось заметить, что отношение градиента правдоподобия к значению правдоподобия — градиент логарифма правдоподобия:

$$\nabla \log p(x) = \frac{\nabla p(x)}{p(x)}
$$

(откуда и название трюка), чем мы и воспользуемся:

$$\nabla_{\theta} J(\theta) = \mathbb{E}_{\mathcal{T} \sim \pi_{\theta}} \nabla_{\theta} \log p(\mathcal{T} \mid \pi_{\theta}) R_0
$$

Вспоминая, как выглядит trajectory distribution для заданной стратегии $\pi_{\theta}$, замечаем, что функция переходов в среде не вносит вклад в наш градиент:

$$\nabla_{\theta} \log p(\mathcal{T} \mid \pi_{\theta}) = \nabla_{\theta} \log \prod_{t \ge 0} p(s_{t + 1} \mid s_t, a_t) \pi_{\theta}(a_t \mid s_t) = \sum_{t \ge 0} \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t)
$$

Итого мы получили следующую формулу:

$$\nabla_{\theta} J(\theta) = \mathbb{E}_{\mathcal{T} \sim \pi_{\theta}} \sum_{t \ge 0} \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) R_0
$$

Отличие этой формулы от приведённой выше в том, что градиент, отвечающий за выбор действия $a_t$ в состоянии $s_t$, взвешивается не на reward-to-go с момента времени $t$, а на всю награду за эпизод, начиная с нулевого момента времени. Это странно, поскольку получается, что награда, собранная в прошлом, до принятия решения в момент времени $t$, как-то влияет на градиент, соответствующий этому выбору. Мы же понимаем, что решение в момент времени $t$ на прошлую награду никак повлиять не могло, и странно, что градиент для решения в момент времени $t$ взвешивается на прошлую награду. Это наблюдение называется **принципом причинности** (causality principle); можно строго показать, что если заменить $R_0$ на $R_t$, то «удалённые» таким образом слагаемые в среднем по траекториям дают нулевой вклад в градиент, и поэтому такая замена математически корректна.

{% endcut %}

Эта формула говорит нам, что градиент нашего функционала — это тоже мат.ожидание по траекториям. А значит, мы можем попробовать посчитать какую-то оценку этого градиента, заменив мат.ожидание на Монте Карло оценку, и просто начать оптимизировать наш функционал самым обычным стохастическим градиентным спуском! А то есть: берём нашу стратегию $\pi_{\theta}$ с текущими значениями параметров $\theta$, играем эпизод (или несколько) в среде, то есть сэмплируем $\mathcal{T} \sim \pi_{\theta}$, и затем делаем шаг градиентного подъёма:

$$\theta \leftarrow \theta + \alpha \sum_{t \ge 0} \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) \gamma^t R_t
$$

Почему эта идея приводит к on-policy подходу? Для каждого шага градиентного шага нам обязательно нужно взять $\mathcal{T} \sim \pi_{\theta}$ с самыми свежими, с текущими весами $\theta$, и никакая другая траектория, порождённая какой-то другой стратегией, нам не подойдёт. Поэтому для каждой итерации алгоритма нам придётся заново играть очередной эпизод со средой. Это **sample-inefficient**: неэффективно по числу сэмплов, мы собираем слишком много данных и очень неэффективно с ними работаем.

Policy Gradient алгоритмы пытаются по-разному бороться с этой неэффективностью, опять же обращаясь к теории оценочных функций и бутстрапированным оценкам, позволяющим предсказывать будущие награды, не доигрывая эпизоды целиком до конца. Большинство этих алгоритмов остаются в on-policy режиме и применимы в любых пространствах действий. К этим алгоритмам относятся такие алгоритмы, как Advantage Actor-Critic (A2C), Trust-Region Policy Optimization (TRPO) и Proximal Policy Optimization (PPO).

## Что там ещё?

Мы до сих пор разбирали **model-free** алгоритмы RL, которые обходились без знаний о $p(s' \mid s, a)$ и никак не пытались приближать это распределение. Однако, в каких-нибудь пятнашках функция переходов нам известна: мы знаем, в какое состояние перейдёт среда, если мы выберем некоторое действие в таком-то состоянии. Понятно, что эту информацию было бы здорово как-то использовать. Существует обширный класс **model-based**, который либо предполагает, что функция переходов дана, либо мы учим её приближение, используя $s, a, s'$ из нашего опыта в качестве обучающей выборки. Алгоритм AlphaZero на основе этого подхода [превзошёл человека в игру Го](https://www.youtube.com/watch?v=WXuK6gekU1Y&ab_channel=DeepMind), которая считалась куда более сложной игрой, чем шахматы; причём этот алгоритм возможно запустить обучаться на любой игре: [как на Го, так и на шахматах или сёги](https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go).

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/model_based_d4787dd274_83413342fe.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <p>источник картинки — курс <a href="http://ai.berkeley.edu/lecture_slides.html">UC Berkeley AI</a></p>
  </figcaption>
</figure>

Обучение с подкреплением стремится построить алгоритмы, способные обучаться решать любую задачу, представленную в формализме MDP. Как и обычные методы оптимизации, их можно использовать в виде чёрной коробочки из готовых библиотек, например, [OpenAI Stable Baselines](https://stable-baselines.readthedocs.io/en/master/). Внутри таких коробочек будет, однако, довольно много гиперпараметров, которые пока не совсем понятно как настраивать под ту или иную практическую задачу. И хотя успехи Deep RL демонстрируют, что эти алгоритмы способны обучаться невероятно сложным задачам вроде [победы над людьми в Dota 2](https://openai.com/projects/five/) и [в StarCraft II](https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii), они требуют для этого колоссального количества ресурсов. Поиск более эффективных процедур — открытая задача в Deep RL.

В ШАДе есть курс Practical RL, на котором вы погрузитесь глубже в мир глубокого обучения с подкреплением, разберётесь в более продвинутых алгоритмах и попробуете пообучать нейронки решать разные задачки в разных средах.

  ## handbook

  Учебник по машинному обучению

  ## title

  Обучение с подкреплением

  ## description

  *null*

- 
  ## path

  /handbook/ml/article/kraudsorsing

  ## content

  ## Вступление

Для обучения и проверки качества ML-модели необходимы данные, размеченные человеком. Студенты обычно получают эти данные уже в готовом виде, но в работе над реальными продуктами задачи по сбору и разметке приходится решать самостоятельно, учитывая специфику конкретного продукта.

Готовые наборы зачастую однообразны, а иногда и вовсе мешают достичь требуемых результатов: так, модели компьютерного зрения для беспилотного транспорта необходимо обучать на данных, собранных в той же среде, где используется модель. Кроме того, высокие темпы развития нейросетевых технологий провоцируют все большую необходимость в крупных объемах данных: чем лучше текущее качество модели, тем больше новых данных требуется, чтобы поднять это качество на новый уровень. Как следствие, сбор и разметка данных становится неотъемлемой частью почти любого ML-производства, а качество и количество этих данных напрямую влияет на качество конечного продукта.

Краудсорсинг зарекомендовал себя, как один из эффективных способов сбора и разметки данных в больших масштабах. Его используют в разработке новых технологий, чтобы создавать обучающие датасеты для ML-моделей беспилотных автомобилей, голосовых помощников, чат-ботов, поисковых систем и других разработок.

Качественные данные удается собрать благодаря краудсорсинговым платформам: они помогают снизить количество ошибок с помощью специальных настроек, которые можно найти на платформе, а также дают доступ к огромному количеству исполнителей, способных в любое время присоединиться к работе.

Также секрет успеха кроется в той последовательности действий, которые нужно соблюдать, создавая проект на карудсорсинговой платформе. Участникам краудсорсинговых платформ под силу выполнить не все задания, а только простые: сложные задания нужно разбивать на несколько небольших. Качество выполняемой ими работы нужно проверять с помощью доступных на платформе инструментов контроля качества. Полученные результаты в некоторых случаях нужно правильно обрабатывать (здесь полезно разобраться в способах агрегации данных). Чтобы исполнители получили оплату только за правильно выполненные задания, нужно сформулировать подходящую модель ценообразования и т. д.

Нюансов в работе с краудсорсингом достаточно много, поэтому мы подготовили этот параграф в учебник. Стоит отметить, что в ней мы уделим внимание нетехническим аспектам краудсорсинга для ML. Такой уклон связан с тем, что использование краудсорсинга в качестве инструмента работы с данными требует не только знания технических и математических методов (они пригодятся в финальной части, когда полученные данные необходимо будет обработать), но и умения правильно организовать процесс сбора данных, понимания самого феномена краудсорсинга, который сегодня используется в разных сферах для решения разных задач.
По этой причине структура этого параграфа будет выглядеть следующим образом:

<ol>
    <li> В первой части мы сделаем общий обзор краудсорсинга в ML и объясним, для каких задач он применим. </li>
    <li> Во второй части мы остановимся на основных этапах запуска краудсорсингового проекта: от деления проекта на небольшие задачи до обработки полученных от исполнителей данных. </li>
    <li> Кроме того, в этом параграфе мы разберем примеры некоторых ML-задач, которые встречаются в проектах в сфере AI и ML. Это сбор данных для поисковых сетей, разметка изображений для беспилотных автомобилей и сбор аудиозаписей для голосовых помощников. </li>
</ol>

Надеемся, что вам будет интересно погружаться в мир краудсорсинга для ML. Будем рады, если мы поможем вам разложить все по полочкам, чтобы вы смогли дальше наращивать свои знания и изучать отдельные темы более глубоко.

## Что такое краудсорсинг в ML?

Существует довольно много определений краудсорсинга, а также близких к нему по значению терминов (например, «человеческие вычисления», «мудрость толпы» и «коллективный разум»). Это связано с тем, что этот метод используется в разных сферах и применяется для решения разного рода задач, в том числе поиска креативных идей, создания контента, сбора денежных средств. Например, автор термина «краудсорсинг», Джефф Хоу, в 2006 году предложил следующее определение этого метода: Краудсорсинг (от англ. crowd — «толпа», source — «использование ресурсов») — это процесс, в котором компания переносит определенные функции, ранее возлагавшиеся на сотрудников, аутсорсинговые предприятия и поставщиков, на неопределенное, достаточно большое количество людей в формате открытого запроса.

Это определение отражает основную идею краудсорсинга. Однако его недостаточно в контексте рассматриваемой нами темы. Мы говорим о краудсорсинге в машинном обучении. Это значит, что мы передаем облаку исполнителей те задачи, которые связаны со сбором и разметкой данных, а также с оценкой этих данных для разного рода проектов. Разработчики используют эти данные, чтобы обучать машины, а именно модели этих машин, выполнять требуемые задачи. Поэтому в машинном обучении краудсорсинг — это дополнительный вычислительный кластер, который помогает командам создавать и улучшать их продукты.

Один из первых проектов, который задействовал краудсорсинг в получении данных для обучения модели, — проект [Distributed Proofreaders](https://www.pgdp.net/c/) (с англ. — «Распределенные корректоры»). Его главная цель — цифровизация печатных книг с помощью программы для оптического распознавания символов (OCR). Вовлекая тысячи волонтёров, проект Distributed Proofreaders оцифровывает печатные книги и улучшает программу для распознавания текстов. Чем больше данных модель этой программы получает от волонтеров, тем лучше она считывает текст с отсканированных страниц книг. Соответственно, чем лучше становится эта модель, тем меньше времени и усилий человека требуется для того, чтобы находить и исправлять ее ошибки.

Рассмотрим этот проект подробнее:

<ol>
    <li>Добровольцам предлагают сравнить отсканированное изображение страницы и текст этой страницы, распознанный с помощью программного обеспечения для оптического распознавания символов (OCR).</li>
    <li>Поскольку программа оптического распознавания текста не справляется с задачей в полном объеме, в тексте часто появляются ошибки. Задача добровольца — исправить ошибки OCR и загрузить файл обратно на сайт.</li>
    <li>Выполненная работа передается второму добровольцу, он проверяет ее, исправляет ошибки.</li>
    <li>Книга аналогичным образом проходит третий этап корректуры и два этапа форматирования с использованием одного и того же веб-интерфейса.</li>
    <li>После того, как все страницы книги прошли через несколько этапов проверки, постпроцессор собирает их в электронную книгу и отправляет в архив проекта <a href="https://www.gutenberg.org/">«Гутенберг»</a>.</li>
    <li>Отредактированные страницы книг в дальнейшем используются разработчиками, как данные для обучения OCR. Модель программы обучается на данных и в дальнейшем совершает меньше ошибок при распознавании текста на изображениях.</li>
</ol>

Другой пример использования краудсорсинга в ML — сервис [reCaptcha](https://www.google.com/recaptcha/about/). Он был запущен учеными Университета Карнеги-Меллона в 2007 году и стал продолжением проекта Captcha, появившегося в 2000 году. Напомним, что Captcha — это программа, которая защищает сайты от интернет-ботов. Посещая сайт и совершая на нем определенные действия, пользователь получает просьбу заполнить веб-форму. Его задача — вписать в эту форму буквы и цифры, которые он видит на изображении. Люди с хорошим зрением могут легко распознать эти символы, а боты не могут. Так сервис определяет, кто из посетителей сайта человек, а кто — бот. Ботам доступ к сайтам закрывается, так как они наносят вред сайтам.

Создатели проекта Captcha пошли дальше. Они подсчитали, что у каждого человека уходит примерно 10 секунд на ввод одной капчи. А у человечества (10 умножаем на 200 млн) — 500 000 часов. Тогда появилась идея о том, что время, потраченное на ввод капчи, можно использовать с пользой для людей. Это стало началом проекта reCaptcha. Отличие этого проекта от проекта Captcha состоит в том, что вы не только печатаете капчу и подтверждаете, что вы человек, но и одновременно делаете минимальное полезное усилие. В 2007 году таким усилием была оцифровка книг, а с 2012 года reCaptcha стали использовать для распознавания изображений из онлайн-карт. Мы расскажем про инициативу, вошедшую в историю под девизом Stop Spam, Read Books. В чем она заключалась?

<ol>
    <li>Каждая страница книги сканируется.</li>
    <li>Компьютер расшифровывает слова на каждом отсканированном изображении. Для этого используется технология OCR — та, же технология, что и в первом проекте.</li>
    <li>При распознавании текста OCR допускает ошибки. Их особенно много в распознанных текстах старых книг, поскольку в некоторых местах чернила выцвели и страницы пожелтели. Например, в книгах, написанных более 50 лет назад, компьютер не может распознать более 30% слов.</li>
    <li>Все нераспознанные слова направляются людям, чтобы они их распознали, когда вводят капчу в интернете. Задача добровольцев — ввести слова, взятые из отсканированных книг, которые компьютер не смог распознать.</li>
    <li>Добровольцу необходимо распознать два слова из книги. Почему именно два? Одно из слов взято из книги, и оно неизвестно компьютеру. Соответственно, проверить ответ добровольца компьютер не может. Поэтому волонтер получает второе слово — его компьютер знает. Мы не говорим, какое из слов известно компьютеру, и просим добровольца ввести оба. Если доброволец вводит известное слово правильно, система получает подтверждение, что он — человек, а также получает уверенность в правильности ввода другого слова.</li>
    <li>Одно и то же слово, которое неизвестно компьютеру, направляется десяти участникам проекта. Если все они вводят его одинаково, то есть их ответы совпадают, то это слово отправляется в книгу.</li>
    <li>Как и в случае с первым проектом, данные, полученные от добровольцев, используются для обучения технологии OCR.</li>
</ol>

Инициативой проекта reCaptcha впечатлилось множество владельцев сайтов. Новый сервис взамен традиционной Captcha установили такие сайты, как Tiketmaster, Facebook, Twitter и примерно 350 000 других сайтов. Каждый день на этих сайтах вплоть до 2012 года люди оцифровывали примерно 100 млн слов в день. Это 2,5 млн книг в год. В результате, в течение пяти лет с момента его запуска в проекте по оцифровке книг поучаствовали минимум 750 млн людей (это 10% всего населения). Книги, оцифрованные в рамках этого проекта сегодня представлены на сайте [books.google.com](https://books.google.com/).

Подводя итоги вышесказанного, сформулируем определение краудсорсинга в ML. Краудсорсинг в ML — это способ сбора данных, которые необходимы разработчикам, чтобы обучать машины выполнять необходимые действия. С помощью краудсорсинга разработчики вовлекают в процесс выполнения задач обычных людей, которые не владеют определенными навыками и экспертизой. В рамках четко заданных инструкций они выполняют нужное количество заданий. Результаты этих заданий — собранные, размеченные или оцененные данные — входят в те датасеты, которые используются для обучения машин.

## Ключевые принципы краудсорсинга в ML

Применение краудсорсинга в машинном обучении значительно ускорило процесс развития AI продуктов. Беспилотные автомобили, голосовые помощники, поисковые системы, онлайн-карты, машинный перевод появились и развиваются во многом благодаря данным, полученным с помощью краудсорсинга. Например, чтобы поисковая система смогла точно отвечать на вопросы пользователей, нужно проделать большую работу по разметке данных: проанализировать запросы и поведение пользователя, оценить возможные результаты на соответствие запросу, сравнить разные варианты поисковых выдач и выбрать лучший. Все эти данные ложатся в основу моделей, которые учатся искать лучшие ответы, опираясь на размеченные людьми образцы. Такие задачи, на первый взгляд, кажутся трудозатратными и продолжительными по времени. Но если воспользоваться возможностями краудсорсинга и подойти к ним, как к инженерной проблеме, эти сложности будут преодолены.

В этом тезисе содержится основная идея краудсорсинга для AI и машинного обучения: **чтобы решить задачу по разметке данных для обучения или оценки качества модели, нужно подойти к ней как к инженерной проблеме**. Это значит, что нужно организовать выполнение задачи таким образом, чтобы конечный результат зависел от качества самого процесса, а не от добросовестности или экспертности отдельных исполнителей.

Такой подход требует соблюдения ряда правил. Прежде всего, чтобы проект был доступен максимальному количеству исполнителей и не зависел от редких компетенций, его необходимо разделить на сценарии или небольшие задачи. Принцип деления сложной задачи на несколько микрозадач называется **декомпозицией**. Это основополагающий принцип для каждого краудсорсингового проекта, создаваемого для задач машинного обучения.

Каждую микрозадачу необходимо детально продумать. Определить элементы, которые будут ее сопровождать. Некоторые из них (например, инструкции или интерфейсы) обязательно должны присутствовать в проекте. Другие — такие как предварительная фильтрация исполнителей или отслеживание их поведения в проекте — используются в случае необходимости. Все эти элементы решают вопрос качества данных: чем лучше продуман проект, чем эффективнее он «сопровождает» исполнителя во время разметки, тем меньше остается пространства для ошибок или недобросовестного поведения.

Детальную схему проекта, состоящую из цепочки микрозадач и сопровождающих их элементов, называют пайплайном (от англ. pipeline — «линия, очередь»). Его создают на этапе планирования проекта и обращаются к нему как к «дорожной карте».

## ML-задачи, где используется разметка

Краудсорсинг помогает решить разнообразный спектр ML-задач. Разделим их на две основные группы — разметка и сбор данных.

## Разметка данных

К этой группе относится целый ряд задач, в рамках которых пользователю краудсорсинговой платформы необходимо выполнить некоторое действие с уже полученными данными. Например, его могут попросить перевести записи из аудио в текст (транскрипция аудио) или выделить в запросе пользователя в поисковой системе определенные смысловые части, такие как тип продукта, цвет, бренд (NLP-задания). Также в эту группу входят задачи по проверке автоматического перевода, модерации контента, разметке видео или сегментации объектов на изображениях.

В качестве примера рассмотрим задачи по сегментации изображений. Как правило, они нужны для обучения алгоритмов компьютерного зрения. Они используются, например, для создания беспилотного транспорта, который должен распознавать всевозможные препятствия на дорогах: людей, светофоры, разметку, дорожные знаки, дома, заборы, искусственные неровности и т. д. Чтобы эти модели были качественными и могли без труда распознавать любые объекты на своем пути, им нужно показать большое количество изображений и в, более сложных случаях, видео с выделенными на них объектами разных классов.

Выделением этих объектов занимаются пользователи краудсорсинговых платформ. На 2D и 3D изображениях, а также видео, снятых во время движения с помощью камер, радаров и лидаров, они находят нужные объекты и обводят их. Изображения и видео, размеченные по требованиям инструкции, используются для обучения моделей компьютерного зрения.

Самый простой пайплайн задачи по сегментации изображений для беспилотных автомобилей состоит из трех проектов (рис. 1). В первом проекте исполнители отвечают на вопрос, есть ли на фото нужные объекты (например, дорожные знаки). Те изображения, на которых эти объекты есть, перенаправляются в проект номер два. В нем вторая группа исполнителей обводит дорожные знаки с помощью прямоугольников. Эту разметку проверяет еще одна группа исполнителей в следующем проекте, третьем по счету. Далее включается схема так называемой отложенной приёмки заданий. В случае отклонения задание отправляется на повторную разметку. Верно выполненная работа включается в итоговый датасет.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/Picture_1_8977ae8a73_f5dff3ce7f.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    Пайплайн проекта по разметке данных для обучения модели компьютерного зрения
  </figcaption>
</figure>

Подобные пайплайны, но еще более многоступенчатые, используются для обучения моделей компьютерного зрения Яндекса. В январе 2020 года инженерам компании удалось продемонстрировать одну из моделей на конференции Consumer Electronics в Лас-Вегасе. Беспилотные автомобили со встроенной моделью проследовали по маршруту с разными дорожными сценариями: нерегулируемыми перекрестками, сложными поворотами со встречным разъездом, пешеходными переходами и многополосными участками. Всего эти автомобили преодолели более 7 тысяч км.

## Сбор данных

Суть задач, связанных со сбором контента, заключается в поиске материалов (изображений, фотографий, фактов), необходимых для решения проблемы. Например, используя краудсорсинг, инженеры собирают фразы для обучения голосового помощника (рис. 2).

Пайплайн такого проекта выглядит довольно просто: исполнители записывают необходимую фразу, например, «Привет, Алиса», и загружают ее в интерфейс задания на краудсорсинговой платформе. Далее другая группа исполнителей проверяет эти записи на предмет ошибок и других требований: если запись соответствует инструкции, вторая группа подтверждает ее, а если в записи допущены ошибки, отклоняет. В следующем проекте еще одна группа исполнителей записывает недостающие фразы, затем они вновь проходят проверку. Этот процесс повторяется по кругу, пока не будет собрано достаточное количество фраз нужного качества.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/Picture_2_4fc89b9858_f97e437e8c.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    Пайплайн проекта по сбору данных для обучения модели распознавания голоса
  </figcaption>
</figure>

## Краудсорсинговые платформы

Масштабируемость и скорость выполнения задач по разметке данных напрямую зависят от доступа заказчика к большому облаку исполнителей. Залог успеха здесь — использование открытых краудсорсинговых платформ, которые позволяют постоянно пополнять это облако и, следовательно, масштабировать процессы сбора или разметки данных.

Открытые краудсорсинговые платформы — например, Amazon Mechanical Turk или Толока — работают по принципу маркетплейсов. Заказчик может создать на такой платформе свой проект, найти для него нужных исполнителей, обучить их и поручить им выполнение задания, контролируя качество результата. Пользователи открытой платформы, в свою очередь, могут выбрать интересующий их проект, выполнить задания и получить за проделанную работу вознаграждение. Свой выбор проекта они могут сделать как на основе рейтинга проекта, так и с учетом итогового вознаграждения — либо просто потому, что какая-то задача им интересна больше других.

Открытые краудсорсинговые платформы — инструмент для тех, кто планирует самостоятельно контролировать разметку данных. А это, как правило, большинство проектов в сфере AI и машинного обучения. Для ML-разработчиков крайне важно, чтобы кропотливая работа по написанию инструкций, проектированию интерфейсов, отбору и обучению участников, настройке контроля качества была выполнена в точности так, как это запланировано в пайплайне проекта. Все эти шаги напрямую влияют на качество тренировочных данных, а от них в немалой степени зависит успех продукта.

При выборе краудсорсинговой платформы важно учесть и то, какими инструментами они располагают. Например, с готовыми шаблонами можно быстрее спроектировать интерфейс задания, а инструменты контроля качества помогут отсеять роботов и недобросовестных исполнителей. Кроме того, выбор платформы во многом определит то, с какими исполнителями будет вестись работа. Изучение их характеристик даст понимание, в каких странах они проживают, на каких языках разговаривают и, что немаловажно, сталкивались ли они с проектами, подобными тому, над которым планируется работа.

Альтернативой платформам-маркетплейсам могут стать проекты, которые предлагают готовые датасеты и помощь в разметке данных для проекта. Это, например, Scale AI, Hive Data, Alegion. Такие платформы подойдут не всем — выше уже шла речь о том, что некоторые проекты (как, например, обучение алгоритмов компьютерного зрения) нуждаются в специфическом контексте для сбора датасета.

Кроме того, построенные по общим принципам краудсорсинга проекты могут запускаться и на внутреннее облако исполнителей, связанных с компанией какими-либо договорными отношениями. Это важно в случаях, если речь идет о разметке чувствительных данных. Однако такой процесс тяжело поддается масштабированию, потому что требует больших ресурсов для сопровождения сотрудников.

## Границы применимости краудсорсинга

Несмотря на все многообразие задач, которые можно решить с помощью краудсорсинга, есть случаи, когда его применение затруднено либо просто нецелесообразно.

Во-первых, необходимо оценить затраты, сопутствующие запуску проекта. Создание и настройка эффективного пайплайна для сбора или обработки данных требуют времени и квалификации высокоуровневого специалиста. Потраченный им ресурс может не окупиться, если требуется лишь один раз разметить небольшое количество данных.

Облаку исполнителей с трудом поддаются задачи, требующие серьезного включения и поддержания контекста. Секрет краудсорсинга — в создании небольших автономных заданий, каждое из которых может быть решено согласно несложной инструкции. Если исполнителю требуется учитывать большой объем сопутствующей информации, чтобы выполнить задачу верно — скорее всего, ее лучше выполнять без использования краудсорсинга. Например, облако исполнителей вряд ли сможет осуществить перевод книги: ее не стоит разбивать на отдельные предложения, ведь перевод должен быть последовательным и согласованным. В то же время, краудсорсинг может помочь при переводе отдельных фраз в конечном контексте, например, отдельных реплик для голосового ассистента.

Наконец, если задача требует крайне специфических навыков, то поиск или обучение подходящего исполнителя на краудсорсинговой платформе сравнится с наймом эксперта. В таких случаях стоит оценить возможность декомпозиции задачи так, чтобы она оказалась разбита на ряд менее сложных действий. Если сделать это невозможно (например, для выполнения задания требуется знание редкого языка), оптимальным способом поиска исполнителя могут стать профессиональные сообщества.

## Этапы создания краудсорсингового проекта

Типичная краудсорсинговая задача состоит из шести этапов:

<ol> 
    <li>Декомпозиция;</li>
    <li>Инструкция и интерфейс;</li>
    <li>Контроль качества;</li>
    <li>Отбор и обучение исполнителей;</li>
    <li>Выбор схемы оплаты и бонусирования;</li>
    <li>Агрегация ответов.</li>
</ol>

Разберем каждый из этапов на примере уже упомянутого проекта по сбору данных для обучения беспилотных автомобилей. Мы запустим этот проект на краудсорсинговой платформе «Толока».

## Декомпозиция

В качестве исходных данных возьмем объемный набор фотографий с изображением улиц. После запуска краудсорсингового проекта мы должны получить те же изображения, но с выделенными на них дорожными знаками. Наша задача — выделить прямоугольниками дорожные знаки на каждой фотографии.

Пример того, как должен выглядеть итоговый датасет с выделенными на них объектами приведен на рисунке 3.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/Picture_3_949da01146_62c973987e.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    Изображение с выделенными на нем дорожными знаками с помощью полигонов
  </figcaption>
</figure>

Можем ли мы поручить нашу задачу участникам краудсорсинговой платформы напрямую? В данном случае — нет. Изображения для разметки могут полностью не соответствовать нашему запросу. Например, на изображениях может не быть нужных объектов. Некоторые фотографии могут не загрузиться в интерфейсе (появится ошибка). Чтобы избежать подобных ситуаций, нам нужно отобрать фотографии с подходящими объектами. Отбор фото или их фильтрация станет первой микрозадачей или **первым пулом** (так называется набор заданий в рамках проекта на платформе «Толока») нашего проекта.

Что дальше? Когда мы получили фотографии с дорожными знаками, мы сможем запустить проект по выделению объектов на изображениях. Наша задача — выделить на фотографиях все дорожные знаки прямоугольниками. Чтобы создать подобное задание на краудсорсинговой платформе «Толока», можно воспользоваться готовым шаблоном. Он предусматривает специальный инструмент, «полигон», который с легкостью позволяет выполнять подобные задания.

На этом мы могли бы остановиться. Получили изображения с выделенными объектами — задача выполнена. Однако для данного проекта потребуется запустить еще одно микрозадание. Фотографии с выделенными объектами необходимо проверить. Кто-то из исполнителей может пропустить некоторые знаки или выделить их неверно. Таким образом, проверка размеченных изображений в конкретном проекте необходима. Но специфика задачи такова, что мы не можем просто сравнить работу отдельного исполнителя с заведомо верным примером: выделенные области могут отличаться на несколько пикселей, но это не будет означать, что ответ неверен.

Итак, что мы делаем? Мы создаем новый пул заданий, в котором спрашиваем «Верно ли выделены объекты на фото?». Участники отвечают на вопрос, после чего фото с верно отмеченными объектами отправляются в итоговый датасет и оплачиваются. Фото с неверно выделенными объектами отклоняются и не оплачиваются. Все фотографии, которые не проходят проверку, отправляются на переразметку (т. е. размечаются повторно).

Какие выводы мы можем сделать по итогу разбора декомпозиции проекта? Самый главный вывод — решение о декомпозиции задачи следует принимать, исходя из типа задачи и данных, которые есть на входе — это могут быть изображения, видео, ссылки, точки на карте, координаты этих точек. Также следует различать типичные случаи, в которых декомпозиция особенно рекомендована для проекта. Речь идет об объемных проектах, многослойных задачах, задачах со множеством вариантов ответов и объемных процессах:

<ul>
    <li><i>Объемные проекты</i>. Если в рамках проекта нужно ответить на несколько вопросов, то лучше сделать это поочередно или в выбранной последовательности.</li>
    <li><i>Многослойные задачи</i>. Если в рамках одной задачи нужно выполнить более одного действия (например, отнести объект к определенной группе и ответить на вопрос, предназначен ли он только для взрослых), то лучше сделать это поочередно или в выбранной последовательности.</li>
    <li><i>Задачи со множеством вариантов ответов</i>. Если в задании есть один вопрос и 10 и более вариантов ответа, то лучшим решением будет группировка ответов по темам, а затем создание отдельного проекта для каждой группы ответов.</li>
    <li><i>Объемные процессы</i>. Если задача включает сложные механизмы контроля качества и отложенную проверку, необходимо создать отдельный проект, в котором одна группа исполнителей будет проверять другую.</li>
</ul>

Есть ли случаи, когда декомпозировать задачу не нужно? Да. Нет необходимости разбивать задачу на части, если соблюдаются два критерия: инструкции к задаче помещаются на половине листа бумаги формата А4, или задача выполнена с помощью одного действия, например, выбора из нескольких категорий.

## Инструкция

После декомпозиции нашего проекта нам необходимо создать для него инструкцию. Инструкция потребуется для каждой микрозадачи. В нашем случае нам необходимо создать три инструкции.

Какие пункты мы обязательно в них укажем?

Первым пунктом инструкции станет описание задачи. В нем мы объясним участнику, что предстоит сделать и где будет использован результат этой работы. Например:

<p style="padding: 0 50px;"><i>
    Вашему вниманию представлен проект, результаты которого помогут сделать беспилотные автомобили безопасным транспортом. Ваша задача — определить, есть ли дорожные знаки на изображении. Выберите ответ «Да», если изображение содержит дорожные знаки. Выберите ответ «Нет», если на изображении дорожных знаков нет. На изображении, представленном ниже, есть несколько дорожных знаков. Значит, правильный ответ — «Да». </i></p>

![Picture](https://yastatic.net/s3/education-portal/media/Picture_3_949da01146_62c973987e.webp)

Далее, мы подробно опишем условия входа в задание: расскажем, будет ли обучение и экзамен, с каким качеством его нужно пройти, есть ли в проекте повторный экзамен для тех, кто не прошел испытание с первого раза. Также опишем ценообразование. Например:

<p style="padding: 0 50px;"><i>
    Чтобы выполнить это задание, вам потребуется пройти обучение на тренировочном пуле. В тренировочный пул войдут задания аналогичные тем, что будут в основном проекте. После обучения мы предложим вам пройти экзамен. В экзамен войдут 5 изображений.</i></p>

Следующий элемент инструкции — технические нюансы. Здесь мы расскажем, с какого устройства потребуется выполнить задание — со смартфона или с компьютера — и какие дополнительные настройки браузера будут необходимы. Этот пункт в особенности важен для второго задания в рамках нашего проекта. Разметить дорожные знаки прямоугольниками участники смогут только с компьютера:

<p style="padding: 0 50px;"><i>
    Мы рекомендуем выполнять это задание с персонального компьютера. Это необходимо, чтобы вы смогли корректно выделить все необходимые объекты на изображении.</i></p>

Краткое описание интерфейса задания — еще один важный пункт в инструкции. Для большей наглядности мы сделаем скриншот с комментариями о том, для чего нужны те или иные блоки и кнопки. Если в задании простой интерфейс, эту часть можно пропустить. Например:

<p style="padding: 0 50px;"><i>
    Используйте желтый квадрат («полигон») в левой части экрана, чтобы выделять дорожные знаки на изображении.</i></p>

Теперь о самом задании. Чтобы избежать ошибок, мы пошагово опишем все частые сценарии, которые могут случиться при выполнении наших задач. Также мы укажем, что делать с нестандартными случаями. Добавим примеры: несколько кейсов сделают теорию намного понятнее. Справочные материалы — глоссарий, faq — важное дополнение к этим сценариям. Наконец, мы расскажем, куда направлять вопросы по заданию или проекту в целом.

На что мы обратим внимание при написании текста?

Первое, за чем стоит проследить — сам язык, которым написана инструкция. Мы откажемся от профессионального сленга и не будем использовать терминологию. Некоторые термины, например, «полигоны», мы объясним или заменим синонимами — «прямоугольники». Наша задача — сделать инструкцию простой и понятной для большого числа участников. Следуя этой же задаче, мы упростим стиль и синтаксис (одна мысль = одно предложение; одна тема = один абзац), не будем использовать пояснения в скобках и сделаем форматирование единообразным.

Готовый текст инструкции мы обязательно проверим, выполнив некоторое количество заданий. Такое упражнение быстро покажет, какие случаи еще не описаны в инструкции, а какие описаны мало. Кроме того, оно позволит проверить как выглядит наше задание на разных устройствах: умещаются ли все картинки и скриншоты на экранах мобильного телефона, планшета и компьютера.

В итоге каждая инструкция не займет больше двух экранов. Это максимальное количество пространства для инструкции, за пределы которого лучше не выходить. Если инструкция все же не вписывается в такой объем, вероятно, задача слишком многосоставная и ее нужно декомпозировать.

## Агрегация результатов

Представим, что мы запустили наш проект и получили необходимые данные. В краудсорсинговых проектах данные обычно собираются в перекрытии (мнения большинства) — это один из распространенных механизмов контроля качества исполнителей и улучшения качества итогового набора данных. Но как выбрать из нескольких оценок финальную?

В данном случае нам помогут механизмы агрегации данных. Что они делают? Они обрабатывают файлы с ответами исполнителей и выбирают из нескольких ответов тот, который с наибольшей вероятностью окажется верным. Рассмотрим принцип работы механизмов агрегации данных на примере первого пула с заданиями (см. рис. 4).

У нас есть набор изображений, и наша цель — отнести каждое изображение к группе «изображения с дорожными знаками» или к группе «изображения без дорожных знаков». В соответствии с принципом краудсорсинга задание должно быть распределено между несколькими исполнителями, каждый из которых размечает некое подмножество изображений. В результате для каждого изображения у нас есть несколько результатов разметки. Цель метода агрегации — объединить эти результаты в один качественный ответ.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/Picture_9_988c50db8c_57b0a666f3.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    Агрегация данных, полученных с помощью краудсорсинга
  </figcaption>
</figure>

### Мнение большинства

Алгоритм агрегации данных «Мнение большинства» основан на предположении, что правильный ответ — этот тот, который выбирают большинство исполнителей (рис. 5). Самый популярный ответ становится финальным ответом.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/Picture_10_b7ecea26b9_72e8ca5e09.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    Агрегация данных по методу, основанному на мнении большинства
  </figcaption>
</figure>

Практика показывает, что при помощи метода, основанного на мнении большинства, можно получить достойные результаты. Поэтому этот метод с успехом применяется во многих проектах. Также одно из преимуществ этого метода заключается в том, что он весьма нагляден и логика его работы понятна. Однако в проектах краудсорсинга существуют определенные временные и бюджетные ограничения. Наша цель в том, чтобы собрать минимальный объем данных, необходимый для достижения желаемой точности. С этой точки зрения, метод, основанный на мнении большинства, далеко не всегда будет оптимальным выбором. Чтобы осознать слабые стороны метода, рассмотрим его модель.

#### Модель

Модель, лежащая в основе метода, проста. Есть N изображений и M исполнителей. Каждое изображение $j \in {1, ..., N}$ подразумевает некий неизвестный ответ («изображения с дорожными знаками» или «изображения без дорожных знаков» в нашем случае). При использовании модели, основанной на мнении большинства, предполагается, что если исполнитель $i$ разметил изображение $j$, его ответ является правильным с некоторой вероятностью $p > \frac{1}{2}$

$$P(Исполнитель\,i\,отвечает\,на\,вопрос\,j\,верно) = p
$$

При этом вероятность правильного ответа полагается одинаковой для каждого исполнителя и вопроса. Допущение, что $p > 1/2$ учитывает, что для каждого исполнителя вероятность правильного ответа выше, чем неправильного. В таком случае, поскольку число разметок для каждого изображения достаточно велико, мнение большинства с высокой вероятностью даст истинные ответы.

#### Ограничения

В силу своей простоты, метод основанный на мнении большинства имеет ряд ограничений:

<ul>
    <li>**Однородность исполнителей.**Во-первых, данный метод предполагает, что все исполнители обладают одинаковыми способностями. Иными словами,<i>для каждого конкретного вопроса вероятность того, что исполнитель правильно ответит на вопрос, одинакова для всех исполнителей.</i> Однако на практике пул исполнителей на краудсорсинговых платформах чрезвычайно разнообразен: кто-то из них очень аккуратно и скрупулезно выполняет задачи, а кто-то небрежен и чаще допускает ошибки. Таким образом, одно из направлений совершенствования модели, основанной на мнении большинства, — это учет различия в способностях исполнителей в рамках модели.</li>
    <li>**Однородность вопросов.**Во-вторых, модель, основанная на мнении большинства, предполагает, что вопросы имеют одинаковую сложность. Другими словами, <i>вероятность того, что исполнитель правильно ответит на вопрос, одинакова для всех вопросов.</i> Однако некоторые вопросы в рамках проекта могут быть сложнее других. Таким образом, еще одно направление по улучшению модели на основании мнения большинства — это учесть в модели разную степень сложности вопросов.</li>
</ul>

Далее мы рассмотрим оба направления развития модели и расскажем о других алгоритмах, учитывающих особенности краудсорсинговых заданий.

### Агрегация с учетом способностей исполнителей

Рассмотрим модель, которая учитывает неоднородность исполнителей при агрегации ответов.

#### Модель

Естественный способ учесть различия в способностях исполнителей — ввести параметр качества для каждого исполнителя. Если есть $M$ исполнителей, то мы можем связать каждого исполнителя $i \in {1, ..., M}$ с неизвестным параметром качества $p_i \in [0, 1]$. Чем выше параметр качества исполнителя, тем больше вероятность того, что исполнитель ответит на вопрос правильно:

$$P(Исполнитель\,i\,отвечает\,на\,вопрос\,j\,верно) = p_i
$$

Другими словами, вероятность того, что исполнитель правильно ответит на вопрос, своя для каждого исполнителя (но от вопроса она все еще не зависит).

#### Методы

В ситуации, когда у исполнителей разные способности, логично присваивать больший вес ответам более сильных исполнителей и меньший вес — ответам более слабых. Однако проблема в том, что параметры качества для исполнителей априори нам не известны. Основная идея двух методов модели агрегации данных с учетом способностей исполнителей заключается том, чтобы одновременно оценить параметры качества для исполнителей и ответы на поставленные вопросы. Рассмотрим каждый их них.

#### Использование большого объема контрольных заданий

Контрольные вопросы (также <i>honeypots</i>, <i>golden sets</i>) — это задания, на которые заказчик заранее знает правильные ответы. На практике мы часто добавляем в набор данных определенное количество контрольных вопросов, чтобы контролировать качество работы исполнителей. Когда этих вопросов достаточно много, мы можем использовать их для оценки качества работы. Предположим, что у нас есть $G$ контрольных вопросов и некий исполнитель $i$, который правильно ответил на $k_i$ вопросов из $G$ контрольных вопросов. Тогда мы можем оценить параметр качества для исполнителя следующим образом:

$$\hat{p_i} = \frac{k_i}{G}
$$

Теперь, когда у нас есть оценка параметра качества, мы можем оценить ответ каждого исполнителя по-разному. Эта идея подводит нас к концепции взвешенного мнения большинства (от англ. <i>Weighted majority vote</i>).

Идея этого метода проиллюстрирована на рисунке ниже (рис. 6). Предположим, что у нас есть нестандартное изображение, на котором столб похож на дорожный знак. В этом случае модель, основанная на простом мнении большинства, не делает отличия между ответами исполнителей с меньшими способностями (первых двух исполнителей) и ответами исполнителя-эксперта (последнего исполнителя) и допускает ошибку. Напротив, модель взвешенного мнения большинства дополнительно взвешивает каждый ответ полученным коэффициентом качества исполнителя. Такая модель приводит к правильному ответу, поскольку мнение исполнителя-эксперта в таком случае перевешивает мнения двух других исполнителей.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/Picture_11_9e711249e4_63a4e528be.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    Агрегация данных по методу, основанному на взвешенном мнении большинства
  </figcaption>
</figure>

### Когда контрольных вопросов не так много

Метод взвешенного мнения большинства подходит для тех случаев, когда в проекте есть достаточное количество контрольных заданий, необходимых для оценки качества работы исполнителя. Однако зачастую контрольных заданий в проекте не хватает, в связи с чем оценки могут быть довольно неточными. Кроме того, исполнители могут коллективно выявить контрольные вопросы и начать обманывать систему, давая правильные ответы на контрольные вопросы и случайные ответы на другие. В этом случае, чтобы оценить параметры качества исполнителей при ответе на неизвестные вопросы, мы можем использовать метод Дэвида — Скина:

### Метод Дэвида-Скина (Dawid, Skene, 1979)

**Метод Дэвида-Скина** одновременно находит значения качества исполнителей и ответы на вопросы, которые согласуются с наблюдаемыми данными в наибольшей степени.
Мы имеем в качестве данных $n_{ik}^u$ — количество раз, при которых разметчик $u \in U$ поставил класс $k \in K$ объекту $i \in I$ (возможно, разметчик видел этот объект несколько раз). Обозначим через

$$Y_{ik} = I\{\text{объект } i \text{ класса } k\},
$$

это наши латентные величины.

В качестве параметров имеем

* $\pi_{k\ell}^u$ — вероятность того, что разметчик $u$ поставил класс $\ell$ вместо правильного класса $k$.
* $\rho_k$ — вероятность класса $k$.

Примем также обозначения:

* $N_i = \{n^u_{ik}\text{ по всем } u \text{ и } k \text{ для объекта } i \}$,
* $N_i ^ u = \{n^u_{ik} \text{ для разметчика } u \text{ и объекта i}\}$,
* $Y_i = \{Y_{ik} \text{ по всем } k \text{ для объекта } i\}$.

Поймём, какой будет функция неполного правдоподобия в этой задаче. Прежде всего,

$$p_{\pi,p}(N, Y) = \prod_{i\in I}p(N_i, Y_i),
$$

Если $k$ – номер класса $i$-го объекта, то

$$p(N_i, Y_i)=\underbrace{p(\text{объект $i$ класса $k$})}_{=\rho_k}p(N_i\mid\text{объект $i$ класса $k$})
$$

(значения $Y_{it}$ однозначно определяются номером истинного класса, поэтому справа $Y_i$ пропадает). Далее, мы считаем, что разметчики действуют независимо, поэтому

$$p(N_i\mid\text{объект $i$ класса $k$}) = \prod_{u\in U}p(N_i^u\mid\text{объект $i$ класса $k$}).
$$

Разберёмся с величиной \$p(N_i\^u\\mid\\text\{объект $i$ класса $k$\})\$. Она отвечает за то, какие классы $u$-й разметчик ставил $i$-му объекту. Мы считаем, что встречи разметчика с объектом упорядочены по времени, тогда

$$p(\text{$u$-й разметчик отнёс $i$-й объект к классам $k'_1,\ldots,k'_r$}\mid\text{объект $i$ класса $k$}) =
$$

$$\begin{matrix}=\prod_{s}p(\text{в $s$-ю встречу с $i$-м объектом $u$-й разметчик отнёс его к классу $k'_s$}\mid \\ \text{объект $i$ класса $k$})\end{matrix}
$$

Эту вероятность можно переписать в виде

$$\prod_{\ell \in K} \left( \pi_{k\ell}^u \right)^{n_{i\ell}^u},
$$

а итоговое неполное правдоподобие предстаёт в виде

$$p_{\pi,p}(N, Y) = \prod_{i\in I}\prod_{k \in K} \left( \rho_k \prod_{u\in U} \prod_{\ell \in K} \left( \pi_{k\ell}^u \right)^{n_{i\ell}^u} \right)^{Y_{ik}}
$$

Его нам нужно максимизировать по $\pi$ и $\rho$

**Пояснение к формуле:**

Вне больших скобок фиксируются объект и его класс, сама скобка возводится в степень 1, если рассматривается правильный класс объекта, и в степень 0 иначе. Внутри сначала записана вероятность того, что объект имеет данный класс, а затем — перебор по всем пользователям и всем классам, которые мог поставить данный пользователь. Наконец, записывается вероятность того, что пользователь нашему объекту поставил некоторый класс, которая возводится в степень того, сколько раз он поставил этот класс. Например, если пользователь видел изображение котика 5 раз, при этом 3 раза он сказал, что котик, а два раза — песик, то вероятность $\pi_{cat,cat}^u$ для данного котика учтется 3 раза, а вероятность $\pi_{cat,dog}^u$ — 2 раза.

Рассмотрим концепцию метода Дэвида-Скина на простом примере (рис. 7).
Предположим, что у нас есть только $N = 4$ вопросов и $M = 3$ исполнителей. Каждый исполнитель отвечает на все вопросы. В этом случае наблюдаемые данные — это ответы исполнителей на вопросы.

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/Picture_12_47efa2f8ab_77489f4daf.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    Агрегация данных по методу Дэвида-Скина
  </figcaption>
</figure>

Давайте разберемся в том, каким образом метод Дэвида — Скина позволяет найти параметры качества для исполнителей и те ответы на вопросы, которые лучше всего соответствуют наблюдаемым данным. Для этого рассмотрим два варианта, показанные на картинках ниже (см. рис. 7.1). Каждая картинка предполагает свой набор параметров. Посмотрим, какой из предложенных вариантов лучше соответствует наблюдаемым данным.

![Picture](https://yastatic.net/s3/education-portal/media/Picture_13_bdb5e67f0e_54cc7bdb1b.webp)

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/Picture_14_6506818225_b4c85e3412.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    Агрегация данных по методу Дэвида-Скина
  </figcaption>
</figure>

Во-первых, обратите внимание, что на обоих изображениях предложенные ответы согласуются с ответами исполнителя, у которого, по оценкам, высокий параметр качества. Но какой выбор параметров подходит данным лучше всего? Чтобы ответить на этот вопрос, обратите внимание, что ответы второго и третьего исполнителей полностью совпадают. Если параметры качества для этих исполнителей соответствуют первой картинке $\hat{p_2} = \hat{p_3} = 0.5$, тогда, если верить этой модели, эти два исполнителя отвечают наугад. В таком случае высокая степень согласия между исполнителями нас бы скорее удивила, поскольку отвечая наугад, они должны время от времени расходиться в своих ответах. Напротив, если исполнители 2 и 3 — эксперты, как на втором изображении $\hat{p_2} = \hat{p_3} = 1$, тогда мы ожидаем, что у них будет высокая степень согласия, и это то, что мы видим в данных. Интуитивно, второй набор параметров лучше согласуется с наблюдаемыми данными. Приведенный простой пример показывает, что концепция согласованности между потенциальными параметрами и наблюдаемыми данными позволяет нам исключить те варианты, которые плохо согласуются с наблюдаемыми данными.

Оба метода — взвешенное мнение большинства и агрегация по методу Дэвида — Скина — входят в стандартный функционал Толоки. В двух наших пулах, в первом и третьем, мы будем использовать метод Дэвида — Скина. Он позволит нам получить наиболее точные данные для нашего проекта. Подробнее узнать о том, как получить агрегированные результаты из размеченного пула, можно в [документации](https://yandex.ru/support/toloka-requester/concepts/result-aggregation.html?lang=ru).

### Агрегация с учетом сложности вопросов

Метод Дэвида-Скина и метод, основанный на мнении взвешенного большинства, — основа современного краудсорсинга. Многие создатели проектов повышают качество данных, используя эти методы агрегации. Однако существуют и другие современные подходы. Например, есть группа подходов, которые учитывают сложность вопроса при агрегировании ответов.

### Параметрический подход

Аналогично тому, как мы замеряли качество для каждого исполнителя, вводя параметр качества $p_i$, точно так же для каждого исполнителя мы можем ввести параметр сложности $d_j$ для каждого вопроса. Тем не менее, главная проблема заключается в том, как описать взаимодействие между качеством исполнителя и сложностью вопроса, и в результате рассчитать вероятность того, что конкретный исполнитель правильно ответит на выбранный вопрос. В работе Уайтхилла с соавторами (2009) предлагается следующее решение.

1. Во-первых, параметр качества для исполнителя, который раньше мерился в диапазоне $[0, 1]$, теперь задается в интервале $(-\infty, \infty)$. В частности, возможно нулевое качество $p = 0$, которое соответствует ситуации, когда исполнитель отвечает на все вопросы наугад. Положительные значения качества подразумевают, что работник с большей вероятностью даст правильный ответ, а отрицательные значения означают, что исполнитель настроен враждебно и с большей вероятностью даст неправильный ответ.
2. Во-вторых, для параметра сложности каждого вопроса $d \in (0, \infty)$ также может быть дана интуитивная интерпретация: низкая сложность вопроса $(d \approx 0)$ означает что вопрос настолько прост, что любой исполнитель ответит на него правильно с вероятностью, близкой к 1. Чем выше уровень сложности, тем меньше вероятность того, что конкретный исполнитель ответит на вопрос правильно.

Объединив эти параметры, модель предполагает, что вероятность для конкретного исполнителя $i$ при ответе на конкретный вопрос $j$ может быть корректно описана следующим параметрическим выражением:

$$P(Исполнитель\,i\,отвечает\,на\,вопрос\,j\,верно) = \frac{1}{\exp{\left(-\frac{p_i}{d_j}\right)}}
$$

Следует заметить, что в таком случае вероятность является функцией и самого исполнителя, и вопроса, на который исполнитель отвечает. Как только мы выбрали параметрическое уравнение для описания взаимосвязи между уровнем качества исполнителя и сложностью вопроса, с одной стороны, и вероятностью правильного ответа, с другой, мы можем применять все те же принципы, что и для расчета параметров по модели Дэвида – Скина. Таким образом мы можем оценить не только параметры модели, но и полученные ответы на вопросы. Более подробно об этом можно почитать в [статье](https://arxiv.org/pdf/1602.03481.pdf).

Несмотря на то, что параметрические модели позволяют делать весьма эффективные выводы, в них неизбежно заложены сильные допущения о когнитивных процессах, присущих исполнителям при ответе на вопросы. Эти допущения обычно невозможно проверить, поэтому неясно, насколько хорошо они согласуются с реальностью. Соответственно, если допущения параметрической модели неверны, то и методы, используемые такой моделью, могут дать неожиданные результаты. Это подводит нас к идее непараметрического подхода, где можно попробовать избежать сильных допущений о мыслительных процессах.

### Непараметрический подход

Непараметрический подход предложил Нихар Б. Шах с коллегами в 2016 году. Вместо моделирования вероятностей, что исполнитель $i$ верно ответит на вопрос $j$, считается, что между этими вероятностями есть взаимосвязь. При этом модель использует два ключевых допущения:

1. Во-первых, предполагается, что исполнителей можно выстроить в ряд в порядке возрастания способностей. Если исполнитель $i_1$ занимает в этом ряду более высокую позицию, чем исполнитель $i_2$, то при ответе на каждый вопрос исполнитель $i_1$ с большей вероятностью даст правильный ответ, чем исполнитель $i_2$.
2. Во-вторых, предполагается, что вопросы можно выстроить в&nbsp;ряд в&nbsp;зависимости от&nbsp;их&nbsp;сложности. Если вопрос $j_1$ сложнее вопроса $j_2$, то&nbsp;любой исполнитель совершит ошибку при ответе на&nbsp;вопрос $j_1$ с&nbsp;не&nbsp;меньшей вероятностью, что и&nbsp;отвечая на&nbsp;вопрос $j_2$.

Стоит заметить, что эти допущения гораздо слабее, чем в параметрической модели. В самом деле, параметрическая модель не только предполагает существование таких упорядоченных рядов, но и задает все вероятности. С другой стороны, непараметрический подход делает всего лишь естественное предположение о существовании последовательных рядов, но не ограничивает набор когнитивных механизмов, характерных для исполнителей. Было показано, что в некоторых случаях непараметрическая модель позволяет лучше делать выводы. Более подробно об этом можно почитать в [полном тексте статьи](https://arxiv.org/pdf/1606.09632.pdf).

Как мы уже говорили, эти подходы еще достаточно новые и не успели стать классикой краудсорсинга. Если сложность вопросов в вашем проекте существенно варьируется, мы рекомендуем более основательно изучить упомянутые методы и лежащие в их основе допущения, а затем опробовать их на практике.

**Использованная литература**

<ol>
    <li>Jeff Howe, <a href='https://www.wired.com/2006/06/crowds/?pg=1&topic=crowds&topic_set='>The Rise of Crowdsourcing</a>, The Wired, 2006.</li>
    <li>Джефф Хау, Краудсорсинг: Коллективный разум как инструмент развития бизнеса, Альпина Паблишер, 2012.</li>
    <li>Omar Alonso, The Practice of Crowdsourcing, 2019.</li>
    <li><a href='https://theoryandpractice.ru/posts/5001-camaya-bogataya-chast-planety-rabotaet-besplatno-vo-vremya-pereryvov-na-kofe-redaktor-wired-dzheff-khau-o-kraudsorsinge'>«Cамая богатая часть планеты работает бесплатно во время перерывов на кофе»: редактор Wired Джефф Хау о краудсорсинге</a>, T&P, 2012.</li>
    <li>Р. А. Долженко, А. В. Бакаленко, Краудсорсинг как инструмент мобилизации интеллектуальных ресурсов: опыт использования в Сбербанке России, Российский журнал менеджмента, Том 14, №3, 2016, С. 77–102.</li>
    <li><a href='https://yandex.ru/company/services_news/2020/2020-01-14-1'>Беспилотные автомобили Яндекса на CES 2020: 7 тысяч км без водителя за рулём по улицам Лас-Вегаса</a>, Новости Яндекса, 2020.</li>
    <li><a href='https://www.jstor.org/stable/2346806?seq=1'>Метод Дэвида и Скина</a></li> 
</ol>


  ## handbook

  Учебник по машинному обучению

  ## title

  Краудсорсинг

  ## description

  *null*

- 
  ## path

  /handbook/ml/article/bias-variance-decomposition

  ## content

  В данном параграфе мы изучим инструмент, который позволяет анализировать ошибку алгоритма в зависимости от некоторого набора факторов, влияющих на итоговое качество его работы. Этот инструмент в литературе называется **bias-variance decomposition** — разложение ошибки на смещение и разброс. В разложении, на самом деле, есть и третья компонента — случайный шум в данных, но ему не посчастливилось оказаться в названии. Данное разложение оказывается полезным в некоторых теоретических исследованиях работы моделей машинного обучения, в частности, при анализе свойств ансамблевых моделей.

Некоторые картинки в тексте кликабельны. Это означает, что они были заимствованы из какого-то источника и при клике вы сможете перейти к этому источнику.

## Вывод разложения bias-variance для MSE

Рассмотрим задачу регрессии с квадратичной функцией потерь. Представим также для простоты, что целевая переменная $y$ — одномерная и выражается через переменную $x$ как:

$$
    y = f(x) + \varepsilon,
$$

где $f$ — некоторая детерминированная функция, а $\varepsilon$ — случайный шум со следующими свойствами:

$$
     \mathbb{E} \varepsilon = 0, \, \mathbb{V}\text{ar} \varepsilon = \mathbb{E} \varepsilon^2 = \sigma^2.
$$

В зависимости от природы данных, которые описывает эта зависимость, её представление в виде точной $f(x)$ и случайной $\varepsilon$ может быть продиктовано тем, что: 

1) данные на самом деле имеют случайный характер;

2) измерительный прибор не может зафиксировать целевую переменную абсолютно точно;

3) имеющихся признаков недостаточно, чтобы исчерпывающим образом описать объект, пользователя или событие.

Функция потерь на одном объекте $x$ равна

$$
    MSE = (y(x) - a(x))^2
$$

Однако знание значения MSE только на одном объекте не может дать нам общего понимания того, насколько хорошо работает наш алгоритм. Какие факторы мы бы хотели учесть при оценке качества алгоритма? Например, то, что выход алгоритма на объекте $x$ зависит не только от самого этого объекта, но и от выборки $X$, на которой алгоритм обучался:

$$
    X = ((x_1, y_1), \ldots, (x_\ell, y_\ell))
$$

$$
    a(x) = a(x, X)
$$

Кроме того, значение $y$ на объекте $x$ зависит не только от $x$, но и от реализации шума в этой точке:

$$
    y(x) = y(x, \varepsilon)
$$

Наконец, измерять качество мы бы хотели на тестовых объектах $x$ — тех, которые не встречались в обучающей выборке, а тестовых объектов у нас в большинстве случаев более одного. При включении всех вышеперечисленных источников случайности в рассмотрение логичной оценкой качества алгоритма $a$ кажется следующая величина:

$$
    Q(a) = \mathbb{E}_x \mathbb{E}_{X, \varepsilon} [y(x, \varepsilon) - a(x, X)]^2
$$

Внутреннее матожидание позволяет оценить качество работы алгоритма в одной тестовой точке $x$ в зависимости от всевозможных реализаций $X$ и $\varepsilon$, а внешнее матожидание усредняет это качество по всем тестовым точкам. 

**Замечание.** Запись $\mathbb{E}_{X, \varepsilon}$ в общем случае обозначает взятие матожидания по совместному распределению $X$ и $\varepsilon$. Однако, поскольку $X$ и $\varepsilon$ независимы, она равносильна последовательному взятию матожиданий по каждой из переменных: $\mathbb{E}_{X, \varepsilon} = \mathbb{E}_{X} \mathbb{E}_{\varepsilon}$, но последний вариант выглядит несколько более громоздко.

Попробуем представить выражение для $Q(a)$ в более удобном для анализа виде. Начнём с внутреннего матожидания:

$$
    \mathbb{E}_{X, \varepsilon} [y(x, \varepsilon) - a(x, X)]^2 = \mathbb{E}_{X, \varepsilon}[f(x) + \varepsilon - a(x, X)]^2 = 
$$

$$
    = \mathbb{E}_{X, \varepsilon} [ \underbrace{(f(x) - a(x, X))^2}_{\text{не зависит от $\varepsilon$}} + 
       \underbrace{2 \varepsilon \cdot (f(x) - a(x, X))}_{\text{множители независимы}} + \varepsilon^2 ] = 
$$

$$
    = \mathbb{E}_X \left[
        (f(x) - a(x, X))^2
    \right] + 2 \underbrace{\mathbb{E}_\varepsilon[\varepsilon]}_{=0} \cdot \mathbb{E}_X (f(x) - a(x, X)) + \mathbb{E}_\varepsilon \varepsilon^2 =
$$

$$
    = \mathbb{E}_X \left[ (f(x) - a(x, X))^2 \right] + \sigma^2
$$

Из общего выражения для $Q(a)$ выделилась шумовая компонента $\sigma^2$. Продолжим преобразования:

$$
    \mathbb{E}_X \left[ (f(x) - a(x, X))^2 \right] = \mathbb{E}_X \left[
        (f(x) - \mathbb{E}_X[a(x, X)] + \mathbb{E}_X[a(x, X)] - a(x, X))^2 
    \right] = 
$$

$$
    = \mathbb{E}_X\underbrace{\left[ 
        (f(x) - \mathbb{E}_X[a(x, X)])^2 
    \right]}_{\text{не зависит от $X$}} + \underbrace{\mathbb{E}_X \left[ (a(x, X) - \mathbb{E}_X[a(x, X)])^2 \right]}_{\text{$=\mathbb{V}\text{ar}_X[a(x, X)]$}} + 
$$

$$
    + 2 \mathbb{E}_X[\underbrace{(f(x) - \mathbb{E}_X[a(x, X)])}_{\text{не зависит от $X$}} \cdot (\mathbb{E}_X[a(x, X)] - a(x, X))] = 
$$

$$
    = (\underbrace{f(x) - \mathbb{E}_X[a(x, X)]}_{\text{bias}_X a(x, X)})^2 + \mathbb{V}\text{ar}_X[a(x, X)] + 2 (f(x) - \mathbb{E}_X[a(x, X)]) \cdot \underbrace{(\mathbb{E}_X[a(x, X)] - \mathbb{E}_X [a(x, X)])}_{=0} =
$$

$$
    = \text{bias}_X^2 a(x, X)+ \mathbb{V}\text{ar}_X[a(x, X)]
$$

Таким образом, итоговое выражение для $Q(a)$ примет вид

$$
    Q(a) = \mathbb{E}_x \mathbb{E}_{X, \varepsilon} [y(x, \varepsilon) - a(x, X)]^2 = \mathbb{E}_x \text{bias}_X^2 a(x, X) + \mathbb{E}_x \mathbb{V}\text{ar}_X[a(x, X)] + \sigma^2,
$$

где

$$
    \text{bias}_X a(x, X) = f(x) - \mathbb{E}_X[a(x, X)]
$$

— **смещение** предсказания алгоритма в точке $x$, усреднённого по всем возможным обучающим выборкам, относительно истинной зависимости $f$;

$$
    \mathbb{V}\text{ar}_X[a(x, X)] = \mathbb{E}_X \left[ a(x, X) - \mathbb{E}_X[a(x, X)] \right]^2
$$

— **дисперсия (разброс)** предсказаний алгоритма в зависимости от обучающей выборки $X$;

$$
    \sigma^2 = \mathbb{E}_x \mathbb{E}_\varepsilon[y(x, \varepsilon) - f(x)]^2
$$

— неустранимый **шум** в данных.

Смещение показывает, насколько хорошо с помощью данного алгоритма можно приблизить истинную зависимость $f$, а разброс характеризует чувствительность алгоритма к изменениям в обучающей выборке. Например, деревья маленькой глубины будут в большинстве случаев иметь высокое смещение и низкий разброс предсказаний, так как они не могут слишком хорошо запомнить обучающую выборку. А глубокие деревья, наоборот, могут безошибочно выучить обучающую выборку и потому будут иметь высокий разброс в зависимости от выборки, однако их предсказания в среднем будут точнее. На рисунке ниже приведены возможные случаи сочетания смещения и разброса для разных моделей:

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/20_1_1d2027dbc2_589b8146f1.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="http://scott.fortmann-roe.com/docs/BiasVariance.html">Источник</a>
  </figcaption>
</figure>

Синяя точка соответствует модели, обученной на некоторой обучающей выборке, а всего синих точек столько, сколько было обучающих выборок. Красный круг в центре области представляет ближайшую окрестность целевого значения. Большое смещение соответствует тому, что модели в среднем не попадают в цель, а при большом разбросе модели могут как делать точные предсказания, так и довольно сильно ошибаться.

Полученное нами разложение ошибки на три компоненты верно только для квадратичной функции потерь. Для других функций потерь существуют более общие формы этого разложения ([Domigos, 2000](https://www.researchgate.net/publication/221345426_A_Unifeid_Bias-Variance_Decomposition_and_its_Applications), [James, 2003](http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=193A2D240404AB056822F188BAB09F94?doi=10.1.1.95.4138&rep=rep1&type=pdf)) с похожими по смыслу компонентами. Это позволяет предполагать, что для большинства основных функций потерь имеется некоторое представление в виде смещения, разброса и шума (хоть и, возможно, не в столь простой аддитивной форме).

### Пример расчёта оценок bias и variance

Попробуем вычислить разложение на смещение и разброс на каком-нибудь практическом примере. Наши обучающие и тестовые примеры будут состоять из зашумлённых значений целевой функции $f(x)$, где $f(x)$ определяется как

$$
    f(x) = x \sin x
$$

В качестве шума добавляется нормальный шум с нулевым средним и дисперсией $\sigma^2$, равной во всех дальнейших примерах 9. Такое большое значение шума задано для того, чтобы задача была достаточно сложной для классификатора, который будет на этих данных учиться и тестироваться. Пример семпла из таких данных:

![20_2_141d5e1768.webp](https://yastatic.net/s3/education-portal/media/20_2_141d5e1768_b27b188f9d.webp)

Посмотрим на то, как предсказания деревьев зависят от обучающих подмножеств и максимальной глубины дерева. На рисунке ниже изображены предсказания деревьев разной глубины, обученных на трёх независимых подвыборках размера 20 (каждая колонка соответствует одному подмножеству):

![20_3_2d4e194d90.webp](https://yastatic.net/s3/education-portal/media/20_3_2d4e194d90_98a6743fb5.webp)

Глядя на эти рисунки, можно выдвинуть гипотезу о том, что с увеличением глубины дерева смещение алгоритма падает, а разброс в зависимости от выборки растёт. Проверим, так ли это, вычислив компоненты разложения для деревьев со значениями глубины от 1 до 15. 

Для обучения деревьев насемплируем 1000 случайных подмножеств $X_{train} = (x_{train}, y_{train})$ размера 500, а для тестирования зафиксируем случайное тестовое подмножество точек $x_{test}$ также размера 500. Чтобы вычислить матожидание по $\varepsilon$, нам нужно несколько экземпляров шума $\varepsilon$ для тестовых лейблов: 

$$
    y_{test} = y(x_{test}, \hat \varepsilon) = f(x_{test}) + \hat \varepsilon
$$

Положим количество семплов случайного шума равным 300. Для фиксированных $X_{train} = (x_{train}, y_{train})$ и $X_{test} = (x_{test}, y_{test})$ квадратичная ошибка вычисляется как

$$
    MSE = (y_{test} - a(x_{test}, X_{train}))^2
$$

Взяв среднее от $MSE$ по $X_{train}$, $x_{test}$ и $\varepsilon$, мы получим оценку для $Q(a)$, а оценки для компонент ошибки мы можем вычислить по ранее выведенным формулам. 

На графике ниже изображены компоненты ошибки и она сама в зависимости от глубины дерева:

![20_4_e63b7825cc.webp](https://yastatic.net/s3/education-portal/media/20_4_e63b7825cc_10f788ced7.webp)

По графику видно, что гипотеза о падении смещения и росте разброса при увеличении глубины подтверждается для рассматриваемого отрезка возможных значений глубины дерева. Правда, если нарисовать график до глубины 25, можно увидеть, что разброс становится равен дисперсии случайного шума. То есть деревья слишком большой глубины начинают идеально подстраиваться под зашумлённую обучающую выборку и теряют способность к обобщению:

![20_5_4a62f68894.webp](https://yastatic.net/s3/education-portal/media/20_5_4a62f68894_18c97c4dd9.webp)

Код для подсчёта разложения на смещение и разброс, а также код отрисовки картинок можно найти в данном [ноутбуке](https://github.com/yandexdataschool/ML-Handbook-materials/blob/main/chapters/ensembles/bias_variance.ipynb).

## Bias-variance trade-off: в каких ситуациях он применим

В книжках и различных интернет-ресурсах часто можно увидеть следующую картинку:

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/20_6_bc989ef14a_4b92bebba1.webp" loading="lazy" decoding="async" alt="">
  <figcaption>
    <a href="http://scott.fortmann-roe.com/docs/BiasVariance.html">Источник</a>
  </figcaption>
</figure>

Она иллюстрирует утверждение, которое в литературе называется **bias-variance trade-off**: чем выше сложность обучаемой модели, тем меньше её смещение и тем больше разброс, и поэтому общая ошибка на тестовой выборке имеет вид $U$-образной кривой. С падением смещения модель всё лучше запоминает обучающую выборку, поэтому слишком сложная модель будет иметь нулевую ошибку на тренировочных данных и большую ошибку на тесте. Этот график призван показать, что существует оптимальная сложность модели, при которой соблюдается баланс между переобучением и недообучением и ошибка при этом минимальна.

Существует достаточное количество подтверждений bias-variance trade-off для непараметрических моделей. Например, его можно наблюдать для метода $k$ ближайших соседей при росте $k$ и для ядерной регрессии при увеличении ширины окна $\sigma$ [(Geman et al., 1992)](http://doursat.free.fr/docs/Geman_Bienenstock_Doursat_1992_bv_NeurComp.pdf):

![20_7_e0e912a783.webp](https://yastatic.net/s3/education-portal/media/20_7_e0e912a783_6127767bd7.webp)

Чем больше соседей учитывает $k$-NN, тем менее изменчивым становится его предсказание, и аналогично для ядерной регрессии, из-за чего сложность этих моделей в некотором смысле убывает с ростом $k$ и $\sigma$. Поэтому традиционный график bias-variance trade-off здесь симметрично отражён по оси $x$.

Однако, как показывают последние исследования, непременное возрастание разброса при убывании смещения не является абсолютно истинным предположением. Например, для нейронных сетей с ростом их сложности может происходить снижение и разброса, и смещения. Одна из наиболее известных статей на эту тему — статья [Белкина и др. (Belkin et al., 2019)](https://arxiv.org/pdf/1812.11118.pdf), в которой, в частности, была предложена следующая иллюстрация:

![20_8_7351c6ce34.webp](https://yastatic.net/s3/education-portal/media/20_8_7351c6ce34_b6564b41f3.webp)

Слева — классический bias-variance trade-off: убывающая часть кривой соответствует недообученной модели, а возрастающая — переобученной. А на правой картинке — график, называемый в статье **double descent risk curve**. На нём изображена эмпирически наблюдаемая авторами зависимость тестовой ошибки нейросетей от мощности множества входящих в них параметров ($\mathcal H$). Этот график разделён на две части пунктирной линией, которую авторы называют interpolation threshold. Эта линия соответствует точке, в которой в нейросети стало достаточно параметров, чтобы без особых усилий почти идеально запомнить всю обучающую выборку. Часть до достижения interpolation threshold соответствует «классическому» режиму обучения моделей: когда у модели недостаточно параметров, чтобы сохранить обобщающую способность при почти полном запоминании обучающей выборки. А часть после достижения interpolation threshold соответствует «современным» возможностям обучения моделей с огромным числом параметров. На этой части графика ошибка монотонно убывает с ростом количества параметров у нейросети. Авторы также наблюдают похожее поведение и для «древесных» моделей: Random Forest и бустинга над решающими деревьями. Для них эффект проявляется при одновременном росте глубины и числа входящих в ансамбль деревьев.

В качестве вывода к этому разделу хочется сформулировать два основных тезиса:
1. Bias-variance trade-off нельзя считать непреложной истиной, выполняющейся для всех моделей и обучающих данных.
2. Разложение на смещение и разброс не влечёт немедленного выполнения bias-variance trade-off и остаётся верным и для случая, когда все компоненты ошибки (кроме неустранимого шума) убывают одновременно. Этот факт может оказаться незамеченным из-за того, что в учебных пособиях часто разговор о разложении дополняется иллюстрацией с $U$-образной кривой, благодаря чему в сознании эти два факта могут слиться в один.

## Список литературы

- [Блог-пост](https://link.medium.com/X5Cpg1WITjb) про bias-variance от 
Йоргоса Папахристудиса
- [Блог-пост](http://scott.fortmann-roe.com/docs/BiasVariance.html) про bias-variance от Скотта Фортмана-Роу 
- Статьи от [Домингоса (2000)](https://www.researchgate.net/publication/221345426_A_Unifeid_Bias-Variance_Decomposition_and_its_Applications) и [Джеймса (2003)](http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=193A2D240404AB056822F188BAB09F94?doi=10.1.1.95.4138&rep=rep1&type=pdf) про обобщённые формы bias-variance decomposition
- [Блог-пост](https://www.bradyneal.com/bias-variance-tradeoff-textbooks-update#double-descent) от Брейди Нила про необходимость пересмотра традиционного взгляда на bias-variance trade-off
- [Статья](http://doursat.free.fr/docs/Geman_Bienenstock_Doursat_1992_bv_NeurComp.pdf) Гемана и др. (1992), в которой была впервые предложена концепция bias-variance trade-off
- [Статья](https://arxiv.org/pdf/1812.11118.pdf) Белкина и др. (2019), в которой был предложен double-descent curve

  ## handbook

  Учебник по машинному обучению

  ## title

  Bias-variance decomposition

  ## description

  Классический взгляд на на то, почему слишком сложные модели переобучаются

- 
  ## path

  /handbook/ml/article/teoriya-glubokogo-obucheniya-vvedenie

  ## content

  Центральной теоретической проблемой обучения с учителем является проблема обобщающей способности.

В самом деле, как можно гарантировать, что модель, обученная на некотором наборе данных, будет показывать хорошие результаты на данных, которых в обучении не было? Для классических моделей было доказано много содержательных результатов, которые, может быть, не давали ответы на все вопросы, но позволяли многое понять о работе моделей. Что же касается нейросетей, для них теория ещё только создаётся, и в этом разделе мы познакомим вас с рядом направлений развития этой науки.

Нейронная сеть фиксированной архитектуры реализует некоторый класс моделей $\mathcal{F}$. Например, разные элементы этого класса могут соответствовать различным наборам весов. Когда такой класс фиксирован, мы обычно решаем задачу минимизации некоторой функции ошибки или, как чаще говорят в теории ML, задачу минимизации **эмпирического риска**

$$\hat R_m(f) = \mathbb{E}_{x,y \in S_m} r(y,f(x))$$ 

среди моделей $f$ из класса $\mathcal{F}$, где $S_m$ – обучающий датасет из $m$ примеров, выбранных независимо из распределения данных $\mathcal{D}$, а $r(y,\hat y)$ – функция риска, например, $\mathbb{I}[y \ne \hat y]$.

Наша цель, однако, минимизировать не эмпирический, а **истинный риск**, то есть

$$R(f) = \mathbb{E}_{x,y \sim \mathcal{D}} r(y,f(x)),$$

где математическое ожидание берётся по распределению данных, а не по выборке (математическое ожидание риска на всех мыслимых данных, не только на выборке). К сожалению, в рамках задачи обучения с учителем доступа к истинному распределению данных у нас нет, поэтому минимизировать истинный риск напрямую не удаётся, но мы можем попробовать его оценить. Часто для этого используют риск на валидации, но в этом разделе учебника мы постараемся получить теоретические оценки.

Пусть $\hat f_m$ – модель из класса $\mathcal{F}$, которую мы построили исходя из выборки $S_m$. Интересно оценить, насколько её истинный риск отличается от эмпирического, то есть оценить разность

$$R(\hat f_m) - \hat R_m(\hat f_m).$$

В случае нейронных сетей очень трудно предсказать, к какой именно модели $\hat f_m$ сойдётся наш метод обучения (например, градиентный спуск) на данной выборке $S_m$. Тем не менее, разницу рисков всегда можно оценить сверху супремумом по всем моделям из класса:

$$R(\hat f_m) - \hat R_m(\hat f_m) \leq \sup_{f \in \mathcal{F}} (R(f) - \hat R_m(f)).$$

В этом случае риск можно оценить сверху величиной

$$\tilde R := \hat R_m + \sup_{f \in \mathcal{F}} (R(f) - \hat R_m(f)).$$

Такие оценки называются **равномерными** (**uniform bounds**); мы рассмотрим их подробно в [соответствующем параграфе](https://academy.yandex.ru/handbook/ml/article/obobshayushaya-sposobnost-klassicheskaya-teoriya).

Понятно, что подобная оценка становится бесполезной (vacuous), если в классе содержится модель, которая идеально работает на фиксированной выборке $S_m$ ($\hat R_m(f) = 0$), но на какой-либо другой (потенциально тестовой) выборке из тех же данных работает плохо ($R(f)$ велик). Так, известно, что модели класса VGG способны выучить ImageNet даже с перемешанными метками классов (см. статью [Understanding deep learning requires rethinking generalization](http://arxiv.org/abs/1611.03530)). Понятно, что истинный риск у такой модели будет близок к риску случайного угадывания. «Плохую» модель можно построить следующим образом. Пусть $\mathcal{A}$ – наш исходный алгоритм обучения, например, градиентный спуск. Он принимает на вход выборку и выдаёт обученную модель. Возьмём датасет, составленный из двух частей: 

* $S_m$ – это самая обычная выборка, в которой объекты насэмплированы из распределения $\mathcal{D}$,

* $\tilde S_M$ – выборка, объекты которой сгенерированы из того же распределения, но метки перепутаны.

Рассмотрим модель $\tilde f_{m,M} = \mathcal{A}(S_m \cup \tilde S_M)$. Чем больше $M$ будет по сравнению с $m$, тем ближе будет построенная модель к случайному угадыванию. При этом, если суммарный размер двух выборок не слишком велик, то наша модель сможет запомнить их обе, в частности, $S_m$. Таким образом, эмпирический риск $\hat R_m(\tilde f_m)$ такой модели окажется мал, а истинный – велик.

Интуитивно понятно, что чем «сложнее» класс $\mathcal{F}$, тем больше шансов найти в нём подобную модель. Одной из классических мер сложности класса моделей является **размерность Вапника-Червоненкиса**, или **VC-размерность**, предложенная в 1971 году в статье В. Н. Вапника и А. Я. Червоненкиса «О равномерной сходимости частот появления событий к их вероятностям». Она даёт следующую равномерную оценку:

$$\sup_{f \in \mathcal{F}} (R(f) - \hat R_m(f)) \leq O\left(\sqrt{\frac{\mathrm{VC}(\mathcal{F})\log{\mathrm{VC}(\mathcal{F})}}{m}}\right).$$

Как и следовало ожидать, правая часть растёт со сложностью модели и падает с размером выборки.
    
Известно, что для полносвязных сетей VC-размерность растёт как $\Theta(n N)$, где $n$ – ширина сети (число нейронов в слое), а $N$ – общее число параметров; см. статью [Nearly-tight VC-dimension and Pseudodimension Bounds for Piecewise Linear Neural Networks](https://arxiv.org/pdf/1703.02930.pdf). Рассмотрим полносвязную сеть с одним скрытым слоем. Тогда общее число параметров сети пропорционально ширине, а значит, VC-размерность пропорциональна квадрату ширины. Соответствующая оценка на истинный риск принимает вид:

$$\tilde R(n) \leq \hat R_m(n) + \frac{C n}{\sqrt{m}},$$

где $C$ – константа из равномерной оценки разницы рисков с помощью VC-размерности, а $\hat R_m(n)$ – эмпирический риск сети ширины $n$, обученной на данной выборке $S_m$.

Как правило, эмпирический риск монотонно убывает с ростом ширины, пока не достигнет нуля (в самом деле, чем больше ширина, тем сложнее класс моделей и тем больше шансов обнаружить в нём модель, запоминающую фиксированную выборку). В результате $\tilde R(n)$ может вести себя немонотонно: у этой величины может обнаружиться минимум строго левее точки, где $\hat R_m(n)$ впервые достигает нуля: 

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/risk_curve_classic_0e42fbd733_8fd79aecea.svg" loading="lazy" decoding="async" alt="">
  <figcaption>
    <p>Схематическое изображение изменения эмпирического риска $\hat R_m$ (синяя кривая) и предсказанного риска $\tilde R$ (красная кривая) в зависимости от ширины сети.</p>
  </figcaption>
</figure>

Правее минимума предсказанный риск монотонно растёт. Но оказывается, что реальный истинный риск, напротив, убывает, выходя на асимптоту: 

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/risk_curve_cifar_2868f12199_cb902d8cae.svg" loading="lazy" decoding="async" alt="">
  <figcaption>
    <p>Зависимость эмпирического риска $\hat R_m$ (бирюзовая кривая) и истинного риска $R$ (синяя кривая) для полносвязной сети с одним скрытым слоем, обученной на наборе данных CIFAR10, в зависимости от ширины сети; подробности см. в работе <a href='http://arxiv.org/abs/1412.6614'>B. Neyshabur, R. Tomioka, N. Srebro, In Search of the Real Inductive Bias: On the Role of Implicit Regularization in Deep Learning</a>.</p>
  </figcaption>
</figure>


Возникает вопрос: если у истинного риска есть асимптота, то как ведут себя нейронные сети в пределе бесконечной ширины? Мы остановимся на этом вопросе в [соответствующем параграфе](https://academy.yandex.ru/handbook/ml/article/seti-beskonechnoj-shiriny).

Упомянутый выше эксперимент с перемешиванием меток классов на части обучающей выборки можно рассматривать как модификацию не данных, а алгоритма обучения. А именно, давайте представим алгоритм, который, получая на вход выборку $S_m$, сделает с ней следующее:

1. Каким-то образом делит её на две части: $S_m = S_{m_1}'\cup S_{m_2}''$,
2. Заменяет в $S_{m_2}''$ метки на случайные.
3. Обучает модель на объединении $S_{m_1}'$ и «испорченной» $S_{m_2}''$.

Такой «испорченный» алгоритм обучения приводит к модели, которая запоминает $S_{m_1}'$ и поэтому на исходной обучающей выборке $S_m$ работает не так уж и плохо. Но на тестовых данных он показывает качество, сравнимое со случайным угадыванием. Работающие на практике алгоритмы, например, градиентный спуск, тоже могут обучить модель, которая «запомнила» обучающую выборку. Тем не менее, на тестовой выборке обученная модель будет давать нормальное качество (см. опять же вторую картинку в начале параграфа). Возникает вопрос: почему так? Почему среди всех конфигураций весов, для которых риск на обучающей выборке равен нулю, градиентный спуск не выбирает те, для которых истинный риск сравним со случайным угадыванием? Явление, при котором среди всех эквивалентных по эмпирическому риску решений алгоритм выбирает определённые, называется **implicit bias**, и будет рассмотрен в [соответствующем параграфе](https://academy.yandex.ru/handbook/ml/article/implicit-bias).

Если даже известно, какие конфигурации весов «предпочитает» наш алгоритм обучения, это никак не повлияет на равномерную оценку разницы рисков. В [параграфе про PAC-байесовские оценки](https://academy.yandex.ru/handbook/ml/article/pac-bajesovskie-ocenki-riska) будет рассмотрен класс оценок, которые позволяют учесть предпочтения алгоритма. А именно, пусть обученная модель случайна (это действительно так из-за случайности инициализации весов и, например, шума стохастического градиентного спуска), то есть алгоритм строит распределение на моделях – назовём его апостериорным распределением. Пусть дано другое распределение, не зависящее от выборки, назовём его априорным. Тогда роль сложности в наших оценках будет играть расстояние Кульбака-Лейблера (KL-дивергенция) между апостериорным и априорным распределениями на моделях. Если априорное распределение покрывает «предпочтительные» решения и не покрывает остальные, то KL-дивергенция мала и оценка разницы рисков невелика. Из-за внешней схожести некоторых величин, возникающих в этой теории, с объектами из байесовской статистики, такие оценки называется **PAC-байесовскими** (**PAC-bayesian bounds**, от **probably approximately correct**).

Выше мы негласно предполагали, что используемый алгоритм обучения успешно решает задачу минимизации эмпирического риска. Для нейронных сетей наиболее популярный алгоритм – градиентный спуск или его разновидности. Если бы функция потерь была выпуклой как функция от весов сети, то это гарантировало бы сходимость в глобальный минимум. В общем случае теория оптимизации не даёт таких гарантий. Тем не менее, для сетей реалистичного размера градиентный спуск успешно сходится в глобальный минимум, что толкает нас на предположение о том, что все локальные минимумы таких сетей глобальны. Это предположение действительно можно доказать в некоторых частных случаях; см. [параграф про ландшафт функции потерь](https://academy.yandex.ru/handbook/ml/article/landshaft-funkcii-poter).

В качестве необходимого дополнения следует также гарантировать, что градиентный спуск (или его разновидности) не сходится в возможные седловые точки. При определённых условиях на минимизируемую функцию можно доказать, что для сходимости в седловую точку необходимо инициализировать функцию на множестве меры ноль. Более подробно вы можете почитать в работе [Gradient descent only converges to minimizers](https://arxiv.org/pdf/1602.04915.pdf) или в её обобщениях [Gradient Descent Only Converges to Minimizers: Non-Isolated Critical Points and Invariant Regions](https://arxiv.org/pdf/1605.00405v1.pdf) и [On the almost sure convergence of stochastic gradient descent in non-convex problems](https://arxiv.org/pdf/2006.11144.pdf).

Впрочем, гарантии сходимости, как правило, формулируются для фиксированной архитектуры сети и ничего не говорят о скорости сходимости. Хотелось бы также иметь гарантии на то, что какой бы широкой или глубокой сеть не была, градиентный спуск сойдётся в минимум за разумное время. А для этого необходимо понимать, что на самом первом шаге градиентного спуска градиент не будет гаснуть или «взрываться» с ростом ширины или глубины. Известен ряд эвристик для инициализации весов, помогающих с этим бороться: например, инициализации Глоро (Xavier Glorot) и Хе (Kaiming He). Подробнее про них вы можете прочитать в параграфе про [тонкости обучения нейросетей](https://academy.yandex.ru/handbook/ml/article/tonkosti-obucheniya).

  ## handbook

  Учебник по машинному обучению

  ## title

  Введение в теорию глубокого обучения

  ## description

  Введение в теорию глубокого обучения

- 
  ## path

  /handbook/ml/article/obobshayushaya-sposobnost-klassicheskaya-teoriya

  ## content

  Как мы уже видели во введении, мы не можем напрямую оптимизировать **истинный риск** модели $f(x)$

$$R(f) = \mathbb{E}_{x,y \sim \mathcal{D}} r(y,f(x)),$$

так как нам недоступно полное распределение данных $\mathcal{D}$. Поэтому вместо задачи минимизации истинного риска, мы будем минимизировать **эмпирический риск**

$$\hat R_m(f) = \mathbb{E}_{x,y \in S_m} r(y, f(x))$$

на доступном нам наборе данных $S_m$.

Классическая теория предлагает оценивать разность эмпирического и истинного равномерно, что даёт
    
$$R(\hat f_m) \leq \hat R_m(\hat f_m) + \sup_{f \in \mathcal{F}} (R(f) - \hat R_m(f)).$$

Такая оценка не зависит от алгоритма обучения; она зависит лишь от класса моделей $\mathcal{F}$, в котором происходит поиск. Так, в случае нейронных сетей в качестве класса $\mathcal{F}$ можно взять класс всех нейронных сетей фиксированной архитектуры, отличающихся только весами.

Если класс $\mathcal{F}$ настолько велик, что для большинства наборов данных размера $m$ содержит модель $f$, у которой эмпирический риск мал, а истинный велик, то оценка выше теряет смысл. Работа <a href="http://arxiv.org/abs/1611.03530">Understanding deep learning requires rethinking generalization</a> показала, что именно это и происходит в нейронных сетях, применяемых на практике, на реальных наборах данных. А именно, пусть $\mathcal{A}$ – алгоритм, применяемый для обучения сети, например, градиентный спуск. Предположим, что с высокой вероятностью $\hat R_m(\mathcal{A}(S_m)) = 0$, если только размер выборки $m$ не слишком велик. Пусть $S_m$ – наша выборка, а $S'_{m'}$ – датасет, в котором примеры берутся из выборки, а разметка случайна. По предположению, модель $\hat f_{m,m'} = \mathcal{A}(S_m \cup S'_{m'})$, обученная на объединённом датасете, имеет нулевой риск на «настоящем» датасете $S_m$, если только $m+m'$ не слишком велик. С другой стороны, если $m' \gg m$, то $\hat f_{m,m'} \approx \mathcal{A}(S'_{m'})$ – истинный риск такой модели близок к риску случайного угадывания.

Тем не менее, если в качестве $\mathcal{F}$ взять не все модели, реализуемые данной архитектурой нейронной сети, а лишь реализуемые данным алгоритмом обучения на наборах данных из распределения с высокой вероятностью, то можно надеятся, что равномерная оценка окажется осмысленной.

Мы говорим «с высокой вероятностью» для того, чтобы исключить «нереалистичные» наборы данных, обучение на которых ведёт к плохим результатам, а также ничтожно-редкие случаи реализации шума в алгоритме обучения, при котором последний сходится в «плохие» решения. Подробнее о том, какие модели реализуются градиентным спуском, мы обсудим в параграфе про [implicit bias](https://academy.yandex.ru/handbook/ml/article/implicit-bias).

## Оценка супремума

Попробуем оценить супремум разницы рисков. Будем считать, что выборка $S_m$ выбирается случайным (и равновероятным) образом из распределения данных $\mathcal{D}$. Некоторые из выборок могут быть катастрофически плохими, поэтому мы будем рассматривать оценки, которые верны не обязательно всегда, а просто с достаточно большой вероятностью.

Предположим сначала, что класс моделей $\mathcal{F}$ конечен. Тогда

$$\mathbb{P}\left(\sup_{f\in\mathcal{F}} (R(f) - \hat R_m(f)) \geq \epsilon\right) =$$

$$= \mathbb{P}(\exists f\in\mathcal{F}: \; (R(f) - \hat R_m(f)) \geq \epsilon) \leq$$

$$\leq\sum_{f\in\mathcal{F}} \mathbb{P}(R(f) - \hat R_m(f) \geq \epsilon) \leq$$

$$\leq\vert\mathcal{F}\vert \sup_{f\in\mathcal{F}} \mathbb{P}(R(f) - \hat R_m(f) \geq \epsilon)
        \quad
        \forall \epsilon > 0$$

Заметим, что $R(f) = \mathbb{E}_{S_m \sim \mathcal{D}^m} \hat R_m(f)$. Поэтому при фиксированном $f$ разницу рисков $\hat R_m(f) - R(f)$ можно оценить с помощью неравенства Хёффдинга.

**Неравенство Хёффдинга (Hoeffding's inequality)**. Пусть $X_1,\ldots,X_m$ – независимые одинаково распределённые случайные величины со значениями в $[0,1]$. Тогда для всех $\epsilon > 0$ имеют место неравенства

$$\mathbb{P}\left(\sum_{i=1}^m X_i - \mathbb{E} \sum_{i=1}^m X_i \geq \epsilon \right) \leq
            e^{-\frac{2\epsilon^2}{m}},$$

$$\mathbb{P}\left(\mathbb{E} \sum_{i=1}^m X_i - \sum_{i=1}^m X_i \geq \epsilon \right) \leq
            e^{-\frac{2\epsilon^2}{m}}.$$


Как следствие неравенства Хёффдинга, получаем, что для любого $\epsilon > 0$ и для любой $f \in \mathcal{F}$.

$$
        \mathbb{P}(R(f) - \hat R_m(f) \geq \epsilon) \leq
        e^{-2 m \epsilon^2}  
$$

Заметим, что тогда для любой $f \in \mathcal{F}$,

$$R(f) - \hat R_m(f) \leq
        \sqrt{\frac{1}{2m} \log \frac{1}{\delta}}
        \quad
        \text{с вероятностью $\geq 1 - \delta$ по $S_m$.}
$$

Несмотря на то, что эта оценка является оценкой на обобщающую способность, она не имеет смысла, так как модель $f$ в ней задана априори и не зависит от $S_m$. Другими словами, она верна для необученных моделей $f$.

Возвращаясь к нашей оценке, получаем:

$$
        \mathbb{P}\left(\sup_{f\in\mathcal{F}} (R(f) - \hat R_m(f)) \geq \epsilon\right)
        \leq |\mathcal{F}| e^{-2 m \epsilon^2}
        \quad
        \forall \epsilon > 0,
$$

где $\vert\mathcal{F}\vert$ – мощность класса $\mathcal{F}$. Следовательно,

$$\color{#348FEA}{\sup_{f\in\mathcal{F}} (R(f) - \hat R_m(f)) \leq
        \sqrt{\frac{1}{2m} \left(\log \frac{1}{\delta} + \log |\mathcal{F}| \right)}
        \quad
        \text{с вероятностью $\geq 1 - \delta$ по $S_m$.}}
$$


В случае бесконечного $\mathcal{F}$ используем следующее обобщение неравенства Хёффдинга:


**Неравенство МакДайармида (McDiarmid's inequality)**. Пусть $X_{1},\ldots,X_m$ – независимые одинаково распределённые случайные величины, $g$ – скалярная функция с $m$ аргументами, такая что

$$\sup_{x_{1},\ldots,x_m,\tilde x_i} |g(x_1,\ldots,x_i,\ldots,x_m) - g(x_{1},\ldots \tilde x_i,\ldots x_{m})| \leq c_i
            \quad
            \forall i = 1,\ldots,m
$$

для некоторых $c_i$. Тогда для любого $\epsilon > 0$ имеет место неравенство

$$
\mathbb{P}\left( g(X_1,\ldots,X_m) - \mathbb{E} g(X_1,\ldots,X_m) \geq \epsilon \right) \leq
            e^{-\frac{2\epsilon^2}{\sum_{i=1}^m c_i^2}}.
$$


Применяя теорему к $g(\{(x_i,y_i)\}_{i=1}^m) = \sup_{f \in \mathcal{F}}(R(f) - \hat R_m(f))$, получаем:



$$\mathbb{P}_{S_m} \left(\sup_{f \in \mathcal{F}}(R(f) - \hat R_m(f)) - \mathbb{E}_{S'_m} \sup_{f \in \mathcal{F}}(R(f) - \hat R'_m(f)) \geq \epsilon\right) \leq
        e^{-2 m \epsilon^2},
$$



из чего следует:


$$
            \sup_{f \in \mathcal{F}}(R(f) - \hat R_m(f)) \leq
        \color{#FFC100}{\mathbb{E}_{S'_m} \sup_{f \in \mathcal{F}}(R(f) - \hat R'_m(f))} + \sqrt{\frac{1}{2m} \log \frac{1}{\delta}}
        \quad
        \text{с вероятностью $\geq 1 - \delta$ по $S_m$,}
$$


где $\hat R'_m(f)$ – эмпирический риск на выборке $S'_m$. В следующем подразделе мы постараемся оценить жёлтое слагаемое.

### Симметризация и сложность Радемахера

Оценим сверху матожидание супремума:


$$\mathbb{E}_{S'_m} \sup_{f \in \mathcal{F}}(R(f) - \hat R'_m(f))
        = \mathbb{E}_{S'_m} \sup_{f \in \mathcal{F}}(\mathbb{E}_{S''_m} \hat R''_m(f) - \hat R'_m(f))
        \leq
$$



$$\leq \mathbb{E}_{S'_m} \mathbb{E}_{S''_m} \sup_{f \in \mathcal{F}}(\hat R''_m(f) - \hat R'_m(f)) =$$



$$= \mathbb{E}_{S'_m} \mathbb{E}_{S''_m} \sup_{f \in \mathcal{F}} \left(\frac{1}{m} \sum_{i=1}^m (r(y''_i,f(x''_i)) - r(y'_i,f(x'_i)))\right).$$


Этот шаг называется «симметризация»: теперь выражение выше зависит от двух равнозначных обучающих выборок $S'_m$ и ${S_m''}$. Ниже для краткости будем обозначать $r'_i(f) = r(y'_i,f(x'_i))$ и ${r_i''}(f) = r({y_i''},f({x_i''}))$.

Как оценить сверху супремум разности рисков? Наивная оценка, супремум суммы, слишком слаба: в самом деле, при фиксированном наборе данных вполне вероятно может существовать модель, имеющая большой риск на нём (достаточно взять модель, обученную на тех же данных, но с «неправильными» метками), поэтому матожидание супремума эмпирического риска может быть велико.

Для обхода этой сложности заметим, что выражение выше симметрично относительно перестановки местами двух выборок:

$$
        \mathbb{E}_{S'_m} \mathbb{E}_{S''_m} \sup_{f \in \mathcal{F}} \left(\frac{1}{m} \sum_{i=1}^m (r''_i(f) - r'_i(f))\right)
        = \mathbb{E}_{S'_m} \mathbb{E}_{S''_m} \sup_{f \in \mathcal{F}} \left(\frac{1}{m} \sum_{i=1}^m (r'_i(f) - r''_i(f))\right).
$$

Более того, так как элементы обеих выборок выбираются независимо, значение выражения не меняется и при перестановке местами отдельно $i$-ых элементов двух выборок. А именно, для любого набора $\sigma_1,\ldots,\sigma_m \in \{-1,1\}^m$

$$
        \mathbb{E}_{S'_m} \mathbb{E}_{S''_m} \sup_{f \in \mathcal{F}} \left(\frac{1}{m} \sum_{i=1}^m (r''_i(f) - r'_i(f))\right) =
$$

$$
        = \mathbb{E}_{S'_m} \mathbb{E}_{S''_m} \sup_{f \in \mathcal{F}} \left(\frac{1}{m} \sum_{i=1}^m \sigma_i (r''_i(f) - r'_i(f))\right).
$$

Будем выбирать $\sigma_i$ независимо и равновероятно из $\{-1,1\}$. Такие случайные величины называются *переменными Радемахера*.

Поскольку оценки выше были верны для любых сигм, они верны и в среднем по переменным Радемахера, выбранным независимо от выборки:

$$
        \mathbb{E}_{S'_m} \mathbb{E}_{S''_m} \sup_{f \in \mathcal{F}} \left(\frac{1}{m} \sum_{i=1}^m (r''_i(f) - r'_i(f))\right) =
$$

$$
        = \mathbb{E}_{\sigma_{1:m}} \mathbb{E}_{S'_m} \mathbb{E}_{S''_m} \sup_{f \in \mathcal{F}} \left(\frac{1}{m} \sum_{i=1}^m \sigma_i (r''_i(f) - r'_i(f))\right).
$$

После введения переменных Радемахера оценка супремума разницы рисков через сумму супремумов становится не такой плохой. В самом деле, рассмотрим бинарную классификацию с помощью линейной модели. Если данные хорошо разделяются плоскостью, то $\mathbb{E}_{S_m} \sup_{f \in \mathcal{F}} \left(\frac{1}{m} \sum_{i=1}^m r_i(f)\right)$ будет большим, так как в качестве $f$ можно взять линейную модель с противоположно ориентированной разделяющей плоскостью для $S_m$. В то же время для того, чтобы $\mathbb{E}_{\sigma_{1:m}} \mathbb{E}_{S_m} \sup_{f \in \mathcal{F}} \left(\frac{1}{m} \sum_{i=1}^m \sigma_i r_i(f)\right)$ было большим, необходимо, чтобы существовала модель, отвечающая правильно на тех примерах, где $\sigma_i=-1$, и неправильно, где $\sigma_i=1$; для линейной модели это невозможно при большинстве конфигураций сигм.

Величина 

$$
\mathrm{Rad}_{\mathcal{D},m}(\mathcal{H}) =
\mathbb{E}_{z_{1:m} \sim \mathcal{D}^m} \mathbb{E}_{\sigma_{1:m} \sim U(\{-1,1\}^m)} \sup_{h \in \mathcal{H}} \left(\frac{1}{m} \sum_{i=1}^m \sigma_i h(z_i)\right).
$$

называется **сложностью Радемахера** класса функций $\mathcal{H}: \, \mathbb{X} \to \mathbb{R}$ (для распределения $\mathcal{D}$ на $\mathbb{X}$ и длины выборок $m$). Она велика, если в классе $\mathcal{H}$ содержатся функции, принимающие большие значения с заданными знаками на любом наборе данных фиксированного размера. Другими словами, сложность Радемахера измеряет, насколько выходы функций из класса $\mathcal{H}$ могут коррелировать со случайным шумом.

Для нас актуальна сложность Радемахера классов вида $\mathcal{H} = r \circ \mathcal{F}$, то есть композиций моделей из класса $\mathcal{F}$ и функции риска $r$. Если $\mathcal{F}$ – класс линейных моделей в пространстве размерности меньшей, чем $m$, то сложность Радемахера невелика. В то же время если $\mathcal{F}$ – множество всех возможных решающих деревьев, то, если только наборы данных непротиворечивы, она равна единице. В самом деле, решающее дерево способно запомнить всю обучающую выборку, то есть добиться единичной корреляции с любым случайным шумом.

Вернёмся к оценке разницы рисков:

$$
\mathbb{E}_{S'_m} \sup_{f \in \mathcal{F}}(R(f) - \hat R'_m(f))
\leq \mathbb{E}_{S'_m} \mathbb{E}_{S''_m} \sup_{f \in \mathcal{F}} \left(\frac{1}{m} \sum_{i=1}^m (r''_i(f) - r'_i(f))\right)
=
$$

$$
= \mathbb{E}_{\sigma_{1:m}} \mathbb{E}_{S'_m} \mathbb{E}_{S''_m} \sup_{f \in \mathcal{F}} \left(\frac{1}{m} \sum_{i=1}^m \sigma_i (r''_i(f) - r'_i(f))\right)
\leq
$$



$$
\leq \mathbb{E}_{S'_m} \mathbb{E}_{S''_m} \mathbb{E}_{\sigma_{1:m}} \left(\sup_{f \in \mathcal{F}} \left(\frac{1}{m} \sum_{i=1}^m \sigma_i r''_i(f)\right) + \sup_{f \in \mathcal{F}} \left(\frac{1}{m} \sum_{i=1}^m \sigma_i r'_i(f)\right)\right)
=
$$

$$= 2 \mathbb{E}_{S'_m} \mathbb{E}_{\sigma_{1:m}} \sup_{f \in \mathcal{F}} \left(\frac{1}{m} \sum_{i=1}^m \sigma_i r(y'_i,f(x'_i))\right) =
$$

$$
        = \color{#FFC100}{2 \mathrm{Rad}_{\mathcal{D},m}(r \circ \mathcal{F})}.
$$

### Оценка для «0/1-риска»

Сложность Радемахера зависит от функции риска. Рассмотрим задачу бинарной классификации с классами $+1$ и $-1$. Возьмём в качестве функции риска индикатор ошибки бинарной классификации, или «0/1-риск»: 

$$r(y,z) = r_{0/1}(y,z) = \mathbb{I}[y z < 0].$$

Название «0/1-риск» обусловлено тем, что риск принимает значения $0$ и $1$.

Заметим следующее:

$$
\mathbb{E}_{\sigma_{1:m}} \sup_{f \in \mathcal{F}} \left(\frac{1}{m} \sum_{i=1}^m \sigma_i r_i(f) \right)
= \mathbb{E}_{\sigma_{1:m}} \max_{f \in \mathcal{F}_{S_m}} \left(\frac{1}{m} \sum_{i=1}^m \sigma_i r_i(f) \right),
$$

где $\mathcal{F}_{S_m}$ – класс эквивалентности функций из $\mathcal{F}$, в котором две функции считаются эквивалентными тогда и только тогда, когда их образы на выборке $S_m$ имеют одинаковые знаки. Другими словами, среди всех функций, принимающих одни и те же знаки на $S_m$, мы выберем по одной и сформируем из них множество $\mathcal{F}_{S_m}$. Заметим, что это множество конечно: $\vert\mathcal{F}_{S_m}\vert \leq 2^m$.

Нам понадобится следующая

**Лемма**. Пусть $X$ – случайная величина со значениями в $[a,b]$ и нулевым средним. Тогда для любых $ s > 0$ имеет место неравенство

$$
        \mathbb{E} e^{sX} \leq
        e^{\frac{(b-a)^2 s^2}{8}}.
$$

С её помощью получаем:

$$
\mathbb{E}_{\sigma_{1:m}} \sup_{f \in \mathcal{F}} \left(\frac{1}{m} \sum_{i=1}^m \sigma_i r_i(f) \right) =
\mathbb{E}_{\sigma_{1:m}} \max_{f \in \mathcal{F}_{S_m}} \left(\frac{1}{m} \sum_{i=1}^m \sigma_i r_i(f) \right)
=
$$

$$= \frac{1}{m s} \log \exp \left(s \mathbb{E}_{\sigma_{1:m}} \max_{f \in \mathcal{F}_{S_m}} \left(\sum_{i=1}^m \sigma_i r_i(f) \right)\right) \leq
$$

$$
\leq\frac{1}{m s} \log \mathbb{E}_{\sigma_{1:m}} \exp \left(s \max_{f \in \mathcal{F}_{S_m}} \left(\sum_{i=1}^m \sigma_i r_i(f) \right)\right)
\leq
$$

$$
\leq \frac{1}{m s} \log \sum_{f \in \mathcal{F}_{S_m}} \mathbb{E}_{\sigma_{1:m}} \exp \left(s \sum_{i=1}^m \sigma_i r_i(f) \right) =
$$

$$
= \frac{1}{m s} \log \sum_{f \in \mathcal{F}_{S_m}} \prod_{i=1}^m \mathbb{E}_{\sigma_i} \left(e^{s \sigma_i r_i(f)}\right)
\leq
$$

$$
\leq \frac{1}{m s} \log \sum_{f \in \mathcal{F}_{S_m}} e^{\frac{m s^2}{2}}
= \frac{1}{m s} \log \left(|\mathcal{F}_{S_m}| e^{\frac{m s^2}{2}}\right) =
$$

$$
= \frac{1}{m s} \log |\mathcal{F}_{S_m}| + \frac{s}{2}.
$$

Эта оценка верна для любого $s > 0$. Минимизируем её по $s$. Легко видеть, что оптимальное $s$ равняется $\sqrt{(2 / m) \log \vert\mathcal{F}_{S_m}\vert}$; подставляя его, получаем:

$$
\color{#348FEA}{
\mathrm{Rad}_{\mathcal{D},m}(r \circ \mathcal{F}) \leq
\mathbb{E}_{S_m} \sqrt{\frac{2}{m} \log |\mathcal{F}_{S_m}|}.
\qquad (1)
}
$$

Определим **функцию роста** класса $\mathcal{F}$ как

$$\Pi_\mathcal{F}(m) = \max_{S_m} |\mathcal{F}_{S_m}|.$$

Эта функция показывает, сколько различных разметок класс функций $\mathcal{F}$ может породить на наборе данных, в зависимости от размера этого набора. Очевидно, что $\Pi_\mathcal{F}(m) \leq 2^m$ и монотонно не убывает.

Например, для линейной модели на $d$-мерном пространстве признаков $\Pi_\mathcal{F}(m) = 2^m$ при $m \leq d+1$ (любое подмножество $d+1$ точек в общем положении в $d$-мерном пространстве всегда можно отделить гиперплоскостью), но строго меньше этого числа при $m > d+1$ (например, если точки – углы квадрата на плоскости, его диагонали нельзя разделить прямой).

Когда $\vert\mathcal{F}_{S_m}\vert = 2^m$, будем говорить, что «$\mathcal{F}$ разделяет $S_m$».

Определим **размерность Вапника-Червоненкиса** (или **VC-размерность**) как максимальное $m$, при котором семейство $\mathcal{F}$ разделяет любой датасет $S_m$:

$$
\mathrm{VC}(\mathcal{F}) =
\max \{m\mid \Pi_\mathcal{F}(m) = 2^m\}.
$$

Таким образом, VC-размерность линейной модели равна $d+1$.

Следующая лемма даёт связь между размерностью Вапника-Червоненкиса и функцией роста:

**Лемма** (Sauer–Shelah, см. подробнее [здесь](https://en.wikipedia.org/wiki/Sauer%E2%80%93Shelah_lemma)) 

$$\Pi_\mathcal{F}(m) \leq \sum_{k=0}^{\mathrm{VC}(\mathcal{F})} \binom{m}{k}$$

Изучим асимптотическое поведение сложности Радемахера при $m\to\infty$. Обозначим $D = \mathrm{VC}(\mathcal{F})$. Для $m \leq D$ имеем $\Pi_\mathcal{F}(m) = 2^m$, а для $m > D$:

$$
\Pi_\mathcal{F}(m) \leq
\sum_{k=0}^{D} \binom{m}{k} \leq
\left(\frac{m}{D}\right)^{D} \sum_{k=0}^{D} \binom{m}{k} \left(\frac{D}{m}\right)^{k} \leq
$$

$$        
\leq
\left(\frac{m}{D}\right)^{D} \sum_{k=0}^{m} \binom{m}{k} \left(\frac{D}{m}\right)^{k} =
\left(\frac{m}{D}\right)^{D} \left(1 + \frac{D}{m}\right)^m \leq
\left(\frac{e m}{D}\right)^D.
$$

Подставляя это выражение в (1), получаем окончательную оценку на сложность Радемахера:

$$
\mathrm{Rad}_{\mathcal{D},m}(r \circ \mathcal{F}) \leq
\sqrt{\frac{2}{m} \mathrm{VC}(\mathcal{F}) \left(1 + \log m - \log(\mathrm{VC}(\mathcal{F}))\right)} = 
$$

$$
=\Theta_{m\to\infty}\left(\sqrt{\mathrm{VC}(\mathcal{F}) \frac{\log m}{m}}\right).
$$

Соответствующая оценка на истинный риск тогда примёт вид:

$$
\color{#348FEA}{R(\hat f_m) 
\leq \hat R_m(\hat f_m) + \sqrt{\frac{1}{2m} \log \frac{1}{\delta}} + \Theta_{m\to\infty}\left(\sqrt{\mathrm{VC}(\mathcal{F}) \frac{\log m}{m}}\right)
\quad
\text{с вероятностью $\geq 1 - \delta$ по $S_m$.}}
$$

Для того, чтобы эта оценка была осмыслена, необходимо гарантировать $\mathrm{VC}(\mathcal{F}) < m/(2\log m)$. Для линейных моделей, при условии $m \gg d$ (данных намного больше, чем признаков), оценки действительно получаются осмысленными. 

К сожалению, для нейронных сетей это подчас неверно. В работе [Nearly-tight VC-dimension and Pseudodimension Bounds for Piecewise Linear Neural Networks](https://arxiv.org/pdf/1703.02930.pdf) показано, что если $\mathcal{F}$ обозначает класс моделей, реализуемых полносвязной сетью ширины $n$ с $N$ параметрами, то $\mathrm{VC}(\mathcal{F}) = \Theta(n N)$. Таким образом, наша оценка на сложность Радемахера становится бесполезной в реалистичных сценариях, когда число весов сети $N$ много больше числа примеров в обучающей выборке $m$.

Если априори известно, что результат обучения лежит в некотором классе $\mathcal{F}_B$, то в оценке сложности Радемахера можно использовать именно этот класс, а не полный класс моделей $\mathcal{F}$. Очевидно, что сложность $\mathcal{F}_B$, лежащего в $\mathcal{F}$, не больше сложности $\mathcal{F}$. Так, в работе [Spectrally-normalized margin bounds for neural networks](https://arxiv.org/pdf/1706.08498.pdf) получены оценки для сложности полносвязной сети с липшицевыми функциями активации при условии, что нормы весов ограничены; см. также [полный конспект лекций](https://arxiv.org/pdf/2012.05760.pdf). В этом случае под $\mathcal{F}_B$ будем понимать класс сетей с весами нормы не больше $B$. Обозначим соответствующую оценку через $\mathcal{B}$:

$$
\sup_{f \in \mathcal{F}_B} (R(f) - \hat R_m(f))
\leq \mathcal{B}(B, \delta)
\quad
\text{с вероятностью $\geq 1-\delta$ по $S_m$.}
$$
    
К сожалению, нет гарантий, что градиентный спуск всегда сходится в решение с нормой меньше какого-то числа. Чтобы обойти это ограничение, используют следующую технику. Возьмём последовательность ограничений $B_j$, такую что 

$$\mathcal{F}_{B_j} \subset \mathcal{F}_{B_{j+1}}\text{ и }\bigcup_{j=1}^\infty \mathcal{F}_{B_j} = \mathcal{F}.$$

Также возьмём последовательность $\delta_j$, монотонно убывающую к нулю и суммирующуюся в $\delta$. Тогда для любого $j \geq 1$

$$
\sup_{f \in \mathcal{F}_{B_j}} (R(f) - \hat R_m(f))
\leq \mathcal{B}(B_j, \delta_j)
\quad
\text{с вероятностью $\geq 1-\delta_j$ по $S_m$.}
$$

А значит,

$$
\sup_{f \in \mathcal{F}_{B_j}} (R(f) - \hat R_m(f))
\leq \mathcal{B}(B_j, \delta_j) \quad \forall j \geq 1
\quad
\text{с вероятностью $\geq 1-\sum_{j=1}^ \infty \delta_j = 1-\delta$ по $S_m$.}
$$

Из этого следует, что

$$
R(\hat f_m) - \hat R_m(\hat f_m)
\leq \mathcal{B}(B_{\hat j_m}, \delta_{\hat j_m})
\quad
\text{с вероятностью $\geq 1-\delta$ по $S_m$,}
$$

где $\hat j_m$ – минимальное $j$, при котором $\hat f_m \in B_{j}$.

Такая техника используется, например, в работах [Spectrally-normalized margin bounds for neural networks](https://arxiv.org/pdf/1706.08498.pdf) и [A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks](https://arxiv.org/pdf/1707.09564.pdf).


## Фундаментальная проблема равномерных оценок

<figure>
  <img src="https://yastatic.net/s3/education-portal/media/uniform_bound_failure_example_738fe9b23c_45b4ff7033_74e9c67df1.svg" loading="lazy" decoding="async" alt="">
  <figcaption>
    <p>Пример модели (розовая кривая), имеющей малый истинный риск, но большой эмпирический на заданном наборе данных (кружочки). Данные одного класса лежат на желтом круге, другого – на голубом; оптимальная разделяющая поверхность обозначена пунктиром. Имея набор из кружочков, мы строим противоположный набор, обозначенный крестиками; заметим, что он мог прийти из того же распределения. Если алгоритм обучения старается отодвинуть границу классов как можно дальше от примеров, то результатом обучения на наборе крестиков может стать розовая кривая. Пример взят из работы <a href='https://arxiv.org/pdf/1902.04742.pdf'>Uniform convergence may be unable to explain generalization in deep learning</a>.</p>
  </figcaption>
</figure>

Напомним, что построение равномерных оценок проходило в несколько шагов:

1. Оценка супремумом
$$
R(\hat{f}_m)-\hat{R}_m(\hat{f}_m)\leq\sup_{f\in\mathcal{F}}(R(f)-\hat{R}_m(f))
$$

2. Применение неравенства макДайармида:

$$
\sup_{f \in \mathcal{F}}(R(f) - \hat R_m(f)) \leq
\color{#FFC100}{\mathbb{E}_{S'_m} \sup_{f \in \mathcal{F}}(R(f) - \hat R'_m(f))} + \sqrt{\frac{1}{2m} \log \frac{1}    {\delta}}
\quad
\text{с вероятностью $\geq 1 - \delta$ по $S_m$,}
$$


3. Оценка матожидания супремума (жёлтое слагаемое выше) с помощью симметризации с дальнейшим выходом на сложность Радемахера:

$$
\mathbb{E}_{S'_m} \sup_{f \in \mathcal{F}}(R(f) - \hat R'_m(f)) =
\mathbb{E}_{S'_m} \sup_{f \in \mathcal{F}}(\mathbb{E}_{S''_m} \hat R''_m(f) - \hat R'_m(f)) \leq
\mathbb{E}_{S'_m} \mathbb{E}_{S''_m} \sup_{f \in \mathcal{F}}(\hat R''_m(f) - \hat R'_m(f)).
$$

На каждом шаге предыдущая величина оценивается сверху, и потенциально каждое из этих неравенств может оказаться слишком слабым и привести к бессмысленной оценке. Давайте это проиллюстрируем.

Выше мы уже отмечали, что если класс $\mathcal{F}$ содержит модель, для которой $\hat R_m(f)$ мал, а $R(f)$ велик, то равномерная оценка становится бессмысленной. По этой причине, имеет смысл выбирать класс $\mathcal{F}$ как можно более маленьким. Самым лучшим из возможных классов мог бы быть класс моделей, к которым сходится наш алгоритм обучения с высокой вероятностью.

Рассмотрим случай, близкий к идеальному: тот, в котором существует $\epsilon > 0$, для которого при любых $f \in \mathcal{F}$ имеем $R(f) < \epsilon$. Иными словами, предположим, что все модели класса $\mathcal{F}$ хорошо обобщают. В этом случае оценка выше близка к идеальной:

$$
R(\hat f_m) - \hat R_m(\hat f_m) \leq
\epsilon
\quad
\text{с вероятностью $\geq 1 - \delta$ по $S_m$.}
$$

Но что будет, если мы начнём честно воспроизводить процесс построения равномерных оценок? После второго шага мы получаем оценку вида

$$
R(\hat f_m) - \hat R_m(\hat f_m) \leq
$$

$$
\sup_{f \in \mathcal{F}}(R(f) - \hat R_m(f)) \leq
\epsilon + \sqrt{\frac{1}{2m} \log \frac{1}{\delta}}
\quad
\text{с вероятностью $\geq 1 - \delta$ по $S_m$,}
$$

которая не сильно хуже предыдущей, но в которой всё равно появилось лишнее слагаемое. 

Но допустим, что мы хотим честно проделать третий шаг процедуры получения равномерных оценок. Для этого нам необходимо было оценить матожидание супремума, которое после симметризации получает вот такую верхнюю оценку:

$$
\mathbb{E}_{S'_m} \mathbb{E}_{S''_m} \sup_{f \in \mathcal{F}}(\hat R''_m(f) - \hat R'_m(f)).
$$

Таким образом, малость истинного риска не гарантирует малость эмпирического риска на любом наборе данных. Так, авторы статьи [Uniform convergence may be unable to explain generalization in deep learning](https://arxiv.org/pdf/1902.04742.pdf) предъявили пример, в котором для любого $S_m''$ существует модель $\tilde f_m \in \mathcal{F}$, такая что $\hat R''_m(\tilde f_m) \approx 1$, но при этом $\hat R_m(\tilde f_m)$ и $R(\tilde f_m)$ малы. Иллюстрация такой ситуации приведена в начале параграфа. Тогда $\sup_{f \in \mathcal{F}}(\hat R''_m(f) - \hat R'_m(f))$ велик, и оценки теряют смысл.

К счастью, даже эта фундаментальная проблема не ставит крест на равномерных оценках. Так, работы [Uniform Convergence of Interpolators: Gaussian Width, Norm Bounds, and Benign Overfitting](https://arxiv.org/pdf/2106.09276.pdf) и [Stability and Deviation Optimal Risk Bounds with Convergence Rate $O (1/n)$](https://arxiv.org/pdf/2103.12024.pdf) рассматривают равномерную оценку в классе интерполирующих моделей, то есть, имеющих нулевой эмпирический риск:

$$
R(\hat f_m) - \hat R_m(\hat f_m)
\leq \sup_{f \in \mathcal{F}: \, \hat R_m(f) = 0} R(f).
$$

Для таких моделей контрпример выше не работает.

  ## handbook

  Учебник по машинному обучению

  ## title

  Обобщающая способность – классическая теория

  ## description

  Обобщающая способность – классическая теория

- 
  ## path

  /handbook/ml/article/pac-bajesovskie-ocenki-riska

  ## content

  В предыдущем параграфе рассматривались равномерные оценки разницы истинного и эмпирического рисков. Если в рассматриваемом классе моделей есть «плохие», то равномерные оценки становятся слишком пессимистичными. Часто нельзя гарантировать, что что наш алгоритм обучения их никогда не выбирает, поэтому класс моделей $\mathcal{F}$ для равномерной оценки не получится сузить до класса только «хороших» моделей. Но можно надеяться, что плохие выучиваются не слишком часто. Например, известно, что градиентный спуск обычно сходится к хорошим моделям (об этом мы ещё поговорим в [параграфе про implicit bias](https://academy.yandex.ru/handbook/ml/article/implicit-bias)). В этом параграфе мы разберём элегантный способ учесть «предпочтения» алгоритма обучения в оценке разницы рисков.

Вспомним равномерную оценку для конечного $\mathcal{F}$:

$$
\mathbb{P}\left(\sup_{f\in\mathcal{F}} (R(f) - \hat R_m(f)) \geq \epsilon\right) =
\mathbb{P}\left(\exists f\in\mathcal{F}: \; (R(f) - \hat R_m(f)) \geq \epsilon\right) \leq$$

$$\leq
\sum_{f\in\mathcal{F}} \mathbb{P}(R(f) - \hat R_m(f) \geq \epsilon) \leq
\vert\mathcal{F}\vert e^{-2 m \epsilon^2}
\quad
\forall \epsilon > 0,
$$

где $\vert\mathcal{F}\vert$ – мощность класса $\mathcal{F}$. Эта оценка формально верна и для бесконечного $\mathcal{F}$, но смысл её теряется. Давайте попробуем исправить это.

Пусть $\mathcal{F}$ не более, чем счётно. Для каждого $f \in \mathcal{F}$ возьмём своё $\epsilon(f)$. Если взять $\epsilon(f)$ таким, чтобы $\sum_{f \in \mathcal{F}} e^{-2 m \epsilon^2(f)}$ было конечным, то приходим к осмысленной оценке:

$$
\mathbb{P}\left(\exists f\in\mathcal{F}: \; (R(f) - \hat R_m(f)) \geq \epsilon(f)\right) \leq
$$

$$
\sum_{f\in\mathcal{F}} \mathbb{P}\left(R(f) - \hat R_m(f) \geq \epsilon(f)\right) \leq
\sum_{f \in \mathcal{F}} e^{-2 m \epsilon^2(f)}. 
$$

Рассмотрим теперь некоторое вероятностное распределение $P(f)$ на $\mathcal{F}$. В качестве $\epsilon(f)$ возьмём

$$e^{-2 m \epsilon^2(f)} = P(f) e^{-2 m \tilde\epsilon^2},$$

где $\tilde\epsilon \in \mathbb{R}_+$. Из этого уравнения получаем следующее выражение для $\epsilon(f)$:

$$
\epsilon(f) =
\sqrt{\tilde\epsilon^2 + \frac{1}{2m} \log\frac{1}{P(f)}}.
$$

В итоге, для любого $\tilde\epsilon > 0$ получаем оценку:

$$
\mathbb{P}\left(\exists f\in\mathcal{F}: \; (R(f) - \hat R_m(f)) \geq \sqrt{\tilde\epsilon^2 + \frac{1}{2m} \log\frac{1}{P(f)}}\right) \leq
e^{-2 m \tilde\epsilon^2}.
$$

Или, что то же самое, с вероятностью $\geq 1 - \delta$ по $S_m$ для любого $f \in \mathcal{F}$:

$$
R(f)-\hat{R}_m(f)\leq\sqrt{\frac{1}{2m}\left(\log\frac{1}{\delta}+\log\frac{1}{P(f)}\right)}
$$

Заметим, что если $\mathcal{F}$ конечно, а $P(f)$ – равномерное распределение, то оценка выше совпадает с равномерной оценкой. Если же наш алгоритм обучения предпочитает выбирать модели, для которых $P(f)$ велико, то оценка улучшается по сравнению с равномерной. Таким образом, распределение $P(f)$ «кодирует» наши представления о предпочтениях алгоритма. Будем называть $P(f)$ «априорным распределением».

Как обобщить оценку выше на несчётные классы моделей? В первую очередь, предположим, что наш алгоритм обучения $\mathcal{A}$ стохастичен, а значит, на выходе даёт не одну модель, а распределение:

$$
\hat f_m \sim 
\hat Q_m =
\mathcal{A}(S_m).
$$

Будем называть это распределение «апостериорным». Такое рассуждение осмысленно, например, для стохастического градиентного спуска: очевидно, что результат его работы на невыпуклой функции потерь недетерминирован (он может сходиться в разные локальные минимумы).
    
Заметим, что главное отличие апостериорного распределения от априорного в том, что первое зависит от данных, а второе – нет. Важно понимать при этом, что, несмотря на названия, эти два распределения не связаны между собой никаким вариантом формулой Байеса. Сходство с байесовским подходом скорее внешнее. Поэтому слова «априорное» и «апостериорное» имеет смысл писать в кавычках, но для экономии места мы их будем в дальнейшем опускать.

Оценки разности рисков, о которых речь пойдёт ниже, называются **PAC-байесовскими** (**PAC-bayesian**, где PAC – probably approximately correct).

Сформулируем одну из классических оценок из этого класса:

**Теорема Макаллестера**. Пусть $\mathcal{F}$ – множество моделей и $P$ – распределение на $\mathcal{F}$. Тогда для любого $\delta \in (0,1)$ с вероятностью $\geq 1 - \delta$ по $S_m$ имеем:

$$
R(\hat Q_m) \leq
\hat R_m(\hat Q_m) + \sqrt{\frac{1}{2m-1} \left(\log \frac{4m}{\delta} + \mathrm{KL}(\hat Q_m\;\vert\vert P) \right)},
$$

где $R(Q) = \mathbb{E}_{f \sim Q} R(f)$ и $\hat R_m(Q) = \mathbb{E}_{f \sim Q} \hat R_m(f)$.

Видим, что оценка тем лучше, чем ближе апостериорное распределение к априорному. Здесь работает следующая интуиция. Если для большинства обучающих наборов данных апостериорное распределение близко к априорному, то оно почти не зависит от данных, а значит, истинный риск и риск на обучающей выборке должны быть близки с высокой вероятностью. Если же апостериорное зависит от данных сильно, то, скорее всего, модель сильно переобучается, а значит, оценка не может быть хорошей; в нашем случае она велика из-за большой KL-дивергенции.

Для доказательства теоремы нам понадобятся две леммы:

**Лемма 1**. Для любого распределения $P$ на $\mathcal{F}$ и для любого $\delta \in (0,1)$ с вероятностью $\geq 1 - \delta$ по $S_m$ имеем:

$$
\mathbb{E}_{f \sim P} e^{(2m-1) (\Delta_m(f))^2} \leq
\frac{4m}{\delta},
$$

где $\Delta_m(f) = \vert R(f) - \hat R_m(f)\vert$.

**Лемма 2 (лемма Донскера-Вередана, Donsker-Varadhan)**. Пусть $P$ и $Q$ – вероятностные распределения на множестве $X$. Тогда для любого $h: \; X \to \mathbb{R}$


$$
\mathbb{E}_{x\sim Q} h(x) \leq
\log\mathbb{E}_{x \sim P} e^{h(x)} + \mathrm{KL}(Q\vert\vert P).
$$

{% cut "Доказательство теоремы" %}

Применим лемму 2 к $X = \mathcal{F}$, $h = (2m-1) \Delta_m^2$ и $Q = \hat Q_m$:

  
$$
        \mathbb{E}_{f \sim \hat Q_m} (2m-1) (\Delta_m(f))^2 \leq
        \log\mathbb{E}_{f \sim P} e^{(2m-1) (\Delta_m(f))^2} + \mathrm{KL}(\hat Q_m\;\vert\vert\;P).
$$
  

Тогда по лемме 1 с вероятностью $\geq 1 - \delta$ по $S_m$ имеем:

$$
        \mathbb{E}_{f \sim \hat Q_m} (2m-1) (\Delta_m(f))^2 \leq
        \log\frac{4m}{\delta} + \mathrm{KL}(\hat Q_m\;\vert\vert\;P).
$$

Доказательство, таким образом, легко завершается:

$$
        R(\hat Q_m) - \hat R_m(\hat Q_m) \leq
        \vert\mathbb{E}_{f\sim \hat Q_m} (R(f) - \hat R_m(f))\vert \leq
$$


$$
\leq\mathbb{E}_{f\sim \hat Q_m} |R(f) - \hat R_m(f)| =
\mathbb{E}_{f\sim \hat Q_m} \Delta_m(f) \leq
$$


$$
        \leq\sqrt{\mathbb{E}_{f\sim \hat Q_m} (\Delta_m(f))^2} \leq
$$

$$
        \sqrt{\frac{1}{2m-1} \left(\log \frac{4m}{\delta} + \mathrm{KL}(\hat Q_m\;\vert\vert\;P) \right)}.
$$

{% endcut %}

{% cut "Доказательство леммы 2" %}

Мы рассмотрим лишь простой случай, когда и у $P$, и у $Q$ есть плотности, и они нигде не обращаются в ноль.

  
$$
        \mathbb{E}_{x \sim Q} h(x) - \mathrm{KL}(Q\vert\vert P) =
        \mathbb{E}_{x \sim Q} \left(h(x) - \log\left(\frac{q(x)}{p(x)}\right)\right) =
$$



$$
        =\mathbb{E}_{x \sim Q} \log\left(e^{h(x)} \frac{p(x)}{q(x)}\right) \leq
        \log \mathbb{E}_{x \sim Q} \left(e^{h(x)} \frac{p(x)}{q(x)}\right) =
        \log \mathbb{E}_{x \sim P} e^{h(x)}.
$$
  
{% endcut %}

{% cut "Доказательство леммы 1" %}

Нам понадобится

**Неравенство Маркова**. Пусть $X$ – неотрицательная случайная величина. Тогда для любого $ a > 0$ имеем

$$
        \mathbb{P}(X \geq a) \leq
        \frac{\mathbb{E} X}{a}.
$$


В качестве $X$ и $a$ из неравенства Маркова возьмём 

$$X = \mathbb{E}_{f \sim P} e^{(2m-1) (\Delta_m(f))^2},\qquad a = 4m / \delta$$

Тогда 

$$
\mathbb{P}\left(\vphantom{\frac14}\mathbb{E}_{f \sim P} e^{(2m-1) (\Delta_m(f))^2} \leq
        \frac{4m}{\delta}\right) \leq
$$


$$
\leq\frac{\delta}{4m}\mathbb{E}_{S_m} \mathbb{E}_{f \sim P} e^{(2m-1) (\Delta_m(f))^2}
$$


Значит, нам достаточно доказать, что


$$
        \mathbb{E}_{S_m} \mathbb{E}_{f \sim P} e^{(2m-1) (\Delta_m(f))^2} \leq
        4m.
$$


Мы докажем даже более сильное соотношение:

$$
\mathbb{E}_{S_m}e^{(2m-1)(\Delta_m(f))^2}\leq4m\quad\forall f\in\mathcal{F}
$$

Заметим, что из неравенства Хёффдинга будет следовать

$$
\mathbb{P}_{S_m}(\Delta_m(f)\geq\epsilon)\leq2e^{-2m\epsilon^2}\quad\forall\epsilon>0\quad\forall f\in\mathcal{F}
$$

Для простоты предположим, что распределение $\Delta_m(f)$ имеет плотность для любого $f \in \mathcal{F}$; обозначим её $p_f(\Delta)$. В этом случае мы можем ограничить матожидания по $S_m$ напрямую:

$$
        \mathbb{E}_{S_m} e^{(2m-1) (\Delta_m(f))^2} =
        \int_0^\infty e^{(2m-1) \epsilon^2} p_f(\epsilon) \, d\epsilon =
$$

$$
        \int_0^\infty e^{(2m-1) \epsilon^2} \frac{d}{d\epsilon} \left(-\int_\epsilon^\infty p_f(\Delta) \, d\Delta\right) \, d\epsilon =
$$


$$
        =
        \left.-\left(e^{(2m-1) \epsilon^2} \int_\epsilon^\infty p_f(\Delta) \, d\Delta\right) \right|_{\epsilon=0}^\infty +
        2 (2m-1) \int_0^\infty \epsilon\,e^{(2m-1) \epsilon^2} \int_\epsilon^\infty p_f(\Delta) \, d\Delta \, d\epsilon \leq
$$


$$
        \leq
        \int_0^\infty p_f(\Delta) \, d\Delta +
        2 (2m-1) \int_0^\infty \epsilon\,e^{(2m-1) \epsilon^2} \int_\epsilon^\infty p_f(\Delta) \, d\Delta \, d\epsilon \leq
$$

$$          
        \leq
        2 + 4 (2m-1) \int_0^\infty \epsilon\,e^{(2m-1) \epsilon^2} e^{-2m \epsilon^2} \, d\epsilon =
$$

$$
        2 + 4 (2m-1) \int_0^\infty \epsilon\,e^{-\epsilon^2} \, d\epsilon =
        2 + 2 (2m-1) =
        4m.
$$

В общем случае, мы не можем предполагать наличие плотности у $\Delta_m(f)$. Доказательство в этом случае можно найти в оригинальной работе D. A. McAllester [Some pac-bayesian theorems](https://link.springer.com/content/pdf/10.1023%2FA%3A1007618624809.pdf), а также в [конспекте лекций](https://arxiv.org/pdf/2012.05760.pdf) автора этого параграфа.

{% endcut %}  

Теорема Макаллестера – не единственная из возможных пак-байесовских оценок. Например, несколько улучшенную версию той же оценки можно найти в работе [Bounds for averaging classifiers](https://www.cs.cmu.edu/~jcl/papers/averaging/averaging_tech.pdf). Другие оценки подобного типа можно найти в монографии [PAC-Bayesian supervised classification: the thermodynamics of statistical learning](https://arxiv.org/pdf/0712.0248.pdf).

## Применение пак-байесовских оценок к детерминированным алгоритмам обучения

Выше были рассмотрены две PAC-байесовские оценки: одна для не более, чем счётного множества моделей, другая – для произвольного. За возможность использования несчётных классов моделей мы заплатили тем, что алгоритм обучения должен быть недетерминированным (для детерминированных алгоритмов KL-дивергенция в Теореме Макаллестера может вырождаться в бесконечность; например, это так, если априорное распределение гауссово). Чаще всего класс моделей $\mathcal{F}$ всё-таки несчетён: например, если это класс всех сетей фиксированной архитектуры, то он индексируется весами, которых несчётное множество. При этом, хотя используемый алгоритм обучения и в самом деле недетерминирован (стохастический градиентный спуск зависит от случайного выбора батчей и от инициализации весов) и теорема Макаллестера выполняется, финальное распределение моделей очень сложно охарактеризовать, и из-за этого непонятно, как считать KL-дивергенцию.

Предположим, что алгоритм обучения всё-таки детерминирован; этого можно добиться, зафиксировав сид генератора случайных чисел при обучении. Как получить осмысленную PAC-байесовскую оценку для детерминированного алгоритма на несчётном множестве моделей?

Мы рассмотрим **два способа**.
    
Первый способ – добавить известный шум в финальную модель, выданную детерминированным алгоритмом. Так, для нейронных сетей, результатом работы алгоритма обучения является набор весов. Если добавить в этот набор гауссовский шум, а также в качестве априорного распределения взять гауссовское, то KL-дивергенцию в теореме Макаллестера можно будет посчитать аналитически.

Дисперсию шума в апостериорном распределении тоже можно обучить с помощью градиентного спуска одновременно с весами, тем самым минимизируя правую часть оценки из вышеупомянутой теоремы.   Если в найденную модель удастся добавить шум так, чтобы KL-дивергенция значительно уменьшилась, но при этом риск на обучающей выборке не сильно вырос, то оценка на истинный риск получится хорошей. 

Это рассуждение связывает PAC-байесовские оценки и гипотезу о том, что «плоские» («широкие») минимумы хорошо обобщают. В самом деле, если минимум «плоский», то в модель из него можно добавить много шума, не испортив качество на обучении. Оценки, основанные на этом принципе, можно найти в работах [Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data](https://arxiv.org/pdf/1703.11008.pdf) и [A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks](https://arxiv.org/pdf/1707.09564.pdf).

Второй способ состоит в том, чтобы взять дискретное кодирование $c$ и применить дискретную PAC-байесовскую оценку к закодированной модели вместо оригинальной. Обозначим закодированную модель $f$ через $f_c$. Следуя работе [Non-vacuous Generalization Bounds at the ImageNet Scale: a PAC-Bayesian Compression Approach](https://arxiv.org/pdf/1804.05862.pdf), возьмём априорное распределение с массой, убывающей с ростом длины кода:

$$
P_c(f_c) =
\frac{1}{Z} m(\vert f_c\vert ) 2^{-|f_c|}.
$$


Здесь $\vert f_c\vert $ – длина кода модели $f$, $m(k)$ – некоторое вероятностное распределение на $\mathbb{N}$, а $Z$ – нормализующая константа. Тогда KL-дивергенция примет следующий вид:

$$
\mathrm{KL}(\delta_{f_c}\vert\vert {P_c}) =
\log Z + \vert f_c\vert  \log 2 - \log(m(\vert f_c\vert )).
$$

Для того, чтобы KL-дивергенция выше была как можно меньше, необходимо, чтобы наш алгоритм обучения на реалистичных данных сходился в модели с маленькой длиной кода. Для этого будем применять наше кодирование не к оригинальной модели, а к сжатой с помощью некоторого алгоритма сжатия. Здесь мы предполагаем, что модели, к которым сходится наш алгоритм обучения, можно сжать с малыми потерями до моделей с малой длиной кода. Другими словами, мы опираемся на предположение, что обученные модели в некоторым смысле «простые».

Если модель параметризована весами $\theta$, типичный алгоритм сжатия выдаст набор $(S,Q,C)$, где


* $S = s_{1:k} \subset [\dim\theta]$ – позиции ненулевых весов;
* $C = c_{1:r} \subset \mathbb{R}$ – «словарь» весов;
* $Q = q_{1:k}$, $q_i \in [r]$ $\forall i \in [k]$ – квантизованные значения весов.

Выход алгоритма будет выглядеть как $\mathcal{C}(\theta)_i = c_{q_j}$, если $i = s_j$, иначе $0$.

Тогда наивное 32-битное кодирование даст следующую длину:

$$
\vert\mathcal{C}(\theta)\vert_c =
\vert S\vert_c + \vert Q\vert_c + \vert C\vert_c \leq
k (\log\dim\theta + \log r) + 32 r.
$$

В работе [Non-vacuous Generalization Bounds at the ImageNet Scale: a PAC-Bayesian Compression Approach](https://arxiv.org/pdf/1804.05862.pdf) описанный выше способ применяется к модели MobileNet (свёрточной сети, сконструированной специально для мобильных устройств), обученной на наборе данных ImageNet, и получают верхнюю оценку на истинный риск, равную $96.5\%$ (риск случайного угадывания – $99.9\%$). Хотя такой результат и выглядит очень скромным, но это первая осмысленная оценка обобщающей способности реально используемой нейронной сети на реалистичном наборе данных.

  ## handbook

  Учебник по машинному обучению

  ## title

  PAC-байесовские оценки риска

  ## description

  PAC-байесовские оценки риска

- 
  ## path

  /handbook/ml/article/seti-beskonechnoj-shiriny

  ## content

  Во введении обсуждалось, что истинный риск нейронной сети выходит на асимптоту при стремлении ширины сети (то есть числа нейронов в слое) к бесконечности. Это намекает нам на то, что существует предельная модель, «бесконечно широкая сеть». В этом параграфе мы обсудим подходы к анализу её поведения.

Динамика обучения нейронной сети описывается эволюцией в пространстве весов – например, правилом обновления весов в градиентном спуске. Но в каком виде можно записать эволюцию бесконечно широкой сети, в которой весов бесконечно много? Есть два способа это сделать.

### Первый способ – ввести меру в пространстве весов

В качестве примера рассмотрим нейронную сеть с одним скрытым слоем, скалярным выходом и скалярным входом:

$$
f(x) = \frac{1}{n} \sum_{i=1}^n a_i \phi(w_i x).
$$

Это выражение можно представить в виде $f(x) = \int_{\mathbb{R}^2} a \phi(w x) \, d\mu_n(a,w)$, где мера $\mu_n$ на $\mathbb{R}^2$ сосредоточена в весах, ассоциированных с каждым из нейронов скрытого слоя:

$$
\mu_n(a,w) = \frac{1}{n} \sum_{i=1}^n \delta_{a_i}(a) \delta_{w_i}(w).
$$

Здесь $\delta_x$ – мера, сосредоточенная в $x$.
    
При стремлении ширины $n$ к бесконечности $\mu_n$ может иметь предел. Так, если все веса насемплированы независимо из стандартного нормального распределения $\mathcal{N}(0,1)$, предельная мера принимает вид двумерного стандартного нормального распределения $\mathcal{N}(0,I_{2\times 2})$, а предсказание предельной сети можно записать в виде $f(x) = \mathbb{E}_{a,w \sim \mathcal{N}(0,1)} a \phi(w x)$.

Заметим, что множитель $1/n$ в определении модели выше принципиально важен для того, чтобы предельная мера и представление предельной сети в виде интеграла по мере были определены. Такая параметризация носит название **mean-field parameterization**.

Динамику эволюции весов также можно представить в виде эволюции меры. В самом деле, в случае конечной ширины градиентный спуск говорит нам о том, как за один шаг оптимизации меняются веса, ассоциированные с каждым из нейронов, или, что то же самое, как меняется мера $\mu_n$. Заменив в этом выражении меру $\mu_n$ на предельную, можно получить эволюцию предельной меры.

К сожалению, представление эволюции предельной сети в виде эволюции меры не даёт сказать много о свойствах предельной модели. Так, известно, что предельная модель всегда сходится в глобальный минимум на обучающей выборке, см статью [On the global convergence of gradient descent for over-parameterized models using optimal transport](https://arxiv.org/pdf/1805.09545.pdf), но мало что известно о её обобщающей способности.

Более того, лишь сети с одним скрытым слоем допускают простую формулировку в форме эволюции меры в пределе бесконечной ширины, см. статьи [Mean field analysis of neural networks: A central limit theorem](https://arxiv.org/pdf/1808.09372.pdf), [On the global convergence of gradient descent for over-parameterized models using optimal transport](https://arxiv.org/pdf/1805.09545.pdf) и [Trainability and accuracy of neural networks: An interacting particle system approach](https://arxiv.org/pdf/1805.00915.pdf). Для сетей с большим числом слоёв подобная формулировка также возможна, см. статьи [A mean-field limit for certain deep neural networks](https://arxiv.org/pdf/1906.00193.pdf) и [A rigorous framework for the mean field limit of multilayer neural networks](https://arxiv.org/pdf/2001.11443.pdf), но анализ усложняется.

Так, сходимость в глобальный минимум для сети с двумя скрытыми слоями была доказана лишь совсем недавно в работе [Global convergence of three-layer neural networks in the mean field regime](https://arxiv.org/pdf/2105.05228.pdf); для более глубоких сетей подобные результаты пока неизвестны.

### Второй способ – вместо эволюции весов рассматривать эволюцию предсказаний модели в каждой точке $x$

Для простоты рассмотрим задачу минимизации квадратичной функции потерь на наборе данных $(\vec x, \vec y)$ размера $m$: 

$$\frac{1}{2} \sum_{j=1}^m (y_j - f(x_j; \theta))^2 \to \min_\theta.$$

Будем оптимизировать эту функцию потерь градиентным спуском с шагом $\eta$:

$$
\theta_{k+1} - \theta_k
= -\eta \nabla_\theta\left(\frac{1}{2} \sum_{j=1}^m (y_j - f(x_j; \theta_k))^2\right)
=
$$

$$
=\eta \sum_{j=1}^m (y_j - f(x_j; \theta_t)) \nabla_\theta f(x_j; \theta_k).
$$

Ниже нам будет удобнее рассматривать градиентный спуск с непрерывным временем вместо дискретного:

$$
\dot\theta_t
= \sum_{j=1}^m (y_j - f(x_j; \theta_t)) \nabla_\theta f(x_j; \theta_t).
$$

Переход к непрерывному времени соответствует устремлению к нулю шага $\eta$, если при этом число шагов растёт как $k = [t / \eta]$, где округление применяется в любую сторону.
    
Обозначим через $f_t(x)$ предсказание в точке $x$ модели в момент времени $t$. Оно зависит от времени следующим образом:

$$
\dot f_t(x)
= \dot\theta_t^T \nabla f_t(x)
= \sum_{j=1}^m (y_j - f_t(x_j)) \nabla_\theta^T f_t(x_j) \nabla_\theta f_t(x).
$$

Введём обозначение:

$$
\color{#348FEA}{\hat\Theta_t(x,x')
= \nabla_\theta^T f_t(x) \nabla_\theta f_t(x').}
$$

С помощью него уравнение выше можно записать более коротко:

$$
\dot f_t(x)
= \hat\Theta_t(x,\vec x) (\vec y - f_t(\vec x)).
\quad(1)
$$

Здесь и дальше мы будем считать, что $\hat\Theta_t(x,\vec x)$ имеет размерность $1 \times m$.

Функция $\hat\Theta_t(x,x')$ называется **эмпирическим нейрокасательным ядром** (**Neural Tangent Kernel**, **NTK**); подробнее о ядрах мы поговорим ниже в параграфе про ядровые методы.

Заметим, что в уравнении $(1)$ вся информация о весах содержится в ядре, которое является отображением из $\mathbb{X} \times \mathbb{X}$ в $\mathbb{R}$. Как мы увидим ниже, при определённых условиях, при стремлении ширины сети к бесконечности ядро имеет предел и он не зависит от $t$.

Обозначив этот предел через $\Theta$, мы приходим к следующему виду эволюции предсказаний бесконечно широкой сети:

$$
\dot f_t(x)
= \Theta(x,\vec x) (\vec y - f_t(\vec x)).\quad(2)
$$

Здесь и далее будем называть $\Theta$ (не эмпирическим) **нейрокасательным ядром** или **NTK**. Такой термин был введён в оригинальной работе [Neural tangent kernel: Convergence and generalization in neural networks](https://arxiv.org/pdf/1806.07572.pdf).

В этом случае динамика предсказаний интегрируется следующим образом. На обучающей выборке

$$
\dot f_t(\vec x)
= \Theta(\vec x,\vec x) (\vec y - f_t(\vec x)),
$$

что даёт

$$
f_t(\vec x)
= f_0(\vec x) - \left(I - e^{-\Theta(\vec x, \vec x) t}\right) (f_0(\vec x) - \vec y).
$$

Подставляя решение в $(2)$, получаем

$$
\dot f_t(x)
= \Theta(x,\vec x) e^{-\Theta(\vec x, \vec x) t} (\vec y - f_0(\vec x)),
$$

и, наконец,

$$
f_t(x) 
= f_0(x) - \Theta(x, \vec x) \Theta^{-1}(\vec x, \vec x) \left(I - e^{-\Theta(\vec x, \vec x) t}\right) (f_0(\vec x) - \vec y).\quad(3)
$$

Прежде, чем доказывать сходимость ядра, мы обсудим, как может применяться предельное ядро и представление эволюции предсказаний в форме (1).


## Применение NTK-анализа

### NTK как математический аппарат

Нам удалось проинтегрировать динамику предсказаний в явном виде. Что это даёт?

Во-первых, мы получаем достаточное условие на сходимость в глобальный минимум на обучающей выборке. Таким условием является положительная определённость матрицы Грама ядра: $\Theta(\vec x,\vec x) \geq \lambda_0$ для некоторого $\lambda_0 > 0$.

В самом деле, в этом случае,

$$
\frac{d}{dt}\left(\frac{1}{2} \| \vec y - f_t(\vec x) \|_2^2\right)
= -(\vec y - f_t(\vec x))^T \Theta(\vec x, \vec x) (\vec y - f_t(\vec x))
\leq -\lambda_0 \| \vec y - f_t(\vec x) \|_2^2,
$$

что даёт

$$
\| \vec y - f_t(\vec x) \|_2^2
\leq e^{-\lambda_0 t} \| \vec y - f_0(\vec x) \|_2^2
\to 0 \quad \text{при $t \to \infty$}.
$$

Во-вторых, раз явное решение известно, можно написать оценку на обобщающую способность.
    
Оба этих результата опираются на то, что ядро постоянно. Как мы покажем ниже, постоянство нейрокасательного ядра нейронной сети можно гарантировать лишь в пределе бесконечной ширины. Тем не менее, если сеть конечна, но достаточно широка, можно показать, что её ядро достаточно близко к предельному, и оценки сохраняют силу.

Например, для обоснования сходимости в глобальный минимум достаточно показать, что наименьшее собственное значение эмпирического ядра с высокой вероятностью остаётся отделённым от нуля в течение обучения: $\hat\Theta_t \geq \lambda_0/2$ $\forall t \geq 0$ с вероятностью $\geq 1-\delta$ для $n \geq n^*(\delta)$. В самом деле, из этого следует, что

$$
\frac{d}{dt}\left(\frac{1}{2} \| \vec y - f_t(\vec x) \|_2^2\right)
= -(\vec y - f_t(\vec x))^T \hat\Theta_t(\vec x, \vec x) (\vec y - f_t(\vec x))
\leq -\frac{\lambda_0}2 \| \vec y - f_t(\vec x) \|_2^2,
$$

а значит,

$$
\| \vec y - f_t(\vec x) \|_2^2
\leq e^{-\lambda_0 t / 2} \| \vec y - f_0(\vec x) \|_2^2
\to 0 \quad \text{при $t \to \infty$}.
$$

Формальное доказательство вы можете найти в работе [Gradient Descent Provably Optimizes Over-parameterized Neural Networks](https://arxiv.org/pdf/1810.02054.pdf), а также в [конспекте лекций](https://arxiv.org/pdf/2012.05760.pdf) автора этого параграфа.

Вот ещё несколько результатов, полученных в этом направлении:

* улучшенные оценки на минимальную ширину в работе [Quadratic suffices for over-parametrization via matrix chernoff bound](https://arxiv.org/pdf/1906.03593v1.pdf); 
* оценки для случая глубоких сетей в работе [Gradient descent finds global minima of deep neural networks](https://arxiv.org/pdf/1811.03804.pdf); 
* оценки на обобщающую способность, полученные через близость ядра к предельному, в работе [Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks](https://arxiv.org/pdf/1901.08584.pdf).


### Определение патологий обучения

Как мы увидим позже, NTK реальных, стандартно параметризованных, имеющих конечную ширину сетей может меняться за время обучения существенным образом: см. эмпирическую работу [Deep learning versus kernel learning](https://arxiv.org/pdf/2010.15110.pdf) и теоретический анализ для сетей с одним скрытым слоем [Dynamically Stable Infinite-Width Limits of Neural Classifiers](https://arxiv.org/pdf/2006.06574.pdf).

Тем не менее, ядро в инициализации может выявить определённые патологии соответствующей нейронной сети. Рассмотрим один из примеров применения.

В некоторых состоящих из однородных блоков архитектурах (скажем, ResNet) можно увеличивать (и даже устремлять к бесконечности) число слоёв или блоков, и логично задаться вопросом о том, как при этом будет вести себя процесс обучения. 

Необходимым условием обучаемости является хороший первый шаг обучения. Если он исчезающе мал, то сеть не обучится ни на первом, ни на каком-либо другом шаге. Если он слишком велик, то обучение разойдётся на первом же шаге. Как мы увидим ниже, индикатором проблем является плохая обусловленность NTK в инициализации. Например, его собственные значения могут с ростом глубины стремиться к нулю или, наоборот, к бесконечности. В первом случае какие-то из компонент выборки никогда не выучатся, во втором обучение невозможно ни при каком конечном темпе обучения.

Чтобы в этом убедиться, рассмотрим разложение матрицы Грама ядра по собственным векторами: 

$$\Theta(\vec x, \vec x) = \sum_{j=1}^m \lambda_j \vec v_j \vec v_j^T,$$

где $\lambda_1 \geq \ldots \geq \lambda_m \geq 0$, а векторы $\vec v_1,\ldots,\vec v_m$ образуют ортонормированный базис. Разложим предсказание сети по этому базису: $f_t(\vec x) = \sum_{j=1}^m u_{t,j} \vec v_j$. Так как базис ортонормированный, каждая из компонент эволюционирует независимо от других. В самом деле, для дискретного градиентного спуска с шагом $\eta$ имеем

$$
u_{k+1,j}
= u_{k,j} + \eta \lambda_j (\vec v_j^T \vec y - u_{t,j}).
$$

Таким образом, если $\lambda_j=0$, то $u_{t,j}$ никогда не сойдётся к $\vec v_k^T \vec y$.

Кроме того, для того, чтобы процесс сходился, шаг $\eta$ должен убывать обратно пропорционально наибольшему собственному числу $\lambda_1$. Если последнее стремится к бесконечности, то $\eta$ стремится к нулю, а значит, $\eta \lambda_j$ будем мало для всех $j$, для которых $\lambda_j$ конечен; соответствующие компоненты также никогда не сойдутся.

Подробности см. в работе [Rapid training of deep neural networks without skip connections or normalization layers using Deep Kernel Shaping](https://arxiv.org/pdf/2110.01765.pdf), а также в более ранних работах [Exponential expressivity in deep neural networks through transient chaos](https://arxiv.org/pdf/1606.05340.pdf), [Deep information propagation](https://arxiv.org/pdf/1611.01232.pdf), [Resurrecting the sigmoid in deep learning through dynamical isometry](https://arxiv.org/pdf/1711.04735.pdf), [Dynamical isometry and a mean field theory of cnns](https://arxiv.org/pdf/1806.05393.pdf), в которых использовалась похожая идея, но не использовалось понятие NTK явно. См. также главу про инициализацию в [конспекте лекций](https://arxiv.org/pdf/2012.05760.pdf).


## NTK и Ядровые методы

Предельное NTK нейронной сети можно использовать в любом ядровом методе, например, в SVM. Обсудим это поподробнее и заодно разберёмся, почему NTK вообще называют ядром.

Рассмотрим задачу линейной регрессии:

$$
\hat\theta_\lambda
= \text{argmin}_{\theta \in \mathbb{R}^d} \sum_{j=1}^m \mathcal{L}(y_j, \theta^T x_j) + \lambda \| \theta \|_2^2\quad(4)
$$

Эту же задачу можно эквивалентно переписать следующим образом:

$$
\hat f_\lambda
= \text{argmin}_{f \in \mathcal{H}} \sum_{j=1}^m \mathcal{L}(y_j, f(x_j)) + \lambda \| f \|_\mathcal{H}^2,\quad(5)
$$

где $\mathcal{H}$ – пространство линейных отображений $f_{\theta}(x) = \theta^T x$ с некоторой нормой $\| f \|_\mathcal{H}$ на нём. 

Сделаем линейное пространство $\mathcal{H}$ евклидовым, введя на нём следующее скалярное произведение. Для $f(x) = \theta^T x$ и $\tilde f(x) = \tilde\theta^T x$ определим

$$\langle f, \tilde f \rangle = \theta^T \tilde\theta.$$

Это скалярное произведение порождает норму $\| f \|_\mathcal{H} = \| \theta \|_2$, что и делает формулировку (5) эквивалентной формулировке (4).

Пространство линейных моделей слишком узко, однако ничто не мешает нам рассмотреть задачу вида (5), в которой $\mathcal{H}$ будет произвольным нормированным пространством функций. Наиболее хорошо изучен случай, когда функции из $\mathcal{H}$ являются линейными моделями в некотором (возможно, бесконечномерном) гильбертовом пространстве признаков: $f(x) = \langle \Phi(x), \theta \rangle$, где $\Phi$ отображает $x$ в это пространство. Если последнее всё же конечномерно, то мы можем использовать матричную запись $f(x) = \theta^T \Phi(x)$; элементы $\theta$ в этой записи обычно называют первичными переменными (primal variables).

Пространство функций $\mathcal{H}$ также оказывается гильбертовым: соответствующее скалярное произведение имеет вид 

$$\langle f_{\theta}, f_{\theta'} \rangle_\mathcal{H} = \langle \theta, \theta' \rangle$$


Таким образом, $\| f_{\theta} \|_\mathcal{H}^2 = \langle f_{\theta}, f_{\theta} \rangle_\mathcal{H} = \langle \theta, \theta \rangle$, если $f_{\theta}(x) = \langle \Phi(x), \theta \rangle$.


Любому отображению $\Phi$ можно сопоставить симметричную положительно-определённую функцию $K(x,x') = \langle\Phi^T(x), \Phi(x')\rangle$; функции такого вида называются ядрами.

В силу [фундаментальной теоремы о представителе](https://en.wikipedia.org/wiki/Representer_theorem) любое решение задачи (4) принимает вид 

$$f(x) = \sum_{j=1}^m \alpha_j K(x,x_j) = K(x, \vec x) \vec\alpha.$$

В отличие от $\theta$, вектор $\alpha$ всегда конечномерен: его размерность равна размеру обучающей выборки. Элементы $\alpha$ называют двойственными (dual) переменными.
    
Упомянутый результат позволяет перейти от минимизации $f$ в бесконечномерном пространстве функций (или, что то же самое, минимизации $\theta$ в бесконечномерном пространстве признаков), к минимизации в конечномерном пространстве двойственных переменных:

$$
\vec\alpha
= \text{argmin}_{\vec\alpha \in \mathbb{R}^m} \sum_{j=1}^m \mathcal{L}\left(y_j, K(x_j, \vec x) \vec\alpha\right) + \lambda \vec\alpha^T K(\vec x, \vec x) \vec\alpha.\quad(6)
$$

Если в качестве функции потерь взять квадратичную $\mathcal{L}(y,z) = \frac{1}{2} (y-z)^2$, то получим ядровую регрессию; если же взять hinge loss $\mathcal{L}(y,z) = [1- yz]_+$, то SVM.
    
Заметим, что двойственная задача полностью сформулирована в терминах ядра $K$: отображение в потенциально бесконечное пространство признаков $\Phi$ более нигде не возникает. Поэтому мы можем использовать в качестве $K$ любую симметричную положительно определённую функцию двух переменных, не думая о том, для какого пространства признаков оно будет ядром (есть теорема, что такие функции всегда являются ядрами). Это может быть очень полезно. Так, если для эмпирического NTK в инициализации $\hat\Theta_0(x,x')$ имеем $\Phi(x) = \nabla_\theta f(x; \theta_0)$, но совершенно неочевидно, какое отображение $\Phi$ соответствует предельному NTK: $\Theta(x,x') = \lim_{n \to \infty} \hat\Theta_0(x,x')$. 
    
Таким образом, мы можем использовать $\Theta$ в качестве ядра $K$ в двойственной задаче (6) наряду с линейным $K(x,x') = x^T x'$ или гауссовским ядром $K(x,x') = e^{-\frac{1}{2\sigma^2} \|x-y\|_2^2}$. Такой подход привлекателен тем, что обучение ядровых методов более устойчиво и имеет меньше гиперпараметров. При этом можно надеяться, что результат обучения ядрового метода с NTK в качестве ядра будет близок к результату обучения соответствующей нейронной сети.

Основная проблема ядровых методов в том, что они требуют вычисления матрицы Грама ядра на обучающем наборе данных $K(\vec x, \vec x)$. Её размер $m \times m$ (где $m$ – размер выборки), так что применение ядровых методов на больших данных сильно усложняется. Более того, наивное вычисление динамики $f_t$ из формулы (3) требует обращения матрицы Грама, которое занимает $O(m^3)$ времени. 

Тем не менее, определённые оптимизации существуют. Так например, в работе [Kernel methods through the roof](https://arxiv.org/pdf/2006.10350.pdf) предлагается способ приближённого вычисления ($f_t$) за $O(m^{3/2} \log m)$ памяти и времени. Другие подходы см. в работах [Fast Finite Width Neural Tangent Kernel](https://arxiv.org/pdf/1806.07572.pdf) и [Neural tangents: Fast and easy infinite neural networks in python](https://arxiv.org/pdf/1912.02803.pdf).
    
Так или иначе, на малых наборах данных выражение (3) можно вычислить точно, см. результаты в работе [Harnessing the power of infinitely wide deep nets on small-data tasks](https://arxiv.org/pdf/1910.01663.pdf). Существуют также примеры задач, в которых матрицу Грама ядра достаточно посчитать только для малых $m$, см., например, [Simple, Fast, and Flexible Framework for Matrix Completion with Infinite Width Neural Networks](https://arxiv.org/pdf/2108.00131.pdf).

Ещё одна проблема использования NTK в ядровых методах состоит в том, что явный подсчёт предельного NTK доступен только для сетей, состоящих из слоёв из определённого класса. В этот класс входят полносвязные и свёрточные слои, average pooling, ряд нелинейностей с одним аргументом (включая, например, ReLU и erf), layer norm, но не входят max pooling и batch norm, часто используемые в реальных архитектурах. Явный подсчёт предельного NTK для «хороших» сетей реализован в [библиотеке NeuralTangents](https://github.com/google/neural-tangents); часть явных формул для подсчёта можно найти в статье [On exact computation with an infinitely wide neural net](https://arxiv.org/pdf/1904.11955.pdf).

Тем не менее, даже в тех случаях, когда посчитать предельное NTK не представляется возможным, в качестве ядра для ядрового метода можно использовать эмпирическое NTK в инициализации 

$$\hat\Theta_0(x,x') = \nabla^T_\theta f(x;\theta_0) \nabla_\theta f(x';\theta_0)$$

Такое ядро можно рассматривать как шумную и смещённую оценку предельного; для уменьшения шума можно использовать Монте-Карло оценку матожидания. Некоторые оптимизации подсчёта эмпирического ядра см. в работе [Neural tangents: Fast and easy infinite neural networks in python](https://arxiv.org/pdf/1912.02803.pdf).

NTK не единственное ядро, которое можно сопоставить нейронной сети. Так, NNGP-ядро $K(x,x') = \mathbb{E} f(x) f(x')$ – это ядро гауссовского процесса, реализуемого сетью в пределе бесконечной ширины. Подробнее можно почитать в работах [Deep Neural Networks as Gaussian Processes](https://arxiv.org/pdf/1711.00165.pdf), [Wide neural networks of any depth evolve as linear models under gradient descent](https://arxiv.org/pdf/1902.06720.pdf), [Random neural networks in the infinite width limit as Gaussian processes](https://arxiv.org/pdf/2107.01562.pdf) или в [конспекте лекций](https://arxiv.org/pdf/2012.05760.pdf). Можно показать, что оно соответствует NTK-ядру для сети, в которой учится лишь выходной слой.

Так как, в отличие от NTK, для подсчёта NNGP-ядра не требуется обратный проход (backward pass), последнее более вычислительно эффективно; [Towards nngp-guided neural architecture search](https://arxiv.org/pdf/2011.06006.pdf) – пример работы, в которой предпочтение отдаётся NNGP-ядру именно по этой причине.

## Сходимость эмпирического ядра

Вы этом параграфе мы покажем, что при определённой параметризации эмпирическое NTK не зависит ни от времени, ни от инициализации. Мы начнём с иллюстративного примера, прежде чем формулировать строгую теорему.

Рассмотрим сеть с одним скрытым слоем, скалярным выходом и гауссовской инициализацией весов; вход для простоты тоже положим скалярным:

$$
f(x; a_{1:n}, w_{1:n})
= \sum_{i=1}^n a_i \phi(w_i x),
\quad
a_{1:n} \sim \mathcal{N}(0, n^{-1} I),
\quad
w_{1:n} \sim \mathcal{N}(0, I).
$$

Здесь $n$ – ширина скрытого слоя.

Следуя одной из стандартных схем инициализации из статьи [Delving deep into rectifiers: Surpassing human-level performance on imagenet classification](https://arxiv.org/pdf/1502.01852.pdf), дисперсия каждого слоя выбирается обратно пропорционально числу входных нейронов (подробнее см. в параграфе про [тонкости обучения нейросетей](https://academy.yandex.ru/handbook/ml/article/tonkosti-obucheniya)).

Назовём описанную выше параметризацию стандартной.

Для сходимости ядра нам придётся несколько её видоизменить:

$$
f(x; a_{1:n}, w_{1:n})
= \frac{1}{\sqrt{n}} \sum_{i=1}^n a_i \phi(w_i x),
\quad
a_{1:n} \sim \mathcal{N}(0, I),
\quad
w_{1:n} \sim \mathcal{N}(0, I).
$$

Назовём новую параметризацию NTK-параметризацией.

Отметим, что распределение выходов нейронов в инициализации остаётся неизменным при переходе от стандартной к NTK-параметризации. Что меняется – это динамика градиентного спуска:

$$
\dot a_k 
= \frac{1}{\sqrt{n}} \sum_{j=1}^m \phi(w_k x_j)(y_j - f_t(x_j)),
$$

$$
\dot w_k
= \frac{1}{\sqrt{n}} \sum_{j=1}^m a_k \phi'(w_k x_j) x_j(y_j - f_t(x_j)).
$$

При $t=0$ приращения весов для такой параметризации имеют порядок $O(n^{-1/2})$, в то время как сами веса имеют порядок $O(1)$ при $t=0$. Поэтому $a_k(t) \to a_k(0)$ и $w_k(t) \to w_k(0)$ при $n \to \infty$ для любого данного $k \in \mathbb{N}$ и $t \in \mathbb{R}_+$. Другими словами, с ростом размера скрытого слоя градиент будет стремиться к нулю, и каждый из весов в пределе останется в начальной точке.

Сравним с градиентным спуском в стандартной параметризации:

$$
\dot a_k 
= \sum_{j=1}^m \phi(w_k x_j)(y_j - f_t(x_j)),
$$

$$
\dot w_k 
= \sum_{j=1}^m a_k \phi'(w_k x_j) x_j(y_j - f_t(x_j))
$$

В этом случае веса выходного слоя имеют порядок $O(n^{-1/2})$ при $t=0$, но получают приращения порядка $O(1)$ в этот момент времени, в то время как веса входного слоя имеют порядок $O(1)$ при $t=0$, но получают в этот момент времени приращения порядка $O(n^{-1/2})$.

В новой параметризации эмпирическое NTK выглядит следующим образом:

$$
\hat\Theta_t(x,x')
= \sum_{i=1}^n \left(\partial_{a_i} f(x) \partial_{a_i} f(x') + \partial_{w_i} f(x) \partial_{w_i} f(x')\right)
=
$$

$$      
= \frac{1}{n} \sum_{i=1}^n \left(\phi(w_i(t) x) \phi(w_i(t) x') + a_i^2(t) \phi'(w_i(t) x) \phi'(w_i(t) x') x x'\right).
$$

Так как $a_k(t) \to a_k(0)$ и $w_k(t) \to w_k(0)$ при $n \to \infty$ для любых заданных $k \in \mathbb{N}$ и $t \in \mathbb{R}_+$, выражение выше асимптотически эквивалентно

$$
\hat\Theta_0(x,x')
= \frac{1}{n} \sum_{i=1}^n \left(\phi(w_i(0) x) \phi(w_i(0) x') + a_i^2(0) \phi'(w_i(0) x) \phi'(w_i(0) x') x x'\right),
$$

а значит, сходится к

$$
\Theta(x,x')
= \mathbb{E}_{a,w \sim \mathcal{N}(0,1)} \left(\phi(w x) \phi(w x') + a^2 \phi'(w x) \phi'(w x') x x'\right)
$$

при $n \to \infty$ в силу закона больших чисел.

Предельное ядро $\Theta(x,x')$ не зависит ни от времени $t$, ни от инициализации. Мы будем называть это ядро **нейрокасательным** или просто **NTK** (его не стоит путать с эмпирическим NTK $\hat\Theta_t$).

Ещё раз подчеркнём, что это работает для NTK-параметризации, но не для стандартной. Для стандартной параметризации эмпирическое NTK в инициализации расходится с шириной:

$$
\hat\Theta_0(x,x')
= \sum_{i=1}^n \left(\phi(w_i(0) x) \phi(w_i(0) x') + a_i^2(0) \phi'(w_i(0) x) \phi'(w_i(0) x') x x'\right)
\sim
$$

$$
\sim n \cdot \mathbb{E}_{w \sim \mathcal{N}(0,1)} \phi(w x) \phi(w x').
$$

Подробнее мы поговорим об этом в одном из следующих параграфов.

Для NTK-параметризации сходимость эмпирического ядра выполняется не только для сетей с одним скрытым слоем. Так, рассмотрим полносвязную сеть с $L$ слоями:

$$
f(x) = h_L(x),
\quad
h_l(x) = \frac{1}{\sqrt{n_{l-1}}} W_l x_{l-1}(x),
\quad
x_{l-1}(x) = \phi(h_{l-1}(x)),
\quad
x_0(x) = x.
$$

Здесь $W_1 \in \mathbb{R}^{n_1 \times n_0}$, $W_L \in \mathbb{R}^{1 \times n_{L-1}}$ и $W_l \in \mathbb{R}^{n_l \times n_{l-1}}$ для всех остальных $l$. 

Положим, что веса инициализируются из стандартного нормального распределения. Поставим задачу оптимизации дифференцируемой функции потерь $\mathcal{L}$:

$$
\dot\theta_t
= -\nabla_\theta\left(\sum_{j=1}^m \mathcal{L}(y_j, f(x_j; \theta_t))\right)
= \sum_{j=1}^m \left.\frac{\partial \mathcal{L}(y_j, z)}{\partial z}\right|_{z=f(x_j; \theta_t)} \nabla_\theta f(x_j; \theta_t),
$$

где $\theta$ – объединение всех весов $W_{1:L}$ сети.

Теорема ниже доказана в [оригинальной работе по NTK](https://arxiv.org/pdf/1806.07572.pdf):

**Теорема**. В предположениях выше, если $\phi$ из $C^2$ и липшицева и $\mathcal{L}$ из $C^1$ и липшицева, то $\hat\Theta_t(x,x')$ сходится к $\Theta(x,x')$ по вероятности при $n_{1:L-1} \to \infty$ последовательно $\forall x,x' \in \mathbb{X}$ $\forall t \geq 0$.

Оказывается, что эта теорема верна не только для полносвязных сетей с гладкими активациями.

Определим **тензорную программу** как начальный набор переменных определённых типов и последовательность команд. Каждая команда порождает новую переменную, действуя на уже имеющиеся.

Переменные бывают трёх типов:

* $\mathsf{A}$: $n \times n$ матрицы с независимыми элементами из $\mathcal{N}(0,1)$;
* $\mathsf{G}$: вектора размера $n$ с асимптотически независимыми нормальными элементами;
* $\mathsf{H}$: образы $\mathsf{G}$-переменных относительно поэлементных нелинейностей.

Для переменной $W$ запись $W: \mathsf{A}$ будет означать, что $W$ имеет тип $\mathsf{A}$.

Команды бывают следующие:

* trspop: $W: \mathsf{A} \to W^T: \mathsf{A}$ (перевести переменную типа $\mathsf{A}$ со значением $W$ в переменную типа $\mathsf{A}$ со значением $W^T$);
*  matmul: $(W: \mathsf{A}, \ x: \mathsf{H}) \to \frac{1}{\sqrt{n}} W x: \mathsf{G}$;
*  lincomb: $(\{x_i: \mathsf{G}, \; a_i \in \mathbb{R}\}_{i=1}^k) \to \sum_{i=1}^k a_i x_i: \mathsf{G}$;
* nonlin: $(\{x_i: \mathsf{G}\}_{i=1}^k, \; \phi: \mathbb{R}^k \to \mathbb{R}) \to \phi(x_{1:k}): \mathsf{H}$ (здесь мы несколько выходных векторов $x_i$ агрегируем в один с помощью покоординатной, возможно, нелинейной функции).

Формализм тензорных программ позволяет представить прямой и обратный проход широкого класса нейронных архитектур, который включает свёрточные сети, рекуррентные сети, сети с residual слоями. Хотя и ни одна из операций выше не может порождать новые $\mathsf{A}$-переменные (веса), любое наперёд заданное число шагов градиентного спуска можно представить в рамках одной тензорной программы (посредством «развёртывания» шагов градиентного спуска).
    
Назовём величину $n$ шириной тензорной программы.
Основная «предельная» теорема тензорных программ представлена ниже:

**Master theorem** (G. Yang, [Tensor programs III: Neural matrix laws](https://arxiv.org/pdf/2009.10685.pdf)). Рассмотрим тензорную программу с $M$ $\mathsf{G}$-величинами, удовлетворяющую определённым начальным условиям. Пусть все нелинейности $\phi$ и функция $\psi: \, \mathbb{R}^M \to \mathbb{R}$ полиномиально ограничены. Тогда

$$
\frac{1}{n} \sum_{\alpha=1}^n \psi(g^1_\alpha,\ldots,g^M_\alpha)
\to \mathbb{E}_{Z \sim \mathcal{N}(\mu,\Sigma)} \psi(Z)
$$

почти наверное при $n \to \infty$, где $\mu$ и $\Sigma$ могут быть вычислены по некоторым рекурентным правилам.

Оказывается, что если тензорная программа выражает прямой и обратной проход в некоторой нейронной сети, то NTK сети в инициализации всегда можно представить в виде $\frac{1}{n} \sum_{\alpha=1}^n \psi(g^1_\alpha,\ldots,g^M_\alpha)$ для некоторой функции $\psi$, см. [Tensor programs II: Neural tangent kernel for any architecture](https://arxiv.org/pdf/2006.14548.pdf).Таким образом, теорема выше доказывает существование и детерминированность предельного ядра в инициализации, а также даёт способ его вычисления. Более того, это верно и для ядра в любой фиксированный момент времени, см. [Tensor Programs IIb](https://arxiv.org/pdf/2105.03703.pdf).

В качестве иллюстрации обратимся вновь к сети с одним скрытым слоем. Рассмотрим тензорную программу, вычисляющую прямой и обратный проходы на входах $x$ и $x'$. Такая программа порождает следующие $\mathsf{G}$-величины: $g^1 = w(0) x$, $g^2 = w(0) x'$, $g^3 = a(0) x$ и $g^4 = a(0) x'$. Напомним, что эмпирическое NTK равно

$$
\hat\Theta_0(x,x')
= \frac{1}{n} \sum_{i=1}^n \left(\phi(w_i(0) x) \phi(w_i(0) x') + a_i^2(0) \phi'(w_i(0) x) \phi'(w_i(0) x') x x'\right).
$$

Положив 

$$\psi(g^1_\alpha,\ldots,g^4_\alpha) = \phi(g^1_\alpha) \phi(g^2_\alpha) + \phi'(g^1_\alpha) \phi'(g^2_\alpha) g^3_\alpha g^4_\alpha,$$

получим выражение как раз в виде, требуемом Master Theorem.


## Стандартная параметризация и эволюция ядра

Как было отмечено в предыдущем параграфе, эмпирическое NTK двухслойной сети расходится с шириной при стандартной параметризации.

$$
\hat\Theta_t(x,x')
= \sum_{i=1}^n \left(\phi(w_i(t) x) \phi(w_i(t) x') + a_i^2(t) \phi'(w_i(t) x) \phi'(w_i(t) x') x x'\right).
$$

При $t=0$, так как $w_i$ независимы и имеют порядок $O(1)$, сумма расходится пропорционально $n$.Так как для квадратичной функции потерь $\dot f_t(x) = \hat\Theta_t(x,\vec x) (\vec y - f_t(\vec x))$, предсказание модели в любой точке $x$ получает приращение порядка $O(n)$ на первом же шаге обучения; для задачи регрессии такая модель теряет смысл.

Однако для классификации величина предсказаний не играет роли: для бинарной классификации важен лишь знак, а для многоклассовой – индекс максимального логита. Таким образом, в этом случае, несмотря на расходящееся ядро, предел при бесконечной ширине имеет смысл, см. [Dynamically Stable Infinite-Width Limits of Neural Classifiers](https://arxiv.org/pdf/2006.06574.pdf).

Рассмотрим нормализованное эмпирическое NTK $\tilde\Theta_t(x,x') = \hat\Theta_t(x,x') / n$. Его предел в инициализации равен $\mathbb{E}_{w \sim \mathcal{N}(0,1)} \phi(w x) \phi(w x')$. Назовём этот предел нормализованным NTK и обозначим $\tilde\Theta(x,x')$.

В отличие от ядра в NTK-параметризации, нормализованное NTK при стандартной параметризации зависит от времени:

$$
\frac{d\tilde\Theta_t(x,x')}{dt}
= \frac{1}{n} \sum_{i=1}^n \left(\phi(w_i(t) x) \phi'(w_i(t) x') x' + \phi'(w_i(t) x) \phi(w_i(t) x') x\right) \frac{dw_i(t)}{dt} 
+
$$

$$      
\phantom{\frac{d\tilde\Theta_t(x,x')}{dt}}
+ \frac{1}{n} \sum_{i=1}^n a_i^2(t) x x' \left(\phi'(w_i(t) x) \phi''(w_i(t) x') x' + \phi''(w_i(t) x) \phi'(w_i(t) x') x\right) \frac{dw_i(t)}{dt}
+
$$

$$
\phantom{\frac{d\tilde\Theta_t(x,x')}{dt}}
+ \frac{1}{n} \sum_{i=1}^n 2 a_i(t) \phi'(w_i(t) x) \phi'(w_i(t) x') x x' \frac{da_i(t)}{dt}.
$$

Напомним, как выглядит градиентный спуск в стандартной параметризации:

$$
\frac{a_k(t)}{dt} 
= \sum_{j=1}^m \phi(w_k(t) x_j),
\quad
\frac{w_k(t)}{dt} 
= \sum_{j=1}^m a_k(t) \phi'(w_k(t) x_j) x_j.
$$

При $t=0$, $\dot a_k = O(1)$, в то время как $\dot w_k = O(n^{-1/2})$. Так как $a_k(0) = O(n^{-1/2})$ и $w_k(0) = O(1)$, для любого $t > 0$, не зависящего от $n$, $a_k(t) = O(1)$, $\dot a_k(t) = O(1)$, $w_k(t) = O(1)$ и $\dot w_k(t) = O(1)$.

Наивная оценка сумм даёт $\frac{d\tilde\Theta_t(x,x')}{dt} = O(1) + O(1) + O(1) = O(1)$ для любого $t > 0$, не зависящего от $n$. Таким образом, нормализованное ядро зависит от времени даже в пределе бесконечной ширины. Экспериментальный анализ эволюции ядра реальной нейронной сети в стандартной параметризации см. в работе [Deep learning versus kernel learning](https://arxiv.org/pdf/2010.15110.pdf).
    
Преимущество нейронных сетей над ядровыми методами, в том числе с NTK, может быть связано, в частности, с зависимостью предельного ядра от времени. В самом деле, ядро измеряет «похожесть» в некотором пространстве признаков. Для NTK это пространство фиксировано, в то время как нейронная сеть меняет своё ядро по ходу обучения, возможно, делая его более подходящим для задачи.

  ## handbook

  Учебник по машинному обучению

  ## title

  Сети бесконечной ширины

  ## description

  Сети бесконечной ширины

- 
  ## path

  /handbook/ml/article/landshaft-funkcii-poter

  ## content

  Задача обучения параметрической модели $f_\theta$ ставится как задача минимизации эмпирического риска 

$$\hat R_m(\theta) = \mathbb{E}_{x,y \in S_m} r(y, f_\theta(x)),$$

где $S_m$ – выборка размера $m$, а $r$ – функция риска, например, $r(y, \hat{y}) = \mathbb{I}[y \neq \hat y]$. Часто интересующая нас функция риска не дифференцируема по второму аргументу, что делает градиентную оптимизацию неприменимой. По этой причине вместо исходной функции риска $r$ вводят её дифференцируемый выпуклый суррогат, то есть некоторую выпуклую и дифференцируемую по второму аргументу функцию $\ell \geq r$. Новый функционал эмпирического риска имеет вид

$$\hat{\mathcal{L}}_m(\theta) = \mathbb{E}_{x,y \in S_m} \ell(y, f_\theta(x)).$$

Если $f_\theta$ дифференцируема по $\theta$, то из дифференцируемости $\ell$ следует дифференцируемость $\hat{\mathcal{L}}_m$, что делает возможной градиентную оптимизацию. А если $f_\theta$ выпукла по $\theta$, то из выпуклости $\ell$ следует выпуклость $\hat{\mathcal{L}}_m$, что даёт гарантии на сходимость градиентного спуска в глобальный минимум.

Увы, в общем случае нейронные сети не выпуклы как функции своих весов. Это можно увидеть на простом примере. Пусть $f_\theta(x) = u v x$, где $u$, $v$ и $x$ – скаляры, а $\theta = (u,v)$. Гессиан $f$ как функции $\theta$ в любой точке равен $\binom{0 \; x}{x \; 0}$; его собственные числа равны $x$ и $(-x)$, что и означает, что для любого ненулевого $x$ функция $f$ не выпукла.

Таким образом, даже для выпуклой $\ell$ функция потерь $\hat{\mathcal{L}}_m$ нейронной сети не обязана быть выпуклой функцией весов.

У невыпуклых функций могут быть минимумы, не являющиеся глобальными, в которых может «застревать» градиентный спуск. Тем не менее, на практике часто оказывается, что градиентный спуск всегда находит точку со сколь угодно близким к глобальному минимуму значением функции потерь.

Это наблюдение приводит к гипотезе, что, хотя поверхность функции потерь не обязана быть выпуклой, все её минимумы глобальны для используемых нами сетей и тех наборов данных, на которых мы их обучаем.

Известны два случая, для которых эту гипотезу удаётся доказать. Первый – это линейные сети. Второй – это достаточно широкие нелинейные сети (ширина одного из слоёв не меньше числа примеров в выборке). К сожалению, оба примера нереалистичны: выразительная способность линейных сетей не выше, чем у обыкновенной линейной модели, а ширина реальных нейронных сетей не настолько велика (порядка $10^3$ нейронов против $10^6$ примеров в ImageNet), причём улучшить оценку на ширину в общем случае невозможно, см. Q. Nguyen [A note on connectivity of sublevel sets in deep learning](https://arxiv.org/pdf/2101.08576.pdf). Возможно, для получения лучших оценок исследователям предстоит научиться учитывать структуру данных обучающей выборки.

Исторически первое доказательство глобальности всех локальных минимумов линейной сети содержится в работе [Deep learning without poor local minima](https://arxiv.org/pdf/1605.07110.pdf). Более простое доказательство в немного более общем случае можно найти в работе [Depth creates no bad local minima](https://arxiv.org/pdf/1702.08580v1.pdf). Ещё более простое доказательство есть в работе [Deep linear networks with arbitrary loss: All local minima are global](https://arxiv.org/pdf/1712.01473.pdf), но оно подходит только для сетей без боттлнеков ($n_l \geq \min(n_0,n_{L+1})$ $\forall l \in 1,\ldots,L$). Последнее подробно разобрано в [конспекте лекций](https://arxiv.org/pdf/2012.05760.pdf) автора этого параграфа.

Здесь мы разберём только второй случай (достаточно широкие нелинейные сети) как потенциально более перспективный.

## Все минимумы достаточно широкой нелинейной сети глобальны

Рассмотрим нейронную сеть с одним скрытым слоем:

$$
f(x) = W_1 x_1(x) \in \mathbb{R}^{n_2},
\quad
x_1(x) = \phi(h_1(x)) \in \mathbb{R}^{n_1},
\quad
h_1(x) = W_0 x \in \mathbb{R}^{n_1},
\quad
x \in \mathbb{R}^{n_0},
$$

где функция активации $\phi$ применяется поэлементно. Рассмотрим набор данных $(X,Y)$ размера $m$, где $X \in \mathbb{R}^{n_0 \times m}$, а $Y \in \mathbb{R}^{n_2 \times m}$. Применяя соотношения выше к этому набору, получим следующие значения выходов слоёв:

$$
\hat Y = W_1 X_1 \in \mathbb{R}^{n_2 \times m},
\quad
X_1 = \phi(H_1) \in \mathbb{R}^{n_1 \times m},
\quad
H_1 = W_0 X \in \mathbb{R}^{n_1 \times m},
\quad
X \in \mathbb{R}^{n_0 \times m}.
$$

Поставим задачу оптимизации квадратичной функции потерь 

$$\mathcal{L}(W_{0:1}) = \vert\vert Y - \hat Y \vert\vert_F^2 \to \min_{W_{0:1}},$$

где $\vert\vert\cdot\vert\vert_F$ – норма Фробениуса.

**Теорема 1** ([On the local minima free condition of backpropagation learning](https://ieeexplore.ieee.org/document/410380)) Если $\phi$ аналитична, ограничена и не тождественно равна нулю, ширина скрытого слоя $n_1$ не меньше $m$ и все столбцы матрицы $X$ различны, то все локальные минимумы $\mathcal{L}(W_{0:1})$ глобальны.


**Доказательство**. Пусть $W_{0:1}^*$ – локальный минимум $\mathcal{L}(W_{0:1})$, и пусть $H_1^*$, $X_1^*$ – соответствующие ему скрытые представления. Тогда $W_1^*$ – локальный минимум $\mathcal{L}_{W_0^*}(W_1) = \vert\vert Y - W_1 X_1^* \vert\vert_F^2$.

Задача оптимизации $\mathcal{L}_{W_0^*}(W_1)$ выпуклая, поэтому $W_1^*$ – глобальный минимум $\mathcal{L}_{W_0^*}(W_1)$.

Если $\text{rk} X_1^* = m$, то система $Y_i = W_{1,i} X_1^*$, где $W_{1,i}$ – неизвестная матрица, гарантировано имеет решение для каждого $i \in 1\ldots,n_2$. Следовательно, 

$$\mathcal{L}(W_{0:1}^*) = \mathcal{L}_{W_0^*}(W_1^*) = \min \mathcal{L}_{W_0^*}(W_1) = 0,$$

а значит, $W_{0:1}^*$ – глобальный минимум $\mathcal{L}(W_{0:1})$.

Заметим, что для выполнения равенства $\text{rk} X_1^* = m$ необходимо $n_1 \geq m$.

Пусть теперь $\text{rk} X_1^* < m$. Если тем не менее $\min \mathcal{L}_{W_0^*}(W_1^*) = 0$, то $W_{0:1}^*$ – по-прежнему глобальный минимум $\mathcal{L}(W_{0:1})$. Пусть 

$$\mathcal{L}(W_{0,1}^*) = \mathcal{L}_{W_0^*}(W_1^*) = \min \mathcal{L}_{W_0^*}(W_1) > 0.$$

Докажем, что $W_{0,1}^*$ не может быть локальным минимумом $\mathcal{L}$ до тех пор, пока выполнены условия следующей леммы, которую мы докажем позже:

**Лемма 1**. Если $n_1 \geq m$, функция $\phi$ аналитична, ограничена и не тождественно равна нулю, а все столбцы матрицы $X$ различны, то лебегова мера множества $\{W_0 \in \mathbb{R}^{n_1 \times n_0}: \; \text{rk} X_1 < m\}$ равна нулю.

Так как $\mathcal{L}(W_{0,1}^*) > 0$ и $\mathcal{L}$ – непрерывна как функция от $W_{0,1}$, существует $\epsilon > 0$, для которого

$$\forall W_{0,1} \in B_\epsilon(W_{0,1}^*):\quad\mathcal{L}(W_{0,1}) > 0,$$

где через $B_\epsilon(W_{0,1}^*)$ мы обозначили $\epsilon$-окрестность точки $W_{0,1}^*$ в пространстве весов.

Из леммы 1 следует, что для любого $\delta > 0$ найдётся $W_0' \in B_\delta(W_0^*)$, для которого $\text{rk} X_1' = m$. Возьмём $\delta \in (0,\epsilon)$. Для соответствующего $W_0'$ имеем $\mathcal{L}(W_0',W_1^*) > 0$; при этом $\text{rk} X_1' = m$. Как было отмечено выше, задача минимизации $\mathcal{L}_{W_0'}(W_1)$ выпуклая, и оптимум её равен нулю, так как $\text{rk} X_1' = m$. Поэтому градиентный спуск, применённый к $\mathcal{L}_{W_0'}$ и стартующий в $W_1^*$, сойдётся в некоторую точку $W_1^{*,\prime}$, для которой $\mathcal{L}_{W_0'}(W_1^{*,\prime}) = 0$.

Мы знаем, что в нашей эпсилон-окрестности функция потерь положительна, значит, найденная точка находится вне её: $(W_0',W_1^{*,\prime}) \notin B_\epsilon(W_{0,1}^*)$.

Таким образом, найдётся $\epsilon > 0$ такое, что для любых $\delta \in (0,\epsilon)$ существует пара $(W_0',W_1^*) \in B_\delta(W_{0,1}^*)$ такая, что градиентный спуск, примененный к $\mathcal{L}$, стартующий в $(W_0',W_1^*)$ и действующий только на $W_1$, сходится в точку $(W_0',W_1^{*,\prime}) \notin B_\epsilon(W_{0,1}^*)$.

Очевидно, что если «для любых $\delta \in (0,\epsilon)$» заменить на «для любых $\delta > 0$», утверждение выше останется верным. Это означает, что динамика градиентного спуска, действующего только на $W_1$, не устойчива по Ляпунову в точке $W_{0,1}^*$. Следовательно, $W_{0,1}^*$ не может быть точкой минимума (иначе градиентный спуск был бы устойчив), а значит, условие $\mathcal{L}(W_{0,1}^*) > 0$ невыполнимо в условиях леммы 1. Таким образом, все локальные минимумы $\mathcal{L}$ глобальны. **Теорема 1 доказана**.

**Доказательство леммы 1**. Пусть $I_m$ – наборов индексов из $1,\ldots,n_1$ длины $m$. Рассмотрим $X_{1,I_m} \in \mathbb{R}^{m \times m}$ – подматрицу матрицы $X_1$, состоящую из строк $X_1$, проиндексированных набором $I_m$. В терминах $I_m$ условие $\text{rk} X_1 < m$ эквивалентно $\det X_{1,I_m} = 0$ $\forall I_m$.

Так как $\phi$ аналитична, а определитель – аналитическая функция элементов матрицы, $\det X_{1,I_m}$ – аналитическая функция от $W_0$ для любого $I_m$.

Нам понадобится следующая лемма, доказательство которой вы можете найти в [The loss surface of deep and wide neural networks](https://arxiv.org/pdf/1704.08045.pdf) (лемма 4.3):

**Лемма 2.** В условиях леммы 1, найдётся $W_0$, для которого $\text{rk} X_1 = m$.

Из леммы 2 и эквивалентности выше следует, что найдётся $W_0$, такой что для некоторого $I_m$ имеет место неравенство $\det X_{1,I_m} \neq 0$. Так как определитель $X_{1,m}$ – аналитическая функция $W_0$, а всякая не тождественно нулевая аналитическая функция принимает значение ноль лишь на множестве меры ноль по Лебегу, то лебегова мера множества $\{W_0: \; \det X_{1,I_m} = 0\}$ равна нулю. Таким образом, **лемма 1 доказана**.

## Обобщения

При доказательстве теоремы 1 мы воспользовались следующими условиями:

* Все обучающие примеры (столбцы матрицы $X$) различны;
* Число скрытых слоёв $L$ равно одному;
* Ширина (последнего) скрытого слоя не меньше числа примеров: $n_L \geq m$;
* Функция активации $\phi$ аналитична, ограничена и не тождественно равна нулю;
* Функция ошибки квадратична.

Можем ли мы ослабить какие-то из них?

Если какие-то из примеров совпадают и соответствующие метки также одинаковы, теорема обобщается тривиально. Если же метки не совпадают, то нулевая ошибка, вообще говоря, недостижима. Тем не менее, доказательство меняется по большому счёту лишь в том, что вместо $m$ будет фигурировать число различных примеров.

Рассмотрим сеть с $L$ скрытыми слоями, действующую на набор данных $X_0$ размера $m$:

$$
\hat Y = W_L X_L \in \mathbb{R}^{n_{L+1} \times m},
\quad
X_l = \phi(H_l) \in \mathbb{R}^{n_l \times m},
\quad
H_l = W_{l-1} X_{l-1} \in \mathbb{R}^{n_l \times m} \quad \forall l \in [L],
\quad
X_0 \in \mathbb{R}^{n_0 \times m}.
$$

Для обобщения теоремы 1 на глубокие сети с широким последним скрытым слоем, достаточно обобщить лемму 1. Например, можно воспользоваться следующим результатом (лемма 4.4 из [The loss surface of deep and wide neural networks](https://arxiv.org/pdf/1704.08045.pdf))

**Лемма 3**. Пусть $\phi$ аналитична, ограничена и не тождественно равна нулю, и пусть $l \in 1,\ldots,L$. Тогда если $n_l \geq m$ и все строки матрицы $X_0$ различны, то лебегова мера множества $\{W_{0:l-1}: \; \text{rk} X_l < m\}$ равна нулю.

Третье предположение можно попытаться ослабить с двух сторон.

Во-первых, можно требовать меньшего числа нейронов в скрытом слое. В общем случае этот подход не работает: в статье [A note on connectivity of sublevel sets in deep learning](https://arxiv.org/pdf/2101.08576.pdf) доказывается, что $m$ – это наименьшая ширина, при которой теорема выполняется для набора данных общего вида с различными примерами. Тем не менее, для реальных нейронных сетей градиентный спуск нередко находит глобальный минимум, хотя их ширина часто гораздо меньше размера набора данных, на которых они обучаются. Возможно, оценки на минимальную ширину удастся улучшить, если учесть структуру данных: например, если все примеры разбиваются на подмножества с элементами, находящимися близко друг к другу и имеющими одинаковые метки.

Во-вторых, можно предположить, что самым широким является не последний скрытый слой, а один из промежуточных: $n_l \geq m$ для некоторого $l < L$. Но тогда задача $\mathcal{L}_{W_{0:l-1}^*}(W_{l:L})$ не выпукла, а значит, из $\text{rk} X_l^* = m$ не следует, что $\mathcal{L}(W_{0:L}^*) = 0$, и градиентный спуск, действующий на $W_{l:L}$, не обязан сходиться в точку, в которой $\mathcal{L} = 0$ (он может застрять в локальном минимуме). Тем не менее, поставив ряд дополнительных условий, теорему 1 можно обобщить:

**Теорема 2**. Пусть $W_{0:L}^{\ast}$ – локальный минимум $\mathcal{L}(W_{0:L}) = \| \hat Y - Y \|_F^2$ и выполнены следующие условия:

1. $\phi$ аналитична, ограничена, не тождественно равна нулю;
2. производная $\phi$ нигде не обращается в ноль;
3. $n_l \geq m$;
4. $\text{rk} W_{l'}^* = n_{l'+1}$ $\forall l' \in \{l+1,\ldots,L\}$;
5. $\det(\nabla^2_{W_{l+1:L}} \mathcal{L}(W_{0:L}^*)) \neq 0$.

Тогда $W_{0:L}^*$ – глобальный минимум $\mathcal{L}(W_{0:L})$.

Условие 4 необходимо, чтобы из $\text{rk} X_l^* = m$ следовало $\mathcal{L}(W_{0:L}^*) = 0$. Отметим, что из условия 4 также следует, что $n_l \geq n_{l+1}$ $\forall l' > L$, то есть нейронная сеть должна сужаться, начиная со следующего после самого широкого слоя.

Условие 5 необходимо, чтобы в случае $\text{rk} X_l^* < m$ построить малое возмущение минимума $W_{0:L}^*$, которое снова является минимумом, но для которого $\text{rk} X_l = m$; невырожденный гессиан позволяет применить для этого теорему об обратной функции.

Если функция активации $\phi$ не аналитична, то лемма 3 неверна. В самом деле, для однородной $\phi$ (например, для ReLU или leaky ReLU) паттерны активаций, $\phi'(H_1)$, не меняются при малом возмущении весов. Значит, мы, вообще говоря, не можем найти такое малое возмущение, для которого ранг $X_1$ будет полным. 

Вместо малого возмущения в работе [On Connected Sublevel Sets in Deep Learning](https://arxiv.org/pdf/1901.07417.pdf) явно строятся пути в пространстве весов, на которых функция потерь не возрастает и достигает нуля. Если такой путь можно построить из произвольной точки, то все (строгие) локальные минимумы глобальны. Оказывается, что для построения такого пути аналитичность функции активации не требуется. Более того, при определённых условиях можно доказать, что из любых двух точек в пространстве весов можно построить соответствующие пути так, чтобы они сходились в одной точке. Это значит, что *множество подуровня* $\mathcal{L}^{-1}((-\infty, \mathcal{E}))$ связно при любом $\mathcal{E} > 0$ – эффект, впервые эмпирически обнаруженный в работах [Loss surfaces, mode connectivity, and fast ensembling of DNNs](https://arxiv.org/pdf/1802.10026.pdf) и [Essentially No Barriers in Neural Network Energy Landscape](https://arxiv.org/pdf/1803.00885.pdf).

Связность множеств подуровня сильнее глобальности всех строгих минимумов. В самом деле, если бы существовал строгий локальный минимум уровня $\mathcal{E} > 0$, то для достаточно малого $\epsilon > 0$ ему бы соответствовала отдельная связная компонента множества подуровня $\mathcal{E} + \epsilon$. С другой стороны, если все локальные минимумы глобальны, но изолированы, то множество подуровня $\epsilon$ несвязно для достаточно малого $\epsilon > 0$.

Вместо того, чтобы строить пути, на которых функция потерь достигает нуля, можно строить пути, на которых функция потерь достигает сколь угодно малого значения $\epsilon > 0$. Это позволяет обобщить результат на функции потерь $\ell(y,\hat y)$, для которых минимум по второму аргументу, ответу сети, не достигается. Пример такой функции – кросс-энтропия. Так мы приходим к следующей теореме:

**Теорема** ([On Connected Sublevel Sets in Deep Learning](https://arxiv.org/pdf/1901.07417.pdf)). Пусть выполнены следующие условия:

1. $\phi(\mathbb{R}) = \mathbb{R}$, $\phi$ строго монотонна и не найдётся ненулевых $(\lambda_i,a_i)_{i=1}^p$ с $a_i \neq a_j$ $\forall i \neq j$, таких что $\forall x \in \mathbb{R}$ $\phi(x) = \sum_{i=1}^p \lambda_i \phi(x - a_i)$;
2. $\ell(y,\hat y)$ выпукла по второму аргументу и $\inf_{\hat y} \ell(y,\hat y) = 0$ для любого $y$;
3. Существует $l \in \{1,\ldots,L\}$, для которого $\text{rk} X_l = m$ и $n_{l'} > n_{l'+1}$ для всех $l' \in \{l+1,\ldots,L\}$.

Тогда если $\mathcal{L}(W_{0:L}) = \sum_{j=1}^m \ell(y_j, f_{W_{0:L}}(x_j))$, то

1. Для каждого $\epsilon > 0$ найдутся веса $W_{0:L}$, для которых $\mathcal{L}(W_{0:L}) < \epsilon$;
2. Множество подуровня $\mathcal{L}^{-1}((-\infty, \mathcal{E}))$ связно для каждого $\mathcal{E} > 0$.

  ## handbook

  Учебник по машинному обучению

  ## title

  Ландшафт функции потерь

  ## description

  Ландшафт функции потерь

- 
  ## path

  /handbook/ml/article/implicit-bias

  ## content

  Под термином **implicit bias** мы будем понимать явление, состоящее в том, что алгоритм обучения среди всех возможных моделей с нулевым эмпирическим риском выбирает определённые. Это явление можно наблюдать уже на очень простом примере. Рассмотрим задачу линейной регрессии с квадратичной функцией потерь. Пусть имеется $m$ обучающих примеров в евклидовом пространстве размерности $N > m$. В этом случае наша задача $\frac{1}{2} \| X w - y \|_2^2 \to \min_w$ недоопределена: семейство решений составляет линейное многообразие в пространстве весов. Предположим, что матрица объекты-признаки $X \in \mathbb{R}^{m \times N}$ имеет полный ранг по строкам: $\text{rk} X = m < N$. 

Рассмотрим динамику градиентного спуска с шагом $\eta$ и нулевой инициализацией весов:

$$
w_{k+1} = w_k + \eta X^T (y - X w_k),
\quad
w_0 = 0.
$$

Нам будет удобнее вместо дискретного градиентного спуска рассматривать его непрерывный аналог

$$
\frac{dw}{dt} = X^T (y - X w),
\quad
w(t) = 0,
$$

переход к которому соответствует стремлению $\eta$ к нулю и выбору параметризации по $t$, для которой $k = [t / \eta]$ (округление берётся в любую сторону).

Рассмотрим сингулярное разложение $X = U \Sigma V^T$, где $U$ и $V$ ортогональны, а $\Sigma$ – прямоугольная диагональная матрица. Тогда

$$X^T X = V \Sigma^T \Sigma V^T \in \mathbb{R}^{N \times N}$$

и $\text{rk}(X^T X) = m$. 

Обозначая $\tilde w = V^T w$ и $\tilde y = V^T X^T y = \Sigma^T U^T y$, получаем:

$$
\frac{d\tilde w}{dt} = \tilde y - \Sigma^T \Sigma \tilde w,
\quad
\tilde w(0) = 0.
$$

В координатном виде имеем следующее:

$$
\frac{d\tilde w_i}{dt} = \tilde y_i - \sigma_i^2 \tilde w_i,
\quad
\tilde w_i(0) = 0
\qquad
\forall i \in [1:m]
$$

и

$$
\frac{d\tilde w_i}{dt} = 0,
\quad
\tilde w_i(0) = 0
\qquad
\forall i \in [m+1:N]
$$

так как $\Sigma^T \Sigma$ – диагональная матрица, у которой лишь первые $m$ элементов на диагонали не равны нулю, и у вектора $\tilde y = \Sigma^T U^T y$ тоже лишь первые $m$ координат ненулевые.

Так как все $\sigma_i$ для $i=1,\ldots,m$ ненулевые, при $t \to \infty$ получаем следующее решение:

$$
\tilde w_i(\infty) = \frac{\tilde y_i}{\sigma_i^2}
\quad \forall i \in\{1,\ldots,m\},
\qquad
\tilde w_i(\infty) = 0 
\quad \forall i \in \{m+1,\ldots,N\}.
$$

Это можно записать в эквивалентном матричном виде: $\tilde w(\infty) = (\Sigma^T \Sigma)^+ \tilde y = \Sigma^+ U^T y$, где «+» обозначает взятие псевдообратной матрицы.

Значит, $w(\infty) = V \Sigma^{+} U^T y = X^{+} y = X^T (X X^T)^{-1} y$ в силу того, что матрица $X$ имеет полный ранг по строкам.

Покажем теперь, что это частное решение, найденное градиентным спуском с нулевой инициализацией, имеет наименьшую евклидову норму среди всех минимумов функции потерь $\frac{1}{2} \vert\vert X w - y \vert\vert_2^2$.

Рассмотрим соответствующую функцию Лагранжа:

$$
L(w; \lambda) = \frac{\| w \|_2^2}{2} + \lambda^T (y - X w).
$$

Здесь $\lambda \in \mathbb{R}^{m}$. Покажем, что пара $(w(\infty), \lambda^* = (X X^T)^{-1} y)$ является критической точкой этой функции:

$$
\nabla_\lambda L(w(\infty); \lambda^*) 
= y - X w(\infty)
= y - X X^T (X X^T)^{-1} y 
= 0;
$$

$$
\nabla_w L(w(\infty); \lambda^*)
= w(\infty) - X^T \lambda^*
= X^T((X X^T)^{-1} y - (X X^T)^{-1} y)
= 0.
$$

В силу выпуклости функции Лагранжа эта критическая точка является точкой глобального минимума.


## Случай линейных сетей

Каков implicit bias нейронных сетей? Сходится ли градиентный спуск в решение наименьшей нормы и если да, то о какой норме идёт речь? Частичный ответ на этот вопрос удаётся получить для линейных сетей.

Следуя работе [Exact solutions to the nonlinear dynamics of learning in deep linear neural networks](https://arxiv.org/pdf/1312.6120.pdf), рассмотрим линейную сеть с одним скрытым слоем:

$$
f(x; W_{1:2}) =
W_2 W_1 x,
$$

где $W_1$ и $W_2$ – матрицы $n\times n$.

Поставим задачу многомерной (метка $y$ – вектор) регрессии с квадратичной функцией потерь: 

$$\mathcal{L}(W_{1:2}) = \mathbb{E}_{x,y} \vert\vert y - f(x; W_{1:2}) \vert\vert_2^2 \to \min_W$$

Шаг градиентного спуска выглядит следующим образом:

$$
\dot{W_1\vphantom{W^T}} =
\eta \mathbb{E}_{x,y} W_2^T (y x^T - W_2 W_1 x x^T),
\quad
\dot{W_2\vphantom{W^T}} =
\eta \mathbb{E}_{x,y} (y x^T - W_2 W_1 x x^T) W_1^T,
$$

где точка над $W_i$ означает производную по времени (то есть по $t$).

Это нелинейная система матричных дифференциальных уравнений второго порядка; чтобы проинтегрировать её аналитически, нам придётся сделать ряд предположений.

Определим ковариационную матрицу входов $\Sigma_{xx} = \mathbb{E} x x^T$ и матрицу ковариации меток со входами $\Sigma_{xy} = \mathbb{E} y x^T$. Предположим, что данные декоррелированы: $\Sigma_{xx} = I$; этого можно добиться, заменив входы $x$ на $\Sigma_{xx}^{-1} x$. Что касается матрицы ковариации меток со входами, рассмотрим её сингулярное разложение:

$$
\Sigma_{xy} =
U_2 S_{2,0} V_0^T =
\sum_{r=1}^{n} s_r u_r v_r^T.
$$

Назовём $s_r$ силой моды с индексом $r$ ковариации между метками и входами.

Сделаем замену координат:

$$
\overline{W_2} =
U_2^T W_2,
\quad
\overline{W_1} =
W_1 V_0.
$$

В новых координатах градиентный спуск принимает вид:


$$
\dot{\overline{W}}_1 =
\eta \overline{W_2}^T (S_{2,0} - \overline{W_2} \overline{W_1}),
\quad
\dot{\overline{W}}_2 =
\eta (S_{2,0} - \overline{W_2} \overline{W_1}) \overline{W_1}^T.
$$


Пусть $\overline{W_1} = [a_1, \ldots, a_n]$ и $\overline{W_2} = [b_1, \ldots, b_n]^T$. Тогда в терминах векторов $a$ и $b$

$$
\frac{1}{\eta} \dot a_{\alpha} =
s_\alpha b_{\alpha} - \sum_{\gamma=1}^n b_\gamma (b_\gamma^T a_\alpha) =
(s_\alpha - (b_\alpha^T a_\alpha)) b_{\alpha} - \sum_{\gamma\neq\alpha} (b_\gamma^T a_\alpha) b_\gamma;
$$

$$
\frac{1}{\eta} \dot b_{\alpha} =
s_\alpha a_{\alpha} - \sum_{\gamma=1}^n (a_\gamma^T b_\alpha) a_\gamma = (s_\alpha - (a_\alpha^T b_\alpha)) a_{\alpha} - \sum_{\gamma\neq\alpha} (a_\gamma^T b_\alpha) a_\gamma.
$$ 

Получилась система векторных дифференциальных уравнений порядка $2n$, всё ещё нелинейная. К счастью, при определённом предположении об инициализации эта система распадается на $n$ независимых систем порядка $2$.

Предположим, что существует ортогональная матрица $R = [r_1, \ldots, r_n]$, такая что при всех $\alpha$ имеет место равенство

$a_\alpha(0) = \tilde a(0) r_\alpha$ и $b_\alpha(0) = \tilde b(0) r_\alpha$

для некоторых скалярных величин $\tilde a(0)$ и $\tilde b(0)$.

Нетрудно заметить, что в этом случае при всех $\alpha$ и в любой момент времени $t$ имеем $a_\alpha(t) = \tilde a(t) r_\alpha$ и $b_\alpha(t) = \tilde b(t) r_\alpha$ для некоторых скалярных величин $\tilde a(t)$ и $\tilde b(t)$ .

Тогда для различных $\alpha$ выражения выше становятся независимыми друг от друга:

$$
\frac{d\tilde a}{dt} =
\eta (s - \tilde a \tilde b) \tilde b,
\qquad
\frac{d\tilde b}{dt} =
\eta (s - \tilde a \tilde b) \tilde a.
$$

Теперь это система нелинейных дифференциальных уравнений второго порядка.

Если $\tilde a = \tilde b$ в начальный момент, то это верно и в любой момент времени. Тогда система выше превращается в одно уравнение первого порядка.

В самом деле, обозначив $u = \tilde a \tilde b$, получаем:

$$
\dot u = 
2\eta (s - u) u.\quad(1)
$$

Это уравнение задаёт следующую динамику градиентного спуска для функции потерь $E(u) = \frac{1}{2} (s - u)^2$ (её глобальный минимум – это $u = s$).

Перед тем, как интегрировать уравнение (1), напомним, как из $u$ перейти обратно к исходным $W_1$ и $W_2$. Имеем для $W_1$:

$$
W_1 = \bar W_1 V_0^T,
\qquad
\bar W_1 = [\tilde a r_1, \ldots, \tilde a r_n],
\qquad
\tilde a = \sqrt{u}.
$$

Аналогично для $W_2$:

$$
W_2 = U_2 \bar W_2,
\qquad
\bar W_2 = [\tilde b r_1, \ldots, \tilde b r_n]^T,
\qquad
\tilde b = \sqrt{u}.
$$

Теперь проинтегрируем уравнение (1) в предположении, что $u(0) = u_0$ и $u(t) = u_f$ для выбранного $t$:

$$
t
= \frac{1}{\eta} \int_{u_0}^{u_f} \frac{du}{2u (s - u)} \, du
= \frac{1}{2 s \eta} \int_{u_0}^{u_f} \left(\frac{du}{u} + \frac{du}{s-u}\right) \, du
=
$$

$$
= \frac{1}{2 s \eta} \left(\ln\left(\frac{u_f}{u_0}\right) - \ln\left(\frac{u_f-s}{u_0-s}\right)\right)
= \frac{1}{2 s \eta} \ln\left(\frac{u_f (u_0-s)}{u_0 (u_f-s)}\right).
$$


Рассмотрим время, необходимое, чтобы выучить фиксированную долю силы данной моды $u_f = \xi s$, где $\xi \in (0,1)$, стартуя из точки из окрестности нуля $u_0 = \epsilon$. Оно равняется

$$
t^{(\xi)} 
= \frac{1}{2 s \eta} \ln\left(\frac{\xi}{1-\xi} \frac{s-\epsilon}{\epsilon}\right) 
= \frac{1}{2 s \eta} (\ln(s/\epsilon - 1) - \ln(\xi^{-1} - 1)).
% \sim \frac{1}{2 s \eta} \ln(s/\epsilon) \; \text{при $\epsilon \to 0$}.
$$

Видим, что чем сильнее мода (то есть чем больше $s$), тем быстрее она сходится.

Рассмотрим две моды с силами $s_1$ и $s_2$, такие что $s_1 > s_2$. Насколько вторая (более слабая) мода выучится к моменту, когда первая уже выучится на долю $\xi$? Из уравнения выше имеем:

$$
\frac{u_f (u_0-s)}{u_0 (u_f-s)} = e^{2 s \eta t};
$$

$$
1 - \frac{s}{u_f} = \left(1 - \frac{s}{u_0}\right) e^{-2 s \eta t}.
$$

Подставляя $s = s_2$ и $t = t_1^{(\xi)}$, получаем:

$$
1 - \frac{s_2}{u_f} 
= \left(1 - \frac{s_2}{\epsilon}\right) e^{-2 s_2 \eta t_1^{(\xi)}}
= \left(1 - \frac{s_2}{\epsilon}\right) e^{-\frac{s_2}{s_1} (\ln(s/\epsilon - 1) - \ln(\xi^{-1} - 1))}
\sim\\\sim -\frac{s_2}{\epsilon} \left(\frac{s_1}{\epsilon}\right)^{-\frac{s_2}{s_1}} \left(\frac{\xi}{1-\xi}\right)^{-\frac{s_2}{s_1}}
= -s_2 s_1^{-\frac{s_2}{s_1}} \left(\frac{\xi}{1-\xi}\right)^{-\frac{s_2}{s_1}} \epsilon^{\frac{s_2}{s_1} - 1}.
$$

Поскольку $s_2 < s_1$, это выражение стремится к минус бесконечности при $\epsilon \to 0$, из чего следует, что $u_f$ стремится к нулю.

Это означает, что если веса в инициализации лежат в окрестности нуля, то к моменту, когда данная мода выучивается на любую фиксированную долю $\xi \in (0,1)$, более слабые моды не успевают выучиться вообще. Таким образом, в любой момент времени $t$ матрица $W_2(t) W_1(t)$ является наилучшим малоранговым приближением заданного ранга матрицы корреляций $\Sigma_{yx}$, причём чем больше $t$, тем больше ранг. Можно сказать, что **градиентный спуск с фиксированным числом шагов «предпочитает» решения малого ранга**.

В выводе выше, мы использовали ряд предположений, в частности, что вектора $a_{1:n}$, образующие матрицу $\overline{W_1}$, ортогональны в инициализации. Эмпирически те же выводы оказываются верными и без этого предположения, см. графики в оригинальной работе [Exact solutions to the nonlinear dynamics of learning in deep linear neural networks](https://arxiv.org/pdf/1312.6120.pdf). Можно ли их обосновать строго математически? В работах [Towards resolving the implicit bias of gradient descent for matrix factorization](https://arxiv.org/pdf/2012.09839.pdf) и [Deep Linear Networks Dynamics](https://arxiv.org/pdf/2106.15933v1.pdf) доказывается, что самая сильная мода выучивается в первую очередь. Тем не менее, на момент написания этого текста остаётся недоказанным, что все моды выучиваются последовательно от сильных к слабым. К сожалению, implicit bias градиентного спуска для нелинейных сетей пока остаётся почти неизученным.

  ## handbook

  Учебник по машинному обучению

  ## title

  Implicit bias

  ## description

  Implicit bias

- 
  ## path

  /handbook/ml/article/optimizaciya-v-ml

  ## content

  ## Введение

Зачастую задачи машинного обучения формулируются таким образом, что «веса» модели, которую мы строим, возникают, как решение оптимизационной задачи. В качестве VIP-примера рассмотрим задачу линейной регрессии:

$$
    \Vert y - Xw \Vert_2^2 \to \min_w,
$$

По сути, мы получили чистейшую задачу квадратичной оптимизации. В чем особенность конкретно этой задачи? Она _выпуклая_. 

{% cut "Для интересующихся определением." %}

Функция $f \colon \mathbb{R}^d \to \mathbb{R}$ является (нестрого) выпуклой (вниз), если для любых $x_1,x_2 \in \mathbb{R}^d$ верно, что 

$$
    \forall t \in [0,1] : f(tx_1 + (1-t)x_2) \leq t f(x_1) + (1-t) f(x_2).
$$

Чтобы запомнить, в какую сторону неравенство, всегда полезно рисовать следующую картинку с графическим определением выпуклой функции.

![21_1_3a9cb89468.webp](https://yastatic.net/s3/education-portal/media/21_1_3a9cb89468_befecd19f9.webp)

Эквивалентное определение, если функция достаточно гладкая – гессиан неотрицательно определен в любой точке, то есть в каждой точке функция хорошо приближается параболоидом ветвями вверх. Отсюда по критерию минимальности второго порядка автоматически следует, что всякая точка локального оптимума является точкой локального минимума, то есть локальных максимумов и сёдел в выпуклом мире попросту не существует.

{% endcut %}

Важное свойство выпуклых функций – локальный минимум автоматически является глобальным (но не обязательно единственным!). Это позволяет избегать уродливых ситуаций, которые с теоретической точки зрения могут встретиться в невыпуклом случае, например, вот такой:

**Теорема** (_No free lunch theorem_) Пусть $A$ – алгоритм оптимизации, использующий локальную информацию (все производные в точке). Тогда существует такая невыпуклая функция $f \colon [0,1]^d \to [0,1]$, что для нахождения глобального минимума на квадрате $[0,1]^d$ с точностью $\frac{1}{m}$ требуется совершить хотя бы $m^d$ шагов.

{% cut "Для интересующихся доказательствами." %}

Будем строить наш контрпример, пользуясь принципом сопротивляющегося оракула (или рассуждениями с противником, кому как привычнее называть).

Разделим нашу область на подкубики размера $1/m \times \ldots \times 1/m$. Зададим функцию следующим образом – она будет тождественно равна $1$ на всех кубиках, кроме одного, в середине которого будет точка с значением $0$ (мы не специфицируем, как значение будет гладко «снижаться» до $0$; можно построить кусочно-линейную функцию, а потом сгладить её). 

А именно поставим ноль в тот кубик, который наш алгоритм оптимизации $A$ посетит последним. Так как кубиков у нас $m^d$, то алгоритм должен всегда совершить как минимум $m^d$ шагов, попробовав все кубики. Итого у нас следующая картинка ($m=3, d=2$):

![21_2_8d876235dc.webp](https://yastatic.net/s3/education-portal/media/21_2_8d876235dc_ba5da709f5.webp)

Отметим дополнительно, что полученный контрпример можно сделать какой угодно гладкости (но не аналитическим).

{% endcut %}

Мы видим, что в общем случае без выпуклости нас ожидает полное разочарование. Ничего лучше перебора по сетке придумать в принципе невозможно. В выпуклом случае же существуют алгоритмы, которые находят глобальный минимум за разумное время.

Встречаются ли в жизни функции невыпуклые? Повсеместно! Например, функция потерь при обучении нейронных сетей, как правило, не является выпуклой. Но отсюда не следует, что любой алгоритм их оптимизации будет обязательно неэффективным: ведь «контрпример» из теоремы довольно специфичен. И, как мы увидим, оптимизировать невыпуклые функции очень даже возможно.

Найти глобальный минимум невыпуклой функции – очень трудная задача, но зачастую нам хватает локального, который является, в частности, стационарной точкой: такой, в которой производная равна нулю. Все теоретические результаты в случае невыпуклых задач, как правило, касаются поиска таких точек, и алгоритмы тоже направлены на их отыскание.

Этим объясняется и то, что большинство алгоритмов оптимизации, придуманных для выпуклого случая, дословно перешли в невыпуклый. Теоретическая причина в следующем: в выпуклом случае поиск стационарной точки и поиск минимума – _буквально_ одна и та же задача, поэтому то, что хорошо ищет минимум в выпуклом случае, ожидаемо будет хорошо искать стационарные точки в невыпуклом. Практическая же причина в том, что оптимизаторы в библиотеках никогда не спрашивают, выпуклую ли им функцию подают на вход, а просто работают и работают хорошо.

Внимательный читатель мог возразить на моменте подмены задачи: подождите-ка, мы ведь хотим сделать функцию как можно меньше, а не стационарную точку искать какую-то непонятную. Доказать в невыпуклом случае тут, к сожалению, ничего невозможно, но на практике мы снова используем алгоритмы изначально для выпуклой оптимизации. Почему?

Причина номер **1**: сойтись в локальный минимум лучше, чем никуда. Об этом речь уже шла.

Причина номер **2**: в окрестности локального минимума функция становится выпуклой, и там мы сможем быстро сойтись.

Причина номер **3**: иногда невыпуклая функция является в некотором смысле «зашумленной» версией выпуклой или похожей на выпуклую. Например, посмотрите на эту картинку (функция Леви):

![21_3_4bf658e519.webp](https://yastatic.net/s3/education-portal/media/21_3_4bf658e519_40da367c0e.webp)

У этой функции огромное количество локальных минимумов, но «глобально» она кажется выпуклой. Что-то отдаленно похожее наблюдается и [в случае нейронных сетей](https://losslandscape.com/). Нашей задачей становится не скатиться в маленький локальный минимум, который всегда рядом с нами, а в большую-большую ложбину, где значение функции минимально и в некотором смысле стабильно.

Причина номер **4**: оказывается, что градиентные методы [весьма часто](http://proceedings.mlr.press/v49/lee16.html) сходятся именно к локальным минимумам.

Сразу отметим важную разницу между выпуклой и невыпуклой задачами: в выпуклом случае работа алгоритма оптимизации не очень существенно зависит от начальной точки, поскольку мы всегда скатимся в точку оптимума. В невыпуклом же случае правильно выбранная точка старта – это уже половина успеха.

Теперь перейдём к разбору важнейших алгоритмов оптимизации.

## Градиентный спуск (GD)

Опишем самый простой метод, который только можно придумать – градиентный спуск. Для того, чтобы его определить, вспомним заклинание из любого курса матанализа: «градиент – это направление наискорейшего локального возрастания функции», тогда антиградиент – это направление наискорейшего локального убывания.

{% cut "Для интересующихся формализмом." %}

Воспользуемся формулой Тейлора для $\Vert h \Vert = 1$ (направления спуска): 

$$
    f(x + \alpha h) = f(x) + \alpha \langle \nabla f(x), h \rangle + o(\alpha).
$$
    
Мы хотим уменьшить значение функции, то есть

$$
    f(x) + \alpha \langle \nabla f(x), h \rangle + o(\alpha) < f(x).
$$

При $\alpha \to 0$ имеем $\langle \nabla f(x), \Delta x \rangle \leq 0$. Более того, мы хотим наискорешйшего убывания, поэтому это скалярное произведение хочется минимизировать. Сделаем это при помощи неравенства Коши-Буняковского:

$$
    \langle \nabla f(x), h \rangle \geq - \Vert \nabla f(x) \Vert_2 \Vert h \Vert_2 = \Vert \nabla f(x) \Vert_2. 
$$
    
Равенство в неравенстве Коши-Буняковского достигается при пропорциональности аргументов, то есть 

$$
    h = - \frac{\nabla f(x)}{\Vert \nabla f(x) \Vert_2}.
$$

{% endcut %}


Тогда пусть $x_0$ – начальная точка градиентного спуска. Тогда каждую следующую точку мы выбираем следующим образом:

$$
    x_{k+1} = x_k - \alpha \nabla f(x_k),
$$

где $\alpha$ – это размер шага (он же learning rate). Общий алгоритм градиентного спуска пишется крайне просто и элегантно:

```python
x = normal(0, 1)                # можно пробовать и другие виды инициализации
repeat S times:                 # другой вариант: while abs(err) > tolerance
   h = grad_f(x)                # вычисляем направление спуска
   x -= alpha * h               # обновляем значение в точке
```

Эту схему в приложении к линейной регрессии можно найти в [параграфе про линейные модели](https://academy.yandex.ru/handbook/ml/article/linejnye-modeli#pochemu-modeli-linejnye). 

После всего этого начинаются тонкости:

* А как вычислять градиент?
* А как выбрать размер шага? 
* А есть ли какие-то теоретические оценки сходимости?

Начнем разбирать вопросы постепенно. Для вычисления градиентов современный человек может использовать инструменты автоматического дифференцирования. Идейно, это вариация на тему алгоритма [обратного распространения ошибки (backpropagation)](https://academy.yandex.ru/handbook/ml/article/metod-obratnogo-rasprostraneniya-oshibki), ведь как правило человек задает функции, составленные из элементарных при помощи умножений/делений/сложений/композиций. Такой метод реализован во всех общих фреймворках для нейронных сетей (Tensorflow, PyTorch, Jax). 

Но, вообще говоря, возникает некоторая тонкость. Например, расмотрим задачу линейной регрессии. Запишем её следующим образом:

$$
    f(w) = \frac{1}{N} \sum_{i=1}^N (w^\top x_i - y_i)^2.
$$

Видим, что слагаемых суммарно $N$ – размер выборки. При $N$ порядка $10^6$ и $d$ (это количество признаков) порядка $10^4$ вычисление градиента за $O(Nd)$ становится жутким мучением. Но если от $d$ избавиться без дополнительных предположений (например, о разреженности) нельзя, то с зависимостью от $N$ в каком-то смысле удастся разделаться при помощи метода стохастического градиентного спуска.

Хранение градиентов тоже доставит нам проблемы. У градиента столько же компонент, сколько параметров у модели, и если мы имеем дело с глубокой нейросетью, это даст значительные затраты дополнительной памяти. Хуже того, метод обратного распространения ошибки устроен так, что нам приходится помнить все промежуточные представления для вычисления градиентов. Поэтому вычислить градиент целиком невозможно ни для какой нормальной нейросети, и от этой беды тоже приходится спасаться с помощью стохастического градиентного спуска.

Теперь перейдем к размеру шага. Теория говорит о том, что если функция гладкая, то можно брать достаточно маленький размер шага, где под достаточно маленьким подразумевается $\alpha \leq \frac1L$, где $L$ – некоторая константа, которая зависит от гладкости задачи (так называемая константа Липшица). Вычисление этой константы может быть задачей сложнее, чем изначальная задача оптимизации, поэтому этот вариант нам не годится. Более того, эта оценка крайне пессимистична – мы ведь хотим размер шага как можно больше, чтобы уменьшить функцию как можно больше, а тут мы будем изменять все очень мало.

Существует так называемый **метод наискорейшего спуска**: выбираем размер шага так, чтобы как можно сильнее уменьшить функцию:

 $$
    \alpha_k = \arg\min_{\alpha \geq 0} f(x_k - \alpha \nabla f(x_k)).
 $$

Одномерная оптимизация является не сильно сложной задачей, поэтому теоретически мы можем её совершать (например, методом бинарного/тернарного поиска или золотого сечения), можно этот шаг также совершать неточно. Но сразу стоит заметить, что это можно делать, только если функция $f$ вычислима более-менее точно за разумное время, в случае линейной регрессии это уже не так (не говоря уже о нейронных сетях).

Также есть всевозможные правила Армихо/Гольдштейна/Вульфа и прочее и прочее, разработанные в давние 60-е, и для их проверки требуется снова вычислять значения функции в точке. Желающие могут посмотреть на эти условия [на википедии](https://ru.wikipedia.org/wiki/%D0%A3%D1%81%D0%BB%D0%BE%D0%B2%D0%B8%D1%8F_%D0%92%D0%BE%D0%BB%D1%8C%D1%84%D0%B5). Про более хитрые вариации выбора шагов мы поговорим позже, но сразу стоит сказать, что эта задача довольно сложная.

По поводу теории: сначала скажем что-то про _выпуклый_ случай.

В максимально общем выпуклом случае без дополнительных предположений оценки для градиентного спуска крайне и крайне пессимистичные: чтобы достичь качества $\varepsilon$, то есть

$$\vert f(x_k) - f(x^*) \vert\leq \varepsilon $$

достаточно сделать $O(R^2/\varepsilon^2)$ шагов, где $R^2$ — это расстояние от $x_0$ до $x^*$. Выглядит очень плохо: ведь чтобы достичь точности $10^{-2}$, необходимо сделать порядка $10^4$ шагов градиентного спуска. Но на практике такого не происходит, потому что на самом деле верны разные предположения, дающие более приятные свойства. Для контраста, укажем оценку в случае гладкой и сильно выпуклой в точке оптимума функции: за $k$ шагов будет достигнута точность

$$
    O\left( \min\left\{R^2 \exp
    \left(-\frac{k}{4\kappa}\right), \frac{R^2}{k} \right\}\right),
$$

где $\kappa$ – это так называемое число обусловленности задачи. По сути, это число измеряет, насколько линии уровня функции вытянуты в окрестности оптимума.

Морали две:
* Скорость сходимости градиентного спуска сильно зависит от обусловленности задачи;
* Также она зависит от выбора хорошей точки старта, ведь везде входит расстояние от точки старта до оптимума.

В качестве ссылки на доказательство укажем на [работу Себастиана Стича](https://arxiv.org/abs/1907.04232v2), где оно довольно простое и общее.

В невыпуклом же случае все куда хуже с точки зрения теории: требуется порядка $O(1/\varepsilon^2)$ шагов в худшем случае даже для гладкой функции, где $\varepsilon$ – желаемая точность уменьшения нормы градиента.

## Стохастический градиентный спуск (SGD)

Теперь мы попробуем сэкономить в случае регрессии и подобных ей задач. Будем рассматривать функционалы вида

$$
    f(x) = \sum_{i=1}^N \mathcal{L}(x, y_i),
$$

где сумма проходится по всем объектам выборки (которых может быть очень много).
Теперь сделаем следующий трюк: заметим, что это усреднение – это по сути взятие матожидания. Таким образом, мы говорим, что наша функция выглядит как

$$
    f(x) = \mathbb{E}[\mathcal{L}(x, \xi)],
$$

где $\xi$ равномерно распределена по обучающей выборке. Задачи такого вида возникают не только в машинном обучении; иногда встречаются и просто задачи _стохастического программирования_, где происходит минимизация матожидания по неизвестному (или слишком сложному) распределению.

Для функционалов такого вида мы также можем посчитать градиент, он будет выглядеть довольно ожидаемо: 

$$
    \nabla f(x) = \mathbb{E} \nabla \mathcal{L}(x, \xi).
$$

Будем считать, что вычисление матожидания напрямую невозможно. 

Новый взгляд из статистики дает возможность воспользоваться классическим трюком: давайте подменим матожидание на его несмещенную Монте-Карло оценку. Получается то, что можно назвать _стохастическим градиентом_:

$$
    \tilde \nabla f(x) = \frac{1}{B} \sum_{i=1}^B \nabla \mathcal{L}(x, \xi_i).
$$

Говоря инженерным языком, мы подменили вычисление градиента по всей выборке вычислением по случайной подвыборке. Подвыборку $\xi_1,\ldots,\xi_B$ часто называют (_мини_)_батчем_, а число $B$ – размером батча.

По-хорошему, наука предписывает нам каждый раз независимо генерировать батчи, но это трудно с вычислительной точки зрения. Вместо этого воспользуемся следующим приёмом: сначала перемешаем нашу выборку (чтобы внести дополнительную случайность), а затем будем рассматривать последовательно блоки по $B$ элементов выборки. Когда мы просмотрели всю выборку – перемешиваем еще раз и повторяем проход. Очередной прогон по обучающей выборке называется _эпохой_. И, хотя, казалось бы, независимо генерировать батчи лучше, чем перемешивать лишь между эпохами, есть несколько результатов, демонстрирующих обратное: [одна работа](https://arxiv.org/abs/1806.10077) и [вторая (более новая)](https://arxiv.org/abs/2006.06946); главное условие успеха – правильно изменяющийся размер шага.

Получаем следующий алгоритм, называемый **стохастическим градиентным спуском** (**stochastic gradient descent**, **SGD**):

```python
x = normal(0, 1)                    # инициализация
repeat E times:                     # цикл по количеству эпох
   for i = 0; i <= N; i += B:
        batch = data[i:i+B]
        h = grad_loss(batch).mean() # вычисляем оценку градиента как среднее по батчу
        x -= alpha * h
```

Дополнительное удобство такого подхода – возможность работы с внешней памятью, ведь выборка может быть настолько большой, что она помещается только на жёсткий диск. Сразу отметим, что в таком случае $B$ стоит выбирать достаточно большим: обращение к данным с диска всегда медленнее, чем к данным из оперативной памяти, так что лучше бы сразу забирать оттуда побольше.

Поскольку стохастические градиенты являются лишь оценками истинных градиентов, SGD может быть довольно шумным:

![21_4_5cc6153d0c.webp](https://yastatic.net/s3/education-portal/media/21_4_5cc6153d0c_81e5a8cdd7.webp)

Поэтому если вы обучаете глубокую нейросеть и у вас в память влезает лишь батч размером с 2-4 картинки, модель, возможно, ничего хорошего не сможет выучить. Аппроксимация градиента и поведение SGD может стать лучше с ростом размера батча $B$ – и обычно его действительно хочется подрастить, но парадоксальным образом слишком большие батчи могут порой испортить дело (об этом дальше в этом параграфе!).

### Теоретический анализ

Теперь перейдем к теоретической стороне вопроса. Сходимость SGD обеспечивается несмещенностью стохастического градиента. Несмотря на то, что во время итераций копится шум, суммарно он зачастую оказывается довольно мал.

Теперь приведем оценки. Сначала, по традиции, в выпуклом случае. Для выпуклой функции потерь за $k$ шагов будет достигнута точность порядка

$$
    O\left( \min\left\{R^2 \exp
    \left(-\frac{k}{4\kappa} \right)+ \frac{\sigma^2}{\mu k}, \frac{R^2}{k} + \frac{\sigma^2 R}{\sqrt{k}} \right\}\right),
$$

где $\sigma^2$ – это дисперсия стохградиента, а $\mu$ – константа сильной выпуклости, показывающая, насколько функция является «не плоской» в окрестности точки оптимума. Доказательство в том же [препринте С. Стича](https://arxiv.org/abs/1907.04232v2). 

Мораль в следующем: дисперсия стохастического градиента, вычисленного по батчу размера $B$ равна $\sigma_0^2/B$, где $\sigma_0^2$ – это дисперсия одного градиента. То есть увеличение размера батча помогает и с теоретической точки зрения.

В невыпуклом случае оценка сходимости SGD просто катастрофически плохая: требуется $O(1/\varepsilon^4)$ шагов для того, чтобы сделать норму градиента меньше $\varepsilon$. В теории есть всевозможные дополнительные способы снижения дисперсии с лучшими теоретическими оценками (Stochastic Variance Reduced Gradient (SVRGD), Spider, etc), но на практике они активно не используются.


## Использование дополнительной информации о функции

### Методы второго порядка

_[Основной раздел.](https://academy.yandex.ru/handbook/ml/article/metody-vtorogo-poryadka)_

Постараемся усовершенствовать метод стохастического градиентного спуска. Сначала заметим, что мы используем явно не всю информацию об оптимизируемой функции. 

Вернемся к нашему VIP-примеру линейной регресии с $\ell_2$ регуляризацией:

$$
    \Vert y - Xw \Vert_2^2 + \lambda \Vert w \Vert_2^2 \to \min_w.
$$

Эта функция достаточно гладкая, и может быть неплохой идеей использовать её старшие производные для ускорения сходимости алгоритма. В наиболее чистом виде этой философии следует метод Ньютона и подобные ему; о них вы можете прочитать [в соответствующем разделе](https://academy.yandex.ru/handbook/ml/article/metody-vtorogo-poryadka). Отметим, что все такие методы, как правило, довольно дорогие (исключая L-BFGS), и при большом размере задачи и выборки ничего лучше вариаций SGD не придумали.

### Проксимальные методы

_[Основной раздел.](https://academy.yandex.ru/handbook/ml/article/proksimalnye-metody)_

К сожалению, не всегда функции такие красивые и гладкие. Для примера рассмотрим Lasso-регресию:

$$
    \Vert y - Xw \Vert_2^2 + \lambda \Vert w \Vert_1 \to \min_w.
$$

Второе, не гладкое слагаемое резко ломает все свойства этой задачи: теоретически оценки для градиентного спуска становятся _гораздо_ хуже (и на практике тоже). С другой стороны, регуляризационное слагаемое устроено очень просто, и эту дополнительную структурную особенность можно и нужно эксплуатировать. Методы решения задачи вида

$$
    f(x) + h(x) \to \min_x,
$$

где $h$ – простая функция (в некотором смысле), а $f$ – гладкая, называются методами _композитной оптимизации_. Глубже погрузиться в них можно в [соответствующем разделе](https://academy.yandex.ru/handbook/ml/article/proksimalnye-metody), посвященном проксимальным методам.

## Использование информации о предыдущих шагах

Следующая претензия к методу градиентного спуска – мы не используем информацию о предыдущих шагах, хотя, кажется, там может храниться что-то полезное.

### Метод инерции, momentum

Начнем с физической аналогии. Представим себе мячик, который катится с горы. В данном случае гора – это график функции потерь в пространстве параметров нашей модели, а мячик – её текущее значение. Реальный мячик не застрянет перед небольшой кочкой, так как у него есть некоторая масса и уже накопленный импульс – некоторое время он способен двигаться даже вверх по склону. Аналогичный прием может быть использован и в градиентной оптимизации. В англоязычной литературе он называется **Momentum**.

![21_5_5006624408.webp](https://yastatic.net/s3/education-portal/media/21_5_5006624408_4e4eebb1ec.webp)

С математической точки зрения, мы добавляем к градиентному шагу еще одно слагаемое:

$$
    x_{k+1} = x_k - \alpha_k \nabla f(x_k) + \color{red}{\beta_k (x_k - x_{k-1})}.
$$

Сразу заметим, что мы немного усугубили ситуацию с подбором шага, ведь теперь нужно подбирать не только $\alpha_k$, но и $\beta_k$. Для обычного, не стохастического градиентного спуска мы можем адаптировать метод наискорейшего и получить **метод тяжелого шарика**:

$$
    (\alpha_k, \beta_k) = \arg\min_{\alpha,\beta} f(x_k - \alpha \nabla f(x_k) + \beta (x_k - x_{k-1})).
$$

Но, увы, для SGD это работать не будет.

Выгода в невыпуклом случае от метода инерции довольно понятна – мы будем пропускать паразитные локальные минимумы и седла и продолжать движение вниз. Но выгода есть также и в выпуклом случае. Рассмотрим плохо обусловленную квадратичную задачу, для которой линии уровня оптимизируемой функции будут очень вытянутыми эллипсами, и запустим на SGD с инерционным слагаемым и без него. Направление градиента будет иметь существенную вертикальную компоненту, а добавление инерции как раз «погасит» паразитное направление. Получаем следующую картинку:

![21_6_aa12e12568.webp](https://yastatic.net/s3/education-portal/media/21_6_aa12e12568_74c75bb6a7.webp)

Также удобно бывает представить метод моментума в виде двух параллельных итерационных процессов:

$$\begin{align}
    v_{k+1} &= \beta_k v_k - \alpha_k \nabla f(x_k)\\
    x_{k+1} &= x_k + v_{k+1}.
\end{align}
$$


### Accelerated Gradient Descent (Nesterov Momentum)

Рассмотрим некоторую дополнительную модификацию, которая была предложена в качестве оптимального метода первого порядка для решения выпуклых оптимизационных задач.

Можно доказать, что в сильно выпуклом и гладком случае найти минимум с точностью $\varepsilon$ нельзя быстрее, чем за 

$$
    \Omega\left( R^2\exp\left(-\frac{k}{\sqrt{\kappa}}\right) \right)
$$

итераций, где $\kappa$ – число обусловленности задачи. Напомним, что для обычного градиентного спуска в экспоненте у нас был не корень из $\kappa$, а просто $\kappa$, то есть, градиентный спуск справляется с плохой обусловленностью задачи хуже, чем мог бы.

В 1983 году Ю.Нестеровым был предложен алгоритм, имеющий оптимальную по порядку оценку. Для этого модифицируем немного моментум и будем считать градиент не в текущей точке, а как бы в точке, в которую мы бы пошли, следуя импульсу:

$$\begin{align}
    v_{k+1} &= \beta_k v_k - \alpha_k \nabla f(\color{red}{x_k + \beta_k v_k})\\
    x_{k+1} &= x_k + v_{k+1}
\end{align}
$$

Сравним с обычным momentum:

![21_7_8744ece0bb.webp](https://yastatic.net/s3/education-portal/media/21_7_8744ece0bb_19560f458f.webp)


_Комментарий: иногда упоминается, что Nesterov Momentum «заглядывает в будущее» и исправляет ошибки на данном шаге оптимизации. Конечно, никто не заглядывает в будущее в буквальном смысле._

В работе Нестерова были предложены конкретные (и довольно магические) константы для импульса, которые получаются из некоторой еще более магической последовательности. Мы приводить их не будем, поскольку мы в первую очередь заинтересованы невыпуклым случаем.

Nesterov Momentum позволяет значительно повысить устойчивость и скорость сходимости в некоторых случаях. Но, конечно, он не является серебряной пулей в задачах оптимизации, хотя в выпуклом мире и является теоретически неулучшаемым.

Также отметим, что ускоренный метод может напрямую примениться к проксимальному градиентному спуску. В частности, применение ускоренного метода к проксимальному алгоритму решения $\ell_1$ регрессии (ISTA) называется FISTA (Fast ISTA).

**Общие выводы:**

* Добавление momentum к градиентному спуску позволяет повысить его устойчивость и избегать маленьких локальных минимумов/максимумов;
* В выпуклом случае добавление моментного слагаемого позволяет доказуемо улучшить асимптотику и уменьшить зависимость от плохой обусловленности задачи.
* Идея ускорения применяется к любым около-градиентным методам, в том числе и к проксимальным, позволяя получить, например, ускоренный метод для $\ell_1$-регрессии.

## Адаптивный подбор размера шага

Выше мы попытались эксплуатировать свойства градиентного спуска. Теперь же пришел момент взяться за больной вопрос: как подбирать размер шага? Он максимально остро встаёт в случае SGD: ведь посчитать значение функции потерь в точке очень дорого, так что методы в духе наискорейшего спуска нам не помогут!

Нужно действовать несколько хитрее.

### Adagrad

Рассмотрим первый алгоритм, который является адаптацией стохастического градиентного спуска. Впервые он предложен в [статье в JMLR 2011 года](http://jmlr.org/papers/v12/duchi11a.html), но она написана в очень широкой общности, так что читать её достаточно сложно.

Зафиксируем $\alpha$ – исходный learning rate. Затем напишем следующую формулу обновления:

$$\begin{align}
    G_{k+1} &= G_k + (\nabla f(x_k))^2 \\
    x_{k+1} &= x_k - \frac{\alpha}{\sqrt{G_{k+1} + \varepsilon}} \nabla f(x_k).
\end{align}
$$

Возведение в квадрат и деления векторов покомпонентные. По сути, мы добавляем некоторую квазиньютоновость и начинаем динамически подбирать размер шага для каждой координаты по отдельности. Наш размера шага для фиксированной координаты – это какая-то изначальная константа $\alpha$ (learning rate), деленная на корень из суммы квадратов координат градиентов плюс дополнительный параметр сглаживания $\varepsilon$, предотвращающий деление на ноль. Добавка $\varepsilon$ на практике оставляется дефолтными `1e-8` и не изменяется.

Идея следующая: если мы вышли на плато по какой-то координате и соответствующая компонента градиента начала затухать, то нам нельзя уменьшать размер шага слишком сильно, поскольку мы рискуем на этом плато остаться, но в то же время уменьшать надо, потому что это плато может содержать оптимум. Если же градиент долгое время довольно большой, то это может быть знаком, что нам нужно уменьшить размер шага, чтобы не пропустить оптимум. Поэтому мы стараемся компенсировать слишком большие или слишком маленькие координаты градиента.

Но довольно часто получается так, что размер шага уменьшается слишком быстро и для решения этой проблемы придумали другой алгоритм.

### RMSProp

Модифицируем слегка предыдущую идею: будем не просто складывать нормы градиентов, а усреднять их в _скользящем режиме_:

$$\begin{align}
    G_{k+1} &= \gamma G_k + (1 - \gamma)(\nabla f(x_k))^2 \\
    x_{k+1} &= x_k - \frac{\alpha}{\sqrt{G_{k+1} + \varepsilon}} \nabla f(x_k).
\end{align}
$$

Такой выбор позволяет все еще учитывать историю градиентов, но при этом размер шага уменьшается не так быстро.

**Общие выводы:**

* Благодаря адаптивному подбору шага в современных оптимизаторах не нужно подбирать последовательность $\alpha_k$ размеров всех шагов, а достаточно выбрать всего одно число – learning rate $\alpha$, всё остальное сделает за вас сам алгоритм. Но learning rate все еще нужно выбирать крайне аккуратно: алгоритм может либо преждевременно выйти на плато, либо вовсе разойтись. Пример приведен на иллюстрации ниже.

![21_8_58c11d1982.webp](https://yastatic.net/s3/education-portal/media/21_8_58c11d1982_67a298fcb7.webp)


## Объединяем все вместе...

### Adam
Теперь покажем гвоздь нашей программы: алгоритм Adam, который считается решением по умолчанию и практически серебряной пулей в задачах стохастической оптимизации. 

Название Adam = ADAptive Momentum намекает на то, что мы объединим идеи двух последних разделов в один алгоритм. Приведем его алгоритм, он будет немного отличаться от [оригинальной статьи](https://arxiv.org/pdf/1412.6980) отсутствием коррекций смещения (bias correction), но идея останется той же самой:

$$\begin{align}
    v_{k+1} &= \beta_1 v_k + (1 - \beta_1) \nabla f(x_k) \\
    G_{k+1} &= \beta_2 G_k + (1 - \beta_2)(\nabla f(x_k))^2 \\
    x_{k+1} &= x_k - \frac{\alpha}{\sqrt{G_{k+1} + \varepsilon}} v_{k+1}.
\end{align}
$$

Как правило, в этом алгоритме подбирают лишь один гиперпараметр $\alpha$ – learning rate. Остальные же: $\beta_1$, $\beta_2$ и $\varepsilon$ – оставляют стандартными и равными `0.9`, `0.99` и `1e-8` соответственно. Подбор $\alpha$ составляет главное искусство. 

Зачастую, при начале работы с реальными данными начинают со значения learning rate равного 3e-4. История данного значения достаточно забавна: в 2016 году Андрей Карпатый (Andrej Karpathy) опубликовал шутливый [пост в Twitter](https://twitter.com/karpathy/status/801621764144971776).

![21_9_f4ea80ad49.webp](https://yastatic.net/s3/education-portal/media/21_9_f4ea80ad49_f2d43c4d2a.webp)

После чего сообщество подхватило эту идею (до такой степени, что иногда число `3e-4` называют Karpathy constant). 

Обращаем ваше внимание, что при работе с учебными данными зачастую полезно выбирать более высокий (на 1-2 порядка) начальный learning rate (например, при классификации MNIST, Fashion MNIST, CIFAR или при обучении языковой модели на примере поэзии выбранного поэта).

Также стоит помнить, что Adam требует хранения как параметров модели, так и градиентов, накопленного импульса и нормировочных констант (cache). Т.е. достижение более быстрой (с точки зрения количества итераций/объема рассмотренных данных) сходимости требует больших объемов памяти. Кроме того, если вы решите продолжить обучение модели, остановленное на некоторой точке, необходимо восстановить из чекпоинта не только веса модели, но и накопленные параметры Adam. В противном случае оптимизатор начнёт сбор всех своих статистик с нуля, что может сильно сказаться на качестве дообучения. То же самое касается вообще всех описанных выше методов, так как каждый из них накапливает какие-то статистики во время обучения.

_Интересный факт_: [Adam расходится на одномерном контрпримере](https://arxiv.org/pdf/1904.09237), что совершенно не мешает использовать его для обучения нейронных сетей. Этот факт отлично демонстрирует, насколько расходятся теория и практика в машинном обучении. В той же работе предложено исправление этого недоразумения, но его активно не применяют и продолжают пользоваться «неправильным» Adamом потому что он быстрее сходится на практике.

### AdamW

А теперь давайте добавим $\ell_2$-регуляризацию неявным образом, напрямую в оптимизатор и минуя адаптивный размер шага: 

$$\begin{align}
    v_{k+1} &= \beta_1 v_k + (1 - \beta_1) \nabla f(x_k) \\
    G_{k+1} &= \beta_2 G_k + (1 - \beta_2)(\nabla f(x_k))^2 \\
    x_{k+1} &= x_k - \left( \frac{\alpha}{\sqrt{G_{k+1} + \varepsilon}} v_{k+1} \color{red}{ + \lambda x_{k}} \right).
\end{align}
$$

Это сделано для того, чтобы эффект $\ell_2$-регуляризации не затухал со временем и обобщающая способность модели была выше. Оставим ссылку на одну [заметку](https://towardsdatascience.com/why-adamw-matters-736223f31b5d) про этот эффект. Отметим, впрочем, что этот алгоритм особо не используется.


## Практические аспекты

### Расписания

Часто learning rate понижают итеративно: каждые условные 5 эпох (LRScheduler в Pytorch) или же при выходе функции потерь на плато. При этом лосс нередко ведет себя следующим схематичным образом:

![21_10_c00d6ba357.webp](https://yastatic.net/s3/education-portal/media/21_10_c00d6ba357_274a5a4ab3.webp)

Помимо этого используют другие варианты «расписаний» для learning rate. Из часто применяемых неочевидных лайфхаков: сначала сделать warmup, то есть увеличивать learning rate, а затем начать постепенно понижать. Использовалось в известной [статье про трансформеры](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf). В ней предложили следующую формулу:

$$
    lr = d^{-0.5}_{\rm{model}} \cdot \min(step\_ num^{-0.5}, step\_ num \cdot warmup\_ steps^{-1.5}).
$$

По сути, первые $warmup\_ steps$ шагов происходит линейный рост размера шага, а затем он начинает уменьшаться как $1/\sqrt{t}$, где $t$ — число итераций.

Есть и вариант с косинусом из [отдельной библиотеки для трансформеров](https://huggingface.co/transformers/main_classes/optimizer_schedules.html).

![21_11_2b8059a205.webp](https://yastatic.net/s3/education-portal/media/21_11_2b8059a205_50aa041e53.webp)

В этой же библиотеке можно также почерпнуть идею рестартов: с какого-то момента мы снова включаем warmup, увеличивая размер шага.

### Большие батчи

Представим ситуацию, что мы хотим обучить свою нейронную сеть на нескольких GPU. Одно из решений выглядит следующим образом: загружаем на каждую видеокарту нейронную сеть и свой отдельный батч, вычисляем стохастические градиенты, а затем усредняем их по всем видеокартам и делаем шаг. Что плохого может быть в этом?

По факту, эта схема в некотором смысле эквивалентна работе с одним очень большим батчем. Хорошо же, нет разве?

На самом деле существует так называемый generalization gap: использование большого размера батча может приводить к худшей обобщающей способности итоговой модели. О причине этого эффекта можно поспекулировать, базируясь на текущих знаниях о ландшафтах функций потерь [при обучении нейронных сетей](https://losslandscape.com/).

Больший размер батча приводит к тому, что оптимизатор лучше «видит» ландшафт функции потерь для конкретной выборки и может скатиться в маленькие «узкие» паразитные локальные минимумы, которые не имеют обобщающий способности — при небольшом шевелении этого ландшафта (distributional shift c тренировочной на тестовую выборку) значение функции потерь резко подскакивает. В свою очередь, широкие локальные минимумы дают модель с лучшей обобщающей способностью. Эту идею можно увидеть на следующей картинке:

![21_12_1c967fa6bc.webp](https://yastatic.net/s3/education-portal/media/21_12_1c967fa6bc_cf78d362ce.webp)

Иными словами, большие батчи могут приводить к переобучению, но это можно исправить правильным динамическим подбором learning rate, как будет продемонстрировано далее. Сразу отметим, что совсем маленькие батчи – это тоже плохо, с ними ничего не получится выучить, так как каждая итерация SGD знает слишком мало о ландшафте функции потерь.

#### LARS

Мы рассмотрим нестандартный оптимизатор для обучения нейронных сетей, которого нет в Pytorch по умолчанию, но который много где используется: [Layer-wise Adaptive Rate Scaling (LARS)](https://arxiv.org/abs/1708.03888). Он позволяет эффективно использовать большие размеры батчей, что очень важно при вычислении на нескольких GPU. 

Основная идея заключена в названии – нужно подбирать размер шага не один для всей сети или каждого нейрона, а отдельный для _каждого слоя_ по правилу, похожему на RMSProp. По сравнению с оригинальным RMSProp подбор learning rate для каждого слоя дает большую стабильность обучения.

Теперь рассмотрим формулу пересчета: пусть $w_l$ – это веса слоя $l$, $l < L$. Параметры алгоритма: базовый learning rate $\eta$ (на который запускается расписание), коэффициент инерции $m$, коэффециент затухания весов $\beta$ (как в AdamW).

```python
for l in range(L):                                              # Цикл по слоям
    g_l = stochgrad(w_prev)[l]                                  # Вычисляем стохградиент из батча для текущего слоя
    lr = eta * norm(w[l]) / (norm(g_l) + beta * norm(w[l]))     # Вычислеяем learning rate для текущего слоя
    v[l] = m * v[l] + lr * (g_l + beta * w[l])                  # Обновляем momentum
    w[l] -= v[l]                                                # Делаем градиентный шаг по всему слою сразу
w_prev = w                                                      # Обновляем веса
```

#### LAMB

Этот оптимизатор введен в статье [Large Batch Optimization For Deep Learning](https://arxiv.org/abs/1904.00962) и является идейным продолжателем LARS, более приближенным к Adam, чем к обычному RMSProp. Его параметры – это параметры Adam $\eta, \beta_1, \beta_2, \varepsilon$, которые берутся как в Adam, а также параметр $\lambda$, который отвечает за затухание весов ($\beta$ в LARS). 

```python
for l in range(L):                                              # Цикл по слоям
    g_l = stochgrad(w_prev)[l]                                  # Вычисляем стохградиент из батча для текущего слоя
    m[l] = beta_1 * m[l] + (1 - beta_1) * g_l                   # Вычисляем моментум
    v[l] = beta_2 * v[l] + (1 - beta_2) * g_l                   # Вычисляем новый размер шага
    m[l] /= (1 - beta_1**t)                                     # Шаг для уменьшения смещения из Adam
    v[l] /= (1 - beta_2**t)
    r[l] = m[l] / sqrt(v[l] + eps)                              # Нормируем моментум как предписывает Adam
    lr = eta * norm(w[l]) / norm(r[l] + llambda * w[l])         # Как в LARS
    w[l] = w[l] - lr *  (r[l] + llambda * w[l])                 # Делаем шаг по моментуму
w_prev = w                                                      # Обновляем веса
```

### Усреднение 

Теперь снова заглянем в теорию: на самом деле, все хорошие теоретические оценки для SGD проявляются, когда берётся усреднение по точкам.

Этот эффект при обучении нейронных сетей был исследован в статье про [алгоритм SWA](https://arxiv.org/abs/1803.05407). Суть очень проста: давайте усреднять веса модели по каждой $c$-й итерации; можно считать, что по эпохам. В итоге, веса финальной модели являются усреднением весов моделей, имевших место в конце каждой эпохи.

В результате такого усреднения сильно повышается обобщающая способность модели: мы чаще попадаем в те самые широкие локальные минимумы, о которых мы говорили в разделе про большие батчи. Вдохновляющая картинка из статьи прилагается:

![21_13_4ffd39e8b7.webp](https://yastatic.net/s3/education-portal/media/21_13_4ffd39e8b7_63f2f47fe5.webp)

На второй и третьей картинке изображено сравнение SGD и SWA при обучении нейронной сети (Preactivation ResNet-164 on CIFAR-100) при одной и той же инициализации.

На первой же картинке изображено, как идеологически должен работать SWA. Также мы видим тут демонстрацию эффекта концентрации меры: после обучения стохастический градиентный спуск становится случайным блужданием по области в окрестности локального минимума. Если, например, предположить, что итоговая точка – это нормальное распределение с центром в реальном минимуме в размерности $d > 10^6$, то все эти точки с большой вероятности будут находиться в окрестности сферы радиуса $\sqrt{d}$. Интуитивную демонстрацию многомерного нормального распределения можно увидеть на следующей картинке из книги Р.Вершинина "High-Dimensional Probability" (слева в размерности 2, справа в большой размерности):

![21_14_c0f6658635.webp](https://yastatic.net/s3/education-portal/media/21_14_c0f6658635_4fc6bb36db.webp)

Поэтому, чтобы вычислить центральную точку этой гауссианы, усреднение просто необходимо, по такому же принципу работает и SWA.

### Предобуславливание

Теперь мы снова обратимся к теории: скорость сходимости градиентного спуска (даже ускоренного) очень сильно зависит от числа обусловленности задачи. Разумной идеей будет попытаться использовать какие-то сведения о задаче и улучшить этот показатель, тем самым ускорив сходимость.

В теории, здесь могут помочь техники [предобуславливания](https://en.wikipedia.org/wiki/Preconditioner). Но, к сожалению, попытки наивно воплотить эту идею приводят к чему-то, похожему на метод Ньютона, в котором нужно хранить большую-большую матрицу для обучения больших моделей. Способ обойти эту проблему рассмотрели в статье о методе [Shampoo](https://arxiv.org/abs/1802.09568), который использует то, что веса нейронной сети зачастую удобно представлять как матрицу или даже многомерный тензор. Таким образом, Shampoo можно рассматривать как многомерный аналог AdaGrad.

  ## handbook

  Учебник по машинному обучению

  ## title

  Оптимизация в ML

  ## description

  Как найти оптимум функции потерь: от градиентного спуска до Adam

- 
  ## path

  /handbook/ml/article/proksimalnye-metody

  ## content

  В этом разделе мы поговорим о том, как оптимизировать негладкие функции в ситуациях, когда «плохую» составляющую удаётся локализовать и она сравнительно несложная.

## Проксимальная минимизация

Для того, чтобы подступиться к проксимальным методам, посмотрим на градиентный спуск с другой стороны. Для простоты рассмотрим константный размер шага $\alpha$. Перепишем шаг градиентного спуска следующим образом:

$$
    \frac{x_{k+1} - x_k}{\alpha} = - \nabla f(x_k).
$$

Посмотрим на это уравнение по-другому. Рассмотрим функцию $x(t)$, равную $x_k$ при $(k-1)\alpha < t \leq \alpha k$ ($t$ мы будем воспринимать, как некоторый временной параметр). Тогда при $t = \alpha k$:

$$
    \frac{x(t + \alpha) - x(t)}{\alpha} = - \nabla f(x(t)).
$$

Теперь слева не что иное, как аппроксимация производной! Если мы устремим $\alpha$ к нулю, то получится так называемое **уравнение градиентного потока**:

$$
    \dot{x} = -\nabla f(x).
$$

Эта динамика в случае выпуклой функции $f$ сходится к точке минимума $x^*$ из любой начальной точки при $t \to +\infty$. Сравнение между динамикой градиентного спуска и градиентного потока можно увидеть на следующем изображении:

![Proksimalnye_metody_267da99b75.webp](https://yastatic.net/s3/education-portal/media/Proksimalnye_metody_267da99b75_90aeece918.webp)

Первый состоит из дискретных шагов, второй же представляет из себя непрерывный процесс.

Нетрудно осознать физический смысл динамики $\dot{x} = -\nabla f(x)$: маленькое тело скатывается по склону графика функции так, что в любой момент её скорость совпадает с антиградиентом, то есть оно катится по направлению наискорейшего спуска.

Теперь представим, что мы сейчас занимается не машинным обучением, а численными методами. Перед нами есть обыкновенное дифференциальное уравнение (ОДУ), и его надо решить. Одним из численных методов решения ОДУ (более стабильным, чем обычная схема Эйлера) является обратная схема Эйлера (backward Euler scheme):

$$
    \frac{x_{k+1} - x_k}{\alpha_k} = -\nabla f(x_{\color{red}{k+1}}).
$$

В обратной схеме Эйлера мы делаем градиентный спуск, только градиент смотрим не в текущей точке (как было бы в обычной схеме Эйлера), а _буквально_ в будущей. Занятная идея, только вот напрямую выразить $x_{k+1}$ из этого уравнения не получится. Нужно поступить чуть хитрее. Заметим, что

$$\frac{(x_{k+1} - x_k)_i}{\alpha_k} = \left.\frac{1}{2\alpha_k} (x - x_k)^2_i \right\vert_{x_{k+1}}$$
 
Это позволяет нам сказать, что весь вектор $\frac{x_{k+1} - x_k}{\alpha_k}$ является градиентом функции $g(u) = \frac{1}{2\alpha_k} \Vert u - x_{k} \Vert^2$, посчитанном в точке $x_{k+1}$. Тогда получаем, что $x_{k+1}$ удовлетворяет следующему условию:

$$
    \nabla\left( g(u) + f(u) \right)(x_{k+1}) = 0.
$$

Если функция $f(x)$ выпуклая, то $f(x) + g(x)$ тоже выпуклая, и её стационарная точка будет точкой минимума. Стало быть, $x_{k+1}$ можно высчитывать по формуле 

$$
    x_{k+1} = \arg\min_{u}\left\{ f(u) + \frac{1}{2\alpha_k}\Vert u - x_{k} \Vert^2   \right\}.
$$

Определим **прокс-оператор** следующим образом:

$$
    \mathrm{prox}_{f}(x)  = \arg\min\left\{ f(u) + \frac{1}{2}\Vert u - x \Vert^2   \right\}.
$$

Тогда, поскольку умножение на $\alpha_k > 0$ внутри арг-минимума не влияет на саму точку минимума, получаем следующую итеративную схему:

$$
    x_{k+1} = \arg\min\left\{ \alpha_k \left(f(u) + \frac{1}{2\alpha_k}\Vert u - x \Vert^2   \right)\right\} = 
$$

$$
    \arg\min\left\{ \alpha_k f(u) + \frac{1}{2}\Vert u - x \Vert^2  \right\}= \mathrm{prox}_{\alpha_k f}(x_k).
$$

Итеративный процесс $x_{k+1} = \mathrm{prox}_{\alpha_k f}(x_k)$ называется **методом проксимальной минимизации**. Вы можете спросить себя: зачем он нужен? Ведь теперь на каждом шаге мы должны решать задачу оптимизации:

$$
    \min_{u} f(u) + \frac{1}{2\alpha_k}\Vert u - x_k \Vert^2
$$

Если $f$ выпуклая, нам есть, что ответить: наличие второго слагаемого гарантирует сильную выпуклость задачи, то есть она решается достаточно эффективно. Но если $f$ не является выпуклой, то мы ничего не достигли этой модификацией.

### Композитная оптимизация, проксимальный градиентный метод (PGM)

Чтобы понять, зачем нам понадобилась проксимальная оптимизация, рассмотрим оптимизацию функций вида 

$$
    \min_{x} \{ f(x) = g(x) + h(x)\},
$$

где $g(x)$ – это гладкая функция, а $h(x)$ – это функция, для которой прокс-оператор считается аналитически. Воспользуемся следующим трюком: по $g$ мы совершим градиентный шаг, а по $h$ – проксимальный. Получаем следующую итеративную процедуру:

$$
    x_{k+1} = \mathrm{prox}_{\alpha_k h} (x_k - \alpha_k \nabla g(x_k));
$$

Эта процедура определяет так называемый проксимальный градиентный метод (Proximal Gradient Method, PGM), который может использоваться, например, для решения задачи регрессии с $\ell_1$-регуляризацией.

### ISTA (Iterative Shrinkage-Thresholding Algorithm)

Теперь решим конкретную задачу $\ell_1$-регрессии. Она выглядит следующим образом:

$$
    \Vert y - Xw \Vert_2^2 + \lambda \Vert w \Vert_1 \to \min_w.
$$

Мы хотим применить PGM к этой задаче, для этого нужно научиться вычислять прокс-оператор для $\ell_1$-нормы. Проделаем эту операцию:

$$
    \mathrm{prox}_{\alpha \Vert \cdot \Vert_1}(x) = \arg\min_{u} \left\{  \Vert u \Vert_1 + \frac{1}{2 \alpha} \Vert u - x \Vert_2^2 \right\} = 
$$

$$
    = \arg\min_{u} \left\{ \sum_{i=1}^d \vert u_i \vert +  \frac{(u_i - x_i)^2}{2\alpha}  \right\}.
$$

Заметим, что каждое слагаемое зависит _только от одной координаты_. Это значит, что каждую координату мы можем прооптимизировать отдельно и получить $d$ одномерных задач минимизации вида 

$$
    \arg\min_{u_i} \left\{ \vert u_i \vert + \frac{(u_i - x_i)^2}{2\alpha} \right\}.
$$

Решение такой одномерной задачи записывается в виде функции _soft thresholding_:

$$
    \mathrm{prox}_{\alpha \Vert \cdot \Vert_1}(x)_i = \begin{cases}
        x_i - \alpha &, x_i \geq \alpha \\
        0 & \vert x_i \vert \leq \alpha \\
        x_i + \alpha & x_i \leq - \alpha
    \end{cases}
$$

Тогда мы получаем следующий алгоритм для $\ell_1$-регрессии, которые называются Iterative Shrinkage-Thresholding Algorithm (ISTA):

```python
w = normal(0, 1)                                            # инициализация
repeat S times:                                             # другой вариант: while abs(err) > tolerance
    f = X.dot(w)                                            # посчитать предсказание
    delta = f - y                                           # посчитать отклонение предсказания
    grad = 2 * X.T.dot(delta) / n                           # посчитать градиент
    w_prime = w - alpha * grad                              # считаем веса, которые отправим в прокс
    for i in range(d):
        w[i] = soft_threshold(w_prime[i], alpha * llambda)  # вычисляем прокс
```

Заметим одну крутую особенность этого алгоритма -- мы явно видим, что решение получается разреженное, ведь какие-то координаты будут явно зануляться при применении soft threshold! Причем чем больше размер и шага, и параметра регуляризации, тем больше прореживается координат.

Конкретно этот метод не применяется на практике, но используются его вариации. Например, [статья](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41159.pdf), которая указана в параграфе про [линейные модели](https://academy.yandex.ru/handbook/ml/article/linejnye-modeli) о том, как работало предсказание CTR в google в 2012 году, также базируется на вычислении soft threshold как прокс-оператора.


### Общие выводы

Подытожим все вышесказанное:

1. Проксимальные методы – теоретически интересная идея для выпуклой оптимизации, которая должна давать более численно стабильные алгоритмы.
2. Проксимальные методы позволяют достаточно эффективно решать задачи композитной оптимизации, в частности, $\ell_1$-регуляризованную задачу регрессии. Более того, используемые на практике решения задачи $\ell_1$-регуляризованной регрессии так или иначе базируются на идее ISTA. 
3. Также есть попытки использовать проксимальные методы для более сложных моделей. Например, [статья о применении их в нейросетях](https://arxiv.org/abs/1706.04638).

Кроме того, имеются применения проксимальных методов для построения распределенных алгоритмов. Все подробности можно найти в [монографии Neal Parikh и Stephen Boyd](https://web.stanford.edu/~boyd/papers/prox_algs.html), мы же только привели применение этих идей в машинном обучении.

  ## handbook

  Учебник по машинному обучению

  ## title

  Проксимальные методы

  ## description

  Как оптимизировать функции потерь с $L_1$-регуляризацией

- 
  ## path

  /handbook/ml/article/metody-vtorogo-poryadka

  ## content

  В этом разделе мы сконцентрируемся сначала на методах, которые используют информацию о гессиане функции, а затем рассмотрим, как, сохраняя высокоуровневую идею метода Ньютона, обойтись без гессиана.

## Метод Ньютона

Итак, наша задача – безусловная оптимизация гладкой функции 

$$
    f(x) \to \min_{x \in \mathbb{R}^d}.
$$

Как и при оптимизации методом градиентного спуска, мы будем искать направление уменьшения функционала. Но в этот раз мы будем использовать не линейное приближение, а квадратичное:

$$
    f(x + \Delta x) \approx f(x) + \langle \nabla f(x), \Delta x \rangle + \frac{1}{2}\langle \Delta x, B(x) \Delta x \rangle.
$$

Формула Тейлора говорит нам брать $B(x) = \nabla^2 f(x)$. Приравняв к нулю градиент этой квадратичной аппроксимации, мы получаем направление спуска для метода Ньютона:

$$
    \Delta x = [B(x)]^{-1} \nabla f(x).
$$

Обозначим $B_k = B(x_k), H_k = B_k^{-1}$. В таком случае мы можем записать итеративный алгоритм спуска:

$$
    x_{k+1} = x_{k} - \alpha_k \cdot H_k \nabla f(x_k).
$$

В литературе методом Ньютона называется такой метод при $\alpha_k = 1$, при другом размере шаге $\alpha_k \in (0, 1)$ этот метод называют **дэмпированным** (damped) методом Ньютона. 

Обсудим, в чем главная особенность метода Ньютона и в чем заключается выигрыш по сравнению с классическим градиентным спуском. Таких особенностей две.

### Скорость сходимости метода Ньютона

Первая связана со скоростью его сходимости. А именно – в окрестности решения он сходится _квадратично_.

**Теорема**. Пусть функция $f$ имеет достаточно гладкий гессиан и сильно выпукла в точке оптимума $x^*$. Тогда $\exists r > 0$, что для всякого $x_0 : \Vert x_0 - x^*\Vert \leq r$ для метода Ньютона с $\alpha_k = 1$ верно $\Vert x_{k+1} - x^* \Vert \leq c \Vert x_k - x^* \Vert^2$ для константы $c$ зависящей только от $f$.

{% cut "Набросок доказательства для интересующихся." %}

Немного поясним терминологию. Под достаточно гладким гессианом мы подразумеваем то, что он должен быть _липшицевым_ и дифференцируемым, из чего следует, что
    
$$
    \Vert \nabla^2 f(x) - \nabla^2 f(y) \Vert \leq L \Vert x - y \Vert,
$$

а также

$$ 
    \Vert \left[\nabla^3 f(x)\right] (x - y, x - y, \cdot) \Vert \leq L \Vert x - y\Vert^2,
$$

где $\left[\nabla^3 f(x)\right] (x - y, x - y, \cdot)$ – результат подстановки $(x - y)$ в качестве двух первых аргументов в трилинейную форму $\left[\nabla^3 f(x)\right]$. Под сильной выпуклостью мы подразумеваем здесь $\mu$-сильную выпуклость, которую здесь строго определять излишне (подробнее см. [в этой статье](https://en.m.wikipedia.org/wiki/Convex_function)), но из которой следует, что гессиан отделён от нуля:

$$\lambda_{\min}(\nabla^2 f(x)) \geq \mu$$

Разложим градиент по формуле Тейлора в точке $x_k$ с остаточным членом в форме Лагранжа:

$$
    0 = \nabla f(x^*) = \nabla f(x_k) + \nabla^2 f(x_k) (x^* - x_k) + \frac{1}{2} \nabla^3 f(\xi_k) [x^* - x_k]^2.
$$

Умножим с обеих сторон на обратный гессиан и получаем:

$$
    x_k - [\nabla^2 f(x_k)]^{-1} \nabla f(x_k) - x^* = \frac{1}{2} [\nabla^2 f(x_k)]^{-1}\nabla^3 f(\xi_k) [x^* - x_k]^2.
$$

Воспользуемся формулой шага $x_{k+1} = x_k - [\nabla^2 f(x_k)]^{-1} \nabla f(x_k)$, тогда в левой части равенства у нас $x_{k+1} - x^*$. Посчитаем норму и воспользуемся гладкостью:

$$
    \Vert x_{k+1} - x^* \Vert \leq \frac{L}{2}  \Vert [\nabla^2 f(x_k)]^{-1} \Vert  \Vert x^* - x_k \Vert^2.
$$

Теперь воспользуемся сильной выпуклостью в точке оптимума. Поскольку мы предполагаем, что точка старта достаточно близко к точке оптимума, то по гладкости можем считать, что в точке $x_k$ у нас есть хотя бы $\mu/2$-сильная выпуклость. Тогда получаем:

$$
    \Vert x_{k+1} - x^* \Vert \leq \frac{L}{4\mu} \Vert x_k - x^* \Vert^2.
$$

{% endcut %}

### Метод Ньютона и плохо обусловленные задачи

Второе приятное свойство заключается в устойчивости метода Ньютона к плохой обусловленности задачи (в отличие от метода градиентного спуска). Разберёмся, что это значит. Когда мы говорим о плохой обусловленности задачи, мы имеем в виду, что гессиан в точке оптимума плохо обусловлен, то есть отношение максимального и минимального собственных чисел является большим числом. Геометрически это значит, что линии уровня функции вблизи оптимума похожи на очень вытянутые эллипсоиды; мы уже обсуждали, что в такой ситуации градиентный спуск может работать медленно. А как справится метод Ньютона? Оказывается, намного лучше. И связано это с его инвариантностью к линейным преобразованиям.

А именно, рассмотрим функцию $\hat f(y) = f(Ay)$ для некоторой невырожденной матрицы $A$. Обозначим $x = Ay$. Посмотрим, как связаны градиент и гессиан новой функции с градиентом и гессианом старой. Воспользуемся производной сложной функции:

$$
    \nabla_y \hat f = A^\top_x \nabla f,
$$

$$
    \nabla^2_y \hat f = A^\top \nabla^2_x f A
$$

Рассмотрим теперь траекторию $x_0, x_1, \ldots, x_K$ метода Ньютона, запущенного из точки $x_0$ для поиска минимума функции $f$, и траекторию $y_0, y_1, \ldots,y_K$ метода Ньютона, запущенного для поиска минимума функции $\hat f$. Если $x_0 = A y_0$, то для всех $k$ будет верно $x_k = A y_k$, то есть траектории получаются одна из другой при помощи этого линейного преобразования, другими словами, траектории исходной и новой функции подобны.


{% cut "Для интересующихся доказательствами." %}
    
Докажем по индукции. Для $k=0$ это дано по условию. Теперь докажем шаг индукции:

$$
    y_{k+1} = y_{k} - \alpha_k [ \nabla^2_{y_k} \hat f]^{-1} \nabla_{y_k} \hat f = 
$$

$$
    y_k - \alpha_k (A^{-1} [\nabla^2_{A y_k} f] A^{-\top}) (A^{\top} \nabla_{A y_k} f).
$$

По предположению индукции $x_k = A y_k$, тогда получаем:

$$
    y_{k+1} = A^{-1} ( x_k - \alpha [\nabla^2 \hat f(x_k)] \nabla f(x_k) ) = A^{-1} x_{k+1} \Rightarrow x_{k+1} = A y_{k+1}.
$$  

{% endcut %}

Вернёмся теперь к плохо обусловленной задаче минимизации функции $f$. Рассмотрим линейное преобразование $A = (\nabla^2_{x^*} f)^{-1/2}$ и функцию $\hat f(x) = f(Ax)$. Тогда для функции $\hat f$ число обусловленности гессиана в точке оптимума равно в точность единице (проверьте это!), а траектории для этой новой, хорошо обусловленной функции, и старой, плохо обусловленной, подобны. В частности, метод Ньютона не будет, как градиентный спуск, долго метаться где-то на задворках вытянутой эллиптической «ямки» вокруг оптимума, а быстро ринется к центру.

Можно сказать, что метод Ньютона правильно улавливает кривизну линий уровня функции и это позволяет ему быстрее сходиться к оптимуму. Эту идею стоит запомнить, она появляется в некоторых вдохновлённых методами второго порядка модификациях SGD.

Также еще можно заметить, что свойства, которые мы требуем от функции в теореме о квадратичной сходимости, вообще говоря, не сохраняются при линейных преобразованиях: могут поменяться константы липшицевости и сильной выпуклости. Это простое замечание побудило исследователей ввести класс самосогласованных функций, более широкий и линейно инвариантный, для которого метод Ньютона также сходится. Подробнее об этом можно узнать в [разделе 9.6 книги S. Boyd & L. Vandenberghe, Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf).

### Слабости метода Ньютона

От хорошего переходим к плохому: к слабостям метода Ньютона. Во-первых, мы имеем квадратичную скорость сходимости только в _окрестности_ оптимума. А если мы стартуем из произвольно удалённой точки, то нам, как и в случае градиентного спуска, требуется подбор шага $\alpha_k$ при помощи линейного поиска (что нам вряд ли по карману). Если подбирать шаг не хочется, можно прибегнуть к интересному теоретическому методу получения гарантий на глобальную сходимость – добавлению кубической регуляризации.

{% cut "Немного деталей для пытливых" %}

В случае кубической регуляризации мы хотим обеспечить не просто аппроксимацию, но и оценку сверху:

$$
    f(x + \Delta x) \leq f(x) + \langle \nabla f(x), \Delta x \rangle + \frac{1}{2}\langle \Delta x, \nabla^2 f(x) \Delta x \rangle + \frac{M}{6} \Vert \Delta x \Vert^3_2.
$$

По сути, мы задаем кубическую модель, которую мы можем уже оптимзировать по $\Delta x$. Тогда мы получаем следующую итеративную процедуру _проксимального вида_:

$$
    x_{k+1} = \arg\min_{y} \biggl\{ f(x_k) + \langle \nabla f(x_k), y - x_k \rangle + \frac{1}{2}\langle y-x_k, \nabla^2 f(x_k) (y - x_k) \rangle + \frac{M}{6} \Vert y - x_k \Vert^3_2
    \biggl\}
$$

В таком случае можно использовать, например, постоянный размер шага и иметь гарантии на сходимость. Этот метод в последнее время стал пользоваться популярностью в теоретических исследованиях, в том числе в распределенной оптимизации. 

Также можно задаться простым вопросом: а можем ли мы пользоваться подобным разложением с регуляризацией для большего числа членов разложения по формуле Тейлора? На самом деле да, только коэффициент $M$ нужно подбирать чуть более специфично. Такие методы называются _тензорными методами_. 

{% endcut %}

Другая проблема кроется в формуле пересчета следующей итерации: _вычисление и обращение гессиана_. Конечно, вместо обращения гессиана можно честно решать систему линейных уравнений, но асимптотика остается прежней: $O(d^3)$, а от затрат памяти на хранение матрицы $O(d^2)$ вообще некуда деться. А это значит, что, например, решать линейную регрессию с ~10000 признаками методом Ньютона попросту невозможно. 

Есть и третья, малозаметная проблема: дословно метод Ньютона не работает для невыпуклых задач, поскольку $\nabla^2 f(x)$ не будет положительно опредленной и $\Delta x$ перестанет быть направлением спуска. Для решения этой проблемы можно немного «подпортить» нашу аппроксимацию и рассмотреть матрицу вида $B_k = \nabla^2 f(x_k) + \Delta_k$, такую что $B_k$ станет положительно определенной, и уже её подставлять в нашу квадратичную модель. Идея подмены гессиана на что-то более подходящее – это главная идея квазиньютоновских методов, обсуждаемых далее.

Итак, общие выводы:
* Метод Ньютона – теоретически оптимальный метод, который автоматически улавливает кривизну функции в окрестности оптимума.
* Для размерности $d > 1000$ он уже не является эффективным, поскольку требует вычисления и хранения гессиана, а также решения системы линейных уравнений с его участием (что может быть в общем случае очень дорого).

## Квазиньютоновские методы

Чтобы придумать, как бороться с проблемами метода Ньютона, нужно посмотреть на него с другой стороны, а для этого мы обратимся ненадолго к решению задачи нахождения нуля векторной функции.

### Метод касательной

Итак, рассмотрим совершенно новую задачу. Пусть дана функция $g \colon \mathbb{R}^n \to \mathbb{R}^n$ и нужно найти её ноль, то есть такое $x^*$, что $g(x^*) = 0$. Связь с оптимизацией (_по крайней мере в выпуклом случае_) довольно проста: если взять $g(x) = \nabla f(x)$, то корень уравнения $g(x) = 0$ и будет точкой оптимума.

Сначала рассмотрим одномерный случай $d=1$. Как найти ноль функции с помощью итеративной процедуры? Логично поступить следующим образом: проводим касательную $y = g'(x_n)(x - x_n) + g(x_n)$ к графику функции и находим точку $x_{n+1}$, в которой линейная аппроксимация обнуляется:

$$0 = g'(x_n)(x_{n+1} - x_n) + g(x_n),$$

откуда получаем формулу пересчета

$$
    x_{n+1} = x_n - \frac{g(x_n)}{g'(x_n)}.
$$

 
![23_1_241d0fc91a.webp](https://yastatic.net/s3/education-portal/media/23_1_241d0fc91a_b91154211d.webp)

Известно, что этот метод обладает квадратичной скоростью сходимости в одномерном мире, что очень перекликается с методом Ньютона для оптимизации – и не просто так.

Если рассмотреть многомерный случай, то вычисление производной заменяется на вычисление якобиана векторнозначной функции $g$. В случае $g = \nabla f$ наш якобиан становится гессианом и получаем в точности обычный метод Ньютона для оптимизации:

$$
    x_{n+1} = x_n - \left[\nabla^2_{x_n} f\right]^{-1} \nabla_{x_n} f.
$$

### Метод секущей и общая схема квазиньютоновских методов


Пусть мы хотим найти такую точку $x^*$, что $g(x^*) = 0$. В одномерном случае мы можем подменить вычисление $g'(x_n)$ вычислением её приближения $g(x_n) - g(x_{n-1}) / (x_n - x_{n-1})$. Откуда получаем формулу пересчета:

$$
    x_{n+1} = x_{n} - \frac{x_n - x_{n-1}}{g(x_n) - g(x_{n-1})} g(x_n)
$$

Графически, этот метод выглядит следующим образом:

![23_2_f753f18d7e.webp](https://yastatic.net/s3/education-portal/media/23_2_f753f18d7e_d22215a18e.webp)

Скорость сходимости этого метода несколько ниже, чем у метода Ньютона (линейная, а не квадратичная), но зато мы теперь не должны вычислять производную! В текущем виде, используя просто подмену градиента на его конечно-разностную аппроксимацию, не очевидно, как обобщить этот метод на произвольную размерность. Но, если посмотреть на название метода и на картинку, как он работает, мы видим, что мы по сути проводим через два предыдущих приближения секущую, а затем выбираем ноль этой секущей в качестве следующей точки. В многомерном случае мы можем выписать соответствующее ей уравнение $y = B_k(x - x^k) + g(x^{k})$, где $B_k$ – матрица размера $d \times d$, которая должна удовлетворять так называемому _уравнению секущей_ (secant equation):

$$
    B_k(x^k - x^{k-1}) = g(x^k) - g(x^{k-1}).
$$

Теперь, чтобы выбрать следующую точку, нужно найти ноль секущей, то есть

$$
    B_k(x^{k+1} - x^{k}) + g(x^k) = 0 \iff  x^{k+1} = x^k - B_k^{-1} g(x^k).
$$

А теперь рассмотрим $g(x) = \nabla f(x)$ и добавим в итеративную схему выше размер шага. Тогда мы получаем общую итеративную схему квазиньютоновских методов:

$$
    x^{k+1} = x^k - \alpha_k B_k^{-1} \nabla f(x^k).
$$

При этом необходимо выбирать такие $B_k$, чтобы они 

(а) были симметричными и положительно определенными и 

(б) удовлетворяли уравнению секущей

$$B_k(x^{k} - x^{k-1}) = \nabla f(x^k) - \nabla f(x^{k-1})$$

Первое требование восходит к двум соображениям. Первое – $B_k$ должно приближать гессиан, а он в идеале в окрестности точки минимума как раз является симметричным и положительно определенным. Второе соображение проще: в противном случае $d_k = -B_k^{-1} \nabla f(x^k)$ попросту не будет направлением спуска. Несмотря на эти два свойства, выбор по прежнему остается достаточно широким, откуда возникает большое разнообразие квазиньютоновских методов. Мы рассмотрим один классический и широко известный метод BFGS (Broyden, Fletcher, Goldfarb, Shanno).

### BFGS 

Сначала заметим, что в самом алгоритме в первую очередь используется обратная матрица к $B_k$, которую мы обозначим $H_k = B_k^{-1}$. Тогда выбирать $B_k$ – это тоже самое, что выбирать $H_k$. Введем еще два стандартных обозначения, чтобы можно было проще записывать все последующие формулы: $s_k = x_{k+1} - x_{k} = \alpha_k d_{k}$ и $y_k = \nabla f(x^{k+1}) - \nabla f(x^k)$. В их терминах уравнение секущей для $H_k$ выглядит максимально просто: $H_{k} y_{k-1} = s_{k-1}$.

Теперь введем некоторое искусственное требование, которое гарантирует единственность $H_{k+1}$ – выберем ближайшую подходящую матрицу к $H_k$, удовлетворяющую описанным выше условиям:

$$
    H_{k+1} = \text{argmin}_H\left\{\left.\frac12\Vert H - H_k \Vert\right| Н = H^\top, \ \ H y_k = s_k\right\}
$$

Вообще говоря, при выборе разных норм $\Vert \cdot \Vert$ мы будем получать разные квазиньютоновские алгоритмы. Рассмотрим один достаточно общий класс норм (аналог взвешенных $\ell_2$ норм в матричном мире):

$$
    \Vert A \Vert := \Vert W^{1/2} A W^{1/2} \Vert_F,
$$

где $\Vert \cdot \Vert_F$ – это Фробениусова норма

$$\Vert C \Vert_F^2 = \langle C, C\rangle_F = \text{tr}(C^\top C) = \sum_{i,j} C_{ij}^2,$$

а $W$ – некоторая симметричная и положительно определенная матрица весов, которую мы выберем таким образом, что она будет сама по себе удовлетворять уравнению секущей $Ws_k = y_k$. 

Сразу уточним, что матрица весов в таком случае меняется на каждой итерации и, по сути, на каждой итерации мы имеем разные задачи оптимизации, само же предположение задает дополнительную похожесть на обратный гессиан, поскольку можно взять в качестве весов усредненый гессиан

$$W = \bar G_k = [\int_0^1 \nabla^2 f(x_k + \tau \alpha_k p_k) d\tau]$$

Решив описанную выше оптимизационную задачу, мы получаем матрицу $H_{k+1}$, не зависящую явным образом от матрицы весов:

Эта формула как раз является ключевой в алгоритме BFGS. Чтобы заметить одно крайне важное свойство этой формулы, раскроем скобки:

$$
    H_{k+1} = H_k - \rho_k (H_k y_k s_k^{\top} + s_k y_k^\top H_k) + \rho_k^2 (s_k y_k^\top H_k y_k s_k^\top)  + \rho_k s_k s_k^\top.
$$

Отсюда мы видим, что нам в этой формуле достаточно умножать матрицу на вектор и складывать матрицы, что можно делать за $O(d^2)$ операций! То есть мы победили один из самых страшных минусов метода Ньютона. Воспользовавшись тем, что $ y_k^\top H_k y_k $ и $1/\rho_k = y_k^\top s_k = s_k^\top y_k$ – числа, перепишем формулу в более computational friendly стиле:

$$
    H_{k+1} = H_k + \rho_k^2 (1/\rho_k + y_k^\top H_k y_k)(s_k s_k^\top) - \rho_k (H_k y_k s_k^{\top} + s_k y_k^\top H_k).
$$

Общие выводы:
* Итерации BFGS вычислительно проще итераций метода Ньютона и не требуют вычисления гессиана;
* По скорости сходимости BFGS уступает методу Ньютона, но все равно является достаточно быстрым;
* По прежнему требуется $O(d^2)$ памяти, что по-прежнему вызывает проблемы при большой размерности ($10^4-10^5$).
* Время выполнения итерации $O(d^2)$ гораздо лучше, чем $O(d^3)$ метода Ньютона, но всё ещё оставляет желать лучшего.

Казалось бы, избавиться от $O(d^2)$ нельзя принципиально, ведь нужно как-то взаимодействовать с матрицей $H_k$ размера $O(d^2)$, а она не факт что разреженная. Но и в этом случае можно добиться улучшения до линейной сложности (как у градиентных методов!).

### L-BFGS

При взаимодействии с матрицами существует два основных способа хранить их дешевле, чем «по-честному». Первый способ – пользоваться разреженностью матрицы, а второй – низкоранговыми разложениями или чем-то близким. Поскольку сейчас мы не хотим добавлять предположений на задачу, которую мы решаем, то единственный выход – это пользоваться структурой $H_k$, возникающей в BFGS.

Если внимательно взглянуть на формулы обновления, то их можно переписать в следующем виде:

$$
    H_{k+1} = V(s_k, y_k)^\top H_k V(s_k, y_k) + U(s_k, y_k),
$$
$$
    V(s_k, y_k) = I - \rho_k y_k s_k^\top, \ \ \ U(s_k,y_k) = \rho_k s_k s_k^\top
$$

Для того, чтобы перейти от $H_k$ к $H_{k+1}$, можно хранить не матрицу $H_{k}$, а набор пар из k пар $(s_i, y_i)_{i=1,\ldots,k}$ и начальное приближение $H_0$ (например, $H_0 = \gamma I$ для некоторого $\gamma > 0$), чтобы «восстановить» $H_k$. Пользуясь такой структурой, мы можем хранить матрицу $H_{k+1}$ при помощи лишь $(k+1) \cdot 2d + 1$ чисел, а не $d^2$. К сожалению, такая структура имеет довольно простую проблему: при $k > d/2$ затраты памяти становятся только выше. 

Возникает простая идея – а давайте хранить только последние $m = \text{const}$ обновлений! Таким образом, мы получаем алгоритм L-BFGS, который имеет уже линейные $O(md)$ затраты памяти и, что немаловажно, такие же линейные затраты $O(md)$ на итерацию, ведь умножение матриц $V$ и $U$ на вектор может осуществляться за линейное время.

Общие выводы:
* L-BFGS обладает линеной сложностью итерации, линейными требованиями по дополнительной памяти и к тому же требует вычислять только градиенты!
* Производительность сильно зависит от константы $m$, отвечающей за точность аппроксимации гессиана;
* Как и все методы из этого раздела, требует точного, а не стохастического вычисления градиентов.

## Практические аспекты 

Из всех перечисленных в этом разделе методов важнее всего отметить L-BFGS как самый практичный. Он реализован в любой* библиотеке, которая имеет дело с оптимизацией чего-либо и может быть эффективным, если удаётся вычислить градиенты (и значения функций для линейного поиска размера шага). К сожалению, это получается не всегда: при больших размерах датасета вычисление честного градиента и значения для функционалов вида суммы

$$
    L(X,Y) = \sum_{i=1}^N L(x_i, y_i)
$$

не представляется возможным за разумное время. В таком случае мы вынуждены вернуться в мир стохастического градиентного спуска. Общая идея более тонкого учёта геометрии линий уровня функции потерь, в чём-то напоминающая происходящее в методе Ньютона, находит применение и в ряде вариаций SGD, но, конечно, порождает совершенно другие методы.

Что же касается самого метода Ньютона, его можно несколько оптимизировать, если смириться с тем, что всё вычисляется неточно. Во-первых, обратную матрицу к гессиану матрицу на самом деле не нужно ни хранить, ни даже вычислять. Давайте разберёмся, почему. Умножить $(\nabla^2f)^{-1}$ на вектор $v$ – это то же самое, что решить систему с левой частью $\nabla^2f$ и правой частью $v$, а для решения систем уравнений существуют эффективные итеративные методы, не меняющие левой части системы, а требующие лишь уметь умножать её на разные векторы. При этом умножать гессиан на вектор можно при помощи автоматического дифференцирования. Кроме того, можно на кажом шаге неточно решать систему, получая таким образом неточный метод Ньютона. Теория предписывает решать систему все точнее с ростом номера итерации, но на практике нередко используют фиксированное и небольшое число шагов итеративных методов решения систем линейных уравнений.

  ## handbook

  Учебник по машинному обучению

  ## title

  Методы второго порядка

  ## description

  От метода Ньютона до LBFGS

- 
  ## path

  /handbook/ml/article/shodimost-sgd

  ## content

  Стохастический Градиентный Спуск (SGD) имеет достаточно простую запись:

$$
x_{k+1} = x_k - \alpha_k g_k.
$$

Здесь $g_k$ &mdash; это некоторая аппроксимация градиента целевой функции $\nabla f(x_k)$ в точке $x_k$, называемая стохастическим градиентом (или просто стох. градиентом), $\alpha_k > 0$ &mdash; это размер шага (stepsize, learning rate) на итерации $k$. Для простоты мы будем считать, что $\alpha_k = \alpha > 0$ для всех $k \geq 0$. Обычно предполагается, что стох. градиент является несмещённой оценкой $\nabla f(x_k)$ при фиксированном $x_k$: $\mathbb{E}\left(g_k \mid x_k\right) = \nabla f(x_k)$. 

## Доказательство сходимости

Зададимся следующим вопросом: с какой скоростью и в каком смысле SGD сходится к решению и сходится ли? Во-первых, как и во многих работах по стохастической оптимизации, нас будет интересовать сходимость метода в среднем, т.е. оценки на $\mathbb{E}\left(\vphantom{\frac14}\vert\vert x_k - x_\ast\vert\vert^2\right)$ или $\mathbb{E}\left(f(x_k) - f(x_\ast)\right)$, где $x_\ast$ &mdash; решение задачи (для простоты будем считать, что оно единственное). 

Во-вторых, чтобы SGD сходился в указанном смысле, необходимо ввести дополнительные предположения. Действительно, например, если дисперсия стох. градиента не ограничена $\mathbb{E}\left(\vphantom{\frac14}\vert\vert g_k - \nabla f(x_k)\vert\vert^2 \mid x_k\right) = \infty$, то $\mathbb{E}\left(\vphantom{\frac14}\vert\vert x_k - x_\ast\vert\vert^2\right) = \infty$ и никаких разумных гарантий доказать не удаётся. Поэтому дополнительно к несмещённости часто предполагается, что дисперсия равномерно ограничена: предположим, что существует такое число $\sigma \ge 0$, что для всех $k \ge 0$ выполнено

$$
\mathbb{E}\left(\left.\vphantom{\frac14}\vert\vert g_k - \nabla f(x_k)\vert\vert^2\right| x_k\right) \leq \sigma^2.
$$

Данное предположение выполнено, например, для задачи логистической регрессии (поскольку в данной задаче норма градиентов слагаемых ограничена), но в то же время является весьма обременительным. Его можно заменить на более реалистичные предположения, что мы немного затронем далее. Однако при данном предположении анализ SGD является очень простым и полезным для дальнейших обобщений и рассуждений.

Для простоты везде далее мы будем считать, что функция $f$ является **$L$-гладкой** и **$\mu$-сильно выпуклой**, т.е. для всех $x, y \in \mathbb{R}^d$ выполнены неравенства

$$
    \vert\vert\nabla f(x) - \nabla f(y)\vert\vert \leq L\vert\vert x-y\vert\vert,
$$

$$
    f(y) \geq f(x) + \langle\nabla f(x), y- x \rangle + \frac{\mu}{2}\vert\vert y - x\vert\vert^2.
$$

**Теорема**. Предположим, что $f$ является $L$-гладкой и $\mu$-сильно выпуклой, стох. градиент $g_k$ имеет ограниченную дисперсию, и размер шага удовлетворяет $0 < \alpha \leq 1/L$. Тогда для всех $k \geq 0$ выполняется неравенство

$$
        \mathbb{E}\left(\vphantom{\frac14}\vert\vert x_k - x_{\ast}\vert\vert^2\right) \leq (1 - \alpha\mu)^k\vert\vert x_0 - x_{\ast}\vert\vert^2 + \frac{\alpha\sigma^2}{\mu}.
$$

**Доказательство.** Используя выражение для $x_{k+1}$, мы выводим

$$
\vert\vert x_{k+1} - x_\ast \vert\vert^2 = \vert\vert x_k - x_{\ast} - \alpha g_k\vert\vert^2\\
= \vert\vert x_k - x_{\ast}\vert\vert^2 - 2\alpha \langle x_k - x_{\ast}, g_k \rangle + \alpha^2 \vert\vert g_k\vert\vert^2
$$

Далее мы берём условное матожидание $\mathbb{E}\left(\cdot\mid x_k\right)$ от левой и правой частей и получаем:

$$
    \mathbb{E}\left(\left.\vphantom{\frac14}\vert\vert x_{k+1} - x_\ast\vert\vert^2\right| x_k\right) = \vert\vert x_k - x_{\ast}\vert\vert^2 - 2\alpha\mathbb{E}\left( \langle x_k - x_{\ast}, g_k \rangle \mid x_k\right) + \alpha^2 \mathbb{E}\left(\left.\vphantom{\frac14}\vert\vert g_k\vert\vert^2\right| x_k\right)\\
    = \vert\vert x_k - x_{\ast}\vert\vert^2 - 2\alpha\left\langle x_k - x_{\ast}, \mathbb{E}\left(g_k \mid x_k\right) \right\rangle + \alpha^2 \mathbb{E}\left(\left.\vphantom{\frac14}\vert\vert g_k\vert\vert^2\right| x_k\right)\\
    = \vert\vert x_k - x_{\ast}\vert\vert^2 - 2\alpha\langle x_k - x_{\ast}, \nabla f(x_k)\rangle + \alpha^2 \mathbb{E}\left(\left.\vphantom{\frac14}\vert\vert g_k\vert\vert^2\right| x_k\right).
$$

Следующий шаг в доказательстве состоит в оценке второго момента $\mathbb{E}\left(\vphantom{\frac14}\vert\vert g_k\vert\vert^2 \mid x_k\right)$. Используя предположение об ограниченности дисперсии стох. градиента, мы выводим:

$$
    \mathbb{E}\left(\left.\vphantom{\frac14}\vert\vert g_k\vert\vert^2\right| x_k\right) = \mathbb{E}\left(\left.\vphantom{\frac14}\vert\vert\nabla f(x_k) + g_k - \nabla f(x_k)\vert\vert^2\right| x_k\right)\\
    = \vert\vert\nabla f(x_k)\vert\vert^2 + \mathbb{E}\left(\langle \nabla f(x_k), g_k - \nabla f(x_k) \rangle \mid x_k\right) + \mathbb{E}\left(\left.\vphantom{\frac14}\vert\vert g_k - \nabla f(x_k)\vert\vert^2 \right| x_k\right)\\
    = \vert\vert\nabla f(x_k)\vert\vert^2 + \left\langle \nabla f(x_k), \mathbb{E}\left(g_k - \nabla f(x_k) \mid x_k\right)\right\rangle  + \mathbb{E}\left(\left.\vphantom{\frac14}\vert\vert g_k - \nabla f(x_k)\vert\vert^2 \right| x_k\right)\\
    = \vert\vert\nabla f(x_k)\vert\vert^2 + \mathbb{E}\left(\left.\vphantom{\frac14}\vert\vert g_k - \nabla f(x_k)\vert\vert^2 \right| x_k\right)\\
    \leq \vert\vert\nabla f(x_k)\vert\vert^2 + \sigma^2
$$

Чтобы оценить сверху $\vert\vert\nabla f(x_k)\vert\vert^2$, мы используем следующий факт, справедливый для любой выпуклой $L$-гладкой функции $f$ (см. книгу Ю. Е. Нестерова "Методы выпуклой оптимизации", 2010):

$$
    \vert\vert\nabla f(x) - \nabla f(y)\vert\vert^2 \leq 2L\left(f(x) - f(y) - \langle \nabla f(y), x- y \rangle\right).
$$

Беря в этом неравенстве $x = x_k$, $y = x_\ast$ и используя $\nabla f(x_\ast) = 0$, получаем

$$
    \mathbb{E}\left(\left.\vphantom{\frac14}\vert\vert g_k\vert\vert^2\right| x_k\right) \leq 2L\left(f(x_k) - f(x_\ast)\right) + \sigma^2.
$$

Далее мы подставляем эту оценку в выражение для $\mathbb{E}\left(\vert\vert x_{k+1} - x_\ast\vert\vert^2 \mid x_k\right)$:

$$
    \mathbb{E}\left(\left.\vphantom{\frac14}\vert\vert x_{k+1} - x_\ast\vert\vert^2\right| x_k\right) \leq \vert\vert x_k - x_{\ast}\vert\vert^2 - 2\alpha\langle x_k - x_{\ast}, \nabla f(x_k)\rangle + 2\alpha^2L\left(f(x_k) - f(x_\ast)\right) + \alpha^2\sigma^2.
$$

Остаётся оценить скалярное произведение в правой части неравенства. Это можно сделать, воспользовавшись сильной выпуклостью функции $f$: из

$$
    f(x_\ast) \geq f(x_k) + \langle\nabla f(x_k), x_\ast - x_k \rangle + \frac{\mu}{2}\vert\vert x_\ast - x_k\vert\vert^2
$$

следует

$$
    \langle\nabla f(x_k), x_k - x_\ast \rangle \geq f(x_k) - f(x_\ast) + \frac{\mu}{2}\vert\vert x_k - x_\ast\vert\vert^2.
$$

Используя это неравенство в выведенной ранее верхней оценке на $\mathbb{E}\left(\vert\vert x_{k+1} - x_\ast\vert\vert^2\mid x_k\right)$, мы приходим к следующему неравенству:

$$
    \mathbb{E}\left(\left.\vphantom{\frac14}\vert\vert x_{k+1} - x_\ast\vert\vert^2\right| x_k\right) \leq (1-\alpha\mu)\vert\vert x_k - x_{\ast}\vert\vert^2 -2\alpha(1 - \alpha L)\left(f(x_k) - f(x_\ast)\right) + \alpha^2\sigma^2\\
    \leq (1-\alpha\mu)\vert\vert x_k - x_{\ast}\vert\vert^2 + \alpha^2\sigma^2,
$$

где в последнем неравенстве мы воспользовались неотрицательностью $2\alpha(1 - \alpha L)\left(f(x_k) - f(x_\ast)\right)$, что следует из $0 < \alpha \leq 1/L$ и $f(x_k) \geq f(x_\ast)$. Чтобы получить результат, заявленный в теореме, нужно взять полное мат. ожидание от левой и правой частей полученного неравенства (воспользовавшись при этом крайне полезным свойством условного мат. ожидания &mdash; tower property: $\mathbb{E}\left(\mathbb{E}\left(\cdot\mid x^k\right)\right) = \mathbb{E}\left(\cdot\right)$)

$$
    \mathbb{E}\left(\vphantom{\frac14}\vert\vert x_{k+1} - x_\ast\vert\vert^2\right) \leq (1-\alpha\mu)\mathbb{E}\left(\vphantom{\frac14}\vert\vert x_k - x_{\ast}\vert\vert^2\right) + \alpha^2\sigma^2,
$$

а затем, применяя это неравенство для $\mathbb{E}\left(\vphantom{\frac14}\vert\vert x_k - x_{\ast}\vert\vert^2\right)$, $\mathbb{E}\left(\vphantom{\frac14}\vert\vert x_{k-1} - x_{\ast}\vert\vert^2\right)$, $\ldots$ , $\mathbb{E}\left(\vphantom{\frac14}\vert\vert x_1 - x_{\ast}\vert\vert^2\right)$, получим

$$
\mathbb{E}\left(\vphantom{\frac14}\vert\vert x_{k+1} - x_\ast\vert\vert^2\right) \leq (1-\alpha\mu)^{k+1}\vert\vert x_0 - x_{\ast}\vert\vert^2 + \alpha^2\sigma^2\sum\limits_{t=0}^k (1-\alpha\mu)^t\\
\leq (1-\alpha\mu)^{k+1}\vert\vert x_0 - x_{\ast}\vert\vert^2 + \alpha^2\sigma^2\sum\limits_{t=0}^\infty (1-\alpha\mu)^t\\
= (1 - \alpha\mu)^{k+1}\vert\vert x_0 - x_{\ast}\vert\vert^2 + \frac{\alpha\sigma^2}{\mu},
$$

что и требовалось доказать.

Данный результат утверждает, что SGD с потоянным шагом сходится линейно к окрестности решения, радиус которой пропорционален $\tfrac{\sqrt{\alpha} \sigma}{\sqrt{\mu}}$. Отметим, что чем больше размер шага $\alpha$, тем быстрее SGD достигает некоторой окрестности решения, в которой продолжает осциллировать. Однако чем больше размер шага, тем больше эта окрестность. Соответственно, чтобы найти более точное решение, необходимо уменьшать размер шага в SGD. Этот феномен хорошо проиллюстрирован [здесь](https://fa.bianp.net/teaching/2018/COMP-652/stochastic_gradient.html).

Теорема выше доказана при достаточно обременительных предположениях: мы предположили, что функция является сильно выпуклой, $L$-гладкой и стох. градиент имеет равномерно ограниченную дисперсию. В практически интересных задачах данные условия (в данном виде) выполняются крайне редко. Тем не менее, выводы, которые мы сделали из доказанной теоремы, справедливы для многих задач, не удовлетворяющих введённым предположениям (во многом потому, что указанные свойства важны лишь на некотором компакте вокруг решения задачи, что в свою очередь не так и обременительно).

Более того, если мы сделаем немного другое предположение о стохастических градиентах, то сможем покрыть некоторые случаи, когда дисперсия не является равномерно ограниченной на всём пространстве. Предположим теперь, что $g_k = \nabla f_{\xi_k}(x_k)$, где $\xi_k$ просэмплировано из некоторого распределения $\cal D$ независимо от предыдущих итераций, $f(x) = \mathbb{E}_{\xi\sim \cal D}\left(f_{\xi}(x)\right)$ и $f_{\xi}(x)$ является выпуклой и $L_{\xi}$-гладкой для всех $\xi$ (данное предположение тоже можно ослабить, но для простоты изложения остановимся именно на такой формулировке). Будем называть данные условия предположением о выпуклых гладких стохастчиеских реализациях. Они выполнены, например, для задач линейно регрессии и логистической регрессии.

В таком случае, для точек, сгенерированных SGD, справедливо, что SGD с потоянным шагом сходится линейно к окрестности решения, радиус которой пропорционален $\tfrac{\sqrt{\alpha} \sigma}{\sqrt{\mu}}$. Отметим, что чем больше размер шага $\alpha$, тем быстрее SGD достигает некоторой окрестности решения, в которой продолжает осциллировать. Однако чем больше размер шага, тем больше эта окрестность. Соответственно, чтобы найти более точное решение, необходимо уменьшать размер шага в SGD. Этот феномен хорошо проиллюстрирован [здесь](https://fa.bianp.net/teaching/2018/COMP-652/stochastic_gradient.html).

**Теорема**. Предположим, что $f$ является $L$-гладкой и $\mu$-сильно выпуклой, стохастчиеские реализации являются выпуклыми и гладкими, и размер шага удовлетворяет $0 < \alpha \leq 1/2L_{\max}$, где $L_{\max} = \max_{\xi\sim \cal D} L_{\xi}$. Тогда для всех $k \geq 0$ выполняется неравенство

$$
    \mathbb{E}\left(\vphantom{\frac14}\vert\vert x_k - x_{\ast}\vert\vert^2\right) \leq (1 - \alpha\mu)^k\vert\vert x_0 - x_{\ast}\vert\vert^2 + \frac{2\alpha\sigma_\ast^2}{\mu},
$$

где $\sigma_\ast^2 = \mathbb{E}_{\xi\sim \cal D}\vert\vert\nabla f_{\xi}(x_\ast)\vert\vert^2$.

**Доказательство**. Аналогично предыдущей доказательству предыдущей теоремы, получаем

$$
    \mathbb{E}\left(\left.\vphantom{\frac14}\vert\vert x_{k+1} - x_\ast\vert\vert^2\right| x_k\right) = \vert\vert x_k - x_{\ast}\vert\vert^2 - 2\alpha\langle x_k - x_{\ast}, \nabla f(x_k)\rangle + \alpha^2 \mathbb{E}\left(\left.\vphantom{\frac14}\vert\vert g_k\vert\vert^2\right| x_k\right).
$$

Поскольку $f_{\xi}(x)$ является выпуклой и $L_\xi$-гладкой, имеем (см. книгу Ю. Е. Нестерова "Методы выпуклой оптимизации", 2010):

$$
    \vert\vert\nabla f_{\xi}(x) - \nabla f_{\xi}(y)\vert\vert^2 \leq 2L_{\xi}\left(f_{\xi}(x) - f_{\xi}(y) - \langle \nabla f_{\xi}(y), x - y \rangle\right).
$$

Применяя это неравенство для $x = x_k$, $y = x_{\ast}$, получаем


$$
    \mathbb{E}\left(\left.\vphantom{\frac14}\vert\vert g_k\vert\vert^2\right| x_k\right) = \mathbb{E}_{\xi_k \sim \mathcal{D}}\left(\vert\vert\nabla f_{\xi_k}(x_k) - \nabla f_{\xi_k}(x_\ast) + \nabla f_{\xi_k}(x_\ast)\vert\vert^2\right)\\
    \leq 2\mathbb{E}_{\xi_k \sim \mathcal{D}}\left(\vert\vert\nabla f_{\xi_k}(x_k) - \nabla f_{\xi_k}(x_\ast)\vert\vert^2\right) + 2\mathbb{E}_{\xi_k \sim \mathcal{D}}\left(\vert\vert\nabla f_{\xi_k}(x_\ast)\vert\vert^2\right)\\
    \leq \mathbb{E}_{\xi_k \sim \mathcal{D}}\left(4L_{\xi_k}\left(f_{\xi_k}(x_k) - f_{\xi_k}(x_\ast) - \langle \nabla f_{\xi_k}(x_\ast), x_k - x_\ast \rangle\right)\right) + 2\mathbb{E}_{\xi \sim \mathcal{D}}\left(\vert\vert\nabla f_{\xi_k}(x_\ast)\vert\vert^2\right)\\
    \leq 4L_{\max} \left(f(x_k) - f(x_\ast) - \langle \nabla f(x_\ast), x_k - x_\ast\rangle\right) + 2\sigma_{\ast}^2 \\
    = 4L_{\max} \left(f(x_k) - f(x_\ast)\right) + 2\sigma_{\ast}^2,
$$


где во втором переходе мы воспользовались стандартным фактом: $\vert\vert a+b\vert\vert^2 \leq \vert\vert a\vert\vert^2 + \vert\vert b\vert\vert^2$ для любых $a, b \in \mathbb{R}^n$. Подставим полученное неравенство в выражение для $\mathbb{E}\left(\vert\vert x_{k+1} - x_\ast\vert\vert^2 \mid x_k\right)$, доказанное ранее:

$$
    \mathbb{E}\left(\left.\vphantom{\frac14}\vert\vert x_{k+1} - x_\ast\vert\vert^2\right| x_k\right) = \vert\vert x_k - x_{\ast}\vert\vert^2 - 2\alpha\langle x_k - x_{\ast}, \nabla f(x_k)\rangle + 4L_{\max}\alpha^2 \left(f(x_k) - f(x_\ast)\right) + 2\alpha^2\sigma_{\ast}^2.
$$

Остаётся оценить скалярное произведение в правой части неравенства. Это можно сделать, воспользовавшись сильной выпуклостью функции $f$: из

$$
    f(x_\ast) \geq f(x_k) + \langle\nabla f(x_k), x_\ast - x_k \rangle + \frac{\mu}{2}\vert\vert x_\ast - x_k\vert\vert^2
$$

следует

$$
    \langle\nabla f(x_k), x_k - x_\ast \rangle \geq f(x_k) - f(x_\ast) + \frac{\mu}{2}\vert\vert x_k - x_\ast\vert\vert^2.
$$

Используя это неравенство в выведенной ранее верхней оценке на $\mathbb{E}\left(\vert\vert x_{k+1} - x_\ast\vert\vert^2 \mid x_k\right)$, мы приходим к следующему неравенству:

$$
    \mathbb{E}\left(\left.\vphantom{\frac14}\vert\vert x_{k+1} - x_\ast\vert\vert^2\right| x_k\right) \leq (1-\alpha\mu)\vert\vert x_k - x_{\ast}\vert\vert^2 -2\alpha(1 - 2\alpha L_{\max})\left(f(x_k) - f(x_\ast)\right) + 2\alpha^2\sigma^2\\
    \leq (1-\alpha\mu)\vert\vert x_k - x_{\ast}\vert\vert^2 + 2\alpha^2\sigma_{\ast}^2,
$$

где в последнем неравенстве мы воспользовались неотрицательностью $2\alpha(1 - 2\alpha L_{\max})\left(f(x_k) - f(x_\ast)\right)$, что следует из $0 < \alpha \leq 1/2L_{\max}$ и $f(x_k) \geq f(x_\ast)$. Действуя по аналогии с доказательством предыдущей теоремы, получаем требуемый результат.

Выводы, которые можно сделать из данной теоремы, очень похожи на те, что мы уже сделали из прошлой теоремы. Главные отличия заключаются в том, что $L_{\max}$ может быть гораздо больше $L$, т.е. максимальный допустимый размер шага $\alpha$ в данной теореме может быть гораздо меньше, чем в предыдущей. Однако размер окрестности теперь зависит от дисперсии стох. градиента в решении $\sigma_\ast^2$, что может быть значительно меньше $\sigma^2$.

Рассмотрим важный частный случай &mdash; задачи минимизации суммы функций:

$$
\min\limits_{x \in \mathbb{R}^d}\left\{f(x) = \frac{1}{n}\sum\limits_{i=1}^n f_i(x)\right\}.
$$

Обычно $f_i(x)$ имеет смысл функции потерь на $i$-м объекте датасета. Предположим, что $f_i(x)$ &mdash; выпуклая и $L_i$-гладкая функция. Тогда выполняется предположение о выпуклых гладких стохастчиеских реализациях: действительно, достаточно задать $\xi$ как случайное число из $\{1,2,\ldots,n\}$, имеющее равномерное распределение. Тогда справедлив результат предыдущей теоремы с $L_{\max} = \max_{i\in 1,\ldots,n)}L_i$ и $\sigma_\ast^2 = \tfrac{1}{n}\sum_{i=1}^n \vert\vert\nabla f_i(x_\ast)\vert\vert^2$.

Для любого $K \ge 0$ можно выбрать шаг в SGD следующим образом:

$$
    \text{если } K \leq \frac{2L_{\max}}{\mu},  \gamma_k = \frac{1}{2L_{\max}},\\
    \text{если } K > \frac{2L_{\max}}{\mu} \text{ и } k < k_0,  \gamma_k = \frac{1}{2L_{\max}},\\
    \text{если } K > \frac{2L_{\max}}{\mu} \text{ и } k \geq k_0,  \gamma_k = \frac{1}{4L_{\max} + \mu(k-k_0)},
$$

где $k_0 = \lceil K/2 \rceil$. Тогда из доказанного выше результата следует (см. Лемму 3 из [статьи С. Стиха](https://arxiv.org/pdf/1907.04232.pdf)), что после $K$ итераций

$$
    \mathbb{E}\left(\vphantom{\frac14}\vert\vert x_K - x_\ast\vert\vert^2\right) = \cal O\left(\frac{L_{\max} \vert\vert x_0 - x_\ast\vert\vert^2}{\mu}\exp\left(-\frac{\mu}{L_{\max}}K\right) + \frac{\sigma_{\ast}^2}{\mu^2 K}\right).
$$

Таким образом, чтобы гарантировать $\mathbb{E}\left(\vphantom{\frac14}\vert\vert x_K - x_\ast\vert\vert^2\right) \leq \varepsilon$, SGD требуется

$$
  \cal O\left(\frac{L_{\max} }{\mu}\log\left(\frac{L_{\max} \vert\vert x_0 - x_\ast\vert\vert^2}{\mu\varepsilon}\right) + \frac{\sigma_{\ast}^2}{\mu^2 \varepsilon}\right)
$$

итераций/подсчётов градиентов слагаемых. Чтобы гарантировать то же самое, градиентному спуску (GD) необходимо сделать

$$
  \cal O\left(n\frac{L}{\mu}\log\left(\frac{\vert\vert x_0 - x_\ast\vert\vert^2}{\varepsilon}\right)\right)
$$

подсчётов градиентов слагаемых, поскольку каждая итерация GD требует $n$ подсчётов градиентов слагаемых (нужно вычислять полный градиент $\nabla f(x) = \tfrac{1}{n}\sum_{i=1}^n \nabla f_i(x)$). Можно показать, что $L \leq L_{\max} \leq nL$, поэтому в худшем случае полученная оценка для SGD заведомо хуже, чем для GD. Однако в случае, когда $L_{\max} = \cal O(L)$, однозначного вывода сделать нельзя: при большом $\varepsilon$ может доминировать первое слагаемое в оценке сложности SGD, поэтому в таком случае SGD будет доказуемо быстрее, чем GD (если пренебречь логарифмическими множителями). 

Иными словами, чтобы достичь не очень большой точности решения, выгоднее использовать SGD, чем GD. В ряде ситуаций небольшой точности вполне достаточно, но так происходит не всегда. Поэтому возникает ествественный вопрос: можно ли так модифицировать SGD, чтобы полученный метод сходился линейно асимптотически к точному решению (а не к окрестности как SGD), но при этом стоимость его итераций была сопоставима со стоимостью итераций SGD? Оказывается, что да и соответствующие методы называются методами редукции дисперсии.

## Методы редукции дисперсии

Перед тем, как мы начнём говорить о методах редукции дисперсии, хотелось бы раскрыть подробнее причину того, что SGD не сходится линейно асимптотически к точному решению. Мы рассмотрели анализ SGD в двух предположениях, и в обоих случаях нам требовалось вывести некоторую верхнюю оценку на второй момент стох. градиента, т.е. на $\mathbb{E}\left(\vert\vert g_k\vert\vert^2 \mid x_k\right)$. В обоих случаях эта оценка содержала некоторый константный член ($\sigma^2$ или $2\sigma_\ast^2$ &mdash; зависит от рассматриваемого предположения), который потом возникал и в финальной оценке на $\mathbb{E}\left(\vphantom{\frac14}\vert\vert x_k - x_\ast\vert\vert^2\right)$, препятствуя тем самым линейно сходимости метода. Конечно, это рассуждение существенно опирается на конкретный способ анализа метода, а потому не является строгим объяснением, почему SGD не сходится линейно.

Однако важно отметить, что оценка на $\mathbb{E}\left(\vert\vert g_k\vert\vert^2 \mid x_k\right)$ достаточно точно отражает поведение метода вблизи решения: даше если точка $x_k$ оказалась по какой-то причине близка к решению $x_\ast$ (или даже просто совпала с решением), то $\mathbb{E}\left(\vert\vert g_k\vert\vert^2 \mid x_k\right)$ и, в частности, $\mathbb{E}\left(\vert\vert g_k - \nabla f(x_k)\vert\vert^2 \mid x_k\right)$ будут порядка $\sigma^2$ или $\sigma_{\ast}^2$. Следовательно, при следующем шаге метод с большой вероятностью отдалится от/выйдет из решения, поскольку $\mathbb{E}\left(\vphantom{\frac14}\vert\vert x_{k+1} - x_k\vert\vert^2\right) = \alpha^2\mathbb{E}\left(\vphantom{\frac14}\vert\vert g_k\vert\vert^2\right) \sim \alpha^2\sigma^2$ или $\alpha^2\sigma_{\ast}^2$.


Из приведённых выше рассуждений видно, что дисперсия стох. градиента мешает методу сходится линейно к точному решению. Поэтому хотелось бы как-то поменять правило вычисления стох. градиента, чтобы выполнялись 3 важных свойства: (1) новый стох. градиент должен быть не сильно дороже в плане вычислений, чем подсчёт стох. градиента в SGD (градиента слагаемого), (2) новый стох. градиент должен быть несмещённой оценкой полного градиента $\nabla f(x_k)$, и (3) дисперсия нового стох. градиента должна уменьшаться в процессе работы метода. Например, можно рассмотреть следующий стох. градиент:

$$
g_k = \nabla f_{j_k}(x_k) + s_k, 
$$

где $j_k$ выбирается случайно равновероятно из множества $\{1, 2, \ldots, n\}$ и $\mathbb{E}\left(s_k\mid x_k\right) = 0$. В таком случае, будет выполнено свойство (2) из списка выше. Чтобы достичь желаемой цели, необходимо как-то специфицировать выбор случайного вектора $s_k$. Исторически одним из первых способов выбора $s_k$ был $s_k = -\nabla f_{j_k}(w_k) + \nabla f(w_k)$, где точка $w_k$ обновляется раз в $m \sim n$ итераций:

$$
    w_{k+1} = \begin{cases} w_k, & \text{if } k+1 \mod m \neq 0,\\ x_{k+1}, & \text{if } k+1 \mod m = 0. \end{cases}
$$

Данный метод называется Stochastic Variance Reduced Gradient (SVRG). Данный методы был предложен и проанализирован в [NeurIPS статье Джонсона и Жанга в 2013 году](https://proceedings.neurips.cc/paper/2013/hash/ac1dd209cbcc5e5d1c6e28598e8cbbe8-Abstract.html). Теперь же убедимся, что метод удовлетворяет всем трём отмеченным свойствам. Начнём с несмещённости:

$$
    g_k = \nabla f_{j_k}(x_k) - \nabla f_{j_k}(w_k) + \nabla f(w_k),
$$

$$
    \mathbb{E}\left(g_k\mid x_k\right) = \frac{1}{n}\sum\limits_{i=1}^n \left(\nabla f_{i}(x_k) - \nabla f_{i}(w_k) + \nabla f(w_k)\right) = \nabla f(x_k).
$$

Далее, вычисление $g_k$ подразумевает 2 подсчёта градентов слагаемых при $k \mod m \neq 0$ и $n+2$ подсчёта градентов слагаемых при $k \mod m \neq 0$. Таким образом, за $m$ последователльных итераций SVRG происходит вычисление $2(m-1) + n + 2 = 2m + n$ градиентов слагаемых, в то время как SGD требуется $m$ подсчётов градиентов слагаемых. Если $m = n$ (стандартный выбор параметра $m$), то $n$ итераций SVRG лишь в 3 раза дороже, чем $n$ итераций SGD. Иными словами, в среднем итерация SVRG не сильно дороже итерации SGD.

Наконец, если мы предположим, что метод сходится $\mathbb{E}\left(\vphantom{\frac14}\vert\vert x_k - x_\ast\vert\vert^2\right) \to 0$ (а он действительно сходится, см., например, доказательство вот [тут](http://proceedings.mlr.press/v108/gorbunov20a/gorbunov20a-supp.pdf)), то получим, что $\mathbb{E}\left(\vphantom{\frac14}\vert\vert w_k - x_\ast\vert\vert^2\right) \to 0$, а значит $\mathbb{E}\left(\vphantom{\frac14}\vert\vert x_k - w_k\vert\vert^2\right) \to 0$ и $\mathbb{E}\left(\vphantom{\frac14}\vert\vert\nabla f(w_k)\vert\vert^2\right) \to 0$. Но тогда в силу Липшицевости градиентов $f_i$ для всех $i=1,\ldots,n$ имеем:

$$
    \mathbb{E}\left(\vphantom{\frac14}\vert\vert g_k\vert\vert^2\right) = \mathbb{E}\left(\vphantom{\frac14}\vert\vert\nabla f_{j_k}(x_k) - \nabla f_{j_k}(w_k) + \nabla f(w_k)\vert\vert^2\right)\\
    \leq 2\mathbb{E}\left(\vphantom{\frac14}\vert\vert\nabla f_{j_k}(x_k) - \nabla f_{j_k}(w_k)\vert\vert^2\right) + 2\mathbb{E}\left(\vphantom{\frac14}\vert\vert\nabla f(w_k)\vert\vert^2\right)\\
    \leq 2L_{\max} \mathbb{E}\left(\vphantom{\frac14}\vert\vert x_k - w_k\vert\vert^2\right) + 2\mathbb{E}\left(\vphantom{\frac14}\vert\vert\nabla f(w_k)\vert\vert^2\right) \to 0,
$$

а значит, дисперсия $g_k$ стремится к нулю.

Приведённые выше рассуждения не являются формальным доказательством сходимости метода, но частично объясняют, почему метод сходится и, самое главное, объясняют интуицию позади формул, задающих метод. Строгое доказательство можно прочитать вот [тут](http://proceedings.mlr.press/v108/gorbunov20a/gorbunov20a-supp.pdf). Мы же здесь приведём результат о сходимости немного другого метода &mdash; Loopless Stochastic Variance Reduced Gradient (L-SVRG), который был предложен в [2015 году](https://proceedings.neurips.cc/paper/2015/hash/effc299a1addb07e7089f9b269c31f2f-Abstract.html) и переоткрыт в [2019 году](http://proceedings.mlr.press/v117/kovalev20a/kovalev20a.pdf). Основное отличие от SVRG состоит в том, что точка $w_k$ теперь обновляется на каждой итерации с некоторой маленькой вероятностью $p \sim 1/n$:

$$
    w_{k+1} = \begin{cases} w_k, & \text{с вероятностью } 1-p,\\ x_{k}, & \text{с вероятностью } p. \end{cases}
$$

Иными словами, L-SVRG имеет случайную длину цикла, в котором $w_k$ не обновляется. Вся интуиция и все наблюдения приведённые для SVRG выше, справедливы и для L-SVRG. 

Можно доказать следующий результат.

**Теорема**. Предположим, что $f$ является $L$-гладкой, $\mu$-сильно выпуклой и имеет вид суммы, функции $f_i$ являются выпуклыми и $L_i$-гладкими для всех $i=1,\ldots, n$, и размер шага удовлетворяет $0 < \alpha \leq 1/6L_{\max}$, где $L_{\max} = \max_{i\in 1,\ldots,n} L_{i}$. Тогда для всех $k \geq 0$ для итераций L-SVRG выполняется неравенство

$$
        \mathbb{E}\left(\vphantom{\frac14}\vert\vert x_k - x_{\ast}\vert\vert^2\right) \leq \left(1 - \min\left\{\alpha\mu, \frac{p}{2}\right\}\right)^kV_0,
$$

где $V_0 = \vert\vert x_0 - x_\ast\vert\vert^2 + \tfrac{4\alpha^2}{p}\sigma_0^2$, $\sigma_0^2 = \tfrac{1}{n}\sum_{i=1}^n \vert\vert\nabla f_i(x_0) - \nabla f_i(x_\ast)\vert\vert$.


**Замечание**. В частности, если $\alpha = 1/6L_{\max}$ и $p = 1/n$, то 

$$
    \mathbb{E}\left(\vphantom{\frac14}\vert\vert x_k - x_{\ast}\vert\vert^2\right) \leq \left(1 - \min\left\{\frac{\mu}{6L_{\max}}, \frac{1}{2n}\right\}\right)^kV_0.
$$

Следовательно, чтобы гарантировать $\mathbb{E}\left(\vphantom{\frac14}\vert\vert x_k - x_\ast\vert\vert^2\right) \leq \varepsilon$, L-SVRG требуется

$$
  \cal O\left(\left(n + \frac{L_{\max} }{\mu}\right)\log\left(\frac{V_0}{\varepsilon}\right)\right)
$$

итераций/подсчётов градиентов слагаемых (в среднем). Напомним, что чтобы гарантировать то же самое, градиентному спуску (GD) необходимо сделать

$$
  \cal O\left(n\frac{L}{\mu}\log\left(\frac{L \vert\vert x_0 - x_\ast\vert\vert^2}{\mu\varepsilon}\right)\right)
$$

подсчётов градиентов слагаемых, поскольку каждая итерация GD требует $n$ подсчётов градиентов слагаемых (нужно вычислять полный градиент $\nabla f(x) = \tfrac{1}{n}\sum_{i=1}^n \nabla f_i(x)$). Можно показать, что $L \leq L_{\max} \leq nL$, поэтому в худшем случае полученная оценка для L-SVRG не лучше, чем для GD. Однако в случае, когда $L_{\max} = \cal O(L)$, L-SVRG имеет сложность значительно лучше, чем GD (если пренебречь логарифмическими множителями).

В заключение этого раздела, хотелось бы отметить, что существуют и другие методы редукции дисперсии. Одним из самых популярных среди них является [SAGA](https://proceedings.neurips.cc/paper/2014/hash/ede7e2b6d13a41ddf9f4bdef84fdc737-Abstract.html). В отличие от SVRG/L-SVRG, в методе SAGA хранится набор градиентов $\nabla f_1(w_k^1), \nabla f_2(w_k^2), \ldots, \nabla f_n(w_k^n)$. Здесь точка $w_k^i$ обозначает точку, в которой в последний раз был подсчитан градиент функции $i$ до итерации $k$. Формально это можно записать следующим образом:

$$
    w_0^1 = w_0^2 = \ldots = w_0^n,
$$

$$
    g_k = \nabla f_{j_k}(x_k) - \nabla f_{j_k}(w_k^{j_k}) + \frac{1}{n}\sum\limits_{i=1}^n \nabla f_{i}(w_k^{i}),
$$

$$
    w_{k+1}^{j_k} = x_k, \quad w_{k+1}^i = w_k^i \text{ для всех } i \neq j_k,
$$

$$
    x_{k+1} = x_k - \alpha g_k.
$$
Основное преимущество SAGA состоит в том, что не требуется вычислять полный градиент всей суммы по ходу работы метода, однако в начале требуется посчитать градиенты всех слагаемых (отмечаем здесь, что эта операция может быть гораздо дороже по времени, чем вычисление полного градиента) и, более того, требуется хранить $n$ векторов, что может быть недопустимо для больших датасетов. В плане теоретических гарантий SAGA и L-SVRG не отличимы.

Ниже приведён график с траекториями SGD (с постоянным шагом), L-SVRG и SAGA при решении задачи логистической регрессии. Как можно видеть из графика, SGD достаточно быстро достигает не очень высокой точности и начинает осциллировать вокруг решения. В то же время, L-SVRG и SAGA достигают той же точности медленнее, но зато не осциллируют вокруг решения, а продолжают сходится (причём линейно).


**Сравнение работы SGD, L-SVRG и SAGA при решении задачи логистической регрессии на датасете gisette из библиотеки [LIBVSM](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html).**

![Shodimost_SGD_2c9e0b99e3.webp](https://yastatic.net/s3/education-portal/media/Shodimost_SGD_2c9e0b99e3_49bbc13459.webp)

  ## handbook

  Учебник по машинному обучению

  ## title

  Сходимость SGD

  ## description

  Почему он всё-таки сходится

- 
  ## path

  /handbook/ml/article/onlajn-obuchenie-i-stohasticheskaya-optimizaciya

  ## content

  ## О чём раздел про онлайн-обучение, кому и зачем его читать?

Во многих случаях обучение ML-модели ― это однократный процесс, после которого она не меняется и только используется для предсказания. А что, если к нам постоянно поступает новая информация и мы должны её учитывать? Тогда модель должна уметь обновляться при поступлении нового объекта или батча объектов. Грубо говоря, этим и занимается онлайн-оптимизация. Можно заметить, что обновление модели на батче объектов проходит и в процессе стохастической оптимизации, ― и это сходство не случайно.

Оказывается, что все известные вам методы стохастической оптимизации первого порядка ― такие как SGD, AdaGrad, Adam, AMSgrad и другие ― являются в первую очередь алгоритмами онлайн-обучения. Чтобы в этом убедиться, достаточно открыть эти статьи и увидеть, для какой задачи выводятся гарантии на сходимость. Постановка задачи онлайн-обучения является одновременно математически простой и очень общей, соединяя три больших темы:
1. «Классическое» онлайн обучение.
2. Стохастическую оптимизацию на фиксированном датасете. Мы покажем, что любой алгоритм онлайн обучения можно переформулировать, как алгоритм стохастической оптимизации; при этом из гарантий на сходимость, полученных для онлайн обучения, автоматически будет следовать сходимость на фиксированном датасете.
3. Adversarial обучение.

Данный текст является в первую очередь систематизирующим. Мы постараемся достичь следующих целей:
1. Подведем единую математическую базу, необходимую для вдумчивого чтения статей по оптимизации. Это будет полезно **ML-теоретикам**.
2. Покажем, как исторически развивались методы оптимизации, как из одного метода получался другой, какие проблемы они решали и ― главное ― актуальны ли эти проблемы сейчас.
3. Разберём все «именные» методы оптимизации на набор базовых концепций и покажем, как вы можете самостоятельно их сочетать, создавая оптимальный метод для решения своей задачи. Спойлер: базовых концепций *намного* меньше, чем наименований методов. Эти знания будут полезны **ML-инженерам**.
4. Пройдемся по относительно нишевым темам, таким как разреженные методы регуляризации $L_1$ и $L_{1/2}$, и рассмотрим наилучшие методы оптимизации для них. Такие методы невозможно получить в стандартной постановке стохастической оптимизации. Эти знания будут полезны **ML-инженерам**, занимающимся рекомендательными системами.

В параграфе «Введение в онлайн-обучение», которую вы читаете сейчас, вы познакомитесь с общей постановкой задачи онлайн-обучения, а также с семейством алгоритмов Follow the Regularized Leader (FTRL), которое включает в себя все методы первого порядка. Кроме того, вы узнаете, как сводить задачи стохастической оптимизации к задачам онлайн-обучения и увидите, что этот переход позволяет строить более эффективные методы стохастической оптимизации, особенно для разреженных регуляризаторов вроде $L_1$.

В параграфе «[Адаптивный FTRL](https://academy.yandex.ru/handbook/ml/article/adaptivnyj-ftrl)» вы узнаете, как улучшить сходимость алгоритмов стохастической оптимизации с помощью регуляризаторов и каковы гарантии сходимости для регуляризованных задач. Это позволит вывести AdaGrad как наилучший адаптивный метод для онлайн-оптимизации.

В параграфе «[Регуляризация в онлайн-обучении](https://academy.yandex.ru/handbook/ml/article/regulyarizaciya-v-onlajn-obuchenii)» мы снова поговорим о регуляризации, но на этот раз речь пойдёт о регуляризаторах, которые накладывают на решение определённые органичения, например, разреженность. Вы сможете с новой стороны взглянуть на разреживающие свойства $L_1$-регуляризаторов. Кроме того, мы получим не достижимые с помощью обычных SGD/AdaGrad результаты для разреженных $L_1$ и $L_{1/2}$ регуляризаторов.

В параграфе «[Стохастическая оптимизация в Deep Learning](https://academy.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning)» мы перейдём к методам оптимизации в глубоких нейросетях. Вас ждёт краткий исторический обзор и мотивация появления двух важных модификаций AdaGrad ― Adam и RMSprop. Мы покажем, что эти методы ломаются вокруг критических точек, и поговорим о том, как починить это и достичь более точной сходимости (этого эффекта можно достичь либо прямой модификацией алгоритмов (AMSgrad и RAdam), либо косвенно с помощью Learning Rate Scheduler'ов). 

В конце параграфа мы соберём воедино все рассмотренные концепции и покажем, как можно комбинировать лучшее из разных методов оптимизации в один новый метод.

## Оглавление

{% cut "<a href="https://academy.yandex.ru/handbook/ml/article/onlajn-obuchenie-i-stohasticheskaya-optimizaciya">Часть 1. Введение</a>" %}

   - [Постановка задачи](https://academy.yandex.ru/handbook/ml/article/onlajn-obuchenie-i-stohasticheskaya-optimizaciya#postanovka-zadachi)
   - [Выпуклая онлайн-оптимизация](https://academy.yandex.ru/handbook/ml/article/onlajn-obuchenie-i-stohasticheskaya-optimizaciya#vypuklaya-onlajn-optimizacziya)
   - [Follow the Leader](https://academy.yandex.ru/handbook/ml/article/onlajn-obuchenie-i-stohasticheskaya-optimizaciya#follow-the-leader)
   - [Follow The Regularized Leader (FTRL)](https://academy.yandex.ru/handbook/ml/article/onlajn-obuchenie-i-stohasticheskaya-optimizaciya#follow-the-regularized-leader)
   - [Линеаризация и вычислительно эффективный FTRL](https://academy.yandex.ru/handbook/ml/article/onlajn-obuchenie-i-stohasticheskaya-optimizaciya#linearizacziya-i-vychislitelno-effektivnyj-ftrl)
   - [Субдифференциал и субградиентные методы](https://academy.yandex.ru/handbook/ml/article/onlajn-obuchenie-i-stohasticheskaya-optimizaciya#subdifferenczial-i-subgradientnye-metody)

{% endcut %}

{% cut "<a href="https://academy.yandex.ru/handbook/ml/article/adaptivnyj-ftrl">Часть 2. Адаптивные методы оптимизации</a>" %}

   - [Аддитивные регуляризаторы](https://academy.yandex.ru/handbook/ml/article/adaptivnyj-ftrl#additivnye-regulyarizatory)
   - [Классы алгоритмов FTRL](https://academy.yandex.ru/handbook/ml/article/adaptivnyj-ftrl#klassy-algoritmov-ftrl)
   - [Гарантии сходимости для алгоритмов FTRL](https://academy.yandex.ru/handbook/ml/article/adaptivnyj-ftrl#garantii-shodimosti-dlya-algoritmov-ftrl)
   - [Построение эффективного адаптивного FTRL](https://academy.yandex.ru/handbook/ml/article/adaptivnyj-ftrl#postroenie-effektivnogo-adaptivnogo-ftrl)
      - [$O(1)$ learning rate](https://academy.yandex.ru/handbook/ml/article/adaptivnyj-ftrl#constant-learning-rate-ftrl)
      - [$O(\frac{1}{\sqrt{t}})$ learning rate](https://academy.yandex.ru/handbook/ml/article/adaptivnyj-ftrl#ftrl-s-learning-rate-scheduling)
      - [Data-Adaptive learning rate.](https://academy.yandex.ru/handbook/ml/article/adaptivnyj-ftrl#data-adaptive-ftrl)
      - [AdaGrad ― оптимальный data-adaptive метод](https://academy.yandex.ru/handbook/ml/article/adaptivnyj-ftrl#ada-grad-nailuchshij-adaptivnyj-metod)

{% endcut %}

{% cut "<a href="https://academy.yandex.ru/handbook/ml/article/regulyarizaciya-v-onlajn-obuchenii">Часть 3. Продвинутые методы регуляризации</a>" %}

   - [Идея неразложения регуляризаторов в субградиентную оценку](https://academy.yandex.ru/handbook/ml/article/regulyarizaciya-v-onlajn-obuchenii#ideya-nerazlozheniya-regulyarizatorov-v-subgradientnuyu-oczenku)
      - [Связь между Composite-Objective FTRL и Proximal Gradient Descent. Lazy vs Greedy представления](https://academy.yandex.ru/handbook/ml/article/regulyarizaciya-v-onlajn-obuchenii#svyaz-mezhdu-composite-objective-ftrl-i-proximal-gradient-descent-lazy-vs-greedy-predstavleniya)
   - [$L_1$-регуляризация](https://academy.yandex.ru/handbook/ml/article/regulyarizaciya-v-onlajn-obuchenii#l-1-regulyarizacziya)
      - [Отбор параметров разреженных моделей](https://academy.yandex.ru/handbook/ml/article/regulyarizaciya-v-onlajn-obuchenii#otbor-parametrov-razrezhennyh-modelej)
      - [Linear Incremental $L_1$. Аналог $L_1$ в Greedy методах](https://academy.yandex.ru/handbook/ml/article/regulyarizaciya-v-onlajn-obuchenii#linear-incremental-lambda-1-t-t-lambda-1)
      - [Global $L_1$. Лучший метод $L_1$ для онлайн обучения]()
   - [$L_2$ регуляризация](https://academy.yandex.ru/handbook/ml/article/regulyarizaciya-v-onlajn-obuchenii#l-2-regulyarizacziya)
      - [Weight Decay. Decoupled Weight Decay](https://academy.yandex.ru/handbook/ml/article/regulyarizaciya-v-onlajn-obuchenii#weight-decay)
   - [Проекция на выпуклое множество $\chi$](https://academy.yandex.ru/handbook/ml/article/regulyarizaciya-v-onlajn-obuchenii#i-chi-w-proekcziya-na-vypukloe-mnozhestvo-chi)

{% endcut %}
   
{% cut "<a href="https://academy.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning">Часть 4. Методы оптимизации в Deep Learning</a>" %}

   - [RMSprop и Adam](https://academy.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning#skolzyashhee-srednee-v-znamenatele-ada-grad-metody-rm-sprop-i-adam)
      - [Мотивация их создания](https://academy.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning#motivacziya)
      - [RMSprop](https://academy.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning#rms-prop)
      - [Adam](https://academy.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning#adam)
      - [Мотивация для bias correction]()
      - [Почему Adam ошибочно считают лучшим методом стохастической оптимизации]()
   - [Как сломать адаптивные методы RMSprop и Adam со скользящим средним](https://academy.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning#kak-slomat-adaptivnye-metody-so-skolzyashhim-srednim)
   - [Чиним RMSprop и Adam](https://academy.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning#chinim-rm-sprop-i-adam)
      - [Метод AMSgrad](https://academy.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning#am-sgrad)
         - [Реализация без дополнительной памяти](https://academy.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning#realizacziya-bez-dopolnitelnoj-pamyati)
         - [Добавление Bias Correction](https://academy.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning#dobavlenie-bias-correction)
      - [Learning Rate Scheduling](https://academy.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning#learning-rate-scheduling)
         - [Влияние learning rate decay на сходимость](https://academy.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning#vliyanie-learning-rate-decay-na-shodimost)
         - [Практические рекомендации](https://academy.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning#prakticheskie-rekomendaczii)
         - [Learning rate scheduling vs AdaGrad](https://academy.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning#learning-rate-scheduling-vs-ada-grad)
   - [SGD vs Adam. Методы AdamW/SGDW. Улучшение методов с помощью проксимального $L_2$](https://academy.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning#sgd-vs-adam)
   - [Momentum](https://academy.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning#momentum)
      - [Nesterov Momentum](https://academy.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning#nesterov-momentum)
      - [Adan](https://academy.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning#adan)

{% endcut %}

{% cut "<a href="https://academy.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning#sobiraem-vse-idei-voedino">Часть 5. Заключение</a>" %}

   - [Таблицы формул. Примеры комбинирования идей](https://academy.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning#primer-tabliczy-s-obshhimi-formulami)

{% endcut %}

## Постановка задачи

**Литература**. Отсюда и далее, пока явно не скажем о переходе на другие источники информации, используется материал из книги Shai Shalev-Shwartz [ Online Learning and Online
Convex Optimization](https://www.cs.huji.ac.il/w~shais/papers/OLsurvey.pdf)

Онлайн-обучение ― это процесс предсказания ответов на последовательность вопросов с учётом знания (возможно, неполного) о предыдущих правильных ответах.

Представим себе следующую игру (назовём её **игра (1)**). На каждом раунде игры $t$ мы:
1. Получаем $x_t$ ― частичную информацию о текущем «вопросе»;
2. Выбираем модель $w_t$, которой будем делать прогноз;
3. Прогнозируем $p_t(w_t, x_t)$;
4. Получаем истинный ответ $y_t$;
5. Получаем обратную связь-лосс $l(p_t, y_t)$. Лоссы обычно имеют семантику функции ошибки: больше ― хуже, меньше ― лучше.

Цель *любого* алгоритма онлайн обучения ― минимизация суммарной ошибки прогнозов $Loss(T) = \sum\limits_{t=1}^Tl(p_t, y_t)$ для любого количества раундов $T$.

Пока рассмотрим интуитивный пример: линейная регрессия (обозначения взяты из [параграфа про линейные модели](https://academy.yandex.ru/handbook/ml/article/linejnye-modeli)). Пусть у нас уже сыграны раунды $1,\ldots,t-1$ и есть выборка данных $x_1,\ldots,x_{t-1}$ и ответов $y_1,\ldots,y_{t-1}$.
1. Получаем новый $x_t$. В данном случае просто получаем и пока не используем;
2. Выбираем модель $w_t$, которая наилучшим образом объясняет всю предыдущую имеющуюся выборку $x_{1..t}$ (алгоритм обучения можем выбирать любой, какой нам нравится);
3. Прогнозируем $p_t = <w_t, x_t>$;
4. Получаем правильный ответ $y_t$;
5. Считаем loss $(y_t - p_t)^2$;

Действуя таким образом, мы делаем интуитивное предположение, что ответы $y_t$ как-то зависят от наших $x_t$ и что эту зависимость мы можем выучить из предыдущей выборки, улучшив прогноз на новых объектах.

### Предположения

Теория онлайн обучения выгодно отличается от классической теории статистического обучения довольно расслабленными и гораздо более простыми (с точки зрения математических формулировок) условиями. Мы **не делаем** предположений о некой статистической зависимости между $x_t$, $y_t$. Зависимость может быть детерминированной, стохастической или даже adversarial:
1. Детерминированная: в самом начала игры делается выбор детерминированной зависимости $x_t \rightarrow y_t$
2. Стохастическая: $x_t$ может быть реализациями случайной величины, зависящей от $y_t$
3. Adversarial: мы играем против активного противника, который может на каждом раунде игры по своему усмотрению менять зависимость $x_t \rightarrow y_t$ и/или подбирать $l(p_t, y_t)$, имея на руках *в том числе текущий ответ $y_t$, не доступный алгоритму онлайн-обучения*

Adversarial постановка включает в себя все остальные как частные случаи, так что сразу будем строить теорию для наиболее общего случая.

### Поведение алгоритма на шаге T
Начнем с введения метрики качества алгоритма на некотором раунде игры $T$, а затем расширим ее на все раунды игры.

Если у противника нет никаких ограничений, то противник всегда выигрывает. Поскольку $l(p_t, y_t)$ выбирается **после** нашего прогноза, он может выбрать любую функцию с сколь угодно большим штрафом.

Чтобы такого не случалось, мы предположим, что все ответы на шаге $T$ должны быть сгенерированы некоторым отображением $h^*: X\rightarrow Y, h^* \in H$, где $H$ ― пространство возможных решений, **известное и онлайн-алгоритму, и противнику**.

С учетом введенного ограничения на поведение противника, введем понятие *regret*:

$$Regret_T(h) = \sum\limits_{t=1}^T l(p_t, y_t) - \sum\limits_{t=1}^Tl(h(x_t), y_t)$$

Regret ― это метрика того, насколько онлайн алгоритм работает хуже, чем некоторая фиксированная модель-бейзлайн h (regret переводится как «сожаление»: насколько мы пожалели о том, что взяли онлайн алгоритм, а не модель h). Поскольку мы работаем в adversarial случае, то логично сравнивать наш онлайн алгоритм с сильнейшим возможным противником, а именно: противник всегда выбирает не «некоторую», а **наилучшую модель-бейзлайн** $h^* \in H$:

$$maxRegret(T) = \max\limits_{h^* \in H}\left[ \sum\limits_{t=1}^T l(p_t, y_t) - \sum\limits_{t=1}^Tl(h^*(x_t), y_t)\right]$$

### Поведение алгоритма на всей последовательности раундов игры

Вспомним, что вообще-то мы играем игру с бесконечным числом раундов. В таком случае, естественно будет анализировать поведение ряда $maxRegret(T), T \in \mathbb{N}, T \rightarrow \infty$. Здесь хочется еще раз подчеркнуть, в чем заключается adversarial поведение: на каждом шаге t maxRegret будет иметь **свою** наилучшую модель $h^*_t$ в бейзлайне:

$$maxRegret(\color{#348FEA}{T_1}) = \max\limits_{h^* \in H} \sum\limits_{t=1}^{\color{#348FEA}{T_1}} l(p_t, y_t) - \sum\limits_{t=1}^{\color{#348FEA}{T_1}}l(h^*(x_t), y_t) = \sum\limits_{t=1}^{\color{#348FEA}{T_1}} l(p_t, y_t) - \sum\limits_{t=1}^{\color{#348FEA}{T_1}}l(h^*_{\color{#348FEA}{T_1}}(x_t), y_t)$$

$$maxRegret(\color{#E06A27}{T_2}) = \max\limits_{h^* \in H} \sum\limits_{t=1}^{\color{#E06A27}{T_2}} l(p_t, y_t) - \sum\limits_{t=1}^{\color{#E06A27}{T_2}}l(h^*(x_t), y_t) = \sum\limits_{t=1}^{\color{#E06A27}{T_2}} l(p_t, y_t) - \sum\limits_{t=1}^{\color{#E06A27}{T_2}}l(h^*_{\color{#E06A27}{T_2}}(x_t), y_t)$$

### Качество онлайн алгоритма на протяжении всей игры

Когда мы говорим про adversarial setting и игру с противником, мы хотим не просто как-то минимизировать кумулятивный $Loss(T) = \sum\limits_{t=1}^Tl(p_t, y_t)$, но еще и хотим быть *не хуже нашего противника*. Потребуем, чтобы

$$\lim\limits_{T \rightarrow \infty} \frac{1}{T}maxRegret(T) = 0$$

Такое условие означает, что regret должен расти медленнее чем линейно (в таком случае говорят, что алгоритм имеет *сублинейный regret*).

Сублинейности бывают разные. Так, $Regret_T$ может быть ограничен сверху рядом с асимптотикой $\sqrt{T}$ или же рядом с асимптотикой $\log{T}$

Асимптотика $\log{T}$, очевидно, приводит к намного лучшей сходимости. Но достичь этого не всегда возможно. *Стандартной* асимптотикой regret в большинстве используемых на практике алгоритмов является $\sqrt{T}$, для этой асимптотики условия на задачу наименее жесткие. Все рассматриваемые нами ниже алгоритмы будут иметь асимптотику $\sqrt{T}$ и отличаться в основном константами в оценках (но, конечно, отличия в константах при оценке Regret часто приводят к существенно разному поведению на практике). Любые более мощные асимптотики требуют условий, которые крайне редко выполняются в практических задачах

### Online to batch conversion

В данном обзоре мы будем анализировать методы, которые гораздо чаще используются для оптимизации в классической постановке: есть фиксированный датасет $(x_i, y_i)_{i=1}^N$, модель $p_w(x)$ с обучаемыми параметрами $w$ и функция потерь $f$, задача ― найти минимум функции

$$\frac1N \sum\limits_{i=1}^N f(p_w(x_i), y_i)$$

Если представить, что все наши $f(p_w(x_i), y_i)$ ― независимые одинаково распределенные случайные величины, то можно считать, что на самом деле мы оптимизируем

$$\frac1N \sum\limits_{i=1}^N f(p_w(x_i), y_i) \approx \mathbb{E}_{(x,y)}f(p_w(x), y)$$

Такую постановку задачи часто называть батчевой (англ. batch). Это означает, что мы можем использовать два класса методов оптимизации:
* методы, которые на каждом шаге смотрят сразу на всю выборку (например, градиентный спуск или метод Ньютона);
* методы, которые на каждом шаге смотрят на случайное подмножество данных в надежде, что, итерируясь по таким подмножествам, мы сможем соптимизировать матожидание $\mathbb{E}f(w)$ (например, SGD). Такие методы называют стохастическими.

Существует специальный класс методов анализа сходимости, называемый online to batch conversion. Они позволяют адаптировать алгоритм онлайн-обучения к постановке задачи стохастической оптимизации на фиксированном датасете; при этом оценка на regret транслируется в асимптотику сходимости стохастической оптимизации. Математически строгий вывод этих методов обычно довольно громоздкий и не дарит более глубокого понимания идей в современных стохастических методах, это чисто технические выкладки, поэтому мы здесь ограничимся интуитивным описанием. Строгий вывод можно найти, например, в упомянутой выше книге Shai Shalev-Schwartz.

Процесс стохастической оптимизации на фиксированном датасете можно представить в виде задачи онлайн обучения, если вытянуть все эпохи (проходы по датасетам) в единую последовательность. Мы получим задачу онлайн обучения, в которой $(x_t, y_t)$ сэмплируются из фиксированного множества $(x_1,y_1),\ldots,(x_N,y_N)$. Строго говоря, тут сэмлпирование двухстадийное:
1. Берем исходное множество функций
2. Сэмплируем из него без возвращения, пока множество не станет пустым
3. Как только оно стало пустым ― заново заполняем его

Таким образом, деление на "эпохи" отчетливо видно и в вытянутой последовательности.

Легко видеть, что эта задача является корректной задачей онлайн обучения. Тут мы активно пользуемся тем, что постановка задачи онлайн обучения математически простая и очень общая. Из корректности данной задачи следует, что все алгоритмы онлайн обучения будут иметь на такой последовательности сублинейный regret.

Следующим шагом давайте взглянем на regret **в момент смены эпохи**. Обозначим за $M$―число эпох, тогда:

$$maxRegret(T) = \max\limits_{h^* \in H}\left[ \sum\limits_{t=1}^T l(p_t, y_t) - \sum\limits_{t=1}^Tl(h^*(x_t), y_t)\right] = \max\limits_{h^* \in H}\left[ \sum\limits_{m=1}^M \sum\limits_{i=1}^N l(p_{m,i}, y_i) - \sum\limits_{m=1}^M \sum\limits_{i=1}^N l(h^*(x_i), y_i)\right] = \max\limits_{h^* \in H}\left[ \sum\limits_{m=1}^M \sum\limits_{i=1}^N l(p_{m,i}, y_i) - M\sum\limits_{i=1}^N l(h^*(x_i), y_i) \right]$$

Из сходимости последовательности следует сходимость любой ее подпоследовательности, а значит, последовательность regret'ов в моменты смены эпох тоже ведет себя сублинейно:

$$\frac{1}{MN}\max\limits_{h^* \in H}\left[ \sum\limits_{m=1}^M \sum\limits_{i=1}^N l(p_{m,i}, y_i) - M\sum\limits_{i=1}^N l(h^*(x_i), y_i) \right] \rightarrow 0$$

$$\max\limits_{h^* \in H}\left[ \frac{1}{MN}\sum\limits_{m=1}^M \sum\limits_{i=1}^N l(p_{m,i}, y_i) - \frac1N\sum\limits_{i=1}^N l(h^*(x_i), y_i) \right] \rightarrow 0$$

$$\frac{1}{MN}\sum\limits_{m=1}^M \sum\limits_{i=1}^N l(p_{m,i}, y_i) - \min\limits_{h^* \in H}\left[ \frac1N\sum\limits_{i=1}^N l(h^*(x_i), y_i) \right] \rightarrow 0$$

Последнее слагаемое уже выглядит практически как постановка задачи стохастической оптимизации на фиксированном датасете! Интуиция на данный момент подсказывает нам, что разрыв между решениями, даваемыми онлайн обучением, и точным решением задачи батч-оптимизации, будет постепенно сокращаться.

В этот момент интуицию можно выключать―остаются только строгие технические выкладки по ссылкам выше.

## Выпуклая онлайн-оптимизация

Выпуклая оптимизация играет центральную роль в анализе алгоритмов онлайн-обучения и позволяет получать эффективные алгоритмы. Вот примеры задач, в которых она хорошо работает:
1. Линейная оптимизация;
2. Expert Advice problem;
3. Линейная/логистическая регрессия.

Для задач, возникающих в глубинном обучении, мы поступим согласно рекомендациям ведущих ученых: возьмем теоретически обоснованный алгоритм выпуклой оптимизации, воткнем в нейросеть и помолимся, чтобы он сохранил свои хорошие свойства. С методами первого порядка, как правило, работает (а здесь мы будем рассматривать только такие методы)

Введём в нашу игру предположение о выпуклости, а заодно попробуем сделать вычисления менее громоздкими. Для этого определим упрощённую **игру (2)**:
1. Выбираем параметрическую модель $w_t$;
2. Получаем извне **выпуклую** функцию потерь $f_t(w)$;
3. Считаем $f_t$ в точке $w_t$ и получаем наш loss $f_t(w_t)$.

Первое упрощение состоит в том, что прогноз $h_t$ и бейзлайн $h^*_t$ мы теперь берём не из абстрактного функционального множества $H$, а из некоторого параметризованного семейства. Говоря «модель $w_t$», мы имеем в виду «модель, заданная параметрами $w_t$». Скажем, для линейной регрессии это может быть вектор весов и bias. Regret будет записываться следующим образом:

$$maxRegret_T = \sum\limits_{t=1}^T f_t(w_t) - \sum\limits_{t=1}^T f_t(w_T^*)$$

Второе упрощение в том, что мы не думаем о признаках $x_t$ и таргетах $y_t$. Вся эта информация спрятана в определение функции $f_t(w)$. Например, для линейной регрессии $f_t(w) = (x_t^Tw - y_t)^2$. При этом теперь у нас нет частичной информации о текущем раунде игры **перед** выбором новой модели $w_t$: ведь мы сначала выбираем $w_t$ и лишь потом получаем $f_t(w)$.

**Обратите внимание**: если вы попробуете себе представить онлайн алгоритм на практике, то, как правило, частичная информация о функции $f_t(w)$ перед выбором $w_t$ вам **доступна**. Например, рассмотрим рекомендательную систему с онлайн-дообучаемой ранжирующей моделью:
1. Пользователь пришел, мы сразу пошли в базу данных за его историей покупок и получили признаковое описание (возможно частичное) $x_t$;
2. С учётом этого признакового описания мы выбираем модель $w_t$ и с её помощью оцениваем релевантность товаров этому пользователю;
3. Смотрим, что купил пользователь и купил ли, это даёт нам $f_t(w_t)$.

Тем не менее, в этом параграфе мы будем считать, что частичной информации нет, потому что хотим разрабатывать наиболее общий фреймворк, а не ad-hoc алгоритмы, использующие конкретный вид этой частичной информации. Если даже для какой-то узкой проблемы можно сформулировать специфический алгоритм, учитывающий частичную информацию, с высокой вероятностью он не будет работать значимо лучше стандартного решения. Если знаете контрпримеры ― напишите, добавим сюда для полноты.

## Follow the Leader

Предположим, что мы провели $t$ шагов игры (2) и теперь выбираем модель $w_{t+1}$ (как условились, без информации о $f_{t+1}(w)$). Наиболее естественным выбором будет алгоритм, минимизирующий ошибку на всех предыдущих раундах

$$w_{t+1} = arg\min\limits_w \sum\limits_{s=1}^{t}f_s(w)$$

Такой алгоритм называется **Follow The Leader (FTL)**, потому что мы идем вплотную за наилучшим возможным алгоритмом-бейзлайном в regret (лидером), который учитывает ещё и информацию с $(t+1)$-го шага:

$$w*_{t+1} = arg\min\limits_w \sum\limits_{s=1}^{\color{#E06A27}{t+1}}f_s(w)$$

К сожалению, для алгоритма в таком виде есть важные примеры выпуклых задач, когда он не работает. Допустим, наши функции потерь линейны $f_t(w) = g_t^Tw$. Вам может показаться, что линейная функция не особенно похожа на функцию потерь, но, забегая вперед, именно такие функции потерь встретятся дальше при изучении градиентных онлайн-алгоритмов ($g_t = \nabla f_t(w_t)$).

Рассмотрим одномерную задачу $f_t(w) = g_tw_t$, $g_t \in \mathbb{R}$, $w_t \in[-1;1]$. Пусть

$$g_t = \begin{cases}
      -0.5 & t=1 \\
      1 & t\%2 = 0 \\
      -1 & t\%2 = 1
   \end{cases}$$

Алгоритм FTL выглядит так:

$$w_{T+1} = arg\min\limits_w\sum\limits_{t=1}^T g_tw = arg\min\limits_w w\Big(\sum\limits_{t=1}^T g_t\Big)$$

Такие осциллирующие суммы коэффициентов будут заставлять FTL выбирать наихудшее возможное решение в каждом раунде. Функция потерь в каждом раунде будет равна $0.5$, а кумулятивная функция потерь примет вид $\sum\limits_{t=1}^T0.5 = 0.5T$. При этом кумулятивная функция потерь константного решения $w^*=0$ будет равна 0. Получаем линейный regret $0.5T$ относительно бейзлайна $w_T^* = w^* = 0$, алгоритм не сходится.

## Follow The Regularized Leader

Чтобы стабилизировать алгоритм, мы добавим регуляризаторы, и назовем получившийся алгоритм **Follow The Regularized Leader (FTRL)**:

$$w_T = arg\min\limits_w\Big[ \sum\limits_{t=1}^T f_t(w) + R(w)\Big]$$

**Упражнение**. Проверьте, что в примере из предыдущего параграфа добавление регуляризатора стабилизирует осцилляцию решения $w$.

Добавка $R(w)$ должна быть выпуклой и неотрицательной. При этом различный выбор $R(w)$ будет приводить к различным алгоритмам и различным оценкам на regret.

Первое, что приходит в голову ― это $L_2$ регуляризатор $R(w) = \vert\vert w\vert\vert_2^2$. Он даёт алгоритм

$$w_T = arg\min\limits_w\Big[ \sum\limits_{t=1}^T f_t(w) +\frac{1}{2\lambda}\vert\vert w\vert\vert_2^2\Big]$$

## Adaptive FTRL

Следующая идея―*сделать регуляризатор зависящим от данных (то есть от $f_t$) и своим на каждом раунде T*:

$$w_T = arg\min\limits_w\Big[ \sum\limits_{t=1}^T f_t(w) + R_T(w)\Big]$$

Забегая вперед―все современные градиентные алгоритмы Adam, RMSProp, AdaGrad и т.д. попадают в это семейство data-dependent регуляризаторов и работают **значительно** эффективнее любых алгоритмов с константными регуляризаторами $R(w)$.

**Обратите внимание**: регуляризаторы являются частью алгоритма FTRL, они **не входят** в формулу для regret, которая по-прежнему имеет вид

$$Regret_T(w^*) = \sum\limits_{t=1}^T f_t(w_t) - \sum\limits_{t=1}^T f_t(w^*)$$

Таким образом, мы не изменили постановку решаемой нами задачи, изменили лишь метод ее решения.

**Обратите внимание**: введение регуляризаторов влияет *только* на онлайн-алгоритм и выбор $w_t$. Бейзлайны $w_T^*$ выбираются как и раньше:

$$w_T^* = arg\min\limits_w \sum\limits_{t=1}^Tf_t(w)$$

## Линеаризация и вычислительно эффективный FTRL

Рассмотрим пример с логистической регрессией $f_t(w) = \log(1 + e^{-y_tw_t^Tx_t})$ и  константным $L_2$ регуляризатором:

$$\sum\limits_{t=1}^T \log(1 + e^{-y_tw_t^Tx_t}) + \frac{1}{2\eta}\vert\vert w\vert\vert_2^2\longrightarrow\min\limits_w$$

Классический пример использования онлайн логистической регрессии ― [предсказание CTR в рекламе]( https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41159.pdf). Миллионы запросов в секунду => миллионы решений этой оптимизационной задачи в секунду (если разбивать на батчи ― тысячи, но сути это не меняет). Успех онлайн-алгоритма в таких задачах определяется его вычислительной эффективностью, как по памяти, так и по скорости. Увы, с этим у нашего алгоритма не всё так хорошо:

* Скорость: аналитически задача не решается => FAIL
* Память: нужно хранить все предыдущие запросы $x_t$, $t\in{1..T}$ => FAIL

Здесь нам на помощь приходит линеаризация задачи. Если фунции $f_t(w)$ выпуклые (вниз) и гладкие (на негладкие посмотрим позже), то они удовлетворяют основному свойству выпуклых функций

$$f(w) \geq f(w_t) + [\nabla f(w_t)]^T(w - w_t)$$

Разложим все функции $f_t(w)$ в точках $w_t$:

$$f_t(w) \geq f_t(w_t) + [\nabla f(w_t)]^T(w - w_t)$$

$$f_t(w_t) - f_t(w) \leq [\nabla f(w_t)]^T(w - w_t)$$

Просуммируем от 1 до $T$

$$\sum\limits_{t=1}^T \Big(f_t(w_t) - f_t(w)\Big) \leq \sum\limits_{t=1}^T \Big([\nabla f_t(w_t)]^Tw_t - [\nabla f_t(w_t)]^Tw^T\Big)$$

Теперь обозначим $g_t = \nabla f_t(w_t)$ и рассмотрим выпуклую *линейную* задачу онлайн обучения с функцией потерь $\widetilde{f}_t(w) = g_t^Tw$. Regret для нее выглядит как

$$LinearizedRegret_T(w^*) = \sum\limits_{t=1}^T g_t^Tw_t - \sum\limits_{t=1}^T g_t^Tw^*$$

Неравенство выше позволяет нам оценить regret исходной задачи через regret линеаризованной:

$$Regret_T(w^*) \leq LinearizedRegret_T(w^*)$$


Минимизируя правую часть неравенства, мы, безусловно, будем минимизировать и левую, так что мы можем выбирать $w_t$ алгоритмом, решающим линеаризованную задачу, и получать хорошо сходящийся метод для исходной задачи.

Посмотрим, будет ли линеаризованный алгоритм вычислительно эффективнее. Посмотрим на линеаризацию задачи с data-depedent регуляризатором:

$$w_T = arg\min\limits_w\Big[ \sum\limits_{t=1}^T \nabla [f_t(w_t)]^Tw + R_T(w)\Big]$$

Линейные задачи имеют аналитическое решение для широкого спектра $R_T(w)$. Собственно, это и есть основное, что нужно помнить на практике ― выбирать регуляризатор так, чтобы эта задача решалась аналитически. Мы рассмотрим простейший случай $R_T(w) = R(w) = \frac{1}{2\eta}\vert\vert w\vert\vert_2^2$:

$$w_T = arg\min\limits_w\Big[ \sum\limits_{t=1}^T \nabla f_t(w_t)^Tw + \frac{1}{2\eta}\vert\vert w\vert\vert_2^2\Big]$$

Справа дифференцируемая функция, так что мы можем найти $w_T$, приравняв к нулю градиент:

$$w_T = -\eta\sum\limits_{t=1}^T \nabla f_t(w_t) = -\eta z_T,$$

где $z_T = \sum\limits_{t=1}^T \nabla f_t(w_t)$ ― это сумма векторов, которую не нужно пересчитывать заново на каждом шаге, а можно инкрементально обновлять. Благодаря этому нам больше не нужно помнить все предыдущие объекты выборки, достаточно хранить лишь некоторую статистику.

Готово, мы построили наш первый вычислительно эффективный алгоритм онлайн обучения! В дальнейшем мы займемся тем, чтобы найти *наилучший* вычислительно эффективный алгоритм.

**Обратите внимание**: теперь вы понимете, почему пример с линейной функцией потерь был так важен: линейные функции соответствуют линеаризованному regret. При этом, как мы уже выяснили, без регуляризатора такие линеаризованные задачи нестабильны.

**Обратите внимание**: если переписать немного формулу для $w_T$, мы получим:

$$w_T = -\eta\sum\limits_{t=1}^T \nabla f_t(w_t) = w_{T-1} - \eta \nabla f_t(w_t)$$

Таким образом, формулы FTRL c константным регуляризатором *эквивалентны формулам обычного стохастического градиентного спуска*. Забегая вперед, скажем, что различия в формулах градиентного спуска и FTRL будут *только* в разделе Composite objective FTRL. В этих отличиях и будет заключаться преимущество FTRL перед привычным SGD.

**Обратите внимание**: концепции FTRL и gradient descent в литературе часто называют *lazy* (ленивая) и *greedy* (жадная) соответственно.

Gradient descent жадный, потому что алгоритм для обновления $w_{t+1}$ использует только текущий $w_t$ и текущий градиент $g_t$. Всё, что было на предыдущих шагах, алгоритм забывает.

FTRL ленивый, потому что алгоритм в явном виде сохраняет всю информацию с начала обучения и рассчитывает $w_{t+1}$, исходя из всей истории $g_1,\ldots,g_t$, и только после этого применяет все регуляризаторы. Подробнее мы расскажем об этом в разделе «Сравнение Composite Objective FTRL-Proximal и Adaptive Gradient Descent».

## Субдифференциал и субградиентные методы

Выше мы рассматривали гладкие функции $f_t(w)$. Гладкость ― сильное ограничение, и оно на самом деле необязательно, можно ослабить условие, если использовать субградиенты.

Когда мы переходили от исходной задачи к линеаризованной, мы использовали основное свойство гладких выпуклых функций

$$f(w) \geq f(w_t) + [\nabla f(w_t)]^T(w - w_t), \quad \forall w_t$$

Гладкость обеспечивает существование $\nabla f(w_t)$ для всех $w_t$. Но нам ведь не нужно, чтобы существовал именно *градиент* функции. Нам достаточно, чтобы существовал *какой-то* вектор $g_t$, для которого выполнено неравенство

$$f(w) \geq f(w_t) + g^T(w - w_t)$$

И в этом помогают следующие два понятия.

**Субдифференциалом** функции $f(w)$ в точке $w_t$ называется множество

$$\partial_{w_t} f(w) = \left\{ g_t \mid f(w) \geq f(w_t) + f^T(w - w_t),\forall w\right\}$$

**Субградиентом** функции $f(w)$ в точке $w_t$ называется любой элемент множества $\partial f(w_t)$.

Потребуем, чтобы для любой точки был непустой субдифференциал, и дело в шляпе, можно вместо $\nabla f_t(w)$ везде подставлять субградиент $g_t$ и обобщить все выкладки выше на негладкий случай.

**Примеры**. Для гладких функций субдифференциал состоит из одной точки: градиента функции, а субградиент равен градиенту. В качестве примера функции с нетривиальным субградиентом рассмотрим функцию $f(x) = \vert x \vert$, где $x$ ― скаляр. Субградиент в точке $0$ ― это можество

$$\partial_0|x| = \left\{ \lambda\mid |x| \geq \alpha x \right\}$$

Легко видеть, что $\partial_0\vert x\vert $ ― это отрезок $[-1, 1]$.

**Замечание**. На практике субдифференциал используют не так часто. Оптимизационные задачи с популярными негладкими регуляризаторами $L_1$ решают «в лоб», без перехода к субградиентной оценке, например, с помощью [проксимальных методов](https://ysda_trove.gitlab.io/ml-handbook/chapters/optimization/proximal).

**Обратите внимание**. В литературе очень часто используется термин Online Mirror Descent. Mirror descent ― это оптимизационная процедура вида

$$w_{t+1} = arg\min\limits_w \left[\vphantom{\frac12}g_t^Tw + \lambda \psi(w) + \vert\vert w-w_t\vert\vert_2^2\right],$$

в которой $\psi$ ― дополнительный негладкий регуляризатор (например, тот же $L_1$), который мы как раз таки не заменяем на субградиентную оценку, а вместо этого оптимизируем всё «в лоб». Заметьте, что эти формулы *идентичны* формулам [Proximal Gradient Descent](https://ysda_trove.gitlab.io/ml-handbook/chapters/optimization/proximal). Если у нас нет регуляризатора $\psi$, то формулы эквивалентны обычному gradient descent.

Как вы увидите дальше, Mirror Descent ― это частный случай общего фреймворка, который мы описываем.

### Субградиентные методы оптимизации.

Почти все градиентные методы оптимизации обобщаются на негладкие функции. Модифицируется необходимое и достаточное условие минимума для выпуклых функций: точка $w^*$ является минимумом, если субдифференциал содержит ноль: $0 \in \partial f(w^*)$. Очевидно, это прямое обобщение условия для гладких функций, где субдифференциал состоит только из градиента функции.

  ## handbook

  Учебник по машинному обучению

  ## title

  Введение в онлайн-обучение

  ## description

  *null*

- 
  ## path

  /handbook/ml/article/adaptivnyj-ftrl

  ## content

  В данном разделе мы рассмотрим широкое семейство алгоритмов, позволяющее делать улучшения в способах введения регуляризации, которые невозможно добиться в классическом градиентном спуске.

### Полезные ссылки
Все написанное ниже (за исключением вывода AdaGrad) — сокращенный пересказ обзора H. Brendan McMahan [A Survey of Algorithms and Analysis for Adaptive Online Learning](https://www.jmlr.org/papers/volume18/14-428/14-428.pdf). Везде, где мы обозначаем Lemma 4, Theorem 10 и т.д. — мы ссылаемся на соответствующие теоремы из этой статьи. То же самое с доказательствами: если мы что-то опускаем, подробности можно найти в обзоре

Интуитивный вывод AdaGrad взят из статьи [Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf) . Вместо оригинальных оценок на метод Regularized Dual Averaging, требующих дополнительных понятий вроде двойственности по Фенхелю, мы использовали аналогичную оценку из обзора выше, сохранив все рассуждения автора. Опять же — строгое доказательство оценок на regret для AdaGrad есть в этом обзоре.

## Синтаксический сахар

В выкладках очень часто используются суммы, и без сокращенных обозначений читать их невозможно. В литературе про онлайн-обучение приняты вот такие сокращения:

* $\color{#348FEA}{r_{0:t}(w)} = \sum\limits_{s=0}^tr_s(w)$;
* Особо отметим обозначение $\color{#348FEA}{r_{0:t}(w_t)} = \sum\limits_{s=0}^tr_s(w_t)$, т.е. точка $w_t$ *фиксирована и не меняется с индексацией в сумме*;
* $\color{#348FEA}{h_{0:t}(w)} = f_{1:t}(w) + r_{0:t}(w)$ (обычно это будет сумма функции потерь и регуляризатора);
* $\color{#348FEA}{g_t}$ — субградиент функции $f_t(w)$ в точке $w_t$.


## Аддитивные регуляризаторы

В новых обозначениях описанные выше алгоритмы примут вид:
* Adaptive FTRL: $w_T = arg\min\limits_w \Big[f_{1:t}(w) + R_T(w)\Big]$
* Adaptive Linearized FTRL: $w_T = arg\min\limits_w \Big[\nabla f_{1:t}(w_t)^Tw + R_T(w)\Big]$

Опишем условия, накладываемые нами на алгоритм. В обзоре они называются Setting 1.

### Setting 1

От функций $R_T(w)$ мы потребуем, чтобы они представлялись в виде:

$$R_T(w) = \sum\limits_{t=0}^Tr_t(w) =  r_{0:T}(w)$$

Слагаемые должны удовлетворять следующим условиям:

1. Все $r_t(w)$ выпуклы (вниз);
2. $r_t(w) \geq 0$;
3. $w_0 = arg\min\limits_w r_0(w)$.

Также наложим следующие требования на $h_{1:t} = f_{1:t}(w) + r_{0:t}(w)$:
1. Область определения $h_{1:t}$ — непустое множество. Это требование может показаться странным, но при желании можно придумать пример $h_{1:t}$ с пустой областью определения: достаточно взять несколько регуляризаторов-проекций $I_{\chi}(w)$ на непересекающиеся выпуклые множества (подробнее о таких регуляризаторах мы расскажем в одном из следующих разделов);
2. Субдифференциал $\partial_{w_t} f_t(w)$ в точке $w_t$ непуст.

## Классы алгоритмов FTRL

Будем рассматривать аддитивные регуляризаторы $r_t(w)$ из двух семейств в зависимости от того, где у них минимум:

* **FTRL-Centered**: $arg\min\limits_w r_t(w) = w_0$;
* **FTRL-Proximal**: $arg\min\limits_w r_t(w) = w_t$;
* **Composite Objective**: смешение первых двух семейств.

**Обратите внимание**: название Proximal напрямую связано с проксимальным градиентным спуском (ссылка на учебник с проксимальными методами). В обоих случаях мы накладываем регуляризатор в текущей точке $w_t$.

**Обратите внимание**: для Proximal регуляризаторов зачастую требуют выполнения более сильного условия: $r_t(w_t) = 0$. Это не такое уж и серьёзное ограничение: все разумные Proximal регуляризаторы (например,  $\vert\vert w - w_t\vert\vert^2$) ему удовлетворяют.

**Обратите внимание**: у обоих семейств есть значимые высокоцитируемые статьи
* FTRL-Centered: метод [Regularized Dual Averaging](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/xiao10JMLR.pdf). Статья получила премию Test of Time Award на NeurIPS 2021, так как огромное количество последующих громких результатов (тот же AdaGrad) напрямую основывались на этих результатах. В названии Dual Averaging под dual average имеется в виду $\frac1t g_{1:t}$, то есть среднее по градиентам. Кардинально других техник оценок regret там нет, обзор McMahan строго улучшает все доступные там результаты.
* FTRL-Proximal: самая известная статья от гугла [Ad Click Prediction](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41159.pdf). Известна она скорее потому, что там выписаны формулы и объяснено, как правильно реализовывать метод для large-scale задач с результатами применения различных дополнительных инженерных идей. Это хороший *инженерный обзор*, а не математическая статья.

Рассмотрим отдельно каждую из разновидностей алгоритмов

### FTRL-Centered

Задача оптимизации имеет вид

$$w_{t+1} = arg\min\limits_{w} h_{0:t} = arg\min\limits_{w}\Big[ g_{1:t}^Tw + r_{0:t}(w)\Big],$$

где $r_t(w)$ таковы, что

$$arg\min\limits_w r_t(w) = w_0$$

**Пример**: Рассмотрим SGD с фиксированным learning rate и стартом в точке $0$. Положим

$$r_1(w) = \frac{1}{2\eta}\vert\vert w\vert\vert_2^2$$

$$r_t(w) = 0, \quad t > 0$$

$$w_0 = 0$$

$$w_{t+1} = arg\min\limits_{w}\Big[ g_{1:t}^Tw + \frac{1}{2\eta}\vert\vert w\vert\vert_2^2\Big].$$

Как мы уже знаем, итеративное обновление весов будет иметь вид

$$w_{t+1} = w_t - \eta g_t.$$

### FTRL-Proximal

Задача имеет похожий вид

$$w_{t+1} = arg\min\limits_{w} h_{0:t} = arg\min\limits_{w}\Big[ g_{1:t}^Tw + r_{0:t}(w)\Big],$$

но $r_t(w)$ выбираются так, чтобы

$$arg\min\limits_w r_t(w) = w_t$$

**Пример**: Рассмотрим SGD с убывающим learning rate:

$$\eta_t = \frac{\alpha}{\sqrt{t}}$$

$$\sigma_t = \frac{1}{\eta_t} - \frac{1}{\eta_{t-1}}$$

Подробный вывод связи $\sigma_t$ и $\eta_t$ мы приведём в одном из следующих разделов, а сейчас просто приведём результат:

$$r_t(w) = \sigma_t\vert\vert w - w_t\vert\vert_w^2$$

$$w_{t+1} = arg\min\limits_{w}\Big[ g_{1:t}^Tw + \sum\limits_{s=1}^t\sigma_s \vert\vert w - w_s\vert\vert_w^2\Big]$$

$$w_{t+1} = w_t - \eta_t g_t = w_t - \frac{\alpha}{\sqrt{t}} g_t$$

**Обратите внимание**: как правило, на практике Proximal методы работают лучше. Интуитивно, центрирование в недавних точках вместо

### Composite-Objective FTRL

Рассмотрим смесь центрированных и проксимальных регуляризаторов:

$$w_{t+1} = arg\min\limits_{w} h_{0:t} = arg\min\limits_{w}\Big[ g_{1:t}^Tw + \psi_{0:t}(w) + r_{0:t}(w)\Big],$$

где $r_t(w)$ и $\psi_t(w)$ таковы, что

$$arg\min\limits_w r_t(w) = w_t$$

$$arg\min\limits_w \psi_t(w) = w_0$$

**Пример**: FTRL-Proximal с L1 и L2 регуляризацией

$$w_{t+1} = arg\min\limits_{w}\Big[ g_{1:t}^Tw + \lambda_{1,t}\vert\vert w\vert\vert_1 + \lambda_{2,t}\vert\vert w\vert\vert_w^2 + \sum\limits_{s=1}^t\sigma_s \vert\vert w - w_s\vert\vert_2^2\Big]$$

**Обратите внимание**: как правило, центрированные регуляризаторы в довесок к проксимальным вводят уже не для «дополнительной стабилизации» алгоритма, а для наложения ограничений на решение $w$.

**Обратите внимание**: наиболее правильные и хорошо работающие на практике способы подбора коэффициентов $\lambda_{1,t}$ и $\lambda_{2,t}$ мы приведём в параграфе про учет дополнительной $L_1$ и $L_2$ регуляризации.

## Гарантии сходимости для алгоритмов FTRL

В этом разделе мы обсудим теоретические оценки на скорость сходимости алгоритма FTRL или, что то же самое, на скорость убывания maxRegret.

Напомним формулу:

$$maxRegret(T) = \max\limits_{w^*}\left[ \sum\limits_{t=1}^Tf_t(w_t) - f_{1:T}(w^*)\right]$$

Чтобы делать оценки на maxRegret, нужно пытаться оценить асимптотику ряда, каждое слагаемое которого — это решение сложной оптимизационной задачи $\min\limits_w f_{1:t}(w)$ с произвольными функциями $f_t(w)$. Работать с такой сущностью крайне сложно. Наша основная цель — сделать верхнюю оценку на regret, в которой не будет этого члена.(???)

#### Strong FTRL Lemma (Lemma 4)

1. Пусть $f_t(w)$ — последовательность произвольных (не обязательно) функций;
2. Пусть $r_t(w)$ — последовательность выпуклых неотрицательных регуляризаторов;
3. Пусть также $w_{t+1} = arg \min\limits_w h_{0:t}(w)$ всегда определен (относительно слабые условия 1-2 требуют от нас это явно проговорить);
Тогда алгоритм, выбирающий $w_{t+1}$ по правилу (3), удовлетворяет неравенству

$$Regret_T(w^*) \leq r_{0:T}(w^*) + \sum\limits_{t=1}^T \left[h_{0:t}(w_t) - h_{0:t}(w_{t+1}) - r_t(w_t)\vphantom{\frac12}\right]$$

Из чего состоит эта лемма?

1. Слагаемое $r_{0:T}(w^*)$ — это суммарная регуляризация в точке $w^*$. Совсем избавиться от вхождения $w^*$ не получится, но мы можем выбирать регуляризатор так, чтобы оценить сверху $r_{0:T}(w^*)$ было не очень сложно.

2. Каждое слагаемое суммы $\sum\limits_{t=1}^T \left[h_{0:t}(w_t) - h_{0:t}(w_{t+1})\vphantom{\frac12}\right] $ отражает, насколько улучшается $t$-й лосс $h_{0:t}$ при замене $w_t$ на $w_{t+1} = arg \min\limits_w h_{0:t}(w)$. Поведение разностей $h_{0:t}(w_t) - h_{0:t}(w_{t+1})$ характеризует стабильность алгоритма. Мы ожидаем, что при больших $t$ у хорошо сходящегося алгоритма на очередном шаге $w_t$ будет достаточно близок к оптимуму $w_{t+1}$, то есть вся сумма будет меняться всё медленнее, и её получится разумно оценить. Пример ситуации, когда это не так, мы уже видели, когда рассматривали FTL без регуляризации для линейной функции потерь (там всё было максимально нестабильно и расходилось). К счастью, введение регуляризации обычно помогает добиться стабильности.

Обе компоненты неразрывно связаны. Добавляя регуляризацию, мы увеличиваем первую компоненту, но улучшает стабильность алгоритма, чем уменьшаем вторую, и наоборот.

**Обратите внимание**: в условиях леммы допускаются невыпуклые $f_t(w)$, и это позволяет применять её в весьма общей ситуации. Впрочем, все наши последующие выкладки все-таки будут опираться на выпуклость $f_t(w)$.

{% cut "Доказательство Strong FTRL Lemma" %}

Преобразуем выражение для regret:

$$regret(T) = \sum\limits_{t=1}^Tf_t(w_t) - f_{1:T}(w^*) =$$

$$= \sum\limits_{t=0}^Th_t(w_t) - h_{0:T}(w^*) - \sum_{t=0}^Tr_t(w_t) + r_{0:T}(w^*) = (\ast)$$

Вспомним, что

$$w_{t+1} = arg \min\limits_w h_{0:t}(w),$$

откуда

$$ h_{0:t}(w^*) \geq h_{0:t}(w_{t+1}).$$

Поэтому выражение выше мы можем оценить как

$$(\ast)\leq \sum\limits_{t=0}^Th_t(w_t) - h_{0:T}(w_{t+1}) - \sum_{t=0}^Tr_t(w_t) + r_{0:T}(w^*) = (\ast\ast)$$

Теперь займёмся первыми двумя компонентами

$$\sum\limits_{t=0}^Th_t(w_t) - h_{0:T}(w_{t+1}) = $$

$$= h_0(w_0) + \color{#E06A27}{\sum\limits_{t=1}^T}\left[h_{0:t}(w_t) \color{#E06A27}{- h_{0:t-1}(w_t)} \vphantom{\frac12}\right] \color{#E06A27}{- h_{0:T}(w_{T+1})}$$

Посмотрим повнимательнее на рыжие слагаемые:

$$\sum\limits_{t=1}^T h_{0:t-1}(w_t) + h_{0:T}(w_{T+1}) = \color{#348FEA}{\sum\limits_{t=1}^T \sum\limits_{i=0}^{t-1}h_i(w_t)} + \sum\limits_{i=0}^T h_i(w_{T+1}) =$$

$$= \left|\vphantom{\frac12}\color{#348FEA}{s = t - 1}\right|
= \color{#348FEA}{\sum\limits_{s=0}^{T-1} \sum\limits_{i=0}^{s}h_i(w_{s+1})} + \sum\limits_{i=0}^T h_i(w_{T+1}) = $$

$$=\sum\limits_{t=0}^{T}\sum\limits_{i=0}^{t}h_i(w_{t+1}) = \sum\limits_{t=0}^Th_{0:t}(w_{t+1}) =
$$

$$= h_0(w_1) + \sum\limits_{t=1}^Th_{0:t}(w_{t+1})$$

Подставим это:

$$(\ast\ast) = h_0(w_0) + \sum\limits_{t=1}^Th_{0:t}(w_t) - h_0(w_1) - \sum\limits_{t=1}^Th_{0:t}(w_{t+1}) - \sum_{t=0}^Tr_t(w_t) + r_{0:T}(w^*) \leq$$

$$\leq h_0(w_0) + r_{0:T}(w^*) + \sum\limits_{t=1}^T \left[h_{0:t}(w_t) - h_{0:t}(w_{t+1}) - r_t(w_t)\vphantom{\frac12}\right].$$

Здесь мы снова воспользовались тем, что $h_0(w_1) = r_0(w_1)\geq 0$, а также сократили $h_0(w_0) = r_0(w_0)$.

Лемма доказана.

{% endcut %}

### Теоретические оценки на Regret (regret bounds)

Ниже мы представим теоремы 1,2 и 10 из [обзора McMahan](https://www.jmlr.org/papers/volume18/14-428/14-428.pdf). Они дают оценки на regret в немного разных исходных предположениях и для разных типов регуляризаторов; асимптотика regret в каждом из случаев $O(\sqrt{T})$, хотя константы будут различными. О важности констант в сходимости мы поговорим в одной из следующих параграфов, когда будем разбирать метод AdaGrad. В самом конце параграфа мы обсудим, какие оценки получаются для линеаризованного regret. А в следующем параграфе мы займёмся выводом конкретных алгоритмов FTRL для разных видов регуляризаторов.

Мы не будем полностью пересказывать обзор (если вам стало интересно, рекомендуем прочитать его самостоятельно) и докажем в качестве примера теорему 2, а для остальных приведём лишь формулировки.

#### Напоминание из выпуклого анализа

**Определение** Выпуклая функция $\psi(x)$ называется **$\sigma$-сильно выпуклой** по отношению к некоторой норме $\vert\vert \cdot\vert\vert $, если выполнено

$$\forall g \in \partial \psi(y) \quad \psi(x) \geq \psi(y) + g^T(x-y) + \frac{\sigma}{2}\vert\vert x-y\vert\vert^2$$

**Определение** **Двойственной нормой** $\vert\vert \cdot\vert\vert_*$ по отношению к норме $\vert\vert \cdot\vert\vert $ называется

$$\vert\vert x\vert\vert_* = \sup\limits_{y:\vert\vert y\vert\vert  \leq 1} x^Ty$$

{% cut "Физический смысл" %}

Эта норма у нас возникнет в контексте работы с градиентами. С одной стороны, конечно, градиент $g_t = \nabla_{w_t}f_t$ — это вектор, но по сути он играет роль линейной функции $w\mapsto g_t^Tw$, и кажется логичным определять для него норму именно как для линейной функции, то есть как для элемента **двойственного пространства**, состоящего из ограниченных линейных функций на исходном пространстве. 
  
Как можно определить норму отображения? Самый, пожалуй, естественный вариант — это рассмотреть **операторую норму** относительно $\vert\vert \cdot\vert\vert $:

$$\vert\vert x\vert\vert_* = \sup\limits_{y\leq 0} \frac{|x^Ty|}{\vert\vert y\vert\vert},$$

Это формула верна, если пространство, в котором живут $x$ и $y$ ненулевое (впрочем, нулевое мы вряд ли рассматриваем). Можно показать, что это выражение равно $\sup\limits_{y:\vert\vert y\vert\vert  \leq 1} x^Ty$.

Построенная норма называется **двойственной** к норме $\vert\vert\cdot\vert\vert$ на исходном пространстве.

{% endcut %}

Более подробно о $\sigma$-сильной выпуклости и двойственных нормах вы можете почитать, например, в книге [Boyd, 2004, Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/).

#### Теорема 1. General FTRL Bound

Пусть
* Обновление параметров происходит по правилу 

$$w_{t+1} = arg\min\limits_{w} h_{0:t} = arg\min\limits_{w}\Big[ g_{1:t}^Tw + r_{0:t}(w)\Big];$$

* Выполнены все условия Setting 1;
* Регуляризатор $r_t(w)$ выбирается так, чтобы выражение $h_{0:t}(w) + f_{t+1}(w) = r_{0:t}(w) + f_{1:t+1}(w)$ было 1-сильно выпукло по отношению к некоторой норме $\vert\vert \cdot\vert\vert_{t}$ (возможно, своей на каждом шаге).

Тогда

$$Regret_T(w^*) \leq r_{0:T-1}(w^*) + \frac{1}{2}\sum\limits_{t=1}^T\vert\vert g_t\vert\vert_{(t-1),*}^2,$$

где $\vert\vert \cdot\vert\vert_{(t-1),*}$ — норма, двойственная к норме $\vert\vert \cdot\vert\vert_{(t-1)}$.

#### Теорема 2. FTRL-Proximal Bound

Пусть
* Обновление параметров происходит по правилу

$$w_{t+1} = arg\min\limits_{w} h_{0:t} = arg\min\limits_{w}\Big[ g_{1:t}^Tw + r_{0:t}(w)\Big];$$

* Выполнены все условия Setting 1;
* Все регуляризаторы $r_t(w)$ лежат в семействе FTRL-Proximal, причём $r_t(w_t) = 0$ для всех $t$;
* $r_t(w)$ выбирается так, чтобы выражение $h_{0:t}(w) = r_{0:t}(w) + f_{1:t}(w)$ было 1-сильно выпукло по отношению к некоторой норме $\vert\vert \cdot\vert\vert_t$ (возможно, своей на каждом шаге).

Тогда

$$Regret_T(w^*) \leq r_{0:T}(w^*) + \frac{1}{2}\sum\limits_{t=1}^T\vert\vert g_t\vert\vert_{t,*}^2,$$

где $\vert\vert \cdot\vert\vert_{t,*}$ — норма, двойственная к норме $\vert\vert \cdot\vert\vert_{t}$.

#### Теорема 10. Composite Objective FTRL-Proximal Bound

Пусть
* Обновление параметров происходит по правилу

$$w_{t+1} = arg\min\limits_{w} h_{0:t} = arg\min\limits_{w} \Big[g_{1:t}^Tw + \alpha_{1:t}\Psi(w) + r_{0:t}(w)\Big];$$

* Выполнены все условия Settning 1;
* $\hat{h}_t(w) = f_t(w) + \alpha_t\Psi(w) + r_t(w)$;
* $\alpha_t$ — неубывающая последовательность;
* $\Psi(w)$ — Centered регуляризатор с минимумом в точке $w_0$;
* $r_t(w)$ — Proximal регуляризаторы;
* $r_t(w)$ выбирается так, чтобы выражение $\hat{h_{0:t}}(w) = r_{0:t}(w) + \alpha_{1:t}\Psi(w) + f_{1:t}(w)$ было 1-сильно выпукло по отношению к некоторой норме $\vert\vert \cdot\vert\vert_t$ (возможно, своей на каждом шаге).

Тогда

* Если мы рассматриваем regret относительно $\hat{f}_t(w) = f_t(w) + \alpha_t\Psi(w)$, то

$$Regret_T(w^*) \leq r_{0:T}(w^*) + \frac{1}{2}\sum\limits_{t=1}^T\vert\vert g_t\vert\vert_{t,*}^2;$$

* Если мы рассматриваем regret относительно $f_t(w)$, то

$$Regret_T(w^*) \leq r_{0:T}(w^*) + \alpha_{1:t}\Psi(w^*) + \frac{1}{2}\sum\limits_{t=1}^T\vert\vert g_t\vert\vert_{t,*}^2,$$

где $\vert\vert \cdot\vert\vert_{(t),*}$ — норма, двойственная к норме $\vert\vert \cdot\vert\vert_{t}$.

**Обратите внимание**. Оценки Proximal и General отличаются индексацией: до $t$ или до $t-1$ соответственно. Это чисто техническое различие, однако именно из-за него с Proximal регуляризаторами удобнее работать как в теоретических выкладках, так и при выведении практических методов.

**Обратите внимание**. На $f_t(w)$ мы не хотим накладывать ограничения сильной выпуклости, но сильную выпуклость функции $h_{0:t}(w) = f_{1:t}(w) + r_{0:t}(w)$ можно обеспечить за счет выбора сильно выпуклых регуляризаторов. В самом деле, сумма выпуклой и сильно выпуклой функций сильно выпукла. Если

$$f_t(w) \geq f_t(w_t) + (w - w_t)^T\nabla f_t(w_t)$$

и

$$r_t(w) \geq r_t(w_t) + (w - w_t)^T\nabla r_t(w_t) + \frac{1}{2}\vert\vert w - w_t\vert\vert^2,$$

то

$$f_t(w) + r_t(w) \geq f_t(w_t) + r_t(w_t) + (w - w_t)^T(\nabla f_t(w_t) + \nabla r_t(w_t)) + \frac{1}{2}\vert\vert w - w_t\vert\vert^2.$$

**Обратите внимание**. Норма $\vert\vert w\vert\vert_{t,*}^2$ является сопряженной к норме, относительно которой 1-сильно выпукла функция $h_{0:t}(w) = f_{1:t}(w) + r_{0:t}(w)$. Это значит, что норму мы будем выбирать по **сумме** регуляризаторов $r_{0:t}(w)$, а не просто по $r_t(w)$.

#### Доказательство на примере теоремы 2

Нам понадобится следующая чисто техническая лемма, доказательство которой мы опустим. Желающие могут прочитать Appendix B в [обзоре](https://www.jmlr.org/papers/volume18/14-428/14-428.pdf). 

**Lemma 7**. Пусть

* $\phi_1$ — выпуклая функция $\mathbb{R}^n \rightarrow \mathbb{R} \cup \{\infty\}$, для которой существует $x_1 = arg\min\limits_x \phi_1(x)$;
* $\psi$ — выпуклая функция;
* $\phi_2(x) = \phi_1(x) + \psi(x)$ — выпуклая функция, для которой существует $x_2 = arg\min\limits_x \phi_2(x)$ и которая, кроме того, 1-сильно выпукла по норме $\vert\vert \cdot\vert\vert $.

Тогда, для любого элемента $b$ субдифференциала $\partial_{x_1} \psi$ имеет место неравенство

$$\vert\vert x_1 - x_2\vert\vert  \leq \vert\vert b\vert\vert_*$$

и для любого $x'$ имеет место неравенство

$$\phi_2(x_1) - \phi_2(x') \leq \frac{1}{2}\vert\vert b\vert\vert_*.$$

**Доказательство теоремы 2**

Рассмотрим соседние раунды $w_t$ и $w_{t+1}$. Имеем

$$w_{t} = arg\min\limits_w h_{0:t-1} = arg\min\limits_w\left[f_{1:t-1} + r_{0:t-1}\right]$$

Обозначим $\phi_1(w) = f_{1:t-1}(w) + r_{0:t}(w) = h_{0:t-1}(w) + r_t(w) = h_{0:t}(w) - f_t(w)$. Поскольку $w_t$ одновременно минимизирует и $r_t(w)$ (т.к. это proximal регуляризатор), и $h_{0:t-1}$, имеем

$$w_t = arg\min\limits_w \Big[h_{0:t-1} + r_t(w)\Big] = arg\min\limits_w \phi_1(w).$$

Далее,

$$w_{t+1} = arg\min\limits_w h_{0:t} = arg\min\limits_w \Big[\phi_1(w) + f_t(w)\Big]$$

Выпишем оценку из Strong FTRL Lemma и постараемся оценить отмеченные рыжим слагаемые

$$\sum\limits_{t=1}^Tf_t(w_t) - f_{1:T}(w^*) \leq r_{0:T}(w^*) + \color{#E06A27}{\sum\limits_{t=1}^Th_{0:t}(w_t) - h_{0:t}(w_{t+1}) - r_t(w_t)}$$

Так как по условию теоремы $r_t(w_t) = 0$, мы можем убрать это слагаемое:

$$h_{0:t}(w_t) - h_{0:t}(w_{t+1}) - r_t(w_t) = \color{#C81D6B}{h_{0:t}(w_t)} - \color{#348FEA}{h_{0:t}(w_{t+1})} = $$

$$= \color{#C81D6B}{\phi_1(w_t) + f_t(w_t)} - \color{#348FEA}{(\phi_1(w_{t+1}) + f_t(w_{t+1}))}$$

Обозначим $\phi_2(w) = \phi_1(w) + f_t(w)$. Применив Лемму 7, получаем

$$\phi_1(w_t) + f_t(w_t) - \phi_1(w_{t+1}) - f_t(w_{t+1}) \leq \frac{1}{2}\vert\vert g_t\vert\vert_{t,*}^2$$

### О связи оценок на regret для обычного и линеаризованного FTRL

Вспомним, что для линеаризованного FTRL имеет место неравенство:

$$Regret_T(w) \leq LinearizedRegret_T(w).$$

Увы, верхняя оценка на левую часть неравенства не помогает оценить правую. Поэтому рассмотрим линеаризованный алгоритм более подробно. Он работает с последовательностью функций $\hat{f}_t(w) = g_t^Tw$, где $g_t \in \partial_{w_t} f_t$. Субдифференциал $\hat{f}_t$ состоит из одного вектора (градиента это функции)

$$\hat{g}_t = \frac{\partial \hat{f}_t(w)}{\partial w} = g_t$$

Применим приведённые выше оценки на regret для исходного и для линеаризованного алгоритма:

$$Regret_T(w^*) \leq r_{0:T}(w^*) + \frac{1}{2}\sum\limits_{t=1}^T\vert\vert g_t\vert\vert_{t,*}^2$$

$$LinearizedRegret_T(w^*) \leq r_{0:T}(w^*) + \frac{1}{2}\sum\limits_{t=1}^T\vert\vert \hat{g}_t\vert\vert_{t,*}^2 = r_{0:T}(w^*) + \frac{1}{2}\sum\limits_{t=1}^T\vert\vert g_t\vert\vert_{t,*}^2$$

Легко убедиться, что **оценки regret для обычного и линеаризованного FTRL совпадают** и выполнено соотношение

$$Regret_T(w^*) \leq LinearizedRegret_T(w^*) \leq TheoremRegret_T(w^*).$$

Таким образом, для линеаризованного варианта любого алгоритма FTRL не нужно доказывать собственные оценки. А поскольку линеаризованный FTRL намного эффективнее, в дальнейшем мы всегда будем сразу переходить от исходного алгоритма к линеаризованному.

## Построение эффективного адаптивного FTRL

Теперь, когда мы получили теоретические оценки на качество работы адаптивного FTRL, настала пора рассмотреть несколько конкретных примеров алгоритмов из этого класса.

### Семейство квадратичных регуляризаторов $r_t(w)$

Во всех дальнейших выкладках мы сразу ограничим себя семейством квадратичных регуляризаторов:
1. Для FTRL-Centered алгоритмов: $r_t(w) = w^TD_tw = \vert\vert w\vert\vert_{D_t}^2$,
2. Для FTRL-Proximal алгоритмов: $r_t(w) = (w - w_t)^TD_t(w - w_t) = \vert\vert w - w_t\vert\vert_{D_t}^2$,

где $D_t$ — некоторая симметричная положительно определённая матрица (возможно, своя для каждого шага).

Помимо того, что они удобны и привычны, таки регуляризаторы позволяют достаточно просто выписывать оценки на regret. Чтобы в этом убедиться, вспомним, какие нетривиальные сущности возникают в теоремах:

* на каждом шаге нам нужно выбрать норму $\vert\vert \cdot\vert\vert_{t}$, по отношение к которой выражение $h_{0:t}(w) + f_{t+1}(w) = r_{0:t}(w) + f_{1:t+1}(w)$ было бы 1-сильно выпуклым;
* во всех оценках участвует $r_{0:T}(w^*)$ (или $r_{0:T-1}(w^*)$), и его хорошо бы уметь оценивать сверху;
* также в оценках фигурирует норма, двойственная к $\vert\vert \cdot\vert\vert_{t}$, и её нужно уметь выводить.

Давайте разберёмся с каждым из пунктов и поймём, почему для квадратичных регуляризаторов всё довольно хорошо.

**Выбор нормы $\vert\vert\cdot\vert\vert_{t}$**

Тут всё просто:

1. Регуляризатор $\vert\vert w\vert\vert_D$ является 1-сильно выпуклым относительно нормы $\vert\vert w\vert\vert_D$ (т.е. относительно себя же);
2. Регуляризатор $\vert\vert w - w_t\vert\vert_D$ является 1-сильно выпуклым относительно *той же самой нормы* $\vert\vert w\vert\vert_D$.

Нам, впрочем, нужна 1-сильная выпуклость всей суммы $r_{0:t}(w)$, но легко убедиться, что $r_{0:t}$ 1-сильно выпукло относительно суммарной нормы $\vert\vert \cdot\vert\vert_{D_{0:t}}^2$. Поскольку $D_{0:t}$ — тоже симметричная положительно определенная матрица, мы остаёмся в том же классе норм Махаланобиса.

**Двойственная норма $r_{0:t}(w)$**

Оказывается, что

$$\vert\vert w\vert\vert_{D,*} = \vert\vert w\vert\vert_{D^{-1}}$$

{% cut "Доказательство" %}

По определению

$$\vert\vert x\vert\vert_{D,*} = sup \{ x^Ty: y^TDy \leq 1 \}$$

Для начала заметим, что ограничение-неравенство можно заменить на ограничение-равенство. А именно, если $\vert\vert y\vert\vert_D < 1$, то, взяв $\alpha > 1: \vert\vert \alpha y\vert\vert_D = 1$, мы получим $\vert\alpha x^Ty\vert > \vert x^Ty\vert$. Значит, супремум можно искать на границе.

Далее, заметим, что мы работаем с конечномерными пространствами (вряд ли у вектора весов бесконечное число компонент), поэтому единичный шар $\vert\vert y\vert\vert_D = 1$ является компактом и, стало быть, супремум на нём достигается. Таким образом, мы можем решать привычную задачу оптимизации с ограничениями и применить для неё метод множителей Лагранжа.

Выпишем функцию Лагранжа:

$$L(y, \lambda) = x^Ty - \lambda(y^TDy - 1)$$

Продифференцируем её и приравняем градиент к нулю:


$$\nabla_y L(y, \lambda) = x - 2\lambda y(D + D^T) = 0$$


Так как матрица $D$ симметрична, имеем $D + D^T = 2D$ и, следовательно,

$$y = \frac{1}{\lambda}D^{-1}x$$

Подставим его в граничное условие-равенство и выразим $\lambda$:

$$1 = y^TDy = \left(-\frac{1}{\lambda}D^{-1}x\right)^TD\left(-\frac{1}{\lambda}D^{-1}x\right) =\frac{1}{\lambda^2}x^TD^{-T}DD^{-1}x$$

Отсюда

$$x^TD^{-T}x = \lambda^2$$

Транспонировав обе части, получаем

$$x^TD^{-1}x = \lambda^2$$

$$\lambda = \sqrt{x^TD^{-1}x}$$

Подставим найденное $\lambda$ обратно в выражение $y = \frac{1}{\lambda}D^{-1}x$:

$$y = \frac{D^{-1}x}{\sqrt{x^TD^{-1}x}},$$

а полученное решение подставим в исходную функцию $x^Ty$:

$$x^Ty = \frac{x^TD^{-1}x}{\sqrt{x^TD^{-1}x}} = \sqrt{x^TD^{-1}x} = \vert\vert x\vert\vert_{D^{-1}}.$$

{% endcut %}

**Ограничение сверху для $r_{0:t}(w^*)$**

Строго говоря, здесь никаких гарантий нет, и, например, очень плохая инициализация может всё сильно испортить. На практике, впрочем, всё работает нормально, но авторы статей не могут себе позволить надеяться на благосклонность судьбы. Поэтому в статьях часто встречается следующий костыль. Для вывода оценок на regret вводится регуляризатор $r_0(w) = I_{R}(w)$, где 

$$
I_{R}(w) = \begin{cases}
      \infty & \vert\vert w \vert\vert > R \\
      0 & \vert\vert w \vert\vert \leq R
   \end{cases}
$$
   
это проекция на шар. Тогда можно доказать, что $\vert\vert w^* \vert\vert \leq R$.


### Семейство логарифмических регуляризаторов

Для ряда частных задач вроде expert advice problem и оптимизаций по вероятностным распределениям используется также семейство энтропийных регуляризаторов

$$r_t(w) = \sum\limits_{i=1}^Nw_i\log{w_i}$$

Более подробно о нём можно почитать в [обзоре Shai-Shalev Schwartz](https://www.cs.huji.ac.il/w~shais/papers/OLsurvey.pdf), пример 2.5.

### Constant learning rate FTRL

Простейший пример — это константный регуляризатор 

$$
r_s(w) = \begin{cases}
\frac{1}{2\eta}\vert\vert w\vert\vert_2^2,\ s=0,\\ 
0,\ s > 0
\end{cases}
$$

Легко показать, что $\frac{1}{2\eta}\vert\vert w\vert\vert_2^2 = \vert\vert w\vert\vert_{\frac{1}{2\eta}I}^2$.

Соответствующий итерационный процесс оптимизации имеет вид

$$w_{t+1} = arg\min\limits_w\Big[ g_{1:t}^Tw + \frac{1}{2\eta}\vert\vert w\vert\vert_2^2\Big]$$

Как мы уже наблюдали ранее, этот метод эквивалентен методу стохастического градиентного спуска с константным learning rate. А именно, шаг обновления весов можно сформулировать двумя способами:

* на языке FTRL: $w_{t+1} = -\eta g_{1:t}^T$;

* на языке градиентного спуска: $w_{t+1} = w_t - \eta g_t$.

**Оценка на Regret** (3.1 Constant Learning Rate Online Gradient Descent). Пусть
* $\vert\vert g_t\vert\vert  \leq G$;
* $\vert\vert w^*\vert\vert  \leq R$.

Тогда, если взять $\eta = \frac{R}{G\sqrt{T'}}$, то для любого $T' \leq T$

$$Regret_T(w^*) \leq RG\sqrt{T'}$$

В целом, такая стратегия регуляризации не самая оптимальная. Интуитивно, наш регуляризатор фиксирован вне зависимости от того, сколько мы уже сыграли раундов, и со временем может перестать компенсировать член $g_{1:t}^Tw$, и тогда стабильность алгоритма может падать.

### FTRL с learning rate scheduling

Чтобы исправить нестабильность алгоритма, возьмём $L_2$-регуляризатор, не равный нулю на каждом шаге.

Процесс оптимизации примет вид:

* Для FTRL-Proximal: $w_{t+1} = arg\min\limits_w \Big[ g_{1:t}^Tw + \sum\limits_{s=0}^t\frac{\sigma_s}{2}\vert\vert w-w_s\vert\vert_2^2 \Big]$;
* Для FTRL-Centered: $w_{t+1} = arg\min\limits_w \Big[ g_{1:t}^Tw + \sum\limits_{s=0}^t\frac{\sigma_s}{2}\vert\vert w\vert\vert_2^2 \Big]$.

Посмотрим, какое обличье примет алгоритм FTRL-Proximal, если его изложить на языке градиентного спуска. Для этого продифференцируем и приравниваем нулю выражение, которое мы минимизируем:

$$0 = g_{1:t} + \sum\limits_{s=0}^t\sigma_s (w - w_s)$$

$$\sum\limits_{s=0}^t\sigma_s w_s - g_{1:t} = \sigma_{0:t} w$$

$$w_{t+1} = \frac{1}{\sigma_{0:t}} \sum\limits_{s=0}^t\sigma_s w_s -\frac{1}{\sigma_{0:t}} g_{1:t}$$

Попробуем получить рекуррентную формулу для выражения $w_{t+1}$ через $w_t$:

$$w_{t+1} = \frac{1}{\sigma_{0:t}} \Big(\sum\limits_{s=0}^t\sigma_s w_s - g_{1:t}\Big)$$

$$w_t = \frac{1}{\sigma_{0:t-1}} \color{#E06A27}{\Big(\sum\limits_{s=0}^{t-1}\sigma_s w_s - g_{1:t-1}\Big)}$$

$$w_{t+1} = \frac{1}{\sigma_{0:t}} \Big(\color{#E06A27}{\sum\limits_{s=0}^{t-1}\sigma_s w_s} + \sigma_t w_t - \color{#E06A27}{g_{1:t-1}} - g_t \Big)$$

$$w_{t+1} = \frac{1}{\sigma_{0:t}} \Big(\sigma_{0:t-1} w_t + \sigma_t w_t - g_t\Big) = \frac{1}{\sigma_{0:t}} \Big( \sigma_{0:t} w_t - g_t\Big) = w_t - \frac{1}{\sigma_{0:t}}g_t$$

Если теперь положить $\eta_t = \frac{1}{\sigma_{0:t}}$, мы получаем формулу градиентного спуска:

$$w_{t+1} = w_t - \eta_t g_t$$

Таким образом, *темп обучения градиентного спуска равен обратной сумме коэффициентов регуляризации ftrl*. Точно так же можно выразить

$$\sigma_t = \frac{1}{\eta_t} - \frac{1}{\eta_{t-1}}$$

В качестве классической непокоординатной последовательности learning rate обычно берут

$$\eta_t = \frac{\alpha}{\sqrt{t}}$$

$$\sigma_t = \frac{\sqrt{t + 1} - \sqrt{t}}{\alpha}$$

**Оценка на Regret** (3.2, Dual Averaging) Пусть
* $\vert\vert g_t\vert\vert  \leq G$,
* $\vert\vert w^*\vert\vert  \leq R$.

Тогда, если выбрать $\eta_t = \frac{R}{\sqrt{2}G\sqrt{t+1}}$, то

$$Regret_T(w^*) \leq \sqrt{2}RG\sqrt{T}$$

Как и в случае с константным learning rate, константа $\frac{R}{\sqrt{2}G}$ в $\eta_t$ на практике никому не известна, так что ее подменяют на $\alpha$ и перебирают руками с learning rate, равным $\frac{\alpha}{\sqrt{t+1}}$.

### Data-Adaptive FTRL

До сих пор мы рассматривали в качестве нормы $\vert\vert \cdot\vert\vert $ стандартное скалярное произведение, в которое различные компоненты вектора весов (которые, грубо говоря, соответствуют различным признакам) вносят равный вклад. Такой подход может быть слишком наивным для «боевых» задач, где геометрия оптимизации имеет форму, например, вытянутого эллипса.

Нетрудно обобщить предыдущие рассуждения на случай произвольного скалярного произведения

$$w_{t+1} = arg\min\limits_w\Big[ g_{1:t}^Tw + \frac{1}{2}\sum\limits_{s=0}^T\vert\vert w - w_s\vert\vert_{D_s}^2\Big]$$

Коэффициенты $\sigma_s$ в этом выражении теперь спрятались в $D_s$. Найдем точку минимума:

$$0 = g_{1:t} + \frac{1}{2}\sum\limits_{s=0}^t(w - w_s)(D_s + D_s^T) = g_{1:t} + \sum\limits_{s=0}^t(w - w_s)D_s$$

$$\Big(\sum\limits_{s=0}^tD_s\Big)w = \sum\limits_{s=0}^tD_sw_s - g_{1:t}$$

Но сразу возникают проблемы:

* Нужно хранить $\sum\limits_{s=0}^tD_s$, в общем случае это квадрат по памяти от числа параметров. Ни в какой реальной задаче мы не сможем себе этого позволить;
* На каждой итерации метода нужно решать гигантскую систему линейных уравнений для поиска $w$. Есть все шансы состариться, так и не успев увидеть решение задачи оптимизации.

Упростим себе жизнь и предположим, что все матрицы $D_s$ диагональны. Тогда $\sum\limits_{s=1}^tD_s$ можно хранить в виде вектора диагональных элементов того же размера, что и $w$, а система на каждой итерации будет решаться за линию.

### AdaGrad: наилучший адаптивный метод

Разрешив себе брать нормы $\vert\vert\cdot\vert\vert_{D_s}$ с диагональными матрицами $D_s$, мы сделали алгоритм более гибким, но при этом приобрели дополнительные степени свободы (выбор диагональных элементов). Попробуем ответить на два вопроса:

* Можно ли матрицы $D_s$ не угадывать, а настраивать по доступной на очередном шаге информации?
* Как выбирать матрицы $D_s$ так, чтобы минимизировать оценки на regret?

В процессе поисков ответов на них мы придём к известному методу оптимизации **AdaGrad**.

Помня, что $\vert\vert .\vert\vert_{D,*} = \vert\vert .\vert\vert_{D^{-1}}$, выпишем общий вид оценки на regret:

$$Regret_T(w^*) \leq r_{0:T-1}(w^*) + \frac{1}{2}\sum\limits_{t=1}^T\vert\vert g\vert\vert_{(t),*}^2 = \sum\limits_{t=1}^T \vert\vert w^* - w_t\vert\vert_{D_t}^2 + \frac{1}{2}\sum\limits_{t=1}^T\vert\vert g_t\vert\vert_{(D_{0:T})^{-1}}^2$$

Чтобы упростить выкладки, введем новую симметричную положительно определенную матрицу $S_T = D_{0:T}^{-1}$ и перепишем формулы

$$Regret_T(w^*) \leq r_{0:T-1}(w^*) + \frac{1}{2}\sum\limits_{t=1}^T\vert\vert g\vert\vert_{t-1,*}^2 = \sum\limits_{t=1}^T \vert\vert w^* - w_t\vert\vert_{D_t}^2 + \frac{1}{2}\sum\limits_{t=1}^T\vert\vert g_t\vert\vert_{S_T}^2$$

С членом $\sum\limits_{t=1}^T \vert\vert w^* - w_t\vert\vert_{D_t}^2$ явно будет очень сложно работать: чтобы им пользоваться, нужно иметь на руках оптимальное решение $w_t^*$ для всей предыдущей выборки. Более перспективным выглядит слагаемое $\frac{1}{2}\sum\limits_{t=1}^T\vert\vert g_t\vert\vert_{S_T}^2$: вычислять их одно удовольствие. Идея метода **AdaGrad** как раз в том, чтобы не пытаться работать с первым членом и минимизировать второй, надеясь, что итоговые оценки на regret при этом тоже улучшатся.

Для начала выведем диагональный AdaGrad как более простой случай. Если все $D_t$ диагональны, то матрица $S_T = D_{1:T}^{-1}$ тоже диагональна и представляется набором диагональных элементов $\frac{1}{s_i}$ (уберем индекс $T$ для сокращения выкладок, так как мы рассматриваем фиксированный раунд).

Распишем второе слагаемое в regret

$$\sum\limits_{t=1}^T\vert\vert g_t\vert\vert_{S_T}^2 = \sum\limits_{t=1}^T\sum\limits_{i=1}^N\frac{g_{t,i}^2}{s_i}$$

Попробуем минимизировать его

$$
\begin{cases}\sum\limits_{t=1}^T\sum\limits_{i=1}^N \frac{g_{t,i}^2}{s_i} \longrightarrow \inf\limits_s\\
s_i \geq 0
\end{cases}
$$

Условие $s_i \geq 0$ возникает из неотрицательной определенности матрицы $S_T$. Решеним такой задачи, очевидно, является $s_i \rightarrow +\infty$. Однако в этом случае член $\sum\limits_{t=1}^T \vert\vert w^* - w_t\vert\vert_{D_t}^2$ из оценки на regret станет, наоборот, бесконечно большим, и нужен какой-то компромисс. Введем довольно слабое ограничение на положительные коэффициенты

$$
\begin{cases}\sum\limits_{t=1}^T\sum\limits_{i=1}^N \frac{g_{t,i}^2}{s_i} \longrightarrow \inf\limits_s\\
s_i \geq 0,\\
\sum\limits_{i=1}^Ns_i \leq c
\end{cases}
$$

и найдём оптимум с помощью метода множителей Лагранжа. Функция Лагранжа имеет вид

$$L(s,\lambda,\theta) = \sum\limits_{t=1}^T\sum\limits_{i=1}^N \frac{g_{t,i}^2}{s_i} + \lambda^Ts + \theta\left(\sum\limits_{i=1}^Ns_i - c\right)$$

Отметим, что здесь $\lambda$ — это вектор, а $\theta$ — число.

Приравняем к нулю частные производные:

$$0=\frac{\partial L(s,\lambda,\theta)}{\partial s_i} = -\frac1{s_i}\sum\limits_{t=1}^T g_{t,i}^2 + \lambda_i + \theta$$

Вспомним про условия дополняющей нежесткости, требующие, чтобы $\lambda_i s_i = 0$. Так как $s_i$ мы нулю приравнять здесь не можем, получаем, что $\lambda_i = 0$:

$$\frac1{s_i}\sum\limits_{t=1}^T g_{t,i}^2 - \theta = 0$$

$$s_i = \theta^{-\frac{1}{2}}\sqrt{\sum\limits_{t=1}^T g_{t,i}^2}$$

Теперь вспомним про условие $\sum\limits_{i=1}^Ns_i \leq c$. Можно показать, что оптимум достигается на границе (то есть когда неравенство превращается в равенство). Тогда

$$c = \sum\limits_{i=1}^Ns_i  = \theta^{-\frac{1}{2}}\sum\limits_{i=1}^N\sqrt{\sum\limits_{t=1}^T g_{t,i}^2}$$

$$\theta^{-\frac{1}{2}} = \frac{c}{\sum\limits_{i=1}^N\sqrt{\sum\limits_{t=1}^T g_{t,i}^2}}$$

$$s_i =  \frac{c\sqrt{\sum\limits_{t=1}^T g_{t,i}^2}}{\sum\limits_{i=1}^N\sqrt{\sum\limits_{t=1}^T g_{t,i}^2}}$$

Вернемся к оценке на regret. Чему равно $c$ мы не знаем, поэтому мы просто констатируем, что оптимальные коэффициенты $s_i$ пропорциональны $\sqrt{\sum\limits_{s=1}^tg^2_{s,i}}$:

$$s_i = \frac1{\alpha}\sqrt{\sum\limits_{t=1}^T g_{t,i}^2}$$

Теперь $S_T$ — диагональная матрица с диагональными элементами $\frac1{s_i}$. Следовательно, $D_{0:T} = (S_T)^{-1}$ - тоже диагональная матрица с диагональными элементами $s_i$:

$$D_{0:T,i} = \frac1{\alpha}{\sqrt{\sum\limits_{t=1}^T g_{t,i}^2}} = \sum\limits_{t=1}^T \left(\frac1{\alpha}{\sqrt{\sum\limits_{t=1}^T g_{t,i}^2}} - \frac1{\alpha}{\sqrt{\sum\limits_{t=1}^{T-1} g_{t,i}^2}}\right) + 0, \quad D_0 = 0,$$

и легко убедиться, что

$$D_{t,i} = \frac1{\alpha}{\sqrt{\sum\limits_{t=1}^T g_{t,i}^2}} - \frac1{\alpha}{\sqrt{\sum\limits_{t=1}^T g_{t,i}^2}}$$

Теперь вспомним, что эти формулы в точности повторяют то, что мы получили выше для соотношения $\sigma_t = \frac{1}{\eta_t} - \frac{1}{\eta_{t-1}}$, только вместо общего коэффициента $\eta_t$ у нас теперь покоординатные коэффициенты $\eta_{t,i}$:

$$\eta_{t,i} = \frac{\alpha}{\sqrt{\sum\limits_{s=1}^t g_{s,i}^2}}$$

Получаем формулы для метода AdaGrad в градиентной постановке:

$$w_{t+1,i} = w_{t,i} - \frac{\alpha}{\sqrt{\sum\limits_{s=1}^t g_{s,i}^2}}g_{t,i},$$

где коэффициент $\alpha$ приобретает значение learning rate.

**Оценка на Regret** (3.4, FTRL-Proximal with Diagonal Matrix Learning Rates)

Если использовать AdaGrad с покоординатными learning rate, то

$$Regret_T(w^*) \leq 2 \sqrt{2} R \sqrt{\sum\limits_{t=1}^Tg_t^2}$$

Отметим, что это оценка отличается от предыдущей тем, что вместо $G\sqrt{T}$ используется $\sqrt{\sum\limits_{t=1}^Tg_t^2}$. Таким образом, если у градиента на какой-то из позиций стоит что-то большое, это повлияет лишь на одно из слагаемых под корнем вместо того, чтобы умножиться на $\sqrt{T}$.

**Эффективный размер шага**. Предположим, что градиенты ограничены по норме $\vert\vert g\vert\vert_2 \leq R$. Перепишем наши формулы в виде

$$\frac{\alpha}{\sqrt{\sum\limits_{s=1}^T g_{s,i}^2}} = \frac{\alpha}{\sqrt{T}\cdot\sqrt{\frac{1}{T}\sum\limits_{s=1}^T g_{s,i}^2}} \leq \frac{\alpha}{R\sqrt{T}}$$

Из этих формул следует, что в среднем learning rate в AdaGrad убывает как $\eta_t = O\left(\frac{1}{\sqrt{T}}\right)$, то есть так же, как в предыдущем методе. Отличие состоит лишь в более правильной покоординатной нормировке, которая улучшает сходимость.

  ## handbook

  Учебник по машинному обучению

  ## title

  Адаптивный FTRL

  ## description

  *null*

- 
  ## path

  /handbook/ml/article/regulyarizaciya-v-onlajn-obuchenii

  ## content

  В этом параграфе мы поговорим о регуляризации, но использовать мы её будем не для стабилизации обучения, а для того, чтобы *накладывать ограничение на получаемое нами решение*. Чтобы отличать их от стабилизирующих слагаемых, для таких регуляризаторов будем использовать обозначение $\psi_t(w)$

В теории от регуляризатора требуется только выпуклость, но на практике широко используются лишь три вида:

* $L_1 = \vert\vert w\vert\vert_1$ и его собрат $L_{1/2}$;
* $L_2 = \vert\vert w\vert\vert_2^2$;
* Проекция на выпуклое множество $\chi$: 

$$
\psi(w) = I_{\chi}(w) = \begin{cases}
      \infty & w \not\in \chi \\
      0 & w \in \chi
   \end{cases}
$$

Классическим способом введения регуляризации является прибавление к оптимизируемому функционалу:

$$\hat{f}_t(w) = f_t(w) + \psi_t(w)$$

с последующим применением любых методов оптимизации «из коробки». Яркий пример — $L_2$ регуляризация:

$$\hat{f}_t(w) = f_t(w) + \frac{1}{2\lambda_2}\vert\vert w\vert\vert_2^2,$$

которая не портит гладкости функционала.

## Идея неразложения регуляризаторов в субградиентную оценку

Вспомним вывод linearized FTRL. В ходе линеаризации мы заменяли все функции $\hat{f}_t(w)$ на их субградиентную оценку в точке $w_t$. Для регуляризованного функционала $\hat{f}_t(w) = f_t(w) + \psi_t(w)$ получалась бы такая оценка:

$$\hat{f}_t(w) \geq \hat{f}_t(w_t) + (g_t + \partial\psi_t)^T (w - w_t),$$

где через $\partial\psi_t$ мы обозначили для краткости субградиент $\psi_t$ в точке $w_t$. Теперь субградиентную оценку можно подставить в метод FTRL:

$$w_{t+1} = arg\min\limits_w \Big[(g_{1:t} + \partial\psi_{1:t})^Tw + \sum\limits_{s=1}^t\vert\vert w - w_s\vert\vert_{\sigma_s}^2\Big]$$

Идея неразложения состоит в следующем: заменим на субградиентную оценку только $f_t(w)$, а регуляризатор будем подбирать так, чтобы задача FTRL решалась аналитически. Интуитивно, оценка

$$\hat{f}_t(w) = f_t(w) + \psi_t(w) \geq f_t(w_t) + g_t^T(w - w_t) + \psi_t(w)$$

должна быть точнее оценки

$$\hat{f}_t(w) = f_t(w) + \psi_t(w) \geq f_t(w_t) + \psi_t(w_t) + (g_t + \partial\psi_t)^T (w - w_t)$$

а значит, и метод оптимизации будет точнее и эффективнее.

Эта идея очень важна для построения регуляризованных алгоритмов онлайн-обучения.

Давайте выпишем, как будут выглядеть с учётом этой идеи регуляризованные алгоритмы. 

**Composite Objective FTRL**

$$w_{t+1} = arg\min\limits_w \Big[g_{1:t}^Tw + \psi_{1:t}(w) + \sum\limits_{s=1}^t\vert\vert w - w_s\vert\vert_{\sigma_s}^2\Big]$$

**Online Mirror Descent, Proximal Gradient Descent, (F)ISTA**

$$w_{t+1} = arg\min\limits_w \Big[g_t^Tw + \psi_t(w) + \vert\vert w - w_t\vert\vert_{\sigma_t}^2 \Big]$$

Напомним, что три названия в заголовке соответствуют трём способам восприятия этой формулы:

* Online Mirror Descent — метод онлайн-обучения;
* Proximal Gradient Descent — метод (стохастической) батч-оптимизации. В стохастическом случае он неотличим от Mirror Descent;
* (F)ISTA — по сути, это название аналитического решения указанного уравнения для $L_1$-регуляризации.

### Связь между Composite-Objective FTRL и Proximal Gradient Descent. Lazy vs Greedy представления

В этом подразделе мы будем проводить рассуждения на примере $L_1$-регуляритора.  для других регуляризаторов выкладки будут аналогичными.

Выпишем Proximal (он же Mirror) Gradient Descent с $L_1$-регуляризацией:

$$w_{t+1} = arg\min\limits_w g_t^Tw + \lambda_1\vert\vert w\vert\vert_1 + \frac{1}{2\eta_t}\vert\vert w - w_t\vert\vert_2^2$$

Необходимым условием минимума явняется равенство нулю градиента (а в данном случае субградиента) всего выражения:

$$0 = g_t + \hat{g}_t + \frac{1}{\eta_t}(w_{t+1} - w_t)$$

где $\hat{g}_t$ - субградиент регуляризатора $\lambda_1\vert\vert w\vert\vert_1$ в точке $w_{t}$. Отсюда получаем

$$w_{t+1} = w_{t} - \eta_t (g_t + \hat{g}^T)$$

Если же переписать формулы в духе FTRL, мы получим

$$w_{t+1} = arg\min\limits_{w} g_{1:t}^Tw + \hat{g}_{1:{t-1}}^Tw + \lambda \vert\vert w\vert\vert_1 + \frac{1}{2}\sum\limits_{s=0}^t\vert\vert w-w_s\vert\vert_{\sigma_s}^2$$

Получился метод, который оптимизирует $L_1$-регуляризатор в явном виде только на текущей итерации $t$, а для остальных использует субоптимальные субградиентные оценки. Заметим, что тем же выражением можно ограничить сверху и функционал:

$$w_{t+1} = arg\min\limits_{w} g_{1:t}^Tw + t\lambda \vert\vert w\vert\vert_1 + \frac{1}{2}\sum\limits_{s=0}^t\vert\vert w-w_s\vert\vert_{\sigma_s}^2$$

Мы получили метод FTRL с incremental $L_1$ — более сильным и стабильным вариантом регуляризации, чем Mirror Descent. Подробнее его анализом мы займемся в параграфе про продвинутую $L_1$-регуляризацию.

## $L_1$-регуляризация

### Отбор параметров разреженных моделей

Предположим, что мы хотим обучить модель минимального размера и при этом как можно лучшего качества. В этом нам поможет отбор параметров. А именно, давайте постараемся оставить только те из них, которые оказывают наиболее влияние на лосс $f_{1:T}(w)$.

**Определение**. Будем называть параметр $w_i$ **разреженным**, если он не используется (пропускается) при предсказании некоторых $f_t(w)$. «Некоторых» может означать как десятую часть, так и $0.99999$ прогнозов $f_t(w)$, главное — что такие объекты просто есть. Частым мы будем называть параметр, у которого частота пропусков низкая (например, $10\%$ пропусков), а редким — тот, у которого она высокая (второй случай).

**Пример**. Рассмотрим модель разреженной линейной регрессии $f_t(w) = (w^Tx_t - y_t)^2$. Обычно она применяется в ситуациях, когда элементы вектора признаков $x_{t,i}$ — это $0$ или $1$ (например, «встретилось ли $i$-е слово в $t$-м документе»), причем на практике доля единиц обычно бывает очень маленькой. Поэтому существенная часть параметров $w_i$ при прогнозе на шаге $t$ будет умножаться на нули и, таким образом, не будет использоваться.

**Обратите внимание**: как правило, в литературе по онлайн-обучению говорят о разреженных *параметрах*, а не признаках. Впрочем, подавляющее большинство моделей на разреженных признаках устроены так, что каждому такому признаку сопоставляется некий набор параметров, поэтому определения «разреженный признак» и «разреженные параметры» взаимозаменяемы. В линейной модели, как в примере выше, каждому признаку $x_i$ сопоставляется параметр $w_i$. В более сложных моделях признаку $x_i$ может сопоставляться вектор параметров $w_i$ — эмбеддинг этого признака.

Давайте теперь поймём, что означает фраза «признак влияет на лосс $f_{1:T}(w)$». Оказывать влияние можно двумя способами:

1. Качеством. Если параметр $w_i$ редкий, но очень хорошо прогнозирует свой небольшой набор объектов, его стоит оставить. За счет того, что мы оставим достаточное количество таких параметров, мы можем покрыть большое число объектов. Такие параметры называются **memorization parameters** (они как будто запоминают «свои» объекты).
2. Количеством. Если параметр $w_i$ часто встречается, то он в любом случае должен остаться в модели и помогать с суммарным качеством прогноза.

Убирать мы хотим только слабые и редкие параметры. Таких, как правило, больше $99\%$.

**Обратите внимание**: мы не хотим убирать *слабые, но часто встречающиеся параметры*. Тому есть две причины:

1. Места они много не занимают, а количества данных в large scale задачах достаточно, чтобы правильно выучить эти параметры. Они будут вносить свой, пусть и небольшой, вклад в общее качество;
2. Частые параметры хорошо запоминают среднее поведение на всех данных, а разреженные — поведение на конкретных объектах. Если наша цель — оставить как можно меньше параметров, то выгоднее хорошо выучить среднее поведение на всех данных, а отклонения от среднего запомнить с помощью memorization parameters. Если в модели есть только супер-разреженные параметры, то из-за огромной вариативности в их возможных комбинациях в данных каждому параметру придется доучивать среднее поведение. Подробнее на этой проблеме мы остановимся в конце параграфа.

### Инициализация разреженных параметров

В обучении разреженных моделей все параметры, на которые накладывается $L_1$-регуляризация, инициализируются нулями. С точки зрения здравого смысла такая инициализация довольно естественна, однако есть и более формальное обоснование;

1. Если параметры инициализируются нулями, то мы по мере обучения смотрим на градиенты этих параметров и в зависимости от градиентов принимаем решение, нужен нам параметр для прогноза или не нужен. Все параметры стартуют в равных условиях, и модель понемногу выходит из состояния «абсолютная разреженность», выучивая что-то содержательное.
2. Если же параметры инициализируются случайно, то нам надо сначала доучить все параметры до какого-то более или менее разумного значения, а потом уже пытаться понять, нужен ли он нам. Момент, когда модель начинает эффективно разреживаться, тем самым очень сильно отдалается.

### Composite-objective FTRL с $L_1$-регуляризацией

Напомним формулировку задачи:

$$w_{t+1} = arg\min\limits_w g_{1:t}^Tw + \lambda_{1,t}\vert\vert w\vert\vert_1 + \frac{1}{2}\sum\limits_{s=1}^T\vert\vert w - w_s\vert\vert_{\sigma_s}^2$$

Решение можно выписать в явном виде. Для этого введём следующие обозначения:

* $z_t$ будет аккумулировать сумму градиентов, $z_0 = 0$,
* $n_t$ будет аккумулировать сумму поэлементных квадратов градиентов, $n_0 = 0$,
* $\alpha$ — это learning rate.

Следующие формулы выписаны отдельно для каждой координаты. В них $i$ — индекс параметра модели, $t$ — номер итерации.

$$\sigma_{t,i} = \frac{1}{\eta_{t,i}} - \frac{1}{\eta_{t-1,i}} = \frac{1}{\alpha}\Big(\sqrt{n_{t,i} + g_{t,i}^2} - \sqrt{n_{t,i}}\Big)$$

$$z_{t+1,i} = z_{t,i}  + g_{t,i} - \sigma_{t,i}w_{t,i}$$

$$n_{t+1,i} = n_{t,i} + g_{t,i}^2$$

$$w_{t+1,i} = \begin{cases}
      0 & \vert z_{t+1,i}\vert  \leq \lambda_{1,t}\\
      -\frac{\alpha}{\sqrt{n_{t+1,i}} + \alpha\lambda_{2,t}} \Big(z_{t+1,i} - sgn(z_{t+1,i})\lambda_{1,t} \Big) & \vert z_{t+1,i}\vert  > \lambda_{1,t}
   \end{cases}\qquad(\ast)$$

Вывод этих формул хорошо расписан в [конспекте курса Д. А. Кропотова](http://www.machinelearning.ru/wiki/images/5/5b/MOMO12_sparse_methods.pdf).

#### Анализ аналитического решения

При регуляризаторе $\vert\vert w\vert\vert_1$ в оптимизируемом функционале стоят коэффициенты $\lambda_{1,t}$, которые могут как-то зависеть от $t$. Обычно рассматривают три вида зависимости:

1. Fixed: $\lambda_{1,t} = \lambda$.
2. Squared incremental: $\lambda_{1,t} = \sqrt{t}\lambda$
3. Linear incremental: $\lambda_{1,t} = t\lambda$

Их также можно комбинировать, получая коэффициенты регуляризации

$$\lambda_{1,t} = \lambda_{1,global} + \sqrt{t}\lambda_{1,sqrt} + t\lambda_{1,incremental}$$

Напомним, что все веса $w_{i}$ мы инициализируем нулями. По формулам $(\ast)$ из нуля на шаге $t$ выводятся веса $w_i$, для которых

$$\vert g_{1:t,i} - \sum\limits\sigma_sw_{s,i}\vert  > \lambda_{1,t}.$$

Таким образом, **начальное условие выхода параметров из нуля** имеет вид

$$\vert g_{1:t,i}\vert  > \lambda_{1,t}.$$

Попробуем понять физический смысл этого неравенства.

**Напоминание**. Говорят, что функция $f(w)$ имеет **липшиц-непрерывный** градиент с константой $L$, если

$$\vert\vert \nabla f(x) - \nabla f(y)\vert\vert_2^2 \leq \frac{L}{2} \vert\vert x - y\vert\vert_2^2$$

Предположим, что это выполняется (ниже мы покажем, что это не слишком обременительное ограничение). Тогда, подставив в качестве $y$ точку оптимума функции $f_t(w)$ (не путайте с глобальным $w_T^*$ из regret!), мы получим

$$\vert\vert \nabla f(x)\vert\vert_2^2 \leq \frac{L}{2}\vert\vert x - x_*\vert\vert_2^2$$

Это означает, что для достаточно хорошей функции **норма градиента является оценкой снизу на расстояние до точки оптимума в пространстве параметров**. Чем больше норма градиента, тем дальше мы от оптимальных параметров $w$.

Вернемся к выражению $\vert g_{1:t,i}\vert  > \lambda_{1,t}$. Здесь мы имеем дело (а) отдельно с каждой из координат и (б) с нормой суммы градиентов (а не с суммой норм). Хорошая новость: утверждение выше верно и для функций одной переменной, то есть $\vert g_{s,i}\vert$, грубо говоря, показывает, насколько мы далеки от оптимума по $i$-й координате. Знак $g_{s,i}$ говорит о том, в какую сторону мы будем сдвигаться по $i$-й координате $w$ на $s$-м шаге. Если сдвиги были в основном в одну сторону, то $g_{1:t,i}$ будет больше, а если они всё время в разную сторону, то отдельные слагаемые могут скомпенсировать друг друга, и $g_{1:t,i}$ может быть малым.

Отметим ещё, что абсолютная величина компоненты $g_{1:t,i}$ на первых итерациях может отражать прогнозирующую силу параметра $w_i$: в самом деле, неверное значение важного для предсказания параметра может вести к большим ошибкам, что будет давать большие градиенты.

Посмотрим теперь, как будет вести себя разреженная модель в зависимости от вида $\lambda_{1,t}$.

### Linear incremental ($\lambda_{1,t} = t\lambda_1$)

Условие выхода $w_i$ из нуля принимает вид

$$\vert g_{1:t,i}\vert  > t\lambda_1,$$

что равносильно

$$\left| \frac{1}{t}g_{1:t,i}\right|  > \lambda_1$$

Ограничение на среднее значение компоненты градиента означает, что для выхода из нуля параметр $w_i$ должен иметь определённую прогнозирующую силу. Это противоречит нашему требованию о том, чтобы частые маломощные параметры все равно присутствовали в модели и выучивали среднее поведение.

**Обратите внимание**. Выше мы показали, что проксимальный градиентный спуск с обычным $L_1$

$$w_{t+1} = arg\min\limits_w g_t^Tw + \lambda_1 \vert\vert w\vert\vert_1 + \frac{1}{\eta_t}\vert\vert w - w_t\vert\vert_2^2$$

в некотором смысле эквивалентен Composite-Objective FTRL с инкрементальным $L_1$. Таким образом, обычная $L_1$-регуляризация в классическом градиентном спуске эквивалентна именно инкрементальному $L_1$, который, как мы выяснили, субоптимален. Ниже мы рассмотрим специфический для FTRL вариант $L_1$-регуляризации, который лишен этих недостатков.

### Фиксированный ($\lambda_{1,t} = t\lambda_1$)

Это самый мощный и полезный на практике режим.

Здесь мы не нормируем на $\frac{1}{t}$ (то есть не берём среднее), и это означает, что выйти из нуля может и слабый, но частый параметр, который за много итераций накопит достаточно большую сумму частных производных.

Свойства фильтрации с фиксированным регуляризатором в точности совпадают с продуктовыми требованиями:

1. Редкий параметр с мощной прогнозирующей силой на старте будет иметь большие по модулю градиенты одного знака, и он выйдет из нуля;
2. Редкий параметр с малой прогнозной силой не выйдет из нуля;
3. Частые параметры в любом случае выйдут из нуля.

### Squared incremental: ($\lambda_{1,t} = \sqrt{t}\lambda$)

В [этой статье](http://www.opt-ml.org/papers/OPT2016_paper_24.pdf) было теоретически обосновано, что если параметр частый, но нерелевантный и абсолютно шумный, то дисперсия $\vert g_{1:t}\vert$ будет иметь асимптотику $O(\sqrt{t})$. Из этого следует, что, если сделать регуляризацию порядка $\sqrt{t}$, мы лишим такой случайный шум почти любых шансов выйти из нуля.

К сожалению, ни в игрушечных примерах вроде Avazu, ни в продакшен задачах улучшений качества прогноза или степени разреживания модели без потери качества достичь не удалось. Возможно, вам повезет больше.

### Полезность частых параметров для разреживания модели

Рассмотрим две линейных модели

$$f_t(w) = w^Tx_t + b,\quad g_t(w) = w^Tx_t,$$

в которых все параметры $w_i$ разреженные. Давайте считать, что в первой модели есть константный (и совсем даже не разреженный) признак $x_b = 1$, которому и соответствует параметр $b$.

Теперь в каждой из моделей наложим на $w$ регуляризацию $L_1$ и сравним, что получится:

1. В модели $f_t$ параметрам $w_i$ нужно запомнить «отклонение» от среднего $b$;
2. В модели $g_t$ параметрам $w_i$ нужно запомнить абсолютное значение предсказания.

Нетрудно понятно, что при наличии bias нормы градиентов в первой модели в среднем будут намного меньше, потому что мы на каждом шаге оптимизации будем стартовать с точки, которая в среднем ближе к точке оптимума (bias и есть наше среднее). Поэтому меньше весов смогут преодолеть порог по модулю суммы градиентов и выйти из нуля. Таким образом, несмотря на одинаковый оптимум без регуляризации, при введении $L_1$-регуляризации модель с bias будет обладать более хорошим соотношением разреженность/качество прогноза.

Эта логика легко обобщается на более сложные случаи, когда вместо bias у нас есть неразреженные контентные признаки. Вывод такой: модели, в которых есть только очень разреженные параметры, обладают гораздо худшим соотношением разреженность/качество, чем модели, в которых есть и контентные, и разреженные параметры.

Убедиться в этих эффектах мы сможем в разделе с практикой на линейных моделях.

## $L_2$ регуляризация


### Weight decay

Рассмотрим обыкновенный SGD.

$$w_{t+1} = w_t - \alpha g_t$$

**Weight decay** состоит во введение штрафа на размер текущих весов:

$$w_{t+1} = (1 - \lambda)w_t - \alpha g_t,\quad 0 \leq \lambda < 1$$

Внимательные читатели уже заметили, что в случае с SGD это эквивалентно введению $L_2$-регуляризации. Давайте разберёмся, как это сделать правильно.

### Decoupled weight decay

Попробуем заменить $f_t(w)$ на

$$\hat{f}_t(w) = f_t(w) + \lambda_2 \vert\vert w\vert\vert_2^2$$

и запустить любой адаптивный метод, например, AdaGrad. Если мы беспечно заменим на градиентную оценку всю функцию $\hat{f}_t(w)$ (забыв, что с регуляризатором этого делать не стоит), то алгоритм примет вид

$$w_{t+1} = w_t - \hat{\eta}_tg_t,$$

где

$$\hat{\eta}_t = \frac{\alpha}{\sqrt{\sum\limits_{s=1}^t (g_s + \lambda_2 w_s)^2 }}$$

В этих формулах нехороши две вещи:

1. Коэффициенты $\alpha$ и $\lambda_2$ нетривиальным образом взаимодействуют. Это крайне неудобно при переборе гиперпараметров: изменение learning rate $\alpha$ должно влечь за собой переподбор коэффициента регуляризации $\lambda_2$ по полной сетке;
2. В квадратах градиентов мы хотим видеть только адаптивность к кривизне самой функции $f_t$, но теперь там ещё добавка $\lambda_2w_s$.

Эта проблема была впервые замечена в [Decoupled weight decay regularization](https://arxiv.org/pdf/1711.05101.pdf). Авторы также рассматривали влияние на momentum, к этому мы вернёмся в параграфе про AdamW.

Авторы статьи предлагают модифицировать метод AdaGrad следующим образом:

$$w_{t+1} = w_t - \frac{\alpha}{\sqrt{\sum\limits_{s=1}^t g_s^2}} g_t - \lambda_2 w_t$$

Сразу отметим сходство с исходными формулами weight decay — его и добивались авторы.

### Decoupled weight decay — это адаптивный $L_2$

Легко видеть, что формула

$$w_{t+1} = w_t - \frac{\alpha}{\sqrt{\sum\limits_{s=1}^t g_s^2}} g_t - \lambda_2 w_t$$

описывает обыкновенный покоординатный градиентный спуск с некоторым линеаризованным $L_2$-регуляризатором. Давайте «проинтегрируем» это выражение обратно до аргминимума, из которого бы получились такие формулы обновления весов:

$$w_{t+1} = arg\min\limits_w \Big[ g_t^Tw + \frac{\lambda_2}{\eta_t}\color{#E06A27}{w_t^T}w + \frac{1}{2\eta_t}\vert\vert w - w_t\vert\vert_2^2 \Big]$$

Получается, что decoupled weight decay — это адаптивный $L_2$-centered регуляризатор. Его можно усовершенствовать, вспомним наше важное правило не заменять регуляризатор на субградиентную оценку. Перейдём к задаче

$$w_{t+1} = arg\min\limits_w \Big[ g_t^Tw + \frac{\lambda_2}{2\eta_t}\vert\vert w\vert\vert_2^2 + \frac{1}{2\eta_t}\vert\vert w - w_t\vert\vert_2^2 \Big]$$

Она отличается от предыдущей заменой $\color{#E06A27}{w_t^T}w$ на $\color{#348FEA}{w^T}w = \vert\vert w\vert\vert^2$. Её решение имеет вид

$$w_{t+1} = \color{#C81D6B}{\frac{1}{1 + \lambda_2}}\Big(w_t - \eta_t g_t \Big)$$

Поскольку мы меньше огрубляем оптимизируемый функционал, обучение может стать немного стабильнее.

Обратите внимание, что в оптимизационной задаче у нас теперь стоит не просто $\lambda_2$, а $\frac{\lambda_2}{2\eta_t}$.

### Decoupled $L_2$-регуляризация в Composite-Objective FTRL

Теперь посмотрим, как decoupled weight decay будте работать с Composite-Objective FTRL. Линеаризованная задача имеет вид:

$$w_{t+1} = arg\min\limits_w \left[g_{1:t}^Tw + \frac{\lambda_2}{2}\vert\vert w\vert\vert_{\sigma_{1:s}}^2 + \frac{1}{2}\sum\limits_{s=1}^t\vert\vert w - w_s\vert\vert_{\sigma_s}^2\right]$$

Перепишем её:

$$w_{t+1} = arg\min\limits_w \left[g_{1:t}^Tw + \frac{1}{2}\sum\limits_{s=1}^t\Big(\vert\vert w - w_s\vert\vert_{\sigma_s}^2 + \lambda_2\vert\vert w\vert\vert_{\sigma_s}^2 \Big)\right]$$

Нетрудно показать, что решение имеет вид

$$z_{t+1} = z_t + g_t - \sigma_t\odot w_t,$$

$$w_{t+1} = -\color{#C81D6B}{\frac{1}{1 + \lambda_2}} \frac{1}{\eta_t}z_t.$$

Для $z_t$ можно написать и явную формулу: $z_{t} = g_{1:t} - \sum_{s=1}^t\sigma_s\odot w_s$.

**Замечание**. Чтобы оценить Regret такого метода, мы не сможем механически воспользоваться оценкой для AdaGrad: ведь она базированась на оценке на Regret, выведенной либо для целиком Proximal, либо для целиком Centered $L_2$-регуляризаторов. Composite objective из теоремы 10 тут не годится, так как Centered регуляризатор в этом случае не поедет в оценку норм градиентов, а мы в текущем представлении рассматриваем Proximal и Centered как равноправные члены. Интуитивно, мы должны применить Lemma 7 к обоим регуляризаторам и получить точно такую же оценку с такой же двойственной нормой (напомним, что centered и proximal регуляризаторы имеют одинаковую двойственную норму). Двойственная норма такая же -> формулы оптимального метода AdaGrad будут такие же. Мы оставляем это читателям в качестве упражнения.

## $I_{\chi}(w)$: проекция на выпуклое множество $\chi$

**Напоминание**: множество $\chi$ называется **выпуклым**, если

$$\forall x,y\in \chi,\ \forall\alpha \in [0; 1]: \quad \alpha x + (1-\alpha)y \in \chi$$

Проекцией на это множество называют функцию

$$I_{\chi}(w) = \begin{cases}
      \infty & w \not\in \chi \\
      0 & w \in \chi
   \end{cases}$$

Докажем, что $I_{\chi}(w)$ — выпуклый регуляризатор. Для этого нам нужно проверить неравенство

$$\alpha I(x) + (1-\alpha)I(y) \geq I(\alpha x + (1 - \alpha) y).$$

Единственный шанс, когда это может быть нарушено — это $I(\alpha x + (1 - \alpha) y) = \infty$, $I(x) = 0$, $I(y) = 0$. Это значит, что $x,y\in \chi$, а $\alpha x + (1 - \alpha) y \notin \chi$, что противоречит выпуклости $\chi$.

Вернемся к формулам FTRL. Здесь ситуация сильно проще — от накидывания любых последовательностей $\alpha_{1:T}$ на регуляризатор ничего не изменится, так что его всегда оставляют просто as is

$$w_{t+1} = arg\min\limits_w g_{1:t}^Tw + I_{\chi}(w) + \frac{1}{2}\sum\limits_{s=1}^T\vert\vert w - w_s\vert\vert_{\sigma_s}^2$$

Аналитические решения для каждого вида $\chi$ нужно искать отдельно. Примерно все решения получаются путем выноса $I_{\chi}(w)$ из оптимизируемого функционала и превращения его в ограничение, после чего можно применить метод множителей Лагранжа.

### Проекция на шар ${x: \vert\vert x\vert\vert  \leq c}$

Решим аналитически задачу проекции на шар

$$ \begin{cases}
g_{1:t}^Tw + \frac{1}{2}\sum\limits_{s=1}^T\vert\vert w - w_s\vert\vert_{\sigma_s}^2\longrightarrow\min_w,\\
\vert\vert w\vert\vert_2 \leq c.
\end{cases}$$

Функция Лагранжа будет иметь вид

$$L(w, \lambda) = g_{1:t}^Tw + \frac{1}{2}\sum\limits_{s=1}^T\vert\vert w - w_s\vert\vert_{\sigma_s}^2 + \lambda(\vert\vert w\vert\vert_2 - c),$$

а её градиент равен

$$\nabla_wL(w, \lambda) = g_{1:t}^T + \sum\limits_{s=1}^T(w - w_s)\odot\sigma_s + \lambda \frac{w}{\vert\vert w\vert\vert_2},$$

где $\sigma_s$ - вектор, а $\odot$ — поэлементное умножение векторов. Приравнивая к нулю градиент, получаем

$$z_t + \sigma_{1:s}\odot w + \lambda \frac{w}{\vert\vert w\vert\vert_2} = 0,$$

где мы, как обычно, обозначили $z_{t} = g_{1:t} - \sum_{s=1}^t\sigma_s\odot w_s$.

Проанализируем условие дополняющей нежесткости $\lambda(\vert\vert w\vert\vert -c) = 0$. Если $\lambda = 0$, то решение $w$ уже находится внутри шара и имеет вид

$$w = \frac{-z_t}{\sigma_{1:s}}$$

При практической реализации мы просто сначала посчитаем это выражение и проверим, не попадаем ли мы в шар. Если попадаем — отлично, если нет — то дальше говорим, что $\vert\vert w\vert\vert  = c$ и решаем продолжаем решение

$$z_t + \sigma_{1:s}*w + \lambda \frac{w}{с} = 0$$

$$w = \frac{-z_t}{\sigma_{1:s} + \frac{\lambda}{c}}$$

Теперь подставим это в $\vert\vert w\vert\vert  = c$ и получим

$$\vert\vert w\vert\vert  = \frac{\vert\vert z_t\vert\vert }{\sigma_{1:s} + \frac{\lambda}{c}} = c$$

$$\lambda = \vert\vert z_t\vert\vert  - \sigma_{1:s}c$$

$$w = c\frac{-z_t}{\vert\vert z_t\vert\vert }$$

Получаем, что если мы находимся внутри шара, то мы действуем согласно обыкновенному adaptive алгоритму со всеми хорошими свойствами, иначе — проекция побеждает.

Аналогично $L_1$ регуляризации, здесь тоже есть различия между lazy и greedy представлением этого регуляризатора. Однако, в классических DL задачах эти методы встречаются не слишком часто и здесь сложно привести какой-нибудь значимый успех, который мог бы улучшить качество в важной задача. Навскидку мы можем вспомнить разве что Adversatial White-Box learning, в котором можно было бы это попробовать.

  ## handbook

  Учебник по машинному обучению

  ## title

  Регуляризация в онлайн-обучении

  ## description

  *null*

- 
  ## path

  /handbook/ml/article/metody-optimizacii-v-deep-learning

  ## content

  ## Напоминания

**Определение** Критической точкой гладкой функции $f_t(w)$ называется точка $w^*$, для которой

$$\nabla f_t(w^*) = 0
$$

В выпуклой оптимизации такая точка обязательно будет точкой глобального минимума. В невыпуклой оптимизации все сильно сложнее:

1. Бывает много локальных минимумов
2. Бывают седловые точки

Локальный минимум — это критическая точка $w^*$, в которой Гессиан $H(w^*) = \nabla^2 f_t(w^*)$ положительно определён. Отметим, что часто в методах глобальной оптимизации рассматривается так называемая «локальная выпуклость», для которой требуется, чтобы функция $f_t(w)$ была выпуклой внутри некоторого шара радиуса $\epsilon$ с центром в точке $w^*$. Критические точки, в которых гессиан не является знакоопределённым, называются седловыми.

**Пример**: функция $f(x_1, x_2) = x_1^2 - x_2^2$ имеет седловую точку $\{0, 0\}$. Гессиан в точке 0

$$H(0, 0) = \begin{pmatrix}
2 & 0\\
0 & -2
\end{pmatrix}
$$

**Обратите внимание**: во многих современных статьях про сходимость методов оптимизации первого порядка на невыпуклых функциях ([пример](https://arxiv.org/pdf/2003.02395.pdf)) в качестве критерия сходимости рассматривают сходимость по норме градиента: $\vert\vert \nabla f(w)\vert\vert_2^2 < \epsilon$ при некотором заранее фиксированном $\epsilon$.

В выпуклой оптимизации этот критерий сходимости эквивалентен двум другим:

- сходимости по расстоянию до оптимума в пространстве параметров: $\vert\vert w - w^*\vert\vert_2^2 < \epsilon$;
- сходимости по расстоянию до оптимума по значениям функции $f(w) - f(w^*) < \epsilon$.

В невыпуклой оптимизации всё не так просто и поиск глобального минимума является в общем случае NP-трудной задачей. Критерий $\vert\vert \nabla f(w)\vert\vert_2^2 < \epsilon$ даёт возможность исследовать сходимость к любой критической точке, но если речь об обучении нейронных сетях, то остается лишь надеяться, что эта критическая точка будет хорошим локальным минимумом.

## Скользящее среднее в знаменателе AdaGrad. Методы RMSprop и Adam

### Мотивация

В далекие 2012-2014е в мире было не так много опыта по построению хороших нейросетевых архитектур. «Канонические» методы оптимизации нейросетей RMSprop и Adam появлялись во времена, когда ещё не придумали основополагающих вещей вроде:

- [Residual connection](https://arxiv.org/pdf/1512.03385.pdf) и [Dense connection](https://arxiv.org/pdf/1608.06993.pdf) (статьи опубликованы в 2015/2016 соответственно, во всех экспериментах используется SGD, в статье и в ссылках не упоминаются методы Adam/RMSprop), плохо решались проблемы взрывов/затуханий градиентов и т.д.
- [Batch Normalization](https://arxiv.org/abs/1502.03167) и [Layer Normalization](https://arxiv.org/abs/1607.06450) (2015/2016 соответственно)

Также люди не умели правильно инициализировать нейросети гигантской глубины. статьи вроде [1000\+ layer fully connected](https://arxiv.org/abs/1711.04735) и [10000\+ layer CNN](https://arxiv.org/pdf/1806.05393.pdf) позже. Кстати, этот цикл статей хочется особо отметить за интересную технику анализа распространения сигнала по нейронной сети.

В общем, в те времена царило архитектурное средневековье со всеми родовыми проблемами нейронных сетей:

1. Взрывы градиентов;
2. Затухания градиентов;
3. Взрывы-затухания сигнала на прямом проходе;
4. Плохие начальные инициализации, нестабильный старт обучения.

При попытках применять метод AdaGrad особо остро стояли проблемы 1 и 4. AdaGrad аккумулирует всю прошедшую историю $\frac{1}{\sqrt{g_{1:t}^2}}$ без затухания. Если в какой-то момент возникает одна из указанных проблем, знаменатель резко возрастает и больше не выправляется.

Чтобы побороть проблемы 1-4, решили поработать над оптимизатором и сделать так, чтобы история в AdaGrad аккумулировалась с затуханием и метод оптимизации мог со временем забыть плохие точки. Самый популярный и простой в реализации метод — экспоненциальное скользящее среднее.

### RMSProp

Самая первая и самая простая модификация метода AdaGrad — метод RMSprop — вместо суммы использует экспоненциальное скользящее среднее в знаменателе:

$$v_0 = 0
$$

$$v_t = \beta v_{t-1} + (1 - \beta) g_t^2
$$

$$w_{t+1} = w_t - \frac{\alpha}{\sqrt{v_t}}g_t
$$

Методу RMSprop не было посвящено ни одной специализированной статьи, равно как и не было никаких доказательств его сходимости даже для выпуклых задач.

### Adam

Авторы Adam в статье [Adam: A Method For Stochastic Optimization](https://arxiv.org/pdf/1412.6980.pdf) вводят два новшества по сравнению с RMSprop. Во-первых, это Momentum. Во вторых — Bias correction term. Напомним, как работает этот метод.

$$v_0 = 0, m_0 = 0
$$

$$m_t = \beta_1 m_{t-1} + (1 - \beta_1)g_t
$$

$$v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
$$

Применяем bias correction

$$\hat{m}_t = \frac{1}{1 - \beta_1^t}m_t
$$

$$\hat{v}_t = \frac{1}{1 - \beta_2^t}v_2
$$

$$w_{t+1} = w_t - \frac{\alpha}{\sqrt{\hat{v}_t}}\hat{m}_t
$$

Сразу перепишем $v_t$ и $m_t$ в нерекурсивной форме с зависимостью только от $g$:

$$\hat{v}_t = \frac{1}{1 - \beta_2^t}(1-\beta_2)\sum\limits_{s=1}^t \beta_2^{t-s} g_s^2
$$

$$\hat{m}_t = \frac{1}{1 - \beta_1^t}(1-\beta_1)\sum\limits_{s=1}^t \beta_1^{t-s} g_s
$$

#### Мотивация для bias correction

Авторы статьи пишут, что для правильной работы метода $m_t$ и $v_t$ должны быть несмещенными оценками $\mathbb{E}[g]$ и $\mathbb{E}[g^2]$ соответственно. Допустим, все $g_t$ — независимые одинаково распредёленные случайные величины. Это довольно сильное предположение, но иначе не получатся красивые формулы. Рассмотрим на примере $v_t$:

$$\mathbb{E}[v_t] = \mathbb{E}\Big[(1-\beta_2)\sum\limits_{s=1}^t \beta_2^{t-s} g_s^2\Big] = (1-\beta_2)\sum\limits_{s=1}^t \beta_2^{t-s}\mathbb{E}[g_s^2] =
$$

$$= \Big((1-\beta_2)\sum\limits_{s=1}^t \beta_2^{t-s}\Big)\mathbb{E}[g^2] = (1 - \beta_2^t)\mathbb{E}[g^2]
$$

Отсюда очевидно, что исходные $v_t$ и $m_t$ смещены на множитель $(1 - \beta_2^t)$, поэтому авторы Adam делят на него $\hat{m}_t$ и $\hat{v}_t$. Так как $\lim\limits_{t\rightarrow \infty} (1 - \beta_2^t) = 1$ при $0 \leq \beta_2 < 1$, эффект смещения сильнее всего заметен в начале итерационного процесса. Например, при классическом $\beta_2 = 0.999$ мы получаем смещение в 0.001 раз.

В начале обучения bias correction призван уменьшить слишком большие шаги оптимизатора.

#### Доказательство сходимости метода

В оригинальной статье приводится теорема с доказательством сублинейного Regret. Доказательство содержало ошибку, в новой работе 2018 года было доказано, что для любого набора гиперпараметров Adam существует *выпуклая* задача, на которой он не сходится. Проблемы со сходимостью, впрочем, не являются специфичными для выпуклых задач: в нейронных сетях Adam тоже может вести себя странно, и об этом мы поговорим ниже в разделе «Как сломать адаптивные методы».

Разбирать доказательство исходной статьи мы не будем, зато обратим внимание на пару неприятных фактов о различиях между «продаваемой» частью статьи и бекендом с экспериментами и доказательствами теорем.

#### Почему Adam стали считать лучшим методом стохастической оптимизации?

После успешного введения метода Adam в эксплуатацию в нейросети его окрестили «method of choice» в задачах стохастической оптимизации. Это было на 100% обусловлено его успехом в обучении нейронных сетей с нестабильными архитектурами.

Структура статьи выглядит следующим образом:

1. Выделенный в большую красивую видную рамочку алгоритм с дефолтными настройками вроде $\alpha = 0.001$;
2. Формулировка теоремы в разделе про доказательства;
3. Эксперименты на нейросетях и выпуклых задачах.

В пункте 1 описан алгоритм, который все нынче знают, как Adam. Мало кто знает, что в доказательствах сходимости и в экспериментах на выпуклых задачах использовался немного другой алгоритм: вместо константного $\alpha$ авторы статьи взяли $\alpha_t = \frac{\alpha}{\sqrt{t}}$. Сравним эти learning rate с AdaGrad:

#|
||

Метод

|

Формулы

||
||

AdaGrad

|

$\frac{1}{\sqrt{\sum\limits_{s=1}^t g_s^2}}$

||
||

Adam

|

$\frac{1 - \beta^t}{\sqrt{t}\sqrt{(1-\beta)}\sqrt{\sum\limits_{s=1}^t \beta^{t-s} g_s^2}}$

||
|#

Авторы в экспериментах на логистической регрессии **убили основное свойство Adam — неубывающие learning rate**. Вспомним, как в разделе про [вывод AdaGrad](https://academy.yandex.ru/handbook/ml/article/adaptivnyj-ftrl#ada-grad-nailuchshij-adaptivnyj-metod) мы анализировали порядок убывания learning rate — он был $\eta_t = O(\frac{1}{\sqrt{t}})$. Отсюда следует, что у такого Adam learning rate убывают так же, как в AdaGrad. Словом, будьте внимательны при чтении статей: смотрите не только в описание алгоритмов, но и в их реализацию.

Настоящий Adam, который в pytorch и tensorflow реализован без множителя $\frac1{\sqrt{t}}$, в выпуклой задаче разреженной логистической регрессии обычно работает намного хуже AdaGrad. Это справедливо как для чисто линейных моделей, так и для комбинированных [Wide & Deep](https://arxiv.org/abs/1606.07792) архитектур, из-за чего в одной и той же нейросети приходится использовать разные методы оптимизации для разных параметров.

### Промежуточный итог по Adam/RMSProp

Тут нужно запомнить три идеи:

1. Momentum
2. Скользящее среднее в learning rate
3. Bias correction

На практике, часто почему-то рассматривают методы RMSprop и Adam как нечто отлитое в граните и не пытаются брать от них лучшее. Например, методу RMSprop обычно идет на пользу добавление bias correction от adam. Так что полезно помнить идеи, стоящие за методами оптимизации, и уметь их комбинировать.

## Как сломать адаптивные методы со скользящим средним

### Как и когда ломаются адаптивные методы

Все диагональные адаптивные методы так или иначе используют покоординатный learning rate $\eta_{t,i} = \frac{\alpha_t}{\sqrt{v_{t,i}}}$. Методы отличаются лишь формулировкой $v_{t,i}$ и $\alpha_t$:

#|
||

Метод

|

Рекуррентные формулы $v_{t,i}$

|

Развернутые формулы $v_{t,i}$

|

$\alpha_t$

||
||

AdaGrad

|

$v_{t-1,i} + g^2_{t,i}$

|

$v_{t,i} = \sum\limits_{s=1}^tg^2_{s,i}$

|

$\alpha$

||
||

RMSprop

|

$\beta v_{t-1,i} + (1 - \beta) g^2_{t,i}$

|

$(1-\beta)\sum\limits_{s=1}^t \beta^{t-s} g_s^2$

|

$\alpha$

||
||

Adam

|

$\beta v_{t-1,i} + (1 - \beta) g^2_{t,i}$

|

$(1-\beta)\sum\limits_{s=1}^t \beta^{t-s} g_s^2$

|

$\alpha\sqrt{1 - \beta^t}$

||
|#

Все эти методы имеют единый вид формул FTRL, аналогичный формулам FTRL-AdaGrad:

$$w_{t+1} = arg\min\limits_w g_{1:t}^Tw + \sum\limits_{s=1}^t\vert\vert w - w_s\vert\vert_{\sigma_s}^2
$$

$$\eta_{s,i} = \frac{\alpha_s}{\sqrt{v_{s,i}}}
$$

$$\sigma_{s,i} = \frac{\sqrt{v_{s,i}}}{\alpha_s} - \frac{\sqrt{v_{s-1,i}}}{\alpha_{s-1}}
$$

Вспомним теоретические ограничения на $r_t(w)$:

- $r_t(w)$ — выпуклый;
- $r_t(w) \geq 0$.

Адаптивные методы с регуляризаторами $r_t(w) = \vert\vert w\vert\vert_{\sigma_t}^2$ будут удовлетворять этим условиям, если все $\sigma_{s,i} \geq 0$. В этом месте и локаются методы со скользящим средним: никто не обещал, что последовательность $v_t$ будет монотонно неубывать. Если же

$$\frac{\sqrt{v_{s,i}}}{\alpha_s} < \frac{\sqrt{v_{s-1,i}}}{\alpha_{s-1}},\qquad(\ast)
$$

то метод может ломаться

**Обратите внимание**. Momentum в методе Adam никак не повлияет на справедливость наших рассуждений, поскольку в формулах для адаптивных learning rate он не используется. Адаптивные методы с такими learning rate сломаются и с momentum, и без него.

**Обратите внимание**. Bias correction в методе Adam уменьшает learning rate в начале обучения, заставляя метод делать меньшие шаги.

Все рекуррентные формулы из таблицы можно переписать в виде

$$v_{t} = C_1v_{t-1} + C_2g_t^2
$$

Тогда неравенство $(\ast)$ можно записать в виде

$$\frac{\sqrt{C_1v_{t-1} + C_2g_t^2}}{\alpha_t} < \frac{\sqrt{v_{t-1}}}{\alpha_{t-1}}
$$

$$g_t^2 < \frac{\alpha_t^2}{C_1}v_{t-1}\left(\frac{1}{\alpha_{t-1}^2} - \frac{C_1}{\alpha_t^2}\right)
$$

Здесь мы можем подвести общую черту и сказать, что методы Adam и RMSprop дают $\sigma_{s,i} < 0$, когда $g_t^2$ становится меньше предыдущей накопленной истории с точностью до некоторой константы.

А когда такое бывает? Уменьшение $g_t^2$, как правило, означает приближение к критическим точкам. Добавление квадратичных регуляризаторов с отрицательным коэффициентом приводит к тому, что метод оптимизации **штрафует за близость к критическим точкам, заставляя убегать от них**. Это приводит к тому, что метод не может нормально сойтись к локальным минимумам (в выпуклых задачах — просто к минимумам, что намного более критично).

Отметим, что по разным координатам $\sigma_{s,i}$ могут вести себя по-разному. Таким образом, можно получить ситуацию, когда мы поощряем близость по одним координатам и штрафуем за близость по другим.

### Вывод условий поломок для конкретных методов

#### AdaGrad

AdaGrad невозможно сломать таким способом: для него гарантируется, что $\sigma_t \geq \sigma_{t-1}$.

#### RMSprop

$$v_{t,i} = \beta v_{t-1,i} + (1-\beta)g_{t,i}^2
$$

Подставим в условие $(\ast)$, сразу сократив константный $\alpha_t = \alpha$:

$$\beta v_{t-1,i} + (1-\beta)g_{t,i}^2 < v_{t-1,i}
$$

$$g_{t,i}^2 < v_{t-1,i}
$$

#### Adam

Чисто технически, при выведении формул можно подумать, что Adam страдает от указанных эффектов гораздо сильнее RMSprop, но на самом деле это не так.

Переобозначим $\beta_2$ из статьи про Adam как просто $\beta$ для общности обозначений.

Распишем неравенство $(\ast)$ для метода Adam:

$$\frac{\beta v_{t-1} + (1 - \beta) g^2_t}{1 - \beta^t} < \frac{v_{t-1}}{1 - \beta^{t-1}}
$$

$$\frac{1 - \beta}{1 - \beta^t}g^2_t < v_{t-1}\Big(\frac{1}{1 - \beta^{t-1}} - \frac{\beta}{1 - \beta^t}\Big)
$$

$$\frac{1 - \beta}{1 - \beta^t}g^2_t < v_{t-1}\frac{1 - \beta^t - \beta + \beta^t}{(1 - \beta^{t-1})(1 - \beta^t)}
$$

$$g_t^2 < \frac{1}{1 - \beta^{t-1}} v_{t-1}
$$

В отличие от RMSprop, у нас появился дополнительный множитель $\frac{1}{1 - \beta^{t-1}} > 1$. С одной стороны, можно подумать, что метод строго хуже. Однако, этот множитель сильно больше нуля только во время первых шагов оптимизации, тогда как рассматриваемая нами проблема играет роль только на поздних стадиях оптимизации при приближении к критическим точкам. А к тому моменту, этот множитель будет практически равен единице и мы получим формулы выше от RMSprop.

Поэтому, на самом деле, методы в одинаковой степени страдают от этих эффектов, но bias correction добавляет стабильности в начале.

### Интерпретации

#### Избегание локальных минимумов или седловых точек

Если представить, что нейросеть — очень плохая и жутко невыпуклая задача, то можно рассматривать подобное поведение как «защиту» от промежуточных плохих критических точек, позволяющую нам «убегать» от них.

Данная интерпретация, к сожалению, имеет множество недостатков:

1. Никто не обещал, что новая критическая точка будет лучше старой и что мы, прыгая таким образом, будем улучшать качество модели.
2. Не каждый локальный минимум плохой. Если текущая критическая точка — хороший локальный минимум с хорошей обобщающей способностью, то мы просто нормально не сойдемся к нему и не достигнем хорошего качества модели.
3. Общественность уже идентифицировала такое поведение как проблему и решила ее в более поздних популярных оптимизаторах (см.раздел про [AMSgrad](https://academy.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning#am-sgrad)).
4. Большинство современных рекомендаций по обучению больших неонлайновых моделей вроде GPT или картиночных моделей содержат в себе learning rate scheduler'ы как обязательный для успеха ингредиент. Эти рекомендации нивелируют проблему отрицательных регуляризаторов.
5. Все learning rate scheduler'ы заставляют learning rate убывать, что позволяет достигать лучших результатов, чем с помощью обычных Adam и RMSprop.
6. В параграфе про FTL мы узнали, что градиентный метод без регуляризации отвратительно работает даже на выпуклых задачах, а если мы начнём вводить отрицательную регуляризацию, да еще и на сложных невыпуклых задачах, то все может стать еще хуже.

В целом, мировой опыт говорит, что полагаться на подобные интерпретации при тюнинге модели не стоит.

#### Нестабильность в выпуклых задачах

Итак, методы RMSprop и Adam плохо работают для выпуклых задач, особенно для разреженных задач, и могут приводить к субоптимальным решениям на train. Тем не менее, есть искушение заявить, что «это такая регуляризация в классическом смысле: не слишком хорошо сходимся к оптимальной точке, не слишком сильно переобучаемся под датасет и можем лучше работать на тесте». Это искушение особенно опасно потому, что подобные эффекты действительно могут иметь место, особенно в классической (не онлайновой) постановке задачи. Любая регуляризация направлена на то, чтобы сдвинуть оптимум решения исходной некорректно поставленной задачи в надежде, что точка оптимума измененной задачи будет обладать лучшей обобщающей способностью на тесте. В частности, такой эффект может иметь ранняя остановка методов оптимизации до их сходимости к точке оптимума.

Однако здесь есть одно очень важное «но». Если введение регуляризации в некорректно поставленную задачу — это полностью осмысленный и контролируемый гиперпараметрами процесс, то хаотично разваливающийся вокруг точки оптимума метод оптимизации — нет. Подумайте: вдруг ваша задача фактически не является некорректно поставленной? Вдруг у вас огромный и очень репрезентативный датасет, благодаря чему оптимум на train всегда отлично работает в проде? В этом случае кривой метод оптимизации способен подпортить качество вашей модели.

#### Нестабильность в разреженных задачах

В задачах с разреженными параметрами ситуацию $g_{t,i}^2 < v_{t-1,i}$ получить еще легче. Допустим, у нас есть некоторый параметр $w_i$, который встречается в 0.1% объектов выборки. В такой ситуации между появлениями этого объекта в выборке и очередным расчетом градиентов для него проходит значительное время. За это значительное время модель дообучалась, и за счет других, менее разреженных параметров могла научиться лучше прогнозировать очередной объект с этим параметром $w_i$. Тогда $\vert\vert g_{t,i}\vert\vert_2$ уменьшается и, следовательно, больше шансов попасть в плохую ситуацию.

Ниже мы рассмотрим метод AMSgrad и наперёд скажем, что для оптимизации разреженных параметров Adam/RMSprop добавление AMSgrad очень часто дает прибавку в качестве.

#### Зависимость нестабильности в регуляризаторе от learning rate $\alpha$

На первый взгляд, парадоксальным кажется следующий факт: чем меньше learning rate, тем в бОльшую сторону может отклониться отрицательный регуляризатор:

$$\sigma_t = \frac{\sqrt{v_t} - \sqrt{v_{t-1}}}{\alpha}
$$

Однако в «жадных» формулах все с точностью до наоборот:

$$w_{t+1} = w_t - \frac{\alpha}{\sqrt{v_t}}g_t
$$

Из жадных формул очевидно, что уменьшение $\alpha$ ведет к уменьшению шага и, как следствие, увеличению стабильности алгоритма.

Чтобы разрешить парадокс, надо вспомнить, что в FTRL решающее значение имеет не один отдельный регуляризатор, а *сумма* $\sigma_{0:t}$. В начале процесса оптимизации $v_0 = 0$, первый регуляризатор точно не сломается. Чем меньше learning rate, тем меньшие шаги мы делаем от начальной точки и, следовательно, *тем меньше должна отличаться норма градиентов*.

Если от шага к шагу норма градиента меняется не слишком сильно, то мы накопим огромную кумулятивную регуляризацию $\sigma_{0:t}$ к моменту, когда регуляризатор решит отклониться в отрицательную сторону. При бОльшем learning rate мы шагаем быстрее, и точки, когда ломается регуляризатор, достигаем тоже быстрее, накопив гораздо меньшую сумму $\sigma_{0:t}$. Если теперь для очередной точки мы получили отрицательный регуляризатор, то насколько сильно он может всё поломать?

Окей, допустим, мы шагнули к критической точке. А насколько сильно может расколбасить *одна* плохая точка в регуляризаторе? Так, чтобы он перекрыл всю предыдущую сумму $\sigma_{0:t}$? Если градиенты ограничены по норме, то катастрофы, очевидно, не будет. Ограниченность градиентов по норме мы, с одной стороны, гарантировать не можем, с другой — проблемам взрыва/затухания градиентов в архитектурах уделяется столько внимания, что на практике это условие зачастую выполняется.

## Чиним RMSprop и Adam

### Мотивация

Время шло, люди учились строить хорошо обучаемые архитектуры. Стали даже появляться революционные идеи вроде [ReZero](https://arxiv.org/pdf/2003.04887.pdf) (не путать с аниме) с полным отказом от batchnorm/layernorm нормализаций в глубоких сетях и с улучшением качества работы и скорости сходимости. Ситуация со стабильностью обучения нейросетей кардинально изменилась.

Несмотря на улучшение стабильности обучения, люди стали замечать, что при длительном процессе оптимизации Adam начинает сбоить. Авторы метода AMSgrad в статье [On the Convergence of Adam and Beyond](https://openreview.net/pdf?id=ryQu7f-RZ) были одними из первых, кто провел почти аналогичный нашему анализ и добавили в Adam костыль, который обеспечивает выполнение условия $v_{t} \geqslant v_{t-1}$ и исключает отрицательные регуляризаторы.

**Обратите внимание**: в разделее про [Learning Rate Scheduling vs AdaGrad](https://academy.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning#learning-rate-scheduling-vs-ada-grad) мы поговорим о «цикличности истории» развития методов оптимизации в deep learning.

### AMSgrad

Авторы статьи [On the Convergence of Adam and Beyond](https://openreview.net/pdf?id=ryQu7f-RZ) анализируют последовательность

$$\Gamma_{t+1} = \Big(\frac{\sqrt{v_{t+1}}}{\alpha_{t+1}} - \frac{\sqrt{v_t}}{\alpha_t} \Big)
$$

и говорят, что отрицательные значения в ней вызывают проблемы с процессом оптимизации. Их анализ в целом аналогичен приведённому выше, поэтому мы не будем его здесь дублировать.

Авторы статьи не стали предлагать новых схем learning rate и просто модифицировали старую: выполнение $v_t >= v_{t-1}$ обеспечивается «в лоб» при помощи $\hat{v}_t = \max\{v_t, \hat{v}_{t-1}\}, \hat{v}_0 = 0$.

Итоговое правило апдейта без momentum и без bias correction (оригинальный Algorithm 2 из статьи bias correction не использует):

$$v_t = \beta v_{t-1} + (1 - \beta) g_t^2
$$

$$\hat{v}_t = \max\{v_t, \hat{v}_{t-1}\}
$$

$$w_{t+1} = w_t - \frac{\alpha}{\sqrt{\hat{v}_t}}g_t
$$

Если нужен метод с momentum, то можно просто заменить $g_t$ в последней формуле на $m_t$

$$m_t = \gamma m_{t-1} + (1 - \gamma) g_t
$$

$$w_{t+1} = w_t - \frac{\alpha}{\sqrt{\hat{v}_t}}m_t
$$

#### Реализация без дополнительной памяти

Оригинальные формулы из статьи $\hat{v}_t = \max\{v_t, \hat{v}_{t-1}\}$ предполагают, что для расчета $\hat{v}_t$ мы держим два параметра: $v_{t-1}$ и $\hat{v}_{t-1}$. RMSprop и Adam хранят только один параметр $v_{t-1}$. Таким образом, включение метода требует дополнительных расходов памяти (х1.5 относительно RMSprop и x1.33 относительно Adam). Выше при разборе методов RMSprop/Adam мы сказали, что на практике AMSgrad помогает разреженным параметрам. Для разреженных моделей потребление памяти — краеугольный камень, поэтому простое включение дефолтной реализации amsgrad из статьи может быть болезненным и, к сожалению, не оправданным.

На практике же эвристика вида $v_t = \max\{v_t, v_{t-1}\}$ для разреженных параметров обычно работает так же хорошо и не требует дополнительной памяти. Никаких теоретических гарантий для нее нет, но на практике она работает.

#### Добавление bias correction

Оригинальная статья (и следующие букве оригинала стандартные реализации алгоритма, например, в PyTorch) предполагает убирание bias correction. Эксперименты на разреженных данных показывают, что убирание bias correction вредит сходимости, это полезная вещь.

С практической точки зрения, есть два способа реализовать bias correction в AMSgrad:

1. Post-correction: $\hat{v}_t = \frac{1}{1 - \beta^t}\max\{v_t, \hat{v}_{t-1}\}$,
2. Pre-correction: $\hat{v}_t = \max\{\frac{1}{1 - \beta^t}v_t, \frac{1}{1 - \beta^{t-1}}\hat{v}_{t-1}\}$.

С точки зрения корректности метода AMSgrad, правильный вариант — pre-correction, так как он не ломает максимум. А вот эксперименты показывают, что добавление pre-correction ничего не даёт, а вот post-correction действительно помогает в том смысле, что AMSgrad \+ post-bias correction лучше, чем просто RMSProp/Adam с bias correction.

#### Соединяем эвристику \+ bias correction

Итоговые формулы можно использовать такие:

$$\hat{v} = \frac{1}{1 - \beta^t}\max\left\{v_t, \frac{1}{1-\beta^{t-1}}v_{t-1}\right\}
$$

### Learning Rate Scheduling

Другим способом улучшения сходимости методов RMSprop/Adam/SGD является learning rate scheduling (расписание learning rate, шедулер). Learning rate scheduler — это мета-алгоритм: они берёт любой стандартный метод оптимизации с константным параметром learning rate $\alpha$ и предписывает схему изменения $\alpha_t$ на каждом шаге $t$, или на каждой эпохе, или на любом другом заданном периоде.

Поскольку мы работаем с одним параметром $\alpha$, мы можем с ним делать всего две вещи: увеличивать или уменьшать. Эти два варианта имеют свои названия:

1. Learning rate decay — уменьшение learning rate с течением времени с целью нивелировать осцилляцию RMSprop/Adam около критических точек.
2. (Warm)Restart — обычно резкое увеличение learning rate. Warm — потому что мы уже сошлись в какую-то хорошую точку и сбрасываем только состояние оптимизатора в ней, но не переинициализируем сами параметры. WarmRestart может заключаться не только в увеличении $\alpha$, но и, например, в дополнительном сбросе состояния оптимизатора (обнуление momentum или $v_{t}$), хотя автор статьи такой подход встречали достаточно редко

Существует огромное количество вариантов расписания, каждый со своим графиком изменения $\alpha_t$ и со своим любовно подобранным множеством задач, на которых данный метод показывает себя лучше других. Приводить здесь их список особого смысла нет, лучше просто откройте [документацию любого фреймворка](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) и наслаждайтесь разнообразием вариантов.

Мы же обсудим влияние learning rate decay на осцилляцию вокруг критических точек и дадим практические рекомендации по подбору расписаний.

#### Влияние learning rate decay на сходимость

Для выпуклых задач в разделе про схемы убывания learning rate для FTRL-методов (константный регуляризатор, $\frac{1}{sqrt{t}}$ и AdaGrad) мы буквально на оценках на regret видели, что это важный аспект для асимптотики сходимости.

В выпуклом случае, при приближении к минимуму мы должны оптимизировать решение с куда большей точностью. Норма градиентов при приближении к минимуму тоже уменьшаются, поэтому даже с константным $O(1)$ learning rate шаги будут становиться меньше, но — как показывают и теоретические оценки на regret, и многочисленные их валидации в статьях — этого недостаточно. Уменьшение learning rate с правильной асимптотикой уменьшения дает куда более хорошие результаты. Для глубинного обучения и оптимизации к каким-то локальным минимумам эта логика тоже применима.

Возвращаясь к методам Adam/RMSprop — напомним, что у них асимптотика learning rate $O(1)$. Им в любом случае пойдет на пользу уменьшение learning rate, даже если не брать во внимание их проблемы вокруг критических точек и взять метод AMSgrad, который от этих проблем не страдает.

Отсюда же очевидно, что проблемы adam/rmsprop начинают стрелять гораздо меньше. Learning rate уменьшается =\> от критической точки мы в плохих ситуациях шагаем на гораздо меньшее расстояние =\> область, вокруг которой мы будем «прыгать», сужается =\> мы худо-бедно, но сходимся.

#### Практические рекомендации

Как мы уже отмечали выше, шедулеров существует поистине фантастическое количество, гораздо больше, чем базовых оптимизаторов, к которым они применяются. Без структуризации подхода к ним работать становится сложно.

Мы хотели бы дать вам следующие рекомендации:

1. Выучите свою модель без learning rate scheduling со стандартными методами оптимизации и посмотрите, как ведёт себя loss для различных learning rate. Обязательно переберите learning rate на этом шаге.
2. Начинать внедрение расписаний рекомендуем с шедулеров, которые только уменьшают learning rate. Классические варианты — ReduceOnPlateou или linear decay. Правильный подбор learning rate и темпа его уменьшения очень важны в любой задаче стохастической оптимизации.
3. Только после того, как вы хорошенько потюните learning rate decay, можно смотреть в сторону WarmRestart. Иногда рестарты могут помочь. Автор статьи занимается в основном рекомендательными моделями и там эту технику практически никто не применяет.

#### Learning rate scheduling vs AdaGrad

У методов SGD/RMSprop/Adam последовательность $\eta_t \sim O(1)$ не является асимптотически убывающей, и для того, чтобы это скомпенсировать, используется расписание learning rate. А вот у AdaGrad с $\eta_t$ и так всё в порядке.

Давайте восстановим хронологию событий:

1. Метод AdaGrad пытаются применять к нейросетям в 2012\+ годах, но тогда архитектуры были нестабильны, градиенты взрывались и навсегда портили знаменатель AdaGrad, сильно уменьшая learning rate.
2. Появляются методы RMSprop/[Adam](https://arxiv.org/abs/1412.6980) (2013/2014) со скользящим средним в знаменателе, которые могут оправиться от взрыва градиента.
3. Развитие архитектур нейронных сетей не стоит на месте, появляются разные виды [residual connection](https://arxiv.org/abs/1512.03385) (2015), [LayerNorm](https://arxiv.org/abs/1607.06450)/[BatchNorm](https://arxiv.org/abs/1502.03167) (2015-2016), крутые методы [начальной инициализации](https://arxiv.org/abs/1711.04735) — огромное количество способов улучшения стабильности обучения.
4. С развитием архитектур люди замечают, что RMSProp/Adam умеют застревать на одном уровне значений функции потерь, и начинают применять техники для уменьшения learning rate.
5. В дальнейших работах метод AdaGrad часто рассматривается наравне с Adam/RSMprop и дает очень похожее, либо даже лучшее качество (см, например, статью про [Shampoo](https://arxiv.org/pdf/1802.09568.pdf)). А дело в том, что архитектуры уже очень хорошо инициализируются и правильно проектируются так, чтобы не было взрывов/затуханий градиентов ни на какой стадии оптимизации.

Развитие методов оптимизации в deep learning сделало небольшой круг, и мы рекомендуем об этом помнить. Порой люди могут одновременно рассуждать о бесценной пользе learning rate decay (особенно с линейным убыванием как $\frac1t$) и корить AdaGrad за бесконечное аккумулирование квадратов градиентов (которые убывают как $\frac1{\sqrt{t}}$). Так что если у вас вдруг хорошо заработал шедулер с $\alpha_t \sim O(\frac{1}{\sqrt{t}})$ — возможно, обычный AdaGrad будет лучше?

### SGD vs Adam

В последнее время в литературе часто появляются заявления, что решения, полученные адаптивными методами в нейросетях, обладают худшей обобщающей способностью. Сразу хотим отметить, что большинство этих статей исследуют эти эффекты только на задачах Computer Vision на одних и тех же датасетах MNIST/CIFAR/ImageNet. В реальной жизни куда большее разнообразие постановок задач и датасетов, что сразу заставляет сомневаться в воспроизводимости этих эффектов. Рекомендация тут одна, как и всегда — досконально сами все проверяйте.

#### AdamW, SGDW

Данные методы предложены авторами в статье [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101), которую мы подробно разобрали в разделее про продвинутую <a href='https://academy.yandex.ru/handbook/ml/article/regulyarizaciya-v-onlajn-obuchenii#l-2-regulyarizacziya'>$L_2$ регуляризацию</a>. Методы AdamW и SGDW — это просто модификации методов Adam и SGD с momentum, которые используют линеаризованный decoupled $L_2$.

Авторы статьи изучали проблему, почему в их экспериментах SGD обобщает лучше Adam (но учится дольше и требует более аккуратной настройки). Они пришли к выводу, что дело не в магии SGD, а в том, что $L_2$-регуляризация у этих двух методов работает по-разному. Добавив decoupling, авторы сумели показать, что decoupled Adam обгоняет SGD.

Эти эффекты, повторимся, были уже рассмотрены ранее в разделее про продвинутую <a href='https://academy.yandex.ru/handbook/ml/article/regulyarizaciya-v-onlajn-obuchenii#l-2-regulyarizacziya'>$L_2$ регуляризацию</a>. Единственное, что мы не обсудили тогда — это momentum. В постановке Proximal Gradient Descent градиент заменяется на momentum

$$w_{t+1} = \color{red}{g_t}^Tw + \frac{\lambda_2}{\eta_t}w_t + \frac{1}{2\eta_t}\vert\vert w - w_s\vert\vert_2^2
$$

$$m_t = \gamma m_{t-1} + (1 - \gamma) g_t
$$

$$w_{t+1} = \color{red}{m_t}^Tw + \frac{\lambda_2}{\eta_t}w_t + \frac{1}{2\eta_t}\vert\vert w - w_s\vert\vert_2^2
$$

$$w_{t+1} = w_t - \eta_t m_t - \lambda_2 w_t
$$

Покоординатные $\eta_t$ могут рассчитываться любыми методами: AdaGrad, RMSprop или Adam, не принципиально.

На всякий случай напомним, что мы вывели потенциально более правильные формулы

$$w_{t+1} = m_t^Tw + \frac{\lambda_2}{2\eta_t}\vert\vert w\vert\vert_w^2 + \frac{1}{2\eta_t}\vert\vert w - w_s\vert\vert_2^2
$$

$$w_{t+1} = \frac{1}{1 + \lambda_2}\Big(w_t - \eta_t m_t\Big)
$$

Метод SGDW получается из формул выше, если убрать покоординатность $\eta_t$

К сожалению, здесь мы не почерпнули новых идей, так как выяснили, что это просто очередная инкарнация Proximal методов оптимизации.

#### RAdam

Этот метод заключается в том, чтобы стартовать с адаптивного метода Adam и в некоторый момент переключиться на SGD. «Некоторый момент» — это, интуитивно, момент стабилизации всех статистик в Adam, когда мы выжали все из ускоренного старта адаптивных методов и хотим получше сойтись к хорошему оптимуму в найденной им окрестности.

Отметим, что позднее переключение на SGD с неубывающими learning rate автоматически починит проблемы расходимости Adam ровно там, где они чаще всего и возникают: при хорошем приближении к локальным минимумам.

Мы не будем здесь подробно рассматривать их анализ, вы можете сами познакомиться с ним в статье [On The Variance Of The Adaptive Learning Rate And Beyond](https://arxiv.org/pdf/1908.03265.pdf)

## Online RMSprop

Особняком стоит метод, описанный в статье [Variants of RMSProp and Adagrad with Logarithmic Regret Bounds](http://proceedings.mlr.press/v70/mukkamala17a/mukkamala17a.pdf). Авторы не придумывали очередной хотфикс, а аккуратно заново выводили формулы. Также важно, что данный метод является строгим обобщением метода AdaGrad.

В работе есть два нововведения:

1. Переформулировка метода RMSprop так, чтобы:\
   — Осталось экспоненциальное скользящее среднее;\
   — Не было проблемы с отрицательными регуляризаторами и взрывающимися learning rate;\
   — Метод AdaGrad являлся частным случаем нового метода;\
   — Чтобы все эмпирически хорошо работало в т.ч. на глубоких моделях
2. Формулировка новых алгоритмов оптимизации SC-AdaGrad и SC-RMSprop для сильно выпуклых функций с логарифмическими гарантиями на regret. SC в названии — Strongly Convex.

Пока рассмотрим только первый пункт. Авторы вводят следующий общий метод:

$$v_t = \beta_t v_{t-1} + (1 - \beta_t) g_t^2
$$

$$\epsilon_t = \frac{\epsilon}{\sqrt{t}}
$$

$$\alpha_t = \frac{\alpha}{\sqrt{t}}
$$

$$w_{t+1} = w_t - \frac{\alpha_t}{\sqrt{v_t} + \epsilon_t}g_t
$$

Нововведение здесь в том, что вместо фиксированного $\beta$ мы будем рассматривать последовательность $\beta_t$. Авторы доказывают сублинейный regret для любых последовательностей, удовлетворяющих

$$1 - \frac{1}{t} \leq \beta_t \leq 1 - \frac{\gamma}{t}
$$

$$0 < \gamma \leq 1
$$

### AdaGrad как частный случай

Докажем, что метод Adagrad — это метод OnlineRMSprop с $\gamma = 1$. Аналогично выводам momentum в FTRL, перепишем рекуррентное выражение для $v_{t+1}$:

$$v_t = \sum\limits_{s=1}^t(1 - \beta_s)\prod\limits_{k=s+1}^t\beta_k g_k^2
$$

Подставив $\beta = 1 - \frac{1}{t}$, получим

$$v_t = \sum\limits_{s=1}^t(1 - \beta_s)\Big(\prod\limits_{k=s+1}^t\beta_k\Big) g_s^2 = \sum\limits_{s=1}^t\frac{1}{s}g_s^2\prod\limits_{k=s+1}^t\frac{t - 1}{t} =
$$

$$= \sum\limits_{s=1}^t\frac{1}{s}g_s^2\prod\limits_{k=s+1}^t\frac{k - 1}{k} = \sum\limits_{s=1}^t\frac{1}{s}g_s^2\frac{s}{t} = \frac{1}{t}\sum\limits_{s=1}^tg_s^2
$$

Далее, подставляя это в формулу $\eta_t = \frac{\alpha_t}{\sqrt{v_t}}$, получаем

$$\eta_t = \frac{\frac{\alpha}{\sqrt{t}}}{\sqrt{\frac{1}{t}\sum\limits_{s=1}^tg_s^2}} = \frac{\alpha}{\sqrt{\sum\limits_{s=1}^tg_s^2}}
$$

### Анализ OnlineRMSprop с $\gamma < 1$ в стиле FTRL. Пригодность для выпуклых задач

Докажем, что OnlineRMSprop не может сломать регуляризаторы в regret. Для этого преобразуем неравенство

$$\frac{\sqrt{v_t}}{\alpha_t} < \frac{\sqrt{v_{t-1}}}{\alpha_{t-1}}
$$

$$tv_t < (t-1)v_{t-1}
$$

$$t(\beta_t v_{t-1} + (1 - \beta_t)g_t^2) < (t-1)v_{t-1}
$$

$$t((1 - \frac{\gamma}{t})v_{t-1} + \frac{\gamma}{t}g_t^2) < (t-1)v_{t-1}
$$

$$\frac{\gamma}{t}g_t^2 < v_{t-1}(t - 1 - t(1 - \frac{\gamma}{t}))
$$

$$\frac{\gamma}{t}g_t^2 < v_{t-1}(\gamma - 1)
$$

Из условия $0 < \gamma \leq 1$ получаем, что правая часть неравенства неположительна, а левая неотрицательно. Значит, последнее неравенство невозможно, то есть все $\sigma_t \geq 0$. Таким образом, регуляризаторы не сломаются, сходимость будет иметь место и данный метод можно использовать в выпуклых задачах. Строгое доказательство сходимости и оценки на Regret можно прочитать в исходной статье.

### Эффективный learning rate

Как и ранее в методе AdaGrad, допустим, что $\vert\vert g\vert\vert_2 < R$. Тогда

$$\eta_t = \frac{\alpha}{\sqrt{t}\sqrt{\sum\limits_{j=1}^t(1-\beta_j)\prod\limits_{k=j+1}^t\beta_kg_j^2}} \leq \frac{\alpha}{R\sqrt{t}\sqrt{\sum\limits_{j=1}^t(1-\beta_j)\prod\limits_{k=j+1}^t\beta_k}}
$$

При $1 - \frac{1}{t} \leq \beta_t \leq 1 - \frac{\gamma}{t}$ выполнено

$$\lim\limits_{t\rightarrow\infty} \sum\limits_{j=1}^t(1-\beta_j)\prod\limits_{k=j+1}^t\beta_k = 1
$$

Докажем, что все элементы предела \< 1. Из этого, в частности, будет следовать, что learning rate у OnlineRMSprop не меньше, чем learning rate в AdaGrad.

Если все все $g_i^2 = 1$, то итерационный процесс OnlineRMSprop превращается в

$$v_t = \beta_t v_{t-1} + (1 - \beta_t)
$$

Предположим, что $v_t \geq 1$. Тогда:

$$\beta_t v_{t-1} + (1 - \beta_t) \geq 1
$$

$$(1 - \frac{\gamma}{t}) v_{t-1} + \frac{\gamma}{t} \geq 1
$$

$$v_{t-1} \geq 1
$$

По индукции разворачиваем вплоть до $v_0 = 0$, получаем противоречие.

Полное доказательство предела оставляем читателям. Надо бы чем-нибудь снизу подпереть, что тоже к 1 сходится. Автор сдавал матан почти 10 лет назад и ему было очень неохота откапывать все эти прекрасные пределы, поэтому ответ был получен с помощью wolfram.

**Вывод**: learning rate у OnlineRMSprop убывает со скоростью $\eta_t = O(\frac{1}{\sqrt{t}})$. Мы исправили ошибку предыдущего RMSprop, изменив *только* перевзвешивание, но не асимптотику в $\eta_t$. Такой RMSprop можно пробовать использовать в выпуклых задачах

## Momentum

Попробуем расписать классический momentum с константным learning rate в стиле FTRL:

$$v_{t + 1} = \beta v_t + (1 - \beta) g_{t}
$$

$$w_{t + 1} = w_t - \alpha v_{t + 1}
$$

Всё, что нам нужно сделать — это взять все рекурсивные зависимости от предыдущей итерации и «размотать» их, получив явное выражение.

Зависимость $w_{t+1}$ от $w_t$ переписать довольно просто, мы это уже делали для обычного градиентного спуска:

$$w_{t+1} = -\alpha\sum\limits_{i=1}^tv_i
$$

Теперь надо размотать $v_t = (1-\beta)\sum\limits_{i=1}^t\beta^{t-i}g_i$

Теперь будет чуть сложнее. Подставим это и попробуем расписать, как сумму $g_i$ с определенными коэффициентами:

$$w_{t+1} = -\alpha\sum\limits_{i=1}^tv_i = -\alpha\sum\limits_{i=1}^t (1-\beta)\sum\limits_{j=1}^i\beta^{i-j}g_j
$$

Множитель $-\alpha(1-\beta)$ сразу выносим за сумму и пока забываем.

$$\sum\limits_{i=1}^t\sum\limits_{j=1}^i\beta^{i-j}g_j = \sum\limits_{i=1}^t\sum\limits_{j=1}^t \mathbb{I}(j\leq i)\beta^{i-j}g_j =
$$

$$= \sum\limits_{j=1}^t\sum\limits_{i=1}^t\mathbb{I}(j\leq i)\beta^{i-j}g_j = \sum\limits_{j=1}^tg_j\sum\limits_{i=1}^t\mathbb{I}(j\leq i)\beta^{i-j} =
$$

$$= \sum\limits_{j=1}^tg_j\sum\limits_{i=j}^t\beta^{i-j}
$$

Отлично, а теперь нам нужно получить последовательность функций. В линеаризованной задаче это фактически эквивалентно получению зависимости $z_{t+1}$ от $z_t$, где, напомним, $z_t$ — это сумма градиентов.

$$z_{t+1} - z_t = \sum\limits_{j=1}^{t+1}g_j\sum\limits_{i=j}^{t+1}\beta^{i-j} - \sum\limits_{j=1}^tg_j\sum\limits_{i=j}^t\beta^{i-j} = g_{t+1} + \sum\limits_{j=1}^{t}g_j\sum\limits_{i=j}^{t+1}\beta^{i-j} - \sum\limits_{j=1}^tg_j\sum\limits_{i=j}^t\beta^{i-j} = g_{t+1} + \sum\limits_{j=1}^{t}\beta^{t+1-j}g_j = \sum\limits_{j=1}^{t+1}\beta^{t+1-j}g_j
$$

Теперь мы можем записать функцию, градиент которой равен $z_{t+1} - z_t$ и онлайн-оптимизация которой эквивалентна процедуре с моментумом:

$$\hat{f}_t(w) = \sum\limits_{j=1}^{t}\beta^{t-j}f_j(w)
$$

Получаем, что для онлайн-обучения мы на самом деле каждую итерацию скармливаем экспоненциально взвешенную последовательность всех предыдущих функций исходной последовательности. В принципе, нечто такое мы и ожидали увидеть. Функции $\hat{f_t(w)}$, очевидно, выпуклы, так что для данной измененной последовательности функций будет сублинейный regret.

### Nesterov Momentum

Рассмотрим классический SGD с momentum, для всех adaptive методов рассуждения аналогичны.

$$m_t = \gamma m_{t-1} + (1 - \gamma)g_t
$$

$$w_{t+1} = w_t - \alpha m_t
$$

Градиент функции $g_t$ посчитан в предыдущей точке $w_t$. Идея nesterov momentum в том, чтобы применить momentum на параметры $w_t$ **до вычисления градиента**:

$$g_t = \nabla f_t(w_t)
$$

$$\hat{g}_t = \nabla f_t(w_t - m_{t-1})
$$

У метода много всяких «интуитивных объяснений», но изначально Nesterov Momentum был выведен сугубо аналитическими методами. Увы, попытки добавлять его в стохастическую оптимизацию «в лоб» обычно улучшением качества не заканчиваются. Анализ того, почему так нельзя и делать и как можно сделать правильно, проводится в работах [Katyusha: The First Direct Acceleration of Stochastic Gradient Methods](https://arxiv.org/pdf/1603.05953.pdf) и [Natasha-2](https://arxiv.org/abs/1708.08694) (мотивация их автора Zeyuan Allen-Zhu для выбора таких наименований доподлинно неизвестна). Katuysha правильным образом использует nesterov momentum для выпуклого случая, Natasha — для невыпуклого. Данные методы используют подход SVRG для улучшения сходимости и ускорение оптимизации происходит **только при приближении к точке оптимума**.

### Adan

До недавнего времени громких историй успеха для nesterov momentum в глубоком обучении не было. Метод Natasha распространения не нашел. Наконец, авторы статьи [Adan](https://arxiv.org/abs/2208.06677) (2022) нашли способ правильной обработки Nesterov Momentum. Метод показал отличные результаты и обновил SOTA метрики на широком спектре задач.

## Собираем все идеи воедино

Авторы данного обзора очень хотят, чтобы читатель ушел не с знанием набора наименований методов оптимизации, а с знанием набора концепций, которые тот или иной метод реализует, и при случае мог сам подстроить метод под свои нужды. Тюнинг методов оптимизации — один из главных способов улучшения качества модели на фиксированном датасете.

 1. Adaptive learning rate — автоматическое подстраивание метода под геометрию задачи оптимизации. Крайне важный класс методов для выпуклых/невыпуклых задач. Must-have для разреженных моделей. Методы: AdaGrad/RMSprop/Adam.
 2. Скользящее среднее в adaptive learning rate представлено в методах RMSprop/Adam. Не забывайте про их плохое поведение вокруг критических точек и проблемы со сходимостью на финальных этапах оптимизации.
 3. BiasCorrection: стабилизация обучения на старте для адаптивных методов со скользящим средним. Большинство экспериментов показывают, что это крайне полезная штука и стоит всегда её использовать. В том числе стоит использовать RMSprop с bias correction, если вам не нужны momentum и Adam.
 4. AMSgrad: способ починить сходимость RMSprop/Adam. Не забывайте, что стандартные реализации при использовании AMSgrad отключают bias correction, а это на самом деле может навредить, а также о том, что можно реализовать AMSgrad без дополнительной памяти, и всё будет хорошо работать.
 5. Learning rate decay: убывание learning rate зачастую является очень важной деталью в стохастической оптимизации. Помните, что можно брать как AdaGrad, в котором это есть из коробки со скоростью $O(\frac{1}{\sqrt{t}})$ (но архитектура нейросети должна быть хорошей), так и комбинацию RMSProp/Adam \+ learning rate scheduler.
 6. WarmRestart: эвристика, резко увеличивающая learning rate после достижения некоторой точки в процессе оптимизации. Практически всегда идет бок о бок с learning rate decay. Где-то помогает
 7. Проксимальные методы для функций потерь с регуляризаторами: ProximalGD/AdamW/SGDW/FTRL-Proximal. Must-have для $L_1$-регуляризаторов, без проксимальности они вообще не работают.
 8. FTRL-Proximal: lazy vs greedy представление. Переписываем представление любого метода оптимизации в не-жадный вид. Позволяет по-новому взглянуть на любые регуляризаторы, особенно негладкие. Must-have для $L_1$-регуляризации.
 9. $L_1$-регуляризация в FTRL-Proximal: Incremental/Fixed/SquareIncremental. Все три имеют разные свойства и разную область применения. Fixed является наилучшим для отбора разреженных признаков/эмбеддингов.
10. $L_{1/2}$-регуляризатор для отбора эмбеддингов или автоматического подбора размерности. Можно использовать как аналог FSTR. Крайне полезный подход для разреженных нейросетей в рекомендательных системах, для которых рекомендуется использовать адаптивную схему SquareIncremental.
11. Heavy-ball Momentum: используется для ускорения процесса оптимизации. В выпуклых задачах имеет доказанные оценки на улучшение скорости сходимости, в нейросетях используется как эвристика (зачастую опциональная).
12. Nesterov momentum: в выпуклом случае гораздо мощнее для batch gradient descent, чем обычный momentum, и это подверждается теоретическими гарантиями. В стохастических методах оптимизации и в онлайн обучении «в лоб» применять нельзя: для выпуклого случая подойдет Katyusha, для нейросетей — Adan.

Главное, что мы хотим подчеркнуть, — эти идеи друг другу не противоречат и их можно свободно комбинировать друг с другом. Например, можно собрать себе FTRL-Proximal метод с $L_1$-регуляризацией, любым momentum и RMSprop learning rate с AMSgrad. Или любую другую комбинацию. Всегда можно выбрать оптимальный набор под задачу.

### Пример таблицы с общими формулами

Эти формулы используют все подходы выше в едином фреймворке, чтобы наглядно убедиться в том, что все можно друг с другом комбинировать.

**Generic FTRL-Proximal**

$$w_{t+1} = arg\min\limits_w \hat{g}_{1:t}^Tw + \lambda_{1,t}\vert\vert w\vert\vert_1 + \frac{1}{2}\vert\vert w\vert\vert_{\lambda_{2,t}}^2 + \frac{1}{2}\sum\limits_{s=1}^t\vert\vert w - w_s\vert\vert_{\sigma_s}^2
$$

$$z_t = g_{1:t} - \sum\limits_{s=1}^t\sigma_s w_s
$$

$$w_{t+1,i} = \begin{cases}
      0 & \vert z_{t,i}\vert  \leq \lambda_{1,t}\\
      -\frac{\sigma_{1:t}}{1 + \lambda_{2,t}\sigma_{1:t}} (z_t - sign(z_t)\lambda_{1,t})& \vert z_{t,i}\vert  > \lambda_{1,t}
\end{cases}
$$

**Generic Mirror (Proximal) Gradient Descent**

$$w_{t+1} = arg\min\limits_w \hat{g}_t^Tw + \lambda_{1,t}\vert\vert w\vert\vert_1 + \frac{1}{2}\vert\vert w\vert\vert_{\lambda_{2,t}}^2 + \frac{1}{2}\vert\vert w - w_t\vert\vert_{\frac{1}{\eta_t}}^2
$$

$$z_t = \frac{1}{\eta_t} w_t - g_t
$$

$$w_{t+1} = \begin{cases}
      0 & \vert z_t\vert  \leq \lambda_{1,t}\\
      -\frac{\eta_t}{1 + \lambda_{2,t}\eta_t} (z_t - sign(z_t)\lambda_{1,t})& \vert z_t\vert  > \lambda_{1,t}
\end{cases}
$$

**Связь**:

$$\sigma_t = \frac{1}{\eta_t} - \frac{1}{\eta_{t-1}}
$$

#|
||

Идея

|

FTRL (lazy)

|

Gradient Descent (greedy)

|

Комментарии

||
||

Momentum

|

$\hat{g}_t = m_t = \gamma m_{t-1} + (1 - \gamma) g_t$ $m_0 = 0$

|

То же самое

|

Не влияет на adaptive $v_t$

||
||

Константный learning rate

|

$\sigma_0 = \frac{1}{\alpha}$, $\sigma_t = 0, t > 0$

|

$\eta_t = \alpha$

|


||
||

Убывающий непокоординатный learning rate

|

$\sigma_t = \frac{1}{\alpha_t} - \frac{1}{\alpha_{t-1}}$

|

$\eta_t = \alpha_t$

|

Обычно берут $\alpha_t = \frac{\alpha}{\sqrt{t}} = O(\frac{1}{\sqrt{t}})$

||
||

Generic Adaptive learning rate

|

$\sigma_t = \frac{\sqrt{v_t}}{\alpha_t} - \frac{\sqrt{v_{t-1}}}{\alpha_{t-1}}$

|

$\eta_t = \frac{\alpha}{\sqrt{v_t}}$

|

$v_t$ и $\eta_t$ — векторы, везде ниже умножение $\eta_tg_t$  означает покоординатное умножение

||
||

Generic Adaptive learning rate с scheduler $S(\alpha, t)$

|

$\sigma_t = \frac{\sqrt{v_t}}{S(\alpha_t, t)} - \frac{\sqrt{v_{t-1}}}{S(\alpha_{t-1}, t)}$

|

$\eta_t = \frac{S(\alpha_t, t)}{\sqrt{v_t}}$

|

Например, в Adam: $\alpha\sqrt{1 - \beta^t} \rightarrow S(\alpha, t)\sqrt{1 - \beta^t}$

||
||

Adaptive learning rate: AdaGrad

|

$v_t = \sum\limits_{s=1}^t g_s^2$</th><th>$v_t = \sum\limits_{s=1}^t g_s^2$

|

$v_t = \sum\limits_{s=1}^t g_s^2$

|

$\eta_t = O(\frac{1}{\sqrt{t}})$

||
||

Adaptive learning rate: RMSprop

|

$v_t = \beta v_{t-1} + (1 - \beta) v_t$, $\alpha_t = \alpha$

|

То же самое

|

$\eta_t = O(1)$, ломается у критических точек

||
||

Adaptive learning rate: Online RMSprop

|

$v_t = \beta_t v_{t-1} + (1 - \beta_t) v_t$, $\alpha_t = \frac{\alpha}{\sqrt{t}}$, $\beta_t = 1 - \frac{\gamma}{t}$

|

То же самое

|

$\eta_t = O(\frac{1}{\sqrt{t}})$

||
||

Adaptive learning rate: Adam

|

$v_t = \beta v_{t-1} + (1 - \beta) v_t$, $\alpha_t = \alpha\sqrt{1 - \beta^t}$

|

То же самое

|

$\eta_t = O(1)$, ломается у критических точек

||
||

Adaptive learning rate: AMSgrad

|

$v_t = \beta v_{t-1} + (1 - \beta) v_t$, $\alpha_t = \alpha$

|

То же самое

|

$\eta_t = O(1)$

||
||

Adaptive learning rate: RAdam

|

Многобукв

|

Многобукв

|

$\eta_t = O(1)$

||
||

Классическая $L_2$ регуляризация

|

$\lambda_{2,t} = \lambda_2$

|

$\lambda_{2,t} = \lambda_2$

|


||
||

Decoupled $L_2$ регуляризация

|

$\lambda_{2,t} = \frac{\lambda_2}{\sigma_{0:t}}$

|

$\lambda_{2,t} = \frac{\lambda_2}{\eta_t}$

|


||
||

Инкрементальная $L_1$ регуляризация

|

$\lambda_{1,t} = t\lambda_1$

|

$\lambda_{1,t} = \lambda_1$

|


||
||

Фиксированная $L_1$ регуляризация

|

$\lambda_{1,t} = \lambda_1$

|

Отсутствует

|


||
|#



  ## handbook

  Учебник по машинному обучению

  ## title

  Методы оптимизации в Deep Learning

  ## description

  *null*

- 
  ## path

  /handbook/ml/article/matrichnoe-differencirovanie

  ## content

  Любая задача машинного обучения — это задача оптимизации, а задачи оптимизации удобнее всего решать градиентными методами (если это возможно, конечно). Поэтому важно уметь находить производные всего, что попадается под руку. Казалось бы, в чём проблема: ведь дифференцирование — простая и понятная штука (чего не скажешь, например, об интегрировании). Зачем же как-то специально учиться дифференцировать матрицы?

Да в принципе-то никаких проблем: в этом параграфе вы не узнаете никаких секретных приёмов или впечатляющих теорем. Но, согласитесь, если исходная функция от вектора $x$ имела вид $f(x) = \vert\vert Ax - b\vert\vert^2$ (где $A$ — константная матрица, а $b$ — постоянный вектор), то хотелось бы уметь и производную выражать красиво и цельно через буквы $A$, $x$ и $b$, не привлекая отдельные координаты $A_{ij}$, $x_k$ и $b_s$. Это не только эстетически приятно, но и благотворно сказывается на производительности наших вычислений: ведь матричные операции обычно очень эффективно оптимизированы в библиотеках, чего не скажешь о самописных циклах по $i, j, k, s$. И всё, что будет происходить дальше, преследует очень простую цель: научиться вычислять производные в удобном, векторно-матричном виде. А чтобы сделать это и не сойти с ума, мы должны ввести ясную систему обозначений, составляющую ядро техники матричного дифференцирования.

## Основные обозначения

Вспомним определение производной для функции $f:\mathbb{R}^m\rightarrow\mathbb{R}^n$. Функция $f(x)$ дифференцируема в точке $x_0$, если

$$f(x_0 + h) = f(x_0) + \color{#348FEA}{\left[D_{x_0} f \right]} (h) + \bar{\bar{o}} \left(\left| \left| h\right|\right|\right),
$$

где $\color{#348FEA}{\big[D_{x_0} f\big]}$ — **дифференциал** функции $f$: линейное отображение из мира $x$-ов в мир значений $f$. Грубо говоря, он превращает «малое приращение $h=\Delta x$» в «малое приращение $\Delta f$» («малые» в том смысле, что на о-малое можно плюнуть):

$$f(x_0 + h) - f(x_0)\approx\color{#348FEA}{\left[D_{x_0} f \right]} (h)
$$

Отметим, что дифференциал зависит от точки $x_0$, в которой он берётся: $\color{#348FEA}{\left[D_{\color{red}{x_0}} f \right]} (h)$. Под $\vert\vert h\vert\vert$ подразумевается норма вектора $h$, например корень из суммы квадратов координат (обычная евклидова длина).

Давайте рассмотрим несколько примеров и заодно разберёмся, какой вид может принимать выражение $\color{#348FEA}{\big[D_{x_0} f\big]} (h)$ в зависимости от формы $x$. Начнём со случаев, когда $f$ — скалярная функция.

{% cut "Примеры конкретных форм $\big[D_{x_0} f\big] (h)$, когда $f$ — скалярная функция" %}

1. $f(x)$ — скалярная функция, $x$ — скаляр. Тогда

$$f(x_0 + h) - f (x_0) \approx f'(x_0) (h)
$$

$$\color{#348FEA}{\left[D_{x_0} f\right] (h)} = f'(x_0) h = h \cdot f'(x_0)
$$

Здесь $h$ и $f'(x_0)$ — просто числа. В данном случае $\color{#348FEA}{\left[D_{x_0} f\right]}$ — это обычная линейная функция.

2. $f(x)$ — скалярная функция, $x$ — вектор. Тогда

$$f(x_0 + h) - f(x_0) \approx \sum\limits_i \left.\frac{\partial f}{\partial x_i} \right|_{x=x_0} h_i,
$$

то есть

$$	\color{#348FEA}{\left[D_{x_0} f\right]}(h) = \left(\color{#FFC100}{\nabla_{x_0} f}\right)^T h = \langle\color{#FFC100}{\nabla_{x_0} f}, h \rangle, 
$$

где $\langle\bullet, \bullet\rangle$ — операция скалярного произведения, а $\color{#FFC100}{\nabla_{x_0} f} = \left(\frac{\partial f}{\partial x_1}, \ldots, \frac{\partial f}{\partial x_n}\right)$ — градиент функции $f$.

3. $f(x)$ — скалярная функция, $X$ — матрица. Дифференциал скалярной функции по матричному аргументу определяется следующим образом:

$$	f(X_0 + H) - f(X_0) \approx \sum\limits_{i,j}\ \left.\frac{\partial f}{\partial X_{ij}} \right|_{X=X_0} H_{ij}
$$

Можно заметить, что это стандартное определение дифференциала функции многих переменных для случая, когда переменные — элементы матрицы $X$. Заметим также один интересный факт:

$$	\sum_{ij} A_{ij} B_{ij} = \text{tr} A^T B,
$$

где $A$ и $B$ — произвольные матрицы одинакового размера. Объединяя оба приведённых выше факта, получаем:

$$	\color{#348FEA}{\left[D_{X_0} f \right]} (H) 
	= \sum \limits_{ij} 
		\left.
			\frac{\partial f}{\partial X_{ij}}
		\right|_{X = X_0} 
		\left( 
			X - X_0
		\right)_{ij}
	= \text{tr}\, 
		\left( \left[\left. \frac{\partial f}{\partial X_{ij}}\right|_{X=X_0}\right]^T H\right).
$$

Можно заметить, что здесь, по аналогии с примерами, где $x$ — скаляр и где $x$ — вектор (и $f(x)$ — скалярная функция), получилось на самом деле скалярное произведение градиента функции $f$ по переменным $X_{ij}$ и приращения. Этот градиент мы записали для удобства в виде матрицы с теми же размерами, что матрица $X$.

{% endcut %}

В примерах выше нам дважды пришлось столкнуться с давним знакомцем из матанализа: **градиентом** скалярной функции (у нескалярных функций градиента не бывает). Напомним, что градиент $\color{#FFC100}{\nabla_{x_0} f}$ функции в точке $x_0$ состоит из частных производных этой функции по всем координатам аргумента. При этом его обычно упаковывают в ту же форму, что и сам аргумент: если $x$ — вектор-строка, то и градиент записывается вектор-строкой, а если $x$ — матрица, то и градиент тоже будет матрицей того же размера. Это важно, потому что для осуществления градиентного спуска мы должны уметь прибавлять градиент к точке, в которой он посчитан.

Как мы уже имели возможность убедиться, для градиента скалярной функции $f$ выполнено равенство

$$    \left[D_{x_0} f \right] (x-x_0) = \langle\color{#FFC100}{\nabla_{x_0} f}, x-x_0\rangle,
$$

где скалярное произведение — это сумма попарных произведений соответствующих координат (да-да, самое обыкновенное).

Посмотрим теперь, как выглядит дифференцирование для функций, которые на выходе выдают не скаляр, а что-то более сложное.

{% cut "Примеры $\big[D_{x_0} f\big] (h)$, где $f$ — это вектор или матрица" %}

1. 

$$f(x) = \begin{pmatrix} f(x_1)\\ \vdots\\ f(x_m) \end{pmatrix}
$$

$x$ — вектор. Тогда

$$f(x_0 + h) - f(x_0) = 
	\begin{pmatrix}
		f(x_{01} + h_1) - f(x_{01})\\
		\vdots \\
		f(x_{0m} + h_m) - f(x_{0m}) 
	\end{pmatrix}
\approx 
	\begin{pmatrix}
		f'(x_{01}) h_1\\
		\vdots  \\
		f'(x_{0m}) h_m
	\end{pmatrix} 
=
	\begin{pmatrix}
		f'(x_{01}) \\
		\vdots \\
		f'(x_{0m})
	\end{pmatrix}
	\odot
		h.
$$

В последнем выражении происходит покомпонентное умножение:

$$\color{#348FEA}{\big[D_{x_0} f\big]} (h) = f'(x_0) \odot h = h \odot f'(x_0)
$$

2. $f(X) = XW$, где $X$ и $W$ — матрицы. Тогда

$$f(X_0 + H) - f(X_0) = (X_0 + H) W - X_0 W = H W,
$$

то есть

$$\color{#348FEA}{\big[D_{X_0} f\big]} (H) = H W
$$

3. $f(W) = XW$, где $X$ и $W$ — матрицы. Тогда

$$f(W_0 + H) - f(W_0) = X(W_0 + H) - XW_0 = X H,
$$

то есть

$$\color{#348FEA}{\big[D_{W_0} f\big]} (H) = X H
$$

4. $f(x) = (f_1(x),\ldots,f_K(x))$ — вектор-строка, $x = (x_1,\ldots,x_D)$ — вектор-строка. Тогда

$$\color{#348FEA}{\big[D_{x_0} f\big]}(h)
= \left(\sum_j 
\left. 
	\frac{\partial f_1}{\partial y_j} 
	\right|_{y=x_0}h_j, \ldots, \sum_j \left. \frac{\partial f_K}{\partial y_j} \right|_{y=x_0}h_j \right) =
$$

$$= h \cdot 
\begin{pmatrix} 
	\left.
		\frac{\partial f_1}{\partial y_1}
	\right|_{y=x_0} & \ldots & 
	\left.
		\frac{\partial f_k}{\partial y_1}
	\right|_{y=x_0} \\
	\vdots & & \vdots \\
	\left.
		\frac{\partial f_1}{\partial y_D}
	\right|_{y=x_0} & \ldots & 
	\left.
		\frac{\partial f_k}{\partial y_D}
	\right|_{y=x_0}\\
\end{pmatrix}
= h \cdot 
\left.
	\frac{\partial f}{\partial y}\right|_{y = x_0}
$$

Матрица, выписанная в предпоследней выкладке, — это знакомая вам из курса матанализа матрица Якоби.

{% endcut %}

## Простые примеры и свойства матричного дифференцирования

1. **Производная константы.** Пусть $f(x) = a$. Тогда

   $$f(x_0 + h) - f(x_0) = 0,
   $$
   
   то есть $\color{#348FEA}{\big[D_{x_0} f\big]}$ — это нулевое отображение. А если $f$ — скалярная функция, то и $\color{#FFC100}{\nabla_{x_0} f} = 0.$

2. **Производная линейного отображения.** Пусть $f(x)$ — линейное отображение. Тогда

   $$f(x_0 + h) - f(x_0) = f(x_0) + f(h) - f(x_0) = f(h)
   $$
   
   Поскольку справа линейное отображение, то по определению оно и является дифференциалом $\color{#348FEA}{\big[D_{x_0} f\big]}$. Мы уже видели примеры таких ситуаций выше, когда рассматривали отображения умножения на матрицу слева или справа. Если $f$ — (скалярная) линейная функция, то она представляется в виде $\langle a, v\rangle$ для некоторого вектора $a$ — он и будет градиентом $f$.

3. **Линейность производной.** Пусть $f(x) = \lambda u(x) + \mu v(x)$, где $\lambda, \mu$ — скаляры, а $u, v$ — некоторые отображения, тогда

   $$\color{#348FEA}{\big[D_{x_0} f\big]} = \lambda \color{#348FEA}{\big[D_{x_0} u\big]} + \mu \color{#348FEA}{\big[D_{x_0} v\big]}
   $$
   
   

{% cut "Попробуйте доказать сами, прежде чем смотреть доказательство. " %}

$$f(x_0 + h) - f(x_0) = (\lambda u(x_0 + h) + \mu v(x_0 + h)) - (\lambda u(x_0) + \mu v(x_0)) =
$$

$$= \lambda(u(x_0 + h) - u(x_0)) + \mu(v(x_0 + h) - v(x_0)) \approx 
$$

$$\approx \lambda \color{#348FEA}{\big[D_{x_0} u\big]}(h) + \mu \color{#348FEA}{\big[D_{x_0} v\big]}(h)
$$

{% endcut %}

4. **Производная произведения.** Пусть $f(x) = u(x) v(x)$, где $u, v$ — некоторые отображения, тогда

   $$\color{#348FEA}{\big[D_{x_0} f\big]} = \color{#348FEA}{\big[D_{x_0} u\big]} \cdot v(x_0) + u(x_0) \cdot \color{#348FEA}{\big[D_{x_0} v\big]}
   $$
   
   

{% cut "Попробуйте доказать сами, прежде чем смотреть доказательство." %}

Обозначим для краткости $x = x_0 + h$. Тогда

$$u(x)v(x) - u(x_0)v(x_0) = u(x)v(x) - u(x_0)v(x) + u(x_0)v(x) - u(x_0)v(x_0) =
$$

$$(u(x) - u(x_0))v(x) + u(x_0)(v(x) - v(x_0))\approx 
$$

$$\approx \color{#348FEA}{\big[D_{x_0} u\big]}(h) \cdot v(x) + u(x_0)\cdot \color{#348FEA}{\big[D_{x_0} v\big]}(h)
$$

И всё бы хорошо, да в первом слагаемом $v(x)$ вместо $v(x_0)$. Придётся разложить ещё разок:

$$\color{#348FEA}{\big[D_{x_0} u\big]}(h) \cdot v(x) \approx 
$$

$$\color{#348FEA}{\big[D_{x_0} u\big]}(h) \cdot \left(v(x_0) + \color{#348FEA}{\big[D_{x_0} v\big]}(h) + o(\vert\vert h\vert\vert)\right) =
$$

$$\color{#348FEA}{\big[D_{x_0} u\big]}(h) \cdot v(x_0) + \bar{\bar{o}}\left(\vert\vert h\vert\vert\right)
$$

{% endcut %}

Это же правило сработает и для скалярного произведения:

$$\color{#348FEA}{\big[D_{x_0} \langle u, v\rangle\big]} = \langle\color{#348FEA}{\big[D_{x_0} u\big]}, v\rangle + \langle u, \color{#348FEA}{\big[D_{x_0} v\big]}\rangle$$

В этом нетрудно убедиться, повторив доказательство или заметив, что в доказательстве мы пользовались лишь дистрибутивностью (= билинейностью) умножения.


5. **Производная сложной функции.** Пусть $f(x) = u(v(x))$. Тогда

   $$f(x_0 + h) - f(x_0) = u(v(x_0 + h)) - u(v(x_0)) \approx 
   $$
   
   $$\approx\left[D_{v(x_0)} u \right] (v(x_0 + h) - v(x_0)) \approx \left[D_{v(x_0)} u \right] \left( \left[D_{x_0} v\right] (h)\right)
   $$
   
   Здесь $D_{v(x_0)} u$ — дифференциал $u$ в точке $v(x_0)$, а $\left[D_{v(x_0)} u \right]\left(\ldots\right)$ — это применение отображения $\left[D_{v(x_0)} u \right]$ к тому, что в скобках. Итого получаем:

   $$\left[D_{x_0} \color{#5002A7}{u} \circ \color{#4CB9C0}{v} \right](h) = \color{#5002A7}{\left[D_{v(x_0)} u \right]} \left( \color{#4CB9C0}{\left[D_{x_0} v\right]} (h)\right)
   $$
   
   

6. Важный частный случай: **дифференцирование перестановочно с линейным отображением**. Пусть $f(x) = L(v(x))$, где $L$ — линейное отображение. Тогда $\left[D_{v(x_0)} L \right]$ совпадает с самим $L$ и формула упрощается:

   $$\left[D_{x_0} \color{#5002A7}{L} \circ \color{#4CB9C0}{v} \right](h) = \color{#5002A7}{L} \left( \color{#4CB9C0}{\left[D_{x_0} v\right]} (h)\right)
   $$
   
   

## Простые примеры вычисления производной

* Вычислим дифференциал и градиент функции $f(x) = \langle a, x\rangle$, где $x$ — вектор-столбец, $a$ — постоянный вектор.

{% cut "Попробуйте вычислить сами, прежде чем смотреть решение." %}

Вычислить производную можно непосредственно:

$$f(x_0 + h) - f(x_0) = \langle a, x_0 + h\rangle - \langle a, x_0\rangle = \langle a, h\rangle
$$

Но можно и воспользоваться формулой дифференциала произведения:

$$\color{#348FEA}{\big[D_{x_0} \langle a, x\rangle\big]} (h) = 
$$

$$=\langle\color{#348FEA}{\big[D_{x_0} a\big]}(h), x\rangle + \langle a, \color{#348FEA}{\big[D_{x_0} x\big]}(h)\rangle
$$

$$= \langle 0, x\rangle + \langle a, h\rangle = \langle a, h\rangle
$$

Сразу видно, что градиент функции равен $a$.

{% endcut %}

* Вычислим производную и градиент $f(x) = \langle Ax, x\rangle$, где $x$ — вектор-столбец, $A$ — постоянная матрица.

{% cut "Попробуйте вычислить сами, прежде чем смотреть решение. " %}

Снова воспользуемся формулой дифференциала произведения:

$$\color{#348FEA}{\big[D_{x_0} \langle Ax, x\rangle\big]}(h) = 
$$

$$= \langle\color{#348FEA}{\big[D_{x_0} Ax\big]}(h), x_0\rangle + \langle Ax_0, \color{#348FEA}{\big[D_{x_0} x\big]}(h)\rangle
$$

$$= \langle Ah, x_0\rangle + \langle Ax_0, h\rangle
$$

Чтобы найти градиент, нам надо это выражение представить в виде $\langle ?, h\rangle$. Для этого поменяем местами множители первого произведения и перенесём $A$ в другую сторону ($A$ перенесётся с транспонированием):

$$\langle A^Tx_0, h\rangle + \langle Ax_0, h\rangle = 
$$

$$= \langle (A^T + A)x_0, h\rangle
$$

Получается, что градиент в точке $x_0$ равен $(A^T + A)x_0$.

{% endcut %}

* Вычислим производную обратной матрицы: $f(X) = X^{-1}$, где $X$ — квадратная матрица.

{% cut "Попробуйте вычислить сами, прежде чем смотреть решение. " %}

Рассмотрим равенство $I = X\cdot X^{-1} = I$ и продифференцируем его:

$$0 = \color{#348FEA}{\big[D_{X_0} \left( X\cdot X^{-1}\right)\big]}(H) = 
$$

$$= \color{#348FEA}{\big[D_{X_0} X\big]}(H)\cdot X_0^{-1} + X_0\cdot \color{#348FEA}{\big[D_{X_0} X^{-1}\big]}(H)
$$

Отсюда уже легко выражается

$$\color{#348FEA}{\big[D_{X_0} X^{-1}\big]}(H) = -X_0^{-1}\cdot\color{#348FEA}{\big[D_{X_0} X\big]}(H)\cdot X_0^{-1}
$$

Осталось подставить $\color{#348FEA}{\big[D_{X_0} X\big]}(H) = H$, но запомните и предыдущую формулу, она нам пригодится.

{% endcut %}

* Вычислим градиент определителя: $f(X) = \text{det}(X)$, где $X$ — квадратная матрица.

{% cut "Попробуйте вычислить сами, прежде чем смотреть решение. " %}

В предыдущих примерах мы изо всех сил старались не писать матричных элементов, но сейчас, увы, придётся. Градиент функции состоит из её частных производных: $\color{#FFC100}{\nabla_{x_0} f} = \left(\frac{\partial f}{\partial{x_{ij}}}\right)_{i,j}$. Попробуем вычислить $\frac{\partial f}{\partial{x_{ij}}}$. Для этого разложим определитель по $i$-й строке:

$$\text{det}(X) = \sum_{k}x_{ik}\cdot(-1)^{i + k}M_{ik},
$$

где $M_{ik}$ — это определитель подматрицы, полученной из исходной выбрасыванием $i$-й строки и $k$-го столбца. Теперь мы видим, что определитель линеен по переменной $x_{ij}$, причём коэффициент при ней равен $\cdot(-1)^{i + k}M_{ik}$. Таким образом,

$$\frac{\partial f}{\partial{x_{ij}}} = (-1)^{i + k}M_{ik}
$$

Чтобы записать матрицу, составленную из таких определителей, покороче, вспомним, что

$$X^{-1} = \frac1{\text{det}(X)}\left((-1)^{i+j}M_{\color{red}{ji}}\right)_{i,j}
$$

Обратите внимание на переставленные индексы $i$ и $j$ (отмечены красным). Но всё равно похоже! Получается, что

$$\color{#FFC100}{\nabla_{x_0} f} = \text{det}(X)\cdot X^{-T},
$$

где $X^{-T}$ — это более короткая запись для $(X^{-1})^T$.

{% endcut %}

* Вычислим градиент функции $f(x) = \vert\vert Ax - b\vert\vert^2$. С этой функцией мы ещё встретимся, когда будем обсуждать задачу линейной регрессии.

{% cut "Попробуйте вычислить сами, прежде чем смотреть решение. " %}

Распишем квадрат модуля в виде скалярного произведения:

$$\vert\vert Ax - b\vert\vert^2 = \langle Ax - b, Ax - b\rangle
$$

Применим формулу дифференциала произведения и воспользуемся симметричностью скалярного произведения:

$$\color{#348FEA}{\big[D_{x_0} \langle Ax - b, Ax - b\rangle\big]}(h) = 
$$

$$\langle \color{#348FEA}{\big[D_{x_0} (Ax - b)\big]}(h), Ax_0 - b\rangle + \langle Ax_0 - b, \color{#348FEA}{\big[D_{x_0} (Ax - b)\big]}(h)\rangle
$$

$$= 2\langle Ax_0 - b, \color{#348FEA}{\big[D_{x_0} (Ax - b)\big]}(h)\rangle =
$$

$$= 2\langle Ax_0 - b, Ah\rangle = \langle 2A^T(Ax_0 - b), h\rangle
$$

Получаем, что

$$\color{#FFC100}{\nabla_{x_0} f} = 2A^T(Ax_0 - b)
$$

{% endcut %}

## Примеры вычисления производных сложных функций

* Вычислим градиент функции $f(X) = \text{log}(\text{det}(X))$.

{% cut "Попробуйте вычислить сами, прежде чем смотреть решение. " %}

Вспомним формулу производной сложной функции:

$$\left[D_{X_0} u \circ v \right](H) = \left[D_{v(X_0)} u \right] \left( \left[D_{X_0} v\right] (H)\right)
$$

и посмотрим, как её тут можно применить. В роли функции $u$ у нас логарифм:

$$u(y) = \text{log}(u),\quad \left[D_{y_0} u\right](s) = \frac1y_0\cdot s,
$$

а в роли $v$ — определитель:

$$v(X) = \text{det}(X),\quad \left[D_{y_0} v\right](H) = \langle \text{det}(X_0)\cdot X_0^{-T}, H\rangle,
$$

где под скалярным произведением двух матриц понимается, как обычно,

$$\langle A, B\rangle = \sum_{i,j}a_{ij}b_{ij} = \text{tr}(A^TB)
$$

Подставим это всё в формулу произведения сложной функции:

$$\left[D_{X_0} u \circ v \right](H) = \frac1{\text{det}(X)}\cdot\langle \text{det}(X)\cdot X^{-T}, H\rangle =
$$

$$= \langle \frac1{\text{det}(X)}\cdot\text{det}(X)\cdot X^{-T}, H\rangle =
\langle X_0^{-T}, H\rangle$$

Отсюда сразу видим, что

$$\color{#FFC100}{\nabla_{X_0} f} = X_0^{-T}
$$

{% endcut %}

* Вычислим градиент функции $f(X) = \text{tr}(AX^TX)$.

{% cut "Попробуйте вычислить сами, прежде чем смотреть решение." %}

Воспользуемся тем, что след — это линейное отображение (и значит, перестановочно с дифференцированием), а также правилом дифференцирования сложной функции:

$$\left[D_{X_0} f \right](H) = \text{tr}\left(\left[D_{X_0} AX^TX \right](H)\right) =
$$

$$=\text{tr}\left(A\cdot\left[D_{X_0} X^T \right](H)\cdot X_0 + AX_0^T\left[D_{X_0} X \right](H)\right) =
$$

$$=\text{tr}\left(AH^TX_0 + AX_0^TH\right)
$$

Чтобы найти градиент, мы должны представить это выражение в виде $\langle ?, H\rangle$, что в случае матриц переписывается, как мы уже хорошо знаем, в виде $\text{tr}(?^T\cdot H) = \text{tr}(?\cdot H^T)$. Воспользуемся тем, что под знаком следа можно транспонировать и переставлять множители по циклу:

$$\ldots=\text{tr}\left(AH^TX_0\right) + \text{tr}\left(AX_0^TH\right) =
$$

$$=\text{tr}\left(X_0AH^T\right) + \text{tr}\left(H^TX_0A^T\right) =
$$

$$=\text{tr}\left(X_0AH^T\right) + \text{tr}\left(X_0A^TH^T\right) =
$$

$$=\text{tr}\left((X_0A + X_0A^T)H^T\right)
$$

Стало быть,

$$\color{#FFC100}{\nabla_{X_0} f} = X_0A + X_0A^T
$$

{% endcut %}

* Вычислим градиент функции $f(X) = \text{det}\left(AX^{-1}B\right)$.

{% cut "Подумайте, почему мы не можем расписать определитель в виде произведения определителей" %}

Расписать у нас может не получиться из-за того, что $A$ и $B$ могут быть не квадратными, и тогда у них нет определителей и представить исходный определитель в виде произведения невозможно.

Воспользуемся правилом дифференцирования сложной функции для $u(Y) = \text{det}(Y)$, $v(X) = AX^{-1}B$. А для этого сначала вспомним, какие дифференциалы у них самих. С функцией $u$ всё просто:

$$\left[D_{Y_0} u\right](S) = \langle \text{det}(Y_0)Y_0^{-T}, S\rangle =
$$

$$= \text{tr}\left(\text{det}(Y_0)Y_0^{-1}S\right)
$$

Функция $v$ сама является сложной, но, к счастью, множители $A$ и $B$ выносятся из-под знака дифференциала, а дифференцировать обратную матрицу мы уже умеем:

$$\left[D_{X_0} v\right](H) = - AX_0^{-1}HX_0^{-1} B
$$

С учётом этого получаем:

$$\left[D_{X_0} f \right](H) = \left[D_{v(X_0)} u \right] \left( \left[D_{X_0} v\right] (H)\right) =
$$

$$=\text{tr}\left(\text{det}(AX_0^{-1}B)(AX_0^{-1}B)^{-1}\left(- AX_0^{-1}HX_0^{-1} B\right)\right)
$$

$$=\text{tr}\left(-\text{det}(AX_0^{-1}B)(AX_0^{-1}B)^{-1}AX_0^{-1}HX_0^{-1} B\right)
$$

Чтобы найти градиент, мы должны, как обычно, представить это выражение в виде $\text{tr}(?^T\cdot H)$.

$$\ldots=\text{tr}\left(-\text{det}(AX_0^{-1}B)X_0^{-1} B(AX_0^{-1}B)^{-1}AX_0^{-1}H\right)
$$

Стало быть,

$${\nabla_{X_0} f} = \left(-\text{det}(AX_0^{-1}B)X_0^{-1} B(AX_0^{-1}B)^{-1}AX_0^{-1}\right)^T =
$$

$$=-\text{det}(AX_0^{-1}B)X_0^{-T} A^T(AX_0^{-1}B)^{-T}B^TX_0^{-T}
$$

{% endcut %}

## Вторая производная

Рассмотрим теперь не первые два, а первые три члена ряда Тейлора:

$$f(x_0 + h) = f(x_0) + \color{#348FEA}{\left[D_{x_0} f \right]} (h) + \frac12\color{#4CB9C0}{\left[D_{x_0}^2 f \right]} (h, h) + \bar{\bar{o}} \left(\left|\left| h\right|\right|^2\right),
$$

где $\color{#4CB9C0}{\left[D_{x_0}^2 f \right]} (h, h)$ — второй дифференциал, квадратичная форма, в которую мы объединили все члены второй степени.

**Вопрос на подумать.** Докажите, что второй дифференциал является дифференциалом первого, то есть

$$\left[D_{x_0} \color{#348FEA}{\left[D_{x_0} f \right]} (h_1) \right] (h_2) = \left[D_{x_0}^2 f \right] (h_1, h_2)
$$

Зависит ли выражение справа от порядка $h_1$ и $h_2$?

Этот факт позволяет вычислять второй дифференциал не с помощью приращений, а повторным дифференцированием производной.

Вторая производная может оказаться полезной при реализации методов второго порядка или же для проверки того, является ли критическая точка (то есть точка, в которой градиент обращается в ноль) точкой минимума или точкой максимума. Напомним, что квадратичная форма $q(h)$ называется положительно определённой (соответственно, отрицательно определённой), если $q(h) \geqslant 0$ (соответственно, $q(h) \leqslant 0$) для всех $h$, причём $q(h) = 0$ только при $h = 0$.

**Теорема.** Пусть функция $f:\mathbb{R}^m\rightarrow\mathbb{R}$ имеет непрерывные частные производные второго порядка $\frac{\partial^2 f}{\partial x_i\partial x_j}$ в окрестности точки $x_0$, причём $\color{#FFC100}{\nabla_{x_0} f} = 0$. Тогда точка $x_0$ является точкой минимума функции, если квадратичная форма $\color{#348FEA}{D_{x_0}^2 f}$ положительно определена, и точкой максимума, если она отрицательно определена.

Если мы смогли записать матрицу квадратичной формы второго дифференциала, то мы можем проверить её на положительную или отрицательную определённость с помощью [критерия Сильвестра](https://ru.wikipedia.org/wiki/%D0%9A%D1%80%D0%B8%D1%82%D0%B5%D1%80%D0%B8%D0%B9_%D0%A1%D0%B8%D0%BB%D1%8C%D0%B2%D0%B5%D1%81%D1%82%D1%80%D0%B0).

## Примеры вычисления и использования второй производной

* Рассмотрим задачу минимизации $f(x) = \vert\vert Ax - b\vert\vert^2$ по переменной $x$, где $A$ — матрица с линейно независимыми столбцами. Выше мы уже нашли градиент этой функции; он был равен $\color{#FFC100}{\nabla_{x_0} f} = 2A^T(Ax - b)$. Мы можем заподозрить, что минимум достигается в точке, где градиент обращается в ноль: $x_* = (A^TA)^{-1}A^Tb$. Отметим, что обратная матрица существует, так как $\text{rk}(A^TA) = \text{rk}{A}$, а столбцы $A$ по условию линейно независимы и, следовательно, $\text{rk}(A^TA)$ равен размеру этой матрицы. Но действительно ли эта точка является точкой минимума? Давайте оставим в стороне другие соображения (например, геометрические, о которых мы упомянем в параграфе про [линейные модели](https://education.yandex.ru/handbook/ml/article/linear-models)) и проверим аналитически. Для этого мы должны вычислить второй дифференциал функции $f(x) = \vert\vert Ax - b\vert\vert^2$.

{% cut "Попробуйте вычислить сами, прежде чем смотреть решение." %}

Вспомним, что

$$\color{#348FEA}{\big[D_{x_0} \vert\vert Ax - b\vert\vert^2\big]}(h_1) = \langle 2A^T(Ax_0 - b), h_1\rangle
$$

Продифференцируем снова. Скалярное произведение — это линейная функция, поэтому можно занести дифференцирование внутрь:

$$\color{#348FEA}{\big[D_{x_0} \langle 2A^T(Ax - b), h_1\rangle\big]}(h_2) = 
\langle \color{#348FEA}{\big[D_{x_0} (2A^TAx - 2A^Tb)\big]}(h_2), h_1\rangle =$$

$$=\langle 2A^TAh_2, h_1\rangle = 2h_2^T A^TA h_1
$$

{% endcut %}

Мы нашли квадратичную форму второго дифференциала; она, оказывается, не зависит от точки (впрочем, логично: исходная функция была второй степени по $x$, так что вторая производная должна быть константой). Чтобы показать, что $x_*$ действительно является точкой минимума, достаточно проверить, что эта квадратичная форма положительно определена.

{% cut "Попробуйте сделать это сами, прежде чем смотреть решение." %}

Хорошо знакомый с линейной алгеброй читатель сразу скажет, что матрица $A^TA$ положительно определена для матрицы $A$ с линейно независимыми столбцами. Но всё же давайте докажем это явно. Имеем $h^TA^TAh = (Ah)^TAh = \vert\vert Ah\vert\vert^2 \geqslant 0$. Это выражение равно нулю тогда и только тогда, когда $Ah = 0$. Последнее является однородной системой уравнений на $h$, ранг которой равен числу переменных, так что она имеет лишь нулевое решение $h = 0$.

{% endcut %}

* Докажем, что функция $f(X) = \log{\text{det}(X)}$ является выпуклой вверх на множестве симметричных, положительно определённых матриц. Для этого мы должны проверить, что в любой точке квадратичная форма её дифференциала отрицательно определена. Для начала вычислим эту квадратичную форму.

{% cut "Попробуйте сделать это сами, прежде чем смотреть решение." %}

Выше мы уже нашли дифференциал этой функции:

$$\color{#348FEA}{\big[D_{X_0} \log{\text{det}(X)}\big]}(H_1) = \langle X_0^{-T}, H_1\rangle
$$

Продифференцируем снова:

$$\color{#348FEA}{\big[D_{X_0} \langle X^{-T}, H_1\rangle\big]}(H_2) = 
\langle \color{#348FEA}{\big[D_{x_0} X^{-T}\big]}(H_2), h_1\rangle =$$

$$=\langle -X_0^{-1}H_2X_0^{-1}, H_1\rangle
$$

{% endcut %}

Чтобы доказать требуемое в условии, мы должны проверить следующее: что для любой симметричной матрицы $X_0$ и для любого симметричного (чтобы не выйти из пространства симметричных матриц) приращения $H\ne 0$ имеем

$$\color{#348FEA}{\big[D^2_{X_0} \log{\text{det}(X)}\big]}(H, H) < 0
$$

Покажем это явно.
Так как $X_0$ — симметричная, положительно определённая матрица, у неё есть симметричный и положительно определённый квадратный корень: $X_0 = X_0^{1/2}\cdot X_0^{1/2} = X_0^{1/2}\cdot \left(X_0^{1/2}\right)^T.$ Тогда

$$\langle -X_0^{-1}HX_0^{-1}, H\rangle = -\text{tr}\left(X_0^{1/2} \left(X_0^{1/2}\right)^THX_0^{1/2} \left(X_0^{1/2}\right)^TH^T\right) =
$$

$$-\text{tr}\left(\left(X_0^{1/2}\right)^THX_0^{1/2} \left(X_0^{1/2}\right)^TH^TX_0^{1/2}\right) = 
$$

$$=-\text{tr}\left( \left(X_0^{1/2}\right)^THX_0^{1/2} \left[\left(X_0^{1/2}\right)^THX_0^{1/2}\right]^T\right) =
$$

$$=-\vert\vert\left(X_0^{1/2}\right)^THX_0^{1/2}\vert\vert^2,
$$

что, конечно, меньше нуля для любой ненулевой $H$.

  ## handbook

  Учебник по машинному обучению

  ## title

  Матричное дифференцирование

  ## description

  Как дифференцировать матрицы и дифференцировать по матрицам: всё, что вам не рассказали про дифференцирование на матанализе

- 
  ## path

  /handbook/ml/article/matrichnaya-faktorizaciya

  ## content

  Если наш датасет таков, что все признаки целочисленные и вещественные, нет пропущенных значений и других приятных сюрпризов, то матрица объекты-признаки — это просто матрица, к которой можно пробовать применять инструменты из линейной алгебры, а среди таковых весьма полезными оказываются матричные разложения, то есть различные способы представить матрицу в виде произведения двух или более матриц, обычно специального вида. Такие разновидности, как LU-разложение, QR-разложение, разложение Холецкого вы несомненно встретите, если откроете код любой библиотеки численной линейной алгебры, но суждено ли матричным разложениям играть роль только лишь винтиков и шестерёнок, запрятанных внутри инструментов машинного обучения, или какие-то из них и сами по себе могут помочь вам анализировать данные? На этот вопрос мы попробуем ответить в данном разделе. Но прежде, чем переходить к конкретным методам, мы разберёмся, к каким моделям данных можно прийти, разложив матрицу в произведение.

## Итак, я разложил матрицу в произведения — и что же?

Предположим, что нашу матрицу объекты-признаки $X$ мы представили в виде произведения (или, более общно, приблизили в каком-либо смысле таким произведением):

$$\underset{N\times D}{\operatorname{X}} \sim \underset{N\times R}{\operatorname{B}} \cdot \underset{R\times D}{\operatorname{C}}
$$

где внизу указаны размеры матриц (то есть в нашем датасете $N$ объектов и $D$ признаков). Что это может означать?

### Смесь признаков

Мы считаем, что каждый из $D$ признаков нашего исходного датасета — это смесь (то есть линейная комбинация) $R$ скрытых (латентных) признаков:

![Decomp1](https://yastatic.net/s3/education-portal/media/Decomp1_52765565e1_37afa1aa3e_f31c5aa293.svg)

По сути это одна из самых простых моделей с латентными переменными, в которой исходные признаки выражаются через латентные линейным образом. Если $R < D$, то мы получаем приближённое описание нашего датасета с помощью меньшего количества признаков. На уровне объектов каждый объект $x_i$ ($D$-мерная строка) приобретает <span style="color:blue">латентное представление</span> $z_i$ ($R$-мерная строка), с которой он связан соотношением $x_i = z_i\color{green}{C}$. Мы можем представлять, что наши объекты $x_i$ представляют из себя не $D$-мерное облако, а лежат на некоторой $R$-мерной плоскости; переходя к $R$-мерным представлениям $z_i$, мы обнажаем эту структуру.

Точность аппроксимации можно измерять по-разному; наиболее популярной (в силу вычислительной простоты) является норма Фробениуса $| A |*{fro}^2  = \sum\limits* A_{i j}^2 = \operatorname{tr}(A^TA)$  — соответствующую модель называют <span style="color:blue"> анализом главных компонент, или PCA (Principal Component Analysis)</span>.

### Понижение размерности признакового пространства

Мы можем захотеть описать наш датасет меньшим чем $D$ количеством признаков (а может быть, и вообще каким-то весьма маленьким). У нас может быть несколько причин для этого, например:

* Признаков очень много, и мы боимся, что обучение на них будет занимать очень много времени или что в процессе обучения нам потребуется слишком много оперативной памяти;

* Мы считаем, что в данных есть шум или что часть признаков связаны соотношением приближённой линейной зависимости — иными словами, мы уверены, что значительную часть информации можно закодировать меньшим числом признаков

Мы уже обсуждали, что это можно получить, построив приближённое разложение:

$$\underset{N\times D}{\operatorname{X}} \sim \underset{N\times T}{\operatorname{B}} \cdot \underset{T\times D}{\operatorname{C}}
$$

*Математика помогает*. Матрица имеет ранг $T$ тогда и только тогда, когда она представляется в виде
$\underset{Ntimes S}{\operatorname{B}}\cdot\underset{S\times D}{\operatorname{C}}$ для $S = T$
и не представляется в таком виде для меньших $S$.

Доказывать это мы не будем, но подметим, что <span style="color:blue"> приблизить датасет линейной смесью $T$ признаков — это то же самое, что приблизить матрицу $X$ матрицей $\hat{X}$ ранга $T$ </span>.

*Качество приближения.* Нам, конечно же, хочется, чтобы приближение было наилучшим — скажем, в том смысле, чтобы разность $X - BC$ была минимальной в каком-либо смысле. Можно предложить много разных метрик; остановимся на двух:

* <span style="color:blue"> Норма Фробениуса.</span>
  Представим, что матрица $A = (X - BC)$ — это просто вектор из $N\times D$ чисел, который зачем-то записали в виде прямоугольной таблицы. Тогда его норму можно записать в виде

  $$\|A\|_{fro} = \sqrt{\sum\limits_{i,j}a_{ij}^2} = \sqrt{\mathrm{tr}\left(A^TA\right)}
  $$
  
  Эту норму (а точнее, её квадрат) легко оптимизировать.

* <span style="color:blue">Операторная  $l_2$-норма.</span>
  Вычислять её тяжко, а уж оптимизировать вообще непонятно как, зато звучит круто. Идея в том, что отображения можно сравнивать в зависимости от того, как оно действует на векторы: чем больше оно умеет удлинять векторы — тем оно «больше»:

  $$\|A\|_2 = \sup \left\{ \frac{\vert Av \vert}{\vert v \vert} \mid v \in\mathbb{R}^D \right\}
  $$
  
  Поскольку $\frac{\vert A(\lambda v)\vert}{\vert \lambda v \vert} = \frac{\vert Av \vert}{ \vert v \vert}$, достаточно брать супремум только по векторам единичной длины, то есть по единичной сфере. Так как это компакт, непрерывная функция $v\mapsto \vert Av \vert$ достигает на нём своего максимального значения, то есть мы можем переписать

  $$\|A\|_2 = \sup \left\{ \vert Av \vert \mid v\in\mathbb{R}^D,\,\vert v \vert = 1 \right\}
  $$
  
  

#### Смесь объектов

Мы считаем, что каждый из $N$ объектов нашего исходного датасета — смесь (то есть линейная комбинация) $R$ скрытых объектов:

![Decomp2](https://yastatic.net/s3/education-portal/media/Decomp2_cc106ea2fc_080a85b20f_5989f3a2a7.svg)

Такая интерпретация может быть полезна, например, в ситуации, когда объекты — это записи с каждого из нескольких микрофонов в помещении, признаки — фреймы, а скрытые объекты — это голоса отдельных людей.

Также данную модель можно интерпретировать как что-то вроде поиска типичных объектов.

#### Отдельные представления для объектов и признаков

Эту интерпретацию лучше всего пояснить на примере. Пусть объекты нашего датасета соответствуют пользователям интернет-магазина, а признаки — товарам, причём в клетке с индексом $(i,j)$ записана единица, если пользователь интересовался товаром, и ноль — если нет (или, в более общей ситуации, рейтинги, которые пользователи ставят товарам).

![Decomp3](https://yastatic.net/s3/education-portal/media/Decomp3_888ac5c12c_68549dfd4d_6d674d650e.svg)

При перемножении матриц $B$ и $C$ на $(i, j)$-м месте произведении стоит скалярное произведение $i$-й строки $B$ и $j$-го столбца $C$. Таким образом, степень релевантности товара пользователю моделируется скалярным произведением (напрашивается сравнение с косинусным расстоянием) вектора, представляющего $i$-го пользователя, и вектора, представляющего $j$-й товар.

$$\begin{array}{l}
X_{\color{red}{i} \color{green}{j}}=\color{red}{b_{i 1}} \color{green}{c_{1 j}} + \ldots + \color{red}{b_{i k}} \color{green}{c_{k j}}= \\
= ( \color{red}{\text{i-ый пользователь}}, \color{green}{ \text{j-ый товар} })
\end{array}
$$

Заметим ещё, что $R$ координат вектора, ответчающего пользователю, равно как и $R$ координат вектора, отвечающего товару, можно рассматривать как $R$ латентных признаков, которые в идеальном мире являются интерпретируемыми и характеризуют «сродство» пользователя и товара с некоторым аспектом бытия:

![Decomp6](https://yastatic.net/s3/education-portal/media/Decomp6_8753618149_1ee134fd6f_8446a2b0ef.svg)

### Матрицы в разложении: физический смысл

Но матрицы в разложении обычно не абы какие — так какие из разновидностей могут быть полезны?

Во всех известных вам матричных разложениях к отдельным сомножителям предъявляются определённые требования: симметричность, треугольность, ортогональность — некоторые из них (скажем, симметричность) не имеют физического смысла ни в одной из указанных выше интерпретаций. Но одно оказывается полезным.

### Ковариация и дисперсия признаков

Для начала — и это важно — <span style="color:blue"> предположим, что матрица $X$ центрирована по столбцам</span>, то есть среднее в каждом из столбцов (= признаков) равно нулю (если это не так, то вычтем из каждого столбца его среднее).

Теперь матрица ковариации признаков может быть с точностью до константы оценена как $X^TX$:

![matrix](https://yastatic.net/s3/education-portal/media/matrix_factorization_noname_1_634f8852a9_8bb69c3ed6.svg)

И мы видим: <span style="color:blue">$i$-й и $j$-й столбцы матрицы $X$ ортогональны тогда и только тогда, когда соответствующие признаки не коррелированы.</span>

При этом <span style="color:blue">$(i,i)$-й диагональный элемент матрицы $X^TX$ — это дисперсия $i$-го признака.</span>

**Вывод: матрица, ортонормированная по столбцам, отвечает датасету, в котором признаки не коррелированы и имеют единичную дисперсию**

## Сингулярное разложение

С помощью сингулярного разложения можно перейти от $D$ исходных признаков к потенциально небольшому количеству «самых важных» , по-быстрому визуализовать данные или построить простенькую рекомендательную систему. Конечно, глубинные автоэнкодеры, TSNE или DSSM справятся с этим гораздо лучше, но если данных относительно немного или если хочется что-нибудь быстро попробовать «на коленке», старое доброе сингулярное разложение всегда подставит плечо.

### Математическое определение

<span style="color:blue">Сингулярным разложением</span> матрицы $X$ называется разложение

$$X = \color{orange}{U} \color{green}{\Sigma}\color{magenta}{V^T},
$$

где $\color{orange}{U}$ и $\color{magenta}{V}$ — матрицы, ортонормированные по столбцам, а $\color{green}{\Sigma} = \mathrm{diag}(\color{green}{\sigma_1},\color{green}{\sigma_2},\ldots)$ — диагональная матрица, у которой $\color{green}{\sigma_1} \geqslant \color{green}{\sigma_2} \geqslant \ldots \geqslant \color{green}{\sigma_R}> \color{green}{\sigma_{R+1}}=0$.

Числа $\color{green}{\sigma_i}$ называются $\color{green}{\text{сингулярными числами}}$, а столбцы $\color{orange}{U}$ и $\color{magenta}{V}$ — $\color{orange}{\text{левыми}}$  и $\color{magenta}{\text{правыми}}$ сингулярными векторами соответственно (их алгебраический смысл станет ясен чуть ниже).

Сингулярное разложение можно записать в полном или в усечённом виде:

![SVD](https://yastatic.net/s3/education-portal/media/SVD_1_d3b8d5a626_27ab400e29_482e8f4a97.svg)

Пара предостережений по поводу ортогональности по столбцам:

$U$ ортогональна по столбцам

$U^TU=E$ (элементы $U^TU$ — скалярные произведения столбцов $U$)

но $UU^T \neq E$ (не обязательно равно; элементы $UU^T$  — скалярные произведения строк $U$)

Ясно, что хранить полное разложение нет смысла: ведь бесполезные, умножающиеся на нули, блоки будут лишь занимать память.

По-английски сингулярное разложение называется SVD (singular value decomposition), и мы будем активно использовать эту аббревиатуру.

*Если вы не любите математику, можете пропустить*. С точки зрения математики сингулярное разложение говорит следующее. Пусть $X$ — матрица линейного отображения $\varphi:\mathbb{R}^D\longrightarrow\mathbb{R}^N$.

Тогда найдётся ортонормированый базис $\color{magenta}{v_1},\dots,\color{magenta}{v_D}$ в пространстве $\mathbb{R}^D$ и ортонормированый базис $\color{orange}{u_1},\dots,\color{orange}{u_N}$ в пространстве $\mathbb{R}^N$, в которых действие оператора записывается следующим образом:

$$\begin{align*}
\varphi(\color{magenta}{v_1}) &= \color{green}{\sigma_1}\color{orange}{u_1},\\
\vdots\\
\varphi(\color{magenta}{v_R}) &= \color{green}{\sigma_r}\color{orange}{u_R},\\
\varphi(\color{magenta}{v_{R+1}}) &= 0,\\
\vdots\\
\varphi(\color{magenta}{v_D}) &= 0
\end{align*}$$

(знатоки функционального анализа могут узнать в этом частный случай теоремы Гильберта-Шмидта).

*Сингулярное разложение и операторная l2-норма*.
Можно показать, что $\|\|A\|\|_2$, эта самая операторная l2-норма матрицы, равна $\sigma_1^2$ — квадрату наибольшего сингулярного числа.

*Сингулярное разложение и норма Фробениуса*. Можно показать, что

$$||A||_{fro} = \sqrt{\sum_i\sigma_i^2}
$$

{% cut "Контрольный вопрос." %}

Единственно ли сингулярное разложение матрицы? Давайте сразу считать, что речь об усечённом разложении.

Есть очень простой источник неоднозначности: если $\varphi(\color{magenta}{v_i}) = \color{green}{\sigma_1}\color{orange}{u_i}$, то и $\varphi(\color{magenta}{-v_i}) = \color{green}{\sigma_1}\color{orange}{(-u_i)}$; при этом умножение вектора на $(-1)$ не попортит ортонормированности базиса. Иными словами, мы можем одновременно поменять знаки $i$-х столбцов матриц $\color{orange}{U}$ и $\color{magenta}{V}$ (без транспонирования!) без ущерба для разложения.

{% endcut %}

Есть и более тонкие, хотя и весьма частные, ситуации. Можете ли вы, например, указать несколько различных сингулярных разложений матрицы $E$? Да-да, для неё сингулярное разложение максимально неоднозначно. Можете ли вы теперь придумать не скалярную матрицу, у которой были бы различные SVD, отличающиеся не только знаками столбцов матриц $\color{orange}{U}$ и $\color{magenta}{V}$?

### Теоретико-вероятностная интерпретация SVD

Если $X =\color{orange}{U}\color{green}{\Sigma} \color{magenta}{V^T}$ (рассмотрим сейчас не усечённое, а полное разложение, в котором матрицы $\color{orange}{U}$ и $\color{magenta}{V}$ квадратные ортогональные), то

$$X^TX = \color{magenta}{V}\color{green}{\Sigma^T}\underbrace{\color{orange}{U^T}\cdot \color{orange}{U}}_{=E}\color{green}{\Sigma} \color{magenta}{V^T} =
\color{magenta}{V}\color{green}{\Sigma}^T\color{green}{\Sigma} \color{magenta}{V^T}$$

Отметим, что в рассматриваемой ситуации $\color{green}{\Sigma}$ не обязательно квадратная, и поэтому нельзя написать, что $\color{green}{\Sigma}^T\color{green}{\Sigma}=\color{green}{\Sigma}^2$; тем не менее, $\color{green}{\Sigma}^T\color{green}{\Sigma}$ — это квадратная матрица с числами $\color{green}{\sigma_1}^2,\color{green}{\sigma_2}^2,\ldots$ на диагонали.

{% cut "Контрольный вопрос" %}

Точно так же мы можем вычислить $XX^T = \color{orange}{U}^T\color{green}{\Sigma}\color{green}{\Sigma}^T\color{orange}{U}$, где опять-таки $\color{green}{\Sigma}\color{green}{\Sigma}^T$ — квадратная матрица с числами $\color{green}{\sigma_1}^2,\color{green}{\sigma_2}^2,\ldots$ на диагонали. И всё бы хорошо, но если $\color{green}{\Sigma}$ не квадратная, матрицы $\color{green}{\Sigma}^T\color{green}{\Sigma}$ и $\color{green}{\Sigma}\color{green}{\Sigma}^T$ имеют разные размеры. Так в чём же подвох?

{% endcut %}

Как бы то ни было, в (ортогональном!) базисе из (ортогональных!) столбцов $\color{magenta}{V}$ матрица $X^TX$ приводится к диагональному виду с числами $\color{green}{\sigma_1}^2,\color{green}{\sigma_2}^2,\ldots$ на диагонали.

Теперь представим, что наши объекты $x_1,x_2,\ldots,x_N$ выбраны из $D$-мерного нормального распределения

$$p(x_i) = \frac{1}{(2\pi)^{D/2}|C|^{1/2}}e^{-\frac12(x_i - \mu)C^{-1}(x_i - \mu)^T}
$$

где $\mu$ — вектор средних, а $C$ — матрица ковариации. Это, в частности, значит, что облако точек представляет из себя нечто вроде эллипсоида в $D$-мерном пространстве с центром $\mu$.

Предположим, что $\mu = 0$ (все признаки центрированы); тогда оценкой матрицы ковариации признаков является матрица $\frac1n X^TX$. Допустим, что эта оценка точная, тогда разложение $X^TX = \color{magenta}{V}\color{green}{\Sigma}^T\color{green}{\Sigma} \color{magenta}{V}^T$ даёт нам аналогичное разложение $C = \color{magenta}{V}(\frac1n\color{green}{\Sigma}^T\color{green}{\Sigma}) \color{magenta}{V}^T$. Теперь замена координат $x = z\color{magenta}{V}^T$ (с матрицей замены $\color{magenta}{V}$ — то есть переход происходит в базис из столбцов матрицы $\color{magenta}{V}$) даёт нам

$$p(x_i) = \mathrm{const}\cdot \exp\left(-\frac12\cdot\color{red}{x_i}\cdot \color{blue}{C^{-1}}\cdot \color{grey}{x_i^T}\right)
$$

Обратите внимание, что $x_i$ и $x_i^T$ стоят в формуле на непривычных местах, как будто их перепутали, но нет — просто $x_i$ у нас является строкой, а не столбцом.

$$p(z) = \mathrm{const}\cdot \exp\left(-\frac12\cdot\color{red}{z_i}\underbrace{\color{red}{V^T}\cdot \color{blue}{V}}_{=E}\color{blue}{(n\Sigma^{-1}\Sigma^{-T})}\underbrace{\color{blue}{V^T}\cdot \color{cyan}{V}}_{=E}\color{grey}{z_i^T}\right)=
$$

$$= \mathrm{const}\cdot \exp\left(-\frac{n}2z_i\Sigma^{-1}\Sigma^{-T}z_i^T\right)=\mathrm{const}\cdot \exp\left(-\frac{n}2\left(\frac1{\sigma_1^2}z_{i1}^2 + \frac1{\sigma_2^2}z_{i2}^2 + \ldots\right)\right)=
$$

$$=p(x'_{i1})\cdot\ldots\cdot p(x'_{iD})
$$

Итак, если наши данные взяты из многомерного нормального распределения, после перехода к базису из столбцов $\color{magenta}{V}$ новые координаты становятся независимыми; вместе с тем это соответствует переходу к главным осям ковариационной матрицы — и геометрически столбцы $\color{magenta}{V}$ соответствуют главным осям эллипсоида-облака точек.

![SVD](https://yastatic.net/s3/education-portal/media/SVD_3_91ed0c9da5_4c32c517f4.webp)

## Использование SVD: латентные признаки

Запишем

$$\underset{N\times D}{\operatorname{X}} = \underset{N\times R}{\operatorname{U\Sigma}} \cdot \underset{R\times D}{\operatorname{V^T}}
$$

и вспомним самую первую интерпретацию матричного разложения.

![matrix](https://yastatic.net/s3/education-portal/media/matrix_factorization_noname_2_435aadc098_8bfc9ce626.svg)
Столбцы $U\Sigma$ ортогональны (так как они пропорциональны столбцам ортогональной по столбцам матрицы $U$) — то есть <span style="color:blue"> латентные признаки не коррелированы</span>. При этом, поскольку длина каждого из столбцов $U$ равна 1, длины столбцов $U\Sigma$ порпорциональны $\sigma_i$ — а, значит, <span style="color:blue"> латентные признаки упорядочены по невозрастанию дисперсии</span> (ведь с точностью до константы оценка дисперсии признака — это квадрат длины вектора его значений).

Заметим, что перед применением SVD признаки лучше центрировать, иначе первая компонента будет указывать в сторону центра масс облака точек (зачем нам это?), а остальные вынуждены будут ей быть ортогональны:

![SVD](https://yastatic.net/s3/education-portal/media/SVD_5_caa8d0689a_a3e3d940da.webp)

### Понижение размерности признакового пространства

Мы уже обсуждали, что это можно получить, построив приближение ранга $T$ или, что то же самое, приближённое разложение

$$\underset{N\times D}{\operatorname{X}} \sim \underset{N\times T}{\operatorname{B}} \cdot \underset{T\times D}{\operatorname{C}}
$$

для некоторого и желательно небольшого $T$. И тут SVD приходится более чем кстати.

#### Теорема Эккарта-Янга

Наилучшее по норме Фробениуса приближение ранга $T$ — это

![SVD](https://yastatic.net/s3/education-portal/media/SVD_6_3cb442dda7_81bc4687d4_b48107a292.svg)

Таким образом, если вы хотите получить $T$ «самых важных» признаков, то вы можете использовать SVD. Но что это за признаки? Что именно означают эти слова «самые важные»? Давайте обратимся к геометрии, которая, как мы помним, тесно связана с теорией вероятностей:

![SVD](https://yastatic.net/s3/education-portal/media/SVD_7_9a0defd674_0d28b2c7a4_1dd9cee298.svg)

![SVD](https://yastatic.net/s3/education-portal/media/SVD_8_291d9eb393_c2ddd38303.webp)

Если применить SVD к датасету, изображённому на последней картинке, и взять два первых латентных признака, то эллипсоид превратится в эллипс; меньшая из полуосей, похожая на шум, будет забыта, останется две бOльших. Видим: самое важное для SVD — это самое масштабное.

*А правда ли у нас получится хорошее приближение с помощью* $T$ новых признаков? Посчитаем норму разности. Везде ниже $\color{orange}{U}$ и $\color{magenta}{V}$ — квадратные ортогональные матрицы; в частности $\color{green}{\Sigma}$ не обязательно квадратная матрица размера $N\times D$.

![SVD](https://yastatic.net/s3/education-portal/media/SVD_9_6a81edb703_564c7cf3ee_d6bbff0b2f.svg)

$$||\Delta||_{fro}^2 = \mathrm{tr}\left((\color{orange}{U}\color{green}{\widetilde{\Sigma}}\color{magenta}{V}^T)^T\color{orange}{U}\color{green}{\widetilde{\Sigma}}\color{magenta}{V}^T\right) = 
\mathrm{tr}\left(\color{magenta}{V}\color{green}{\widetilde{\Sigma}}^T\underbrace{\color{orange}{U}^T\color{orange}{U}}_{=E}\color{green}{\widetilde{\Sigma}}\color{magenta}{V}^T\right) =$$

$$=\mathrm{tr}\left(\color{magenta}{V}\color{green}{\widetilde{\Sigma}}^T\color{green}{\widetilde{\Sigma}}\color{magenta}{V}^T\right) = \mathrm{tr}\left(\color{green}{\widetilde{\Sigma}}^T\color{green}{\widetilde{\Sigma}}\underbrace{\color{magenta}{V}^T\color{magenta}{V}}_{=E}\right) = ||\color{green}{\widetilde{\Sigma}}||^2_{fro} = \color{green}{\sigma_{T+1}}^2 + \ldots + \color{green}{\sigma_{R}}^2
$$

Аналогичным образом

$$||\Delta||_2 = \color{green}{\sigma_{T+1}}
$$

потому что умножение на ортогональную матрицу не меняет операторную $l_2$-норму. Таким образом, если сингулярные значения убывают достаточно медленно (например, линейно), то мы вряд ли сможем приблизить исходную матрицу матрицей маленького ранга с очень хорошей точностью.

*Как избавиться от иллюзий*.
Сгенерируйте матрицу $100\times100$ с помощью *np.random.rand* или *np.random.randn*. Для какого $T$ вы сможете найти матрицу ранга $T$, приближающую исходную с относительной точностью $10^{-6}$?

К счастью, в реальных датасетах сингулярные значения убывают достаточно быстро или же нам хватает довольно грубого приближения.

#### Переход из исходного признакового пространства в новое и обратно

Допустим, мы построили приближённое разложение ранга $T$:

$$ X\sim\color{orange}{\widehat{U}}\color{green}{\widehat{\Sigma}}\color{magenta}{\widehat{V}}^T =
\color{orange}{U}\cdot\underbrace{\color{green}{\begin{pmatrix}
\sigma_1 &\\
 & \ddots & \\
 & & \sigma_T &\\
& & & 0 & \\
& & & & \ddots\end{pmatrix}}}_{=:\color{green}{\Sigma'}}\cdot\color{magenta}{V}^T$$

Матрица $\color{orange}{\widehat{U}}\color{green}{\widehat{\Sigma}}$ — это первые $T$ столбцов матрицы $\color{orange}{U}\color{green}{\Sigma'}$, и они же первые $T$ столбцов матрицы $\color{orange}{U}\color{green}{\Sigma} = X\cdot\color{magenta}{V}$. Таким образом, <span style="color:blue">для перевода объекта $x_i$ в новое признаковое пространство нужно произвести $x_i\mapsto x_i\cdot\color{magenta}{V}$ и взять первые $T$ столбцов или, что то же самое, $x_i\mapsto x_i\cdot\color{magenta}{V[:,:T]}$.</span>

Теперь пусть задана вектор-строка $z_i$ длины $T$ — латентное представление, соответствующего некоторому объекту, то есть одна из строк матрицы $\color{orange}{\widehat{U}}\color{green}{\widehat{\Sigma}}$. Тогда точно восстановить исходный $x_i$ мы не сможем: ведь равенство $X\sim\color{orange}{\widehat{U}}\color{green}{\widehat{\Sigma}}\color{magenta}{\widehat{V}}^T$ не точное, но <span style="color:blue">для приближённого восстановления $x_i$ мы должны произвести $z_i\mapsto z_i\cdot \color{magenta}{\widehat{V}}^T = z_i\cdot\color{magenta}{V[:,:T]}^T$. </span>

## На что не способно сингулярное разложение

Сингулярное разложение умеет находить дающие самый существенный вклад в дисперсию линейные комбинации признаков, притом некоррелированные; в случае нормально распределённых данных эти направления оказываются главными осями эллипсоида, которым является облако данных. К сожалению, эта суперспособность SVD столь же охотно превращается в слабость, ведь:

* Данные не всегда распределены нормально, они могут обладать сложной геометрией, но SVD будет упрямо искать эллипсоид.
* Самое важное не всегда самое масштабное. Забыть привести признаки к одному масштабу — хороший способ выстрелить себе в ногу при работе с сингулярным разложением.
* Новые признаки не обязаны быть хорошо интерпретируемыми. Линейная комбинация возраста, стажа работы и зарплаты — это не то, что хотелось бы показывать банковскому регулятору.
* Выбросы почти наверняка усложнят вам жизнь, хотя, возможно, SVD поможет вам их увидеть.

## Практические кейсы

### MNIST и путешествие по латентному пространству

Возьмём большой датасет MNIST, состоящий из чёрно-белых изображений рукописных цифр размера $28\times28$ пикселей (его можно загрузить, к примеру, [отсюда](https://ru-keras.com/datasets)), вытянем каждое из изображений в вектор, получив тем самым матрицу размера $60000\times(28\cdot28)$, и применим к этой матрице SVD. Теперь возьмём первые два латентных признака (то есть первые два столбца матрицы $U\Sigma$) — получается, что каждая рукописная цифра у нас теперь кодируется вектором из двух чисел. Нарисуем на плоскости точки, соответствующие этим векторам (скажем, по 100 из каждого класса, чтобы хоть что-нибудь было понятно):

![MNIST](https://yastatic.net/s3/education-portal/media/MNIST_SVD_latent_space_bad_d03acdc84a_eb4ea55797.webp)

Что же мы видим? Единицы и нули оказались особенными, то есть уже первые два латентных признака хорошо их различают, правда, с середине какая-то каша. А почему? Да потому, что мы забыли центрировать данные. Давайте перед применением SVD вычтем из каждого признака (то есть из каждого пикселя) его среднее по всем картинкам, а потом нарисуем всё заново:

![MNIST](https://yastatic.net/s3/education-portal/media/MNIST_SVD_latent_space_2468ef3c4f_db1bd7b105.webp)

Теперь стало получше: например, семёрки, девятки и четвёрки сгуппировались вместе с другой стороны от восьмёрок и троек (собственно говоря, это отражает тот факт, что рукописные написания семёрок, девяток и четвёрок могут быть похожи друг на друга, так и человек не сразу отличит — а вот с тройкой их спутать намного труднее).

Заметим ещё вот что. В $(28\cdot28)$-мерном пространстве наборов пикселей совсем не каждая точка соответствует какой-то рукописной цифре — то, что может приходить из реального мира, лежит на некоторой хитрой поверхности в этом пространстве (если выражаться корректнее, то на подмногообразии). Если же мы попробуем нарисовать «изображения», лежащие на отрезке, соединяющем два изображения цифр, то получим нечто не слишком интересное:

![mnist](https://yastatic.net/s3/education-portal/media/mnist_movie_svd_4b36164193_e5db6ad932.gif)

Одно изображение просто наложилось и затем сменило другое — скучно! Но если мы сделаем то же самое в двумерном пространстве, образованном первыми двумя латентными признаками SVD, то мы будем получать, может быть, не совсем реалистичные изображения цифр, но что-то явно из мира рукописных символов:

![mnist](https://yastatic.net/s3/education-portal/media/mnist_movie_svd1_6b42a304ae_fa63250ec5.gif)

{% cut "Контрольный вопрос" %}

Если $x_1$ — вектор-строка длины $28\cdot28$, отвечающая первой картинке, $x_2$ — второй, а $U$, $\Sigma$, $V^T$ — матрицы из сингулярного разложения, то как получить картинку, соответствующую середине отрезка, соединяющего в пространстве двух первых латентных признаков SVD точки $x_1'$ и $x_2'$, отвечающие этим картинкам?

{% endcut %}

### Химический состав рек

Посмотрим на небольшой кусок вот [этого датасета](https://data.europa.eu/euodp/en/data/dataset/data_waterbase-rivers-10), который доступен для скачивания нигде (ха-ха), и попробуем что-нибудь понять про химических состав рек европейского союза, а заодно соберём шишки, которые могут попасться при визуализации с помощью SVD.

Конечно, сразу хочется нарисовать все объекты датасета в виде точек на плоскости. Мы знаем, что в этом может помочь SVD — попробуем же! Центрируем признаки — и рисуем:

![Rivers](https://yastatic.net/s3/education-portal/media/Rivers_bad_SVD_af94fc5e75_b268a69313.webp)

Ой, что-то пошло не так. Но почему же?! Наверное, надо хотя бы посмотреть на данные...

* Объекты имеют вид «GBPKER0059», «GB20227», «LVV0120100» и так далее — это коды станций, измеряющих состав воды;
* Признаки имеют вид «1985 BOD5», «1985 Chlorophyll a», «1985 Orthophosphates» и так далее — тут указан год измерения и показатель;
* Посмотрев статистики, убеждаемся, что все показатели неотрицательны (то есть уж точно распределены не нормально — но может, и так сработает); при этом почти все элементы нашей матрицы находятся в пределах 1000, но три значения космически огромны, причём в одном столбце «2008 Total oxidised nitrogen» (а строки соответствуют каким-то греческим станциям, с которыми вообще всё странно), и ещё одно тоже очень большое («2005 Total organic carbon (TOC)») — вот они-то и дали нам четыре точки на графике, отличных от начала координат.
  Кстати говоря, если космически большие значения, по-видимому, являются результатам поломки, то по поводу четвёртого, не столь злостного, выброса есть подозрение, что это реальные значения. Посмотрев в данные, мы видим, что показатель был измерен на станции Zidlochovice, на реке Srvatka ниже Брно — а, как говорит нам википедия: *As a result of water pollution by communal sewage, the reservoir suffered from an extensive amount of cyanobacteria for a long time*. Так или иначе, все четыре станции мы уберём, чтобы они не портили нам SVD.
* Один из признаков «2002 Kjeldahl Nitrogen» принимает только нулевые значения. Уберём его, чтобы не мешался.

Почистив выбросы в исходных данных, опять центрируем и рисуем:

![Rivers](https://yastatic.net/s3/education-portal/media/Rivers_SVD_1_b7a6bf8940_2a8f3bbdbd.webp)

Уже лучше. Попробуем понять, что за вещества внесли вклад в первые два латентных признака. Как это сделать? Латентные признаки — это столбцы матрицы $U\Sigma = XV$; линейная алгебра говорит нам, что $i$-й столбец произведения $XV$ — это линейная комбинация столбцов $X$ с коэффициентами из $i$-го столбца $V$. Находим номера самых больших по модулю координат $V$ — и оказывается, что первые два латентных признака складываются почти сплошь из насыщения воды кислородом, только за разные годы (первый за более старые, второй за чуть более свежие):

**First latent feature**

#|
||

Признак

|

элемент $V$

||
||

2001 Oxygen saturation

|

0\.27

||
||

2002 Oxygen saturation

|

0\.27

||
||

2000 Oxygen saturation

|

0\.26

||
||

2004 Oxygen saturation

|

0\.26

||
||

$\vdots$

|

$\vdots$

||
|#

**Second latent feature**

#|
||

Признак

|

элемент $V$

||
||

2001 Oxygen saturation

|

\-0.62

||
||

2002 Oxygen saturation

|

\-0.49

||
||

2000 Oxygen saturation

|

\-0.45

||
||

2004 Oxygen saturation

|

\-0.26

||
||

$\vdots$

|

$\vdots$

||
|#

Неужели насыщение кислородом действительно так важно? Нет, просто мы не отмасштабировали признаки. Оказывается, что насыщение кислородом имеет на порядок больший масштаб, чем многое другое, и потому забивает все остальные признаки. Тем не менее, мы можем попробовать сделать вывод и из имеющейся картинки. По оси "у" что-то не очень интересное, а по оси "х" видим большой кластер (напомним, это меньшие значения насыщения воды кислородом в начале 2000-х), содержащий, если проверить, примерно три четверти всех точек, и ещё некоторой размазанный шлейф.

Итак, на многих станциях насыщение воды кислородом в начале 2000-х было примерно в одинаковой степени мало — проверив глазами, обнаруживаем, что там просто нули. Поскольку вряд ли это так на самом деле, видимо, стоит сделать вывод, что в первой половине 2000-х насыщение кислородом измерялось из рук вон плохо.

Теперь вдобавок к центрированию поделим каждый признак на его стандартное отклонение и снова нарисуем:

![Rivers](https://yastatic.net/s3/education-portal/media/Rivers_SVD_2_d0e42b323f_fa2bedcc09.webp)

Опять видим тесный кластер. При этом первый латентный признак складывается в основном из «Nitrate» , «pH»  и «Dissolved oxygen» за разные годы, все с положительными коэффициентами, а второй — из «Total ammonium», «Total phosphorus» и «Kjeldahl Nitrogen» за разные годы, причём с отрицательными коэффициентами. В частности, справа у нас точки с высоким содержанием нитратов и высокой кислотностью. Среди этих точек:

* [Река Тейм](https://en.wikipedia.org/wiki/River_Tame,_West_Midlands), про которую Википедия пишет: *The Tame was once one of Britain's dirtiest rivers.*
* Река Кёрёш, про которую тоже можно найти вот такую информацию: *For some time the municipal government of Kanjiža (to which the mouth of the river belongs) protests about the extreme pollution of the Kereš's water, as it represents the single largest polluter of the Tisa river*
* Темза (станция немного выше Лондона).

Что ещё можно было бы сделать? Например, мы можем посмотреть распределения признаков и увидеть, что многие из них далеки от нормальных и в целом выиграли бы от логарифмирования — тогда, возможно, итоговая картинка стала бы красноречивей.

## Использование SVD: разделённые представления и рекомендательная система для бедных

Мы уже обсуждали, что, вообще говоря, любое матричное разложение можно с той или иной степенью успеха использовать для построения рекомендательной системы. Основанные на этом модели называются **моделями латентных факторов** (Latent factor models). В 2006 году SVD-подобный алгоритм даже помог Саймону Фанку (Simon Funk; под этим псевдонимом скрывался Brandyn Webb) занять высокое место на соревновании Netflix Prize.

### Подход на чистом SVD

Вернёмся к примеру из пункта 1.3. Пусть вновь объекты нашего датасета соответствуют пользователям интернет-магазина, а признаки — товарам, причём в клетке с индексом $(i,j)$ записаны рейтинги $\rho_{ij}$, которые пользователи ставят товарам. На основе этих данных мы хотим порекомендовать некоторому $n$-му пользователю $k$ очередных товаров. Если бы нам были известны $\rho_{nj}$ для всех индексов товаров $j$, задача не стоила бы выеденного яйца: мы бы просто взяли $k$ товаров с максимальными значениями рейтингов. Более того, мы могли бы с помощью матричного разложения построить модель и надеяться, что координаты латентных представлений пользователей и товаров окажутся интерпретируемыми (нет).

![Decomp31](https://yastatic.net/s3/education-portal/media/Decomp31_908a5a0bc7_29f6c494ef_befa61800c.svg)

А именно, если бы мы знали все $\rho_{nj}$, построить отдельные представления для пользователей и для товаров некоторой (подбираемой; это гиперпараметр модели) длины $T$ мы могли бы с помощью SVD и приближения из теоремы Эккарта-Янга:

$$X\sim\widehat{X}\widehat{U}\widehat{\Sigma}\widehat{V}^T = \color{red}{\widehat{U}\widehat{\Sigma}^{\frac12}}\cdot
\color{green}{\widehat{\Sigma}^{\frac12}\widehat{V}^T}$$

Но на деле матрица $\left(\rho_{ij}\right)_{i,j}$ обычно разреженная: в ней лишь сравнительно немного известных рейтингов, а в остальных ячейках стоят пропуски. Что же делать?

Наивный вариант — заменить все пропуски нулями (то есть положить, что если пользователь не ставил рейтинг товару, то он ему вдребезги не интересен, что не всегда правдоподобно) или средними по строке/столбцу, после чего сделать SVD и радоваться жизни. В этой ситуации наша приближённая модель предсказывает рейтинг, выставленный $i$-м пользователем $j$-му товару, как скалярное произведение представлений пользователя и товара — то есть $i$-й строки матрицы $\color{red}{\widehat{U}\widehat{\Sigma}^{\frac12}}$ и $j$-го столбца матрицы $\color{green}{\widehat{\Sigma}^{\frac12}\widehat{V}^T}$

Теперь <span style="color:blue"> чтобы порекомендовать n-му пользователю k очередных товаров, мы просто берём n-ю строку матрицы $\widehat{X}$ и находим номера её наибольших элементов.</span>

К сожалению, у этого метода есть как минимум две проблемы:

* Пропусков обычно очень много; если их все заменить какими попало значениями, оценка будет очень шумной;
* При таком подходе нет простого способа обновить рекомендации при добавлении новых данных — SVD придётся переучивать заново.

### Развиваем идею: как побороть разреженность

К счастью, есть и другой путь. Давайте подумаем: чего вообще мы требуем от матриц $\color{red}{B} := \color{red}{\widehat{U}\widehat{\Sigma}^{\frac12}}$ и $\color{green}{C}^T := \color{green}{\widehat{\Sigma}^{\frac12}\widehat{V}^T}$? По сути нам нужны две вещи:

* $\color{red}{B}\cdot\color{green}{C}^T \sim X$;
* Обе матрицы ортогональны по столбцам.

Последнее можно опустить. Ясной пользы для рекомендательной системы от этого нет; да, это давало бы нам некоррелированность латентных признаков, но мы уже видели, что интерпретируемости это не влечёт. Первое же условие удобно сформулировать в терминах векторов латентных представлений пользователей (обозначим их $\color{red}{b_i}$; это строки $\color{red}{B}$) и товаров (обозначим их $\color{green}{c_i}$ — это строки $\color{green}{C}$). А именно, нам нужно, чтобы *скалярное произведение* $(\color{red}{b_i},\color{green}{c_j})$ было как можно ближе к $\rho_{ij}$ **для всех пар** $(i, j)$, для которых $\rho_{ij}$ нам известно.

Вот именно! Мы можем просто не обращать внимания на неизвестные значения, оптимизируя только по тем клеткам $X$, для которых нам что-то известно:

$$\sum_{i,j:,\rho_{ij}\neq\mathrm{NA}}(\rho_{ij}-(b_i,c_j))^2\longrightarrow\min_{b_i,c_i}
$$

Но как решить эту оптимизационную задачу? Разумеется, с помощью стохастического градиентного спуска. В базовом варианте мы случайным образом перебираем пары $(i, j)$, для которых $\rho_{ij}$ нам известно, и обновляем координаты векторов $\color{red}{b_i}$ и $\color{green}{c_j}$ следующим образом:

$$b_{it}:=b_{it}+\eta\varepsilon_{ij}c_{jt}\\c_{jt}:=c_{jt}+\eta\varepsilon_{ij}b_{it}, t=1,\ldots,T,\quad\text{где }\varepsilon_{ij}=\rho_{ij}-(b_i,c_j)
$$

где $\eta$ — гиперпараметр, отвечающий за темп обучения.

Приятное свойство такого подхода: в нём легко добавлять новые товары/пользователей (дообучаем их векторы, заморозив остальные), а также новые оценки $\rho_{ij}$ (добавляем в оптимизируемый функционал и проводим дооптимизацию).

Отметим, что в ходе оптимизации мы попеременно осуществляем градиентный спуск, обновляя то $\color{red}{B}$, то $\color{green}{C}$. Эту идею можно развить следующим образом. Заметим, что при фиксированной матрице $C$ задача минимизации по $B$ выражения

$$\sum_{i,j:,\rho_{ij}\neq\mathrm{NA}}(\rho_{ij}-(b_{i},c_{j}))^{2}\longrightarrow\min_{b_{i},c_{i}}
$$

превращается по сути в обычный метод наименьших квадратов, для которого можно даже выписать «точное» решение (а вы можете это сделать?). Точно так же и при фиксированном $B$ легко находится минимум по $C$. Чередуя эти два шага, мы будем сходиться к решению быстрее и надёжнее, чем с помощью SGD. Данный алгоритм носит название <span style="color:blue">Alternating Least Squares (ALS)</span>.

### Развиваем идею: как ещё усовершенствовать модель

Можно ввести много дополнительных эвристик и предположений, которые уведут нас совсем далеко от старого доброго SVD. Например:

* Рейтинг не всегда является продуктом чистого взаимодействия пользователя с товаром. Бывают товары, которые сами по себе ужасно популярны (скажем, человек купит туалетную бумагу даже если не очень интересуется товарами для дома) или так ужасны, что даже интересующийся данной «латентной категорией» покупатель не станет их высоко оценивать. Это можно промоделировать, добавив к скалярному произведению члены, зависящие только от пользователя и только от товара соответственно:

$$\rho_{ij}\sim \color{red}{\overline{b}_i} +\color{green}{\overline{c}_j} + (\color{red}{b_i},\color{green}{c_j})
$$

Тогда наша задача оптимизации примет вид:

$$\sum_{i,j:\rho_{ij}\neq\mathrm{NA}}(\rho_{ij}-\overline{b}_i-\overline{c}_j-(b_i,c_j))^2\longrightarrow\min_{b_i,c_i,\overline{b}_i,\overline{c}_j}
$$

* Можно добавлять регуляризационные члены. Например:

$$\sum_{i,j:,\rho_{ij}\neq\mathrm{NA}}(\rho_{ij}-(b_i,c_j))^2+\lambda\sum_i||b_i||^2+\mu\sum_j||c_j||^2\longrightarrow\min_{b_i,c_i}
$$

* Мы можем не игнорировать неизвестные нам элементы матрицы $X$, а присвоить им нулевые значения и ставить более низкие веса соответствующим слагаемым функции потерь:

$$\color{blue}{\sum_{i,j}w(\rho_{ij})(\rho_{ij} - (\color{red}{b_i},\color{green}{c_j}))^2\longrightarrow \min\limits_{\color{red}{b_i},\color{green}{c_i}}}
$$

где $w(\rho_{ij})$ маленькое, если $\rho_{ij}=\mathrm{NA}$, и большое в противном случае. Это имеет смысл, например, если отсутствие данных в самом деле может быть логично интерпретировать, как отсутствие интереса.

* Можно ввести требования неотрицательности: $\color{red}{b_{it}}\geqslant 0$, $\color{green}{c_{js}}\geqslant0$. Подробнее об этом в параграфе про неотрицательное матричное разложение.
* Или даже всё это вместе :D

{% cut "Контрольный вопрос" %}

Предположим, что каждый рейтинг $\rho_{ij}$ имеет также временную метку $t_{ij}$. Можете ли вы придумать, как их использовать?

{% endcut %}

## Вероятностное обличье модели латентных факторов

Вы могли заметить, что задача

$$\sum_{i,j:,\rho_{ij}\neq\mathrm{NA}}(\rho_{ij}-(b_i,c_j))^2\longrightarrow\min_{b_i,c_i}
$$

подозрительно напоминает задачу наименьших квадратов, и неспроста. В базовой формулировке мы предполагаем, что

$$\rho_{ij}=(b_i,c_j)+\xi_{ij},\quad\text{где }\xi_{ij}\sim\mathcal{N}(0,\sigma^2)\text{ - нормальный шум}
$$

Иными словами,

$$\color{blue}{\rho_{ij} \sim\mathcal{N}\left((\color{red}{b_i},\color{green}{c_j}), \sigma^2\right)}
$$

По крайней мере, те из них, которые нам известны. Нахождение $\color{red}{b_i}$ и $\color{green}{c_j}$ методом максимального правдоподобия как раз и приводит к описанной выше оптимизационной задаче.

Как обычно, мы можем добавить априорную информацию о распределении латентных векторов $\color{red}{b_i}$ и $\color{green}{c_j}$. Например, такую:

$$\color{red}{b_i}\sim\mathcal{N}(0, \sigma_bE),\qquad\color{green}{c_j}\sim\mathcal{N}(0, \sigma_cE)
$$

Расписывая логарифм правдоподобия

$$p(\rho;\color{red}{b},\color{green}{c}) = p(\rho|\color{red}{b},\color{green}{c})p(\color{red}{b})p(\color{green}{c})
$$

и убирая константные члены, которые содержат только сигмы, приводим задачу максимизации логарифма правдоподобия к виду

$$-\frac{1}{2\sigma^{2}}\sum_{i,j:,\rho_{ij}\neq\mathrm{NA}}(\rho_{ij}-(b_{i},c_{j}))^{2}-\frac{1}{2\sigma_{1}^{2}}\sum_{i}||b_{i}||^{2}-\frac{1}{2\sigma_{2}^{2}}\sum_{j}||c_{j}||^{2}\longrightarrow\max_{b_{i},c_{i}}
$$

вполне объясняющему, почему в предыдущем пункте у нас могла появляться L2-регуляризация.

## Анализ независимых компонент (ICA)

ICA изначально был придуман для задачи разделения сигналов («blind source separation»). Рассмотрим [пример из sklearn](https://scikit-learn.org/stable/auto_examples/decomposition/plot_ica_blind_source_separation.html#sphx-glr-auto-examples-decomposition-plot-ica-blind-source-separation-py)

![sklearn](https://yastatic.net/s3/education-portal/media/sklearn_ica_bb0edd0f70_128c3a9ef5.webp)

Изначально были три сигнала (красный, рыжий и синий на второй сверху картинке), их смешали, получив три линейных комбинации (на верхней картинке). Теперь попробуем их разделить. Первая мысль, которая нам приходит в голову: воспользуемся SVD (проинтерпретировав моменты времени как объекты, а сигналы из смеси как признаки — то есть взяв матрицу $2000\times3$)! Но на нижней картинке мы видим результат, который не радует, но не радует ожидаемо, и вот почему:

* В первый латентный признак SVD старается собрать максимально возможную дисперсию — мы видим, что красный график на нижней картинке действительно ловит самые значительные колебания сигналов из смеси; при этом в третий (рыжий) сигнал уже попадает более или менее случайный шум.
* Если посмотреть на значения исходных сигналов, то они распределены не нормально (распределения значений синего и красного имеют две моды, а у рыжего близко к равномерному), а мы помним, что SVD плохо приспособлено к работе с не гауссовскими данными.

<span style="color:blue">Анализ независимых компонент (ICA)</span> состоит в аппроксимации $x_i\sim z_iV^T$ наблюдаемых признаков линейной смесью латентных, которые являются **независимыми** как случайные величины.

*Замечание*. Оригинальная формулировка несколько другая: изначально ICA — это аппроксимация наблюдаемых сигналов линейной смесью некоторого числа независимых сигналов, то есть речь шла о смеси объектов. Описываемые далее методы можно точно также использовать и для разделения смеси объектов, конечно.

Важно, что в данном случае предъявляется требование **независимости**, а не просто **некоррелированности** — более сильное, впрочем, труднодостижимое и столь же трудно проверяемое.

### Как построить ICA? Путешествие в мир удивительных эвристик

Мы будем излагать алгоритм FastICA по [статье его создателей](https://www.cs.helsinki.fi/u/ahyvarin/papers/TNN99new.pdf), она же реализована в библиотеке sklearn; в статье вас ждёт гораздо больше подробностей и тонкостей реализации.

Алгоритм базируется на следующем эвристическом соображении: <span style="color:blue">линейная комбинация нескольких независимых негауссовских величин в большей степени гауссовская, чем сами эти величины </span> — довольно смелый вывод из   Центральной предельной теоремы. Таким образом, мы будем искать <span style="color:blue">линейную комбинацию исходных признаков, которая была бы в наименьшей степени гауссовской</span> — это и будет первая из независимых компонент. Но как померить близость к нормальности?

Пусть $z$ — некоторая (одномерная) случайная величина с плотностью $p(z)$. Рассмотрим её энтропию

$$H(z) = -\int p(t)\log{p(t)}dt
$$

Имеет место теорема: <span style="color:blue">гауссовская случайная величина имеет максимальную энтропию среди всех случайных величин с заданной дисперсией</span>. Рассмотрим теперь

$$J(z) = H(z_{gauss}) - H(z)
$$

где $z_{gauss}$ — гауссовская случайная величина с той же дисперсией, что и у $z$. Величина $J(z)$ всегда неотрицательна и равна нулю в том случае, если $z$ гауссовская. Решая задачу

$$J(Xw)\longrightarrow\max\limits_{w}
$$

мы могли бы найти самую негауссовскую линейную комбинацию наших признаков. Проблема в том, что $J(z)$ трудно посчитать. Авторы статьи предлагают использовать приближение

$$J(z)\sim\left(\mathbb{E}G(z) - \mathbb{E}G(w)\right)^2,
$$

где $w\sim\mathcal{N}(0,1)$, а $G$ неквадратичная функция (в статье предлагаются конкретные варианты). Последующие независимые компоненты можно искать в ортогональном подпространстве (всё-таки они должны быть и некоррелированными).

### Подготовка данных для ICA

Перед тем, как строить разложение нужно центрировать данные (вычесть из признаков их средние) и убедиться, что ковариационная матрица признаков является единичной.

{% cut "Контрольный вопрос: как добиться последнего с помощью линейной замены?" %}

При линейной замене $x = Cx'$ матрица ковариации меняется, как $\Sigma' = C^T\Sigma C$. Осталось вспомнить, что, поскольку матрица ковариации симметричная и положительно определённая, существует линейная замена, для которой $C^T\Sigma C = E$. Например, в качестве $C$ можно взять (симметричный положительно определённый) квадратный корень из $\Sigma^{-1}$.

{% endcut %}

## Неотрицательное матричное разложение (NMF)

### Мотивация: тематическое моделирование

Допустим, что у нас есть датасет, в котором объекты — тексты, признаки — токены (например, слова), а на $(i,j)$-м месте написана частота встречаемости $j$-го токена в $i$-м тексте (то есть $\frac{n_{ij}}{n_j}$, где $n_{ij}$ — сколько раз $i$-й токен встретился в $j$-м документе, а $n_j$ — общее число токенов в этом документе).

![matrix](https://yastatic.net/s3/education-portal/media/matrix_factorization_noname_3_539bfbc386_0c10957e0f.svg)

Приблизим нашу матрицу произведением

$$\underset{N\times D}{\operatorname{X}} \sim \underset{N\times R}{\operatorname{B}} \cdot \underset{R\times D}{\operatorname{C}}
$$

Одна из возможных интерпретаций такова. Есть $D$ тем:

![matrix](https://yastatic.net/s3/education-portal/media/matrix_factorization_noname_4_e158549ceb_c50e43a574.svg)

За этим стоит вполне ясная вероятностная модель:

$$p(word\mid document)\sim\sum_{theme}p(word\mid theme)\cdot p(theme\mid document)
$$

Вопрос в том, как получить такое разложение. Конечно, чисто технически можно использовать SVD. Но тогда элементы матриц разложения вряд ли будут иметь вероятностный смысл: они же даже не обязаны быть неотрицательными. С другой стороны, если потребовать, чтобы все элементы $\color{red}{B}$ и $\color{green}{C}$ были неотрицательными, ситуация исправится.

### Определение NMF

<span style="color:blue">Неотрицательное матричное разложение</span> неотрицательной матрицы $X$ — это произведение $BC$ матриц с неотрицательным элементами, наилучшим образом приближающее $X$ по норме Фробениуса

$$||X - BC||^2_{fro}\longrightarrow\min\limits_{\begin{smallmatrix}B, C\\b_{ij}, c_{kl}\geqslant0\,\forall i,j,k,l\end{smallmatrix}}
$$

### Alternating Least Squares (ALS)

ALS — один из популярных методов для решения факторизационных задач. Несмотря на то, что оптимизационная задача в целом не является выпуклой, по отдельности задача поиска каждого из сомножителей является выпуклой и может решаться с помощью привычных нам методов. Таким образом, мы можем чередовать поиск $B$ при фиксированном $C$ и поиск $C$ при фиксированном $B$, итеративно сходясь к итоговому решению:

![als](https://yastatic.net/s3/education-portal/media/als_algo_5bff8ae1c2_6c08e4a1f1.webp)

Заметим, что из-за насильного обнуления элементов будут получаться разреженные матрицы.

Разумеется, можно рассматривать и более сложные функционалы, прибавляя к $\Vert X - BC\Vert^2$ различные регуляризационные члены, скажем, поощряющие большую разреженность матриц $B$ и $C$.

  ## handbook

  Учебник по машинному обучению

  ## title

  Матричная факторизация

  ## description

  Матричная факторизация

- 
  ## path

  /handbook/ml/article/veroyatnostnye-raspredeleniya

  ## content

  Принимая то или иное решение в условиях недостаточной информации, нам часто приходится взвешивать шансы, просчитывать риски, а то и вовсе уповать на удачу. Теория вероятностей предоставляет математические инструменты для проведения корректных рассуждений в условиях неопределённости, количественного измерения характеристик случайных событий и оценки правдоподобия их реализации.

Этот и последующий параграфы следует рассматривать как расширенный справочник, позволяющий освежить знания по вероятности и статистике, сделав при этом упор на приложении к машинному обучению. За более систематическим курсом по теории вероятностей читателю следует обратиться к серьёзным учебникам вроде Ширяева или Феллера.
Для погружения в статистику смотри, например, книгу Лагутина. А особо нетерпеливым рекомендуем взглянуть на короткий и ёмкий Probability and Statistics Cookbook.

## Вероятностное пространство

В учебниках вероятность традиционно поставляется в комплекте с [вероятностным пространством](https://ru.wikipedia.org/wiki/%D0%92%D0%B5%D1%80%D0%BE%D1%8F%D1%82%D0%BD%D0%BE%D1%81%D1%82%D0%BD%D0%BE%D0%B5_%D0%BF%D1%80%D0%BE%D1%81%D1%82%D1%80%D0%B0%D0%BD%D1%81%D1%82%D0%B2%D0%BE). Не увлекаясь чрезмерным формализмом, можно сказать, что для задания вероятностного пространства нужны:

- непустое множество $\Omega$, называемое пространством **элементарных событий** (**исходов**);

- [алгебра](https://ru.wikipedia.org/wiki/%D0%90%D0%BB%D0%B3%D0%B5%D0%B1%D1%80%D0%B0_%D0%BC%D0%BD%D0%BE%D0%B6%D0%B5%D1%81%D1%82%D0%B2) множеств $\mathcal F \subset 2^{\Omega}$ — набор подмножеств $\Omega$, замкнутый относительно дополнений, объединений и пересечений; каждый элемент $A\in\mathcal F$ называется **событием**;

- **вероятностная мера** $\mathbb P \colon \mathcal F \to [0, 1]$, приписывающая каждому событию $A \in \mathcal F$ некоторую **вероятность** $\mathbb P(A) \in [0,1]$.

К вероятностному пространству $(\Omega, \mathcal F, \mathbb P)$ предъявляются следующие требования:

- $\varnothing \in \mathcal F$ (**невозможное** событие), $\Omega \in \mathcal F$ (**достоверное** событие);
- $\mathbb P(\Omega) = 1$;
- $\mathbb P(A\cup B) = \mathbb P(A) + \mathbb P(B)$, если $A, B \in \mathcal F$ и $A\cap B= \varnothing$ (**аддитивность**).

**Упражнение**. Докажите, что $\mathbb P(\varnothing) = 0$ и $\mathbb P(\bar A) = 1 - \mathbb P(A)$, если $A\in\mathcal F$.

{% cut "Решение (не открывайте сразу, попробуйте сначала решить самостоятельно)" %}

События $A$ и $\bar A = \Omega \setminus A$ не пересекаются (или, как говорят, **несовместны**), поэтому из аддитивности вероятностной меры следует, что
$\mathbb P(A) + \mathbb P(\bar A) = P(\Omega) = 1$. Полагая $A = \varnothing$, получаем $\mathbb P(\varnothing) + 1 = 1$, т.е. $\mathbb P(\varnothing) = 0$.

{% endcut %}

Аддитивность вероятности легко обобщается по индукции до свойства **конечной аддитивности**: если события $A_1, \ldots, A_n$ попарно несовместны, то

$$
    \mathbb P\Big(\bigcup\limits_{k=1}^n A_k\Big) = \sum\limits_{k=1}^n \mathbb P(A_k).
$$

{% cut "Примечание" %}

Если подходить совсем строго, то система множеств $\mathcal F$ должна быть <a href="https://ru.wikipedia.org/wiki/%D0%A1%D0%B8%D0%B3%D0%BC%D0%B0-%D0%B0%D0%BB%D0%B3%D0%B5%D0%B1%D1%80%D0%B0">сигма-алгеброй</a>, а вероятностная мера $\mathbb P$ — сигма-аддитивной, т.е.

$$
  \mathbb P\Big(\bigcup\limits_{n=1}^\infty A_n\Big) = \sum\limits_{n=1}^\infty \mathbb P(A_n), \quad A_i \cap A_j = \varnothing \text{ при } i\ne j.
$$

Впрочем, эти свойства носят преимущественно теоретичеческий интерес, поскольку в
прикладных задачах редко встречаются бесконечные наборы событий.

{% endcut %}

Множество $\Omega$ часто называют **носителем**; говорят также, что вероятностная мера (масса) $\mathbb P$ **сосредоточена**, или **распределена**, на носителе $\Omega$. В зависимости от типа носителя $\Omega$ распределения делятся на два типа: **дискретные** и **непрерывные**.

## Дискретные распределения

Вероятность на не более чем счётном пространстве элементарных событий $\Omega = \{\omega_1, \omega_2, \ldots\}$ задаётся просто приписыванием неотрицательного числа $p_k$ каждому элементарному исходу $\omega_k$ с условием $\sum\limits_k p_k = 1$. Для произвольного события $A \subset \Omega$ полагают $\mathbb P(A) =  \sum\limits_{\omega_k \in A} p_k$. Набор чисел $\{p_k\}$ называют также **распределением вероятностей** на множестве $\Omega$. В англоязычной литературе распространён термин **probability mass function** (сокращённо **pmf**).

Зачастую в результате эксперимента нас интересуют не вероятности событий сами по себе, а значения некоторой связанной с ними **случайной величины**, принимающей числовые значения. Например:

- сумма очков, выпавших при броске двух кубиков;
- число метеоритов диаметром более одного метра, падающих на Землю в течение года;
- ежедневный доход от показа рекламных объявлений в интернете.

На каждом элементарном исходе $\omega_k$ случайная величина $\xi$ принимает некоторое числовое значение $\xi_k = \xi(\omega_k)$. Иными словами, случайная величина — это функция $\xi \colon \Omega \to \mathbb R$, принимающая значение $\xi_k$ с вероятностью $p_k$; её **математическое ожидание** (**среднее**) и **дисперсия** (**среднеквадратичное отклонение**) вычисляются по формулам

$$
\mathbb E\xi = \sum\limits_{k} \xi_k p_k \text{ и }\mathbb V\xi = \mathbb E\big(\xi - \mathbb E \xi\big)^2 = \mathbb E \xi^2 - \big(\mathbb E \xi\big)^2
$$

соответственно. Корень из дисперсии $\sqrt{\mathbb V \xi}$ назвают **стандартным отклонением** случайной величины $\xi$. Стандартное отклонение и дисперсия показывают, насколько далеко значения случайной величины могут отклоняться от среднего значения. Стандартное отклонение хорошо тем, что, в отличие от дисперсии, измеряется в тех же единицах, что и сама случайная величина.

### Равномерное распределение

Так называется распределение вероятностней на множестве $\Omega = \{\omega_1, \ldots, \omega_n\}$, у которого $p_k =\mathbb P(\omega_k) = \frac 1n$, $1\leqslant k\leqslant n$. Тогда $\mathbb P(A) = \sum\limits_{\omega_k \in A} \frac 1n  = \frac{\vert A\vert}{\vert \Omega\vert}$, и мы получили формулу из классического подхода к вероятности, при котором вероятность события равна отношению числа благоприятных исходов к общему их количеству.

Равномерным распределением моделируются различные игровые ситуации:

- подбрасывание симметричной монеты ($\omega_1 = «орёл»$, $\omega_2 = «решка»$);
- подбрасывание кубика ($\omega_k = k$, $1\leqslant k \leqslant 6$);
- вращение рулетки в казино ($n=37$ для европейской, $n=38$ для американской).

**Упражнение.** У европейской рулетки по $18$ чёрных и красных секторов и один сектор «зеро». Игрок ставит €10 на чёрное. В случае успеха казино выплачивает ему ещё €10, в противном случае забирает ставку. Чему равно математическое ожидание, дисперсия и стандартное отклонение выигрыша?

{% cut "Решение (не открывайте сразу, попробуйте сначала решить самостоятельно)" %}

Здесь выигрыш — это случайная величина $\xi$, принимающая значение $-10$ евро в $19$-ти исходах и $+10$ евро в $18$-ти. Распределение равномерное (если верить в честность казино), поэтому $\mathbb E\xi = 10\cdot\frac{18}{37} - 10\cdot\frac{19}{37} = -\frac {10}{37}$ евро.

Случайная величина $\xi^2$ постоянна (а вовсе не «случайна») и при любом исходе равна 100 («квадратных евро»?), поэтому

$$
  \mathbb V\xi = \mathbb E \xi^2 - \big(\mathbb E \xi\big)^2 = 100 \cdot\Big(1 - \frac{1}{37^2}\Big)\approx 99.927.
$$

Стандартное отклонение выигрыша равно $\frac{10 \sqrt{1368}}{37} \approx 9.996$ евро, что почти совпадает со ставкой игрока. Это отражает сущность игры в рулетку: либо пан, либо пропал. Причём последнее чуть вероятнее из-за отрицательности математического ожидания.

{% endcut %}

**Вопрос на подумать**. Бывают ли равномерные распределения в пространствах со счётным носителем?

{% cut "Ответ (не открывайте сразу, сначала подумайте самостоятельно)" %}

Нет, поскольку условие нормировки требует сходимости ряда $\sum\limits_{k} p_k = 1$, а это равенство никак не может быть выполнено при одинаковых числах $p_k$. Либо все эти вероятности равны нулю, и тогда их сумма тоже равна нулю; либо же получается бесконечная сумма одинаковых положительных слагаемых, которая равна $+\infty$.

{% endcut %}

Равномерные распределения преимущественно встречаются в разного сорта играх. В более жизненных ситациях случайность обычно распределена отнюдь не равномерно.

### Распределение Бернулли

Так называется очень простое распределение всего лишь с двумя исходами:

$$
    \mathbb P(\text{«успех»}) = p, \quad \mathbb P(\text{«неудача»}) = 1-p, \quad 0\leqslant p \leqslant 1.
$$

**Бернуллиевская** случайная величина $\xi\sim\mathrm{Bern}(p)$ — это просто индикаторная функция успешного события: $\xi = 1$, если случился «успех», $\xi = 0$, если нас постигла «неудача». Несложные вычисления показывают, что

$$
\mathbb E \xi = 1\cdot p + 0 \cdot (1-p) = p, \quad
\mathbb V \xi = p-p^2 = p(1-p).
$$

Если $p=\frac 12$, то снова получается равномерное распределение с двумя исходами. При $p\ne \frac 12$ бернуллиевская случайная величина моделирует подбрасывание несимметричной монеты. В машинном обучении часто встречается задача бинарной классификации, и разбиение на классы обычно кодируется с помощью $\mathrm{Bern}(p)$, например:

- диагностика болезни (болен — $1$, здоров — $0$);
- оценка кредитоспособности клиента (одобрить кредит — $0$, отказать $1$);
- предсказание поведения пользователя (кликнет на рекламу — $1$, пропустит — $0$).

В этих примерах вероятности классов явно не равны, поэтому несимметричное распределение Бернулли — типичная ситуация в реальных задачах.

### Биномиальное распределение

**Биномиальное распределение** $\mathrm{Bin}(n, p)$ имеет сумма независимых бернуллиевских случайных величин $\xi_k\sim \mathrm{Bern}(p)$: $\eta \sim \mathrm{Bin}(n, p)$, если $\eta = \xi_1 + \ldots + \xi_n$. Другими словами, случайная величина $\eta$ равна количеству успехов в $n$ независимых испытаниях Бернулли с вероятностью успеха $p$. Случайная величина $\eta$ принимает значения от $0$ до $n$, и

$$
  p_k=P(\eta = k) = \binom nk p^k (1-p)^{n-k},\quad 0\leqslant k \leqslant n.
$$

<iframe src="https://yastatic.net/s3/education-portal/media/slider_binomial_28dc938f6a_94f0fea15b.html" width=100% height=500 frameborder=0></iframe>

Отметим, что согласно [биному Ньютона](https://ru.wikipedia.org/wiki/%D0%91%D0%B8%D0%BD%D0%BE%D0%BC_%D0%9D%D1%8C%D1%8E%D1%82%D0%BE%D0%BD%D0%B0)

$$
  \sum\limits_{k=0}^n p_k = \big(p + (1-p)\big)^n = 1,
$$

поэтому числа $\{p_k\}$ действительно представляют собой распределение вероятностей, называемое также **биномиальным**. Если $\xi \sim\mathrm{Bin}(n, p)$, то

$$
\mathbb E \xi = np, \quad \mathbb V \xi = np(1-p).
$$

**Пример**. Каждый день рекламу компании А поисковой выдаче Яндекса видят ровно $1000$ человек. Вчера $50$ из них кликнули на рекламу. Для прогнозирования объемов продаж компании А хочется знать, с какой вероятностью не менее 50 людей кликнут на ее рекламу сегодня.

Если моделировать наличие или отсутствие клика бернуллиевской случайной величиной, то общее количество кликов за день моделируется случайной величиной $\xi \sim \mathrm{Bin}(n, p)$ с параметрами $n=1000$ и $p = \frac{50}{1000} = 0.05$. Тогда с помощью вычислительной техники получаем, что

$$
  \mathbb P(\xi \geqslant 50) = \sum\limits_{k = 50}^{n} \binom{n}{k} p^k (1 - p)^{n-k} = 1 - \sum\limits_{k = 0}^{49} \binom {1000}{k} 0.05^k 0.95^{1000 - k} \approx 0.52.
$$

Отметим, что параметр $p$ в предыдущем примере нам, строго говоря, не был известен, и вместо него мы использовали частотную оценку.

### Распределение Пуассона

Это распределение имеет счётный носитель $\Omega = \mathbb N \cup \{0\}$. Случайная величина $\xi$ имеет **пуассоновское распределение** с параметром $\lambda$, $\xi \sim \mathrm{Pois}(\lambda)$, если

$$
  \mathbb P(\xi = k) = e^{-\lambda} \frac{\lambda^k}{k!}, \quad k \in \mathbb N \cup \{0\}.
$$

<iframe src="https://yastatic.net/s3/education-portal/media/slider_poisson_1566c16c9e_05597862cc.html" width=100% height=500 frameborder=0></iframe>

Известное разложение экспоненты в ряд Тейлора $e^\lambda = \sum\limits_{k=0}^\infty  \frac{\lambda^k}{k!}$ позволяет заключить, что вероятности распределения Пуассона действительно суммируются в единицу. Этот же ряд позволяет вычислить, что

$$
\mathbb E \xi = \mathbb V \xi = \lambda.
$$

Пуассоновская случайная величина моделирует число редких событий, происходящих в течение фиксированного промежутка времени: если события наступают со средней скоростью $r$, то

$$
  \mathbb P(k \text{ событий на промежутке } t) = e^{-rt} \frac{(rt)^k}{k!}, \quad k \in \mathbb N \cup \{0\}.
$$

Иногда приходится рассматривать биномиальное распределение $\mathrm{Bin}(n, p)$ с большим числом попыток $n$ и вероятностью успеха $p$ с условием $np \approx \lambda > 0$. Оказывается, что вне зависимости от $n$ такое распределение быстро стабилизируется, сходясь к пуассоновскому распределению с параметром $\lambda$. Точнее говоря, справедлива следующая теорема.

**Теорема (Пуассон)**. Пусть $\xi \sim \mathrm{Bin}(n, p_n)$ и $\lim\limits_{n \to \infty} np_n = \lambda > 0$. Тогда

$$
  \lim\limits_{n \to \infty} \mathbb{P}(\xi = k) =  e^{-\lambda} \frac{\lambda^k}{k!}, \quad k \in \mathbb N \cup \{0\}.
$$

**Пример**. Известно, что на поисковой выдаче яндекса на рекламу компании А кликает в среднем примерно 50 пользоваталей в день. Количество показов достаточно большое и может меняться изо дня в день. Требуется оценить вероятность того, что сегодня будет совершено не менее 50 кликов по рекламным объявлениям.

Распределение количества кликов снова будем моделировать биномиальным распределением $\mathrm{Bin}(n, p)$. На этот раз число $n$ нам неизвестно, но сказано, что оно велико и $np \approx 50$ (вспомним, что $\mathbb E\xi = np$, если $\xi \sim \mathrm{Bin}(n, p)$). Поэтому можно воспользоваться теоремой Пуассона и заменить биномиальное распределение пуассоновским с параметром $\lambda = 50$. Тогда искомая вероятность равна

$$
  1 - \sum\limits_{k = 0}^{49} e^{-50} \frac{50^k}{k!} \approx 0.518,
$$

что практически совпадает ответом, полученным с помощью биномиального распределения при $n = 1000$.

### Геометрическое распределение

Пусть монетка с вероятностью «успеха» $p$ подбрасывается до тех пор, пока впервые не случится «успех». Случайная величина $\xi$, равная общему количеству попыток на этом пути, имеет **геометрическое** распределение, т.е.

$$
  \mathbb P(\xi = k) = q^{k-1}p, \quad q = 1-p, \quad k \in \mathbb N.
$$

<iframe src="https://yastatic.net/s3/education-portal/media/slider_geom_46e10aae89_bd7f66c833.html" width=100% height=500 frameborder=0></iframe>

По формуле геометрической прогрессии находим, что

$$
  \sum\limits_{k=1}^\infty \mathbb P(\xi = k) = \sum\limits_{k=0}^\infty q^kp = \frac p{1-q} = 1,
$$

поэтому с нормировкой тут всё в порядке. Чем меньше $p$, тем больше геометрическое распределение похоже на равномерное, что подтверждают и формулы для среднего и дисперсии:

$$
\mathbb E \xi = \frac 1p, \quad \mathbb V \xi = \frac{1-p}{p^2}.
$$

**Пример**. По оценкам за предыдущие дни пользователь нажимает на рекламу с вероятностью $p=0.05$. Сегодня компания B планирует показать очень важное рекламное объявление и требует от Яндекса, чтобы с вероятностью не менее $99\%$ на него кликнули хотя бы раз. Скольким различным людям следует показать это объявление?

Здесь мы имеем дело с геометрическим распределением с вероятностью «успеха» (клика) $p$: именно так распределена случайная величина $\xi$, равная количеству показов объявления до первого клика по нему. Следовательно,

$$
  \mathbb P(\xi \leqslant n) = \sum\limits_{k=1}^n \mathbb P(\xi = k) = \sum\limits_{k=1}^n q^{k-1}p = p\cdot \frac{1 - q^{n}}{1-q} = 1-q^n.
$$

Эта вероятность должна быть не меньше $99\%$, т. е. $0.95^n \geqslant 0.01$. Отсюда находим, что
$n \geqslant \frac{\log 0.01}{\log 0.95} \approx 89.78$. Таким образом, рекламу надо показать как минимум $90$ раз.

### Гипергеометрическое распределение

**Пример**. Известно, что партия из $N$ деталей содержит $K$ бракованных. Какова вероятность того, что среди выбранных наугад $n$ деталей окажется ровно $k$ бракованных?

Всего есть $\binom N n$ способов выбора $n$ деталей из партии. Число вариантов выбрать $k$ деталей из $K$ бракованных и $n-k$ из $N-K$ деталей без дефектов равно $\binom K k \binom{N-K}{n-k}$. По классическому определению вероятности получаем, что искомая вероятность равна

$$
  p_k = \frac{\binom K k \binom{N-K}{n-k}}{\binom N n}, \quad 0 \leqslant k \leqslant \min\{K, n\}.
$$

Такое распределение называется **гипергеометрическим**. Равенство

$$
\sum\limits_{k=0}^{\min\{K, n\}} p_k = 1
$$

следует из [тождества Вандермонда](https://ru.wikipedia.org/wiki/%D0%A2%D0%BE%D0%B6%D0%B4%D0%B5%D1%81%D1%82%D0%B2%D0%BE_%D0%92%D0%B0%D0%BD%D0%B4%D0%B5%D1%80%D0%BC%D0%BE%D0%BD%D0%B4%D0%B0). Если случайная величина $\xi$ имеет гипергеометрическое распределение с параметрами $N$, $K$, $n$, то

$$
  \mathbb E \xi = \frac{nK}{N}, \quad \mathbb V\xi = n\frac{K(N-K)(N-n)}{N^2(N-1)}.
$$

Гипергеометрическое распределение является аналогом биномиального, при котором моделируется выбор без возвращения с вероятностью успеха $p\approx \frac KN$.

## Непрерывные распределения

Вероятностная модель с конечным или счётным носителем не подходит в тех случаях, когда результатом эксперимента удобно считать произвольное действительное число, например, распределение людей по росту или по весу. Для этого требуется пересмотреть подход к построению пространства элементарных событий $\Omega$: ведь множество действительных чисел $\mathbb R$ континуально, и поэтому вероятность события не получится определить как сумму вероятностей всех составляющих исходов, коих тоже может оказаться континуум. Приходится искать другие способы задания вероятности.

Наиболее часто встречающийся на практике класс непрерывных распределений на числовой прямой задаётся с помощью неотрицательной интегрируемой функции **плотности** (**probability density function**, **pdf**) $p(x)$ со свойством

$$
  \int\limits_{-\infty}^{+\infty} p(x) dx = 1.
$$

Вероятность события $A$ определяется как

$$
  \mathbb{P}(A) = \int\limits_{A} p(x) dx
$$

при условии, что этот интеграл имеет смысл. В частности,

$$
  \mathbb{P}([a, b)) = \int\limits_a^b p(x) dx.
$$

**Замечание**. Связь между вероятностью и плотностью распределения весьма напоминает связь между массой и физической плотностью. Когда плотность объекта всюду одинакова, то масса равна плотности, умноженной на объём. Если же объект неоднороден, то плотность становится функцией, сопоставляющей каждой точке некое число (что-то вроде предела отношения массы малого шарика вокруг этой точки к объёму шарика). Тогда масса любого куска объекта может быть вычислена, как интеграл функции плотности по объёму этого куска.

С плотностью вероятности $p(x)$ автоматически связана случайная величина $\xi\colon  \mathbb R  \to  \mathbb R$, для которой $\mathbb P(a\leqslant \xi < b) = \int\limits_a^b p(x)\,dx$. Функция $p(x)$ называется плотностью случайной величины $\xi$, и обозначается также как $p_\xi(x)$. Иногда используется запись $\xi \sim p(x)$. Среднее и дисперсия случайной величины $\xi \sim p(x)$ вычисляются по формулам

$$
  \mathbb E \xi = \int\limits_{-\infty}^{\infty} xp(x)\,dx,\quad
  \mathbb V \xi = \int\limits_{-\infty}^{\infty} x^2p(x)\,dx - (\mathbb E \xi)^2.
$$

### Равномерное распределение

**Равномерное** распределение на отрезке $[a;b]$, которое часто обозначают $U[a,b]$, имеет постоянную плотность на этом отрезке:

$$
  p(x) = \frac {\mathbb I(a\leqslant x \leqslant b)}{b-a} = \begin{cases}
  \frac 1{b-a},& x\in[a, b],\\
  0, & x \notin [a, b].
  \end{cases}
$$

Если $\xi \sim U[a,b]$, то

$$
\mathbb E \xi = \frac {a+b}2,\quad \mathbb V \xi = \frac{(b-a)^2}{12}.
$$

**Вопрос на подумать**. Можно ли задать равномерное распределения на неограниченном промежутке, например, на $\mathbb R$ или на $\mathbb [0, +\infty)$?

{% cut "Ответ (не открывайте сразу, сначала подумайте самостоятельно)" %}

Строго говоря, нет, покольку интеграл от константы по неограниченному промежутку расходится, и потому не может равняться единице. Однако при байесовском выводе в качестве априорного распределения иногда приходится выбирать ненормируемое распределение, в том числе, например, равномерное на $[0, +\infty)$. Такое «распределение» (которое на самом деле никакое не распределение) также называют **несобственным**.

{% endcut %}

Аналогичным образом вводится равномерное распределение в многомерном пространстве: если множество $V \subset \mathbb R^n$ имеет объём $\vert V\vert$, то плотность равномерно распределённой на $V$ случайной величины $\xi$ задаётся как $p_\xi(\boldsymbol x) = \frac{\mathbb I(\boldsymbol x \in V)}{\vert V\vert}$. Если $A \subset V$, то

$$
\mathbb P(A) = \frac 1{\vert V\vert}\int\limits_A d\boldsymbol x = \frac {\vert A\vert}{\vert V\vert},
$$

и мы получили формулу **геометрической вероятности**.

### Нормальное распределение

Случайная величина $\xi$ имеет **нормальное** (**гауссовское**) распределение $\mathcal N(\mu, \sigma^2)$, если её плотность равна

$$
p_\xi(x) = \frac 1{\sqrt{2\pi}\sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}}.
$$

Параметры нормального распределения $\mathcal N(\mu, \sigma^2)$ представляют собой его среднее и дисперсию:

$$
\mathbb E \xi = \mu,\quad \mathbb V \xi = \sigma^2.
$$

Параметр $\sigma$ отвечает за выраженность «колокола» плотности нормального распределения:

- при $\sigma \to 0$ «колокол» приобретает очертания резко выраженного пика, то есть практически вся вероятностная масса сосретдоточена в малой окрестности точки $x = \mu$;
- при $\sigma \to +\infty$ «колокол», наоборот, размывается, и распределение становится больше похоже на равномерное.

Гауссиана, у которой $\mu=0$ и $\sigma = 1$, называется **стандартным нормальным распределением**.

<iframe src="https://yastatic.net/s3/education-portal/media/button_gaussian_78c43b7d42_4272dd7886.html" width=100% height=500 frameborder=0></iframe>

Иногда бывает полезно тесно связанное с гауссовским **логнормальное** распределение.
Случайная величина $\xi \colon (0, +\infty) \to \mathbb R$ имеет логнормальное распределение, $\xi \sim \mathcal{LogN}(\mu, \sigma^2)$, если $\log \xi \sim \mathcal N(\mu, \sigma^2)$. Плотность логнормальной случайной величины равна

$$
p_\xi(x) = \frac 1{\sqrt{2\pi}\sigma x} e^{-\frac{(\log x-\mu)^2}{2\sigma^2}}, \quad x > 0,
$$

а её среднее и дисперсию можно вычислить по формулам

$$
  \mathbb E\xi = e^{\mu + \frac{\sigma^2}2}, \quad  \mathbb V\xi=\big(e^{\sigma^2} - 1\big) e^{2\mu + \sigma^2}.
$$

### Показательное распределение

Плотность **показательного** (**экспоненциального**) распределения $\mathrm{Exp}(\lambda)$ сосредоточена на луче $[0, +\infty)$ и имеет параметр $\lambda > 0$: $p(x) = \lambda e^{-\lambda x}$, $x \geqslant 0$. Если $\xi \sim \mathrm{Exp}(\lambda)$, то

$$
\mathbb E \xi = \frac 1\lambda,\quad \mathbb V \xi = \frac 1{\lambda^2}.
$$

Плотность показательного распределения является убывающей функцией на $[0, +\infty)$, а параметр $\lambda$ отвечает за скорость этого убывания:

- при $\lambda \to 0$ убывание очень медленное, и распределение больше похоже на равномерное;
- при $\lambda \to +\infty$, наоборот, вся вероятностная масса сосредоточена около точки $0$.

<iframe src="https://yastatic.net/s3/education-portal/media/button_exp_9b7b050329_f487e69793.html" width=100% height=500 frameborder=0></iframe>

Показательное распределение моделирует временные интервалы между случайными событиями, наступающими с постоянной скоростью, например:

- время ожидания автобуса на остановке;
- время между телефонными звонками в колл-центре;
- время до выхода из строя вычислительного узла в дата-центре.

**Гамма-распределение** с положительными параметрами $\alpha$ и $\beta$ имеет плотность

$$
  p(x) = \frac 1{\Gamma(\alpha) \beta^\alpha} x^{\alpha - 1} e^{-\frac x \beta},\quad x \geqslant 0,
$$

где $\Gamma(\alpha)$ — [гамма-функция Эйлера](https://ru.wikipedia.org/wiki/%D0%93%D0%B0%D0%BC%D0%BC%D0%B0-%D1%84%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D1%8F). При $\alpha =1$ гамма-распределение превращается в показательное с параметром $\lambda = \frac 1\beta$. Среднее и дисперсия случайной величины $\xi$, имеющей гамма-распределение с параметрами $\alpha$ и $\beta$, равны

$$
\mathbb E\xi = \alpha\beta, \quad \mathbb V\xi = \alpha\beta^2.
$$

### Бета-распределение

Плотность **бета-распределения** с параметрами $\alpha, \beta > 0$ равна

$$
  p(x) = \frac 1{B(\alpha, \beta)} x^{\alpha - 1} (1-x)^{\beta -1}, \quad 0 < x < 1,
$$

где $B(\alpha, \beta)$ — [бета-функция Эйлера](https://ru.wikipedia.org/wiki/%D0%91%D0%B5%D1%82%D0%B0-%D1%84%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D1%8F).

Бета-распределение имеет следующее статистическое приложение. Выберем случайным образом точки $x_1, \ldots, x_n \in [0,1]$, и упорядочим их по возрастанию. Получим набор значений

$$
  0\leqslant x_{(1)} \leqslant x_{(2)} \leqslant \ldots \leqslant x_{(k)} \leqslant \ldots \leqslant x_{(n)} \leqslant 1.
$$

Оказывается, что случайная величина $\xi = x_{(k)}$, называемая **$k$-й порядковой статистикой**, имеет бета распределение с параметрами $k$ и $n+1 - k$:

$$
  p_\xi(x) = \frac{n!}{(k-1)!(n-k)!} x^{k-1}(1-x)^{n-k} = k \binom nk x^{k-1}(1-x)^{n-k}.
$$

{% cut "Доказательство " %}

Пусть $y\in [0, 1]$. Неравенство $x_{(k)}\leqslant y$ означает, что из $n$ элементов выборки хотя бы $k$ не превосходят $y$. Заметим, что для каждого $i$ индикатор $\mathbb{I}(x_i\leqslant y)$ является бернуллиевской случайной величиной с вероятностью успеха $y$. У нас есть $n$ таких бернуллиевских величин, и неравенство $x_{(k)}\leqslant y$ равносильно тому, что среди $n$ испытаний Бернулли случилось не менее $k$ успехов. Вероятность того, что случилось ровно $j$ успехов, равна, как мы уже знаем,

$$\binom {n}{j}y^j(1 - y)^{n-j}.$$

Чтобы получить вероятность того, что успехов хотя бы $k$, надо просуммировать эти числа по $j$ от $k$ до $n$:

$$F_\xi(y) = \mathbb P (x_{(k)} \leqslant y) = \sum_{j=k}^n\binom {n}{j}y^j(1 - y)^{n-j}.$$

Чтобы получить плотность, продифференцируем функцию распределения:

$$
p_{\xi}(y) = \sum_{j=k}^n j\binom nj y^{j-1}(1 - y)^{n-j} - \sum_{j=k}^{n-1} (n-j)\binom nj y^{j}(1 - y)^{n-1-j}.
$$

Легко проверяется, что $j\binom nj = n\binom {n-1}{j-1}$, $(n-j)\binom nj = n\binom{n-1}j$, и поэтому

$$
  \sum_{j=k}^n j\binom nj y^{j-1}(1 - y)^{n-j}  = \sum_{j=k-1}^{n-1} n\binom{n-1}j y^{j}(1 - y)^{n-1-j}
$$

И мы видим, что большинство слагаемых сокращается и выживает лишь одно:

$$
p_{\xi}(y) = \frac{n!}{(k-1)!(n-k)!}y^{k-1}(1 - y)^{n-k}.
$$

{% endcut %}

### Распределение Стьюдента

При проверке статистических гипотез бывает полезно **распределение Стьюдента** (**t-distribution**) с $\nu$ степенями свободы, плотность которого равна

$$
  p(x) = \frac{\Gamma(\frac{\nu+1}{2})} {\sqrt{\nu\pi}\,\Gamma(\frac{\nu}{2})} \left(1+\frac{x^2}\nu \right)^{-(\nu+1)/2}, \quad \nu > 0,
$$

где $\Gamma(\alpha)$ — гамма-функция Эйлера. Распределение Стьюдента похоже на стандартное нормальное распределение; более того, при $\nu\to +\infty$ оно превращается в $\mathcal N(0, 1)$.

<iframe src="https://yastatic.net/s3/education-portal/media/sliders_t_c618ee8b26_bca1415c8b.html" width=100% height=500 frameborder=0></iframe>

Однако при малых значениях $\nu$ распределение Стьюдента имеет гораздо более тяжёлые «хвосты»: например, при $\nu \leqslant 2$ его дисперсия бесконечна, а при $\nu \leqslant 1$ та же участь постигает и математическое ожидание (всё из-за расходимости соответствующих интегралов). В остальных случаях

$$
  \mathbb E \xi = 0, \quad \mathbb V \xi = \frac{\nu}{\nu-2},
$$

если $\xi$ имеет распределение Стьюдента с $\nu$ степенями свободы.

### Распределение Лапласа

Плотность **распределения Лапласа** с параметрами $\mu, b$ равна

$$
   p(x) = \frac 1{2b} e^{-\frac{\vert x - \mu\vert}b}.
$$

Такое распределение иногда обозначают $\mathrm{Laplace}(\mu, b)$. Если $\xi \sim \mathrm{Laplace}(\mu, b)$, то

$$
  \mathbb E \xi = \mu, \quad \mathbb V \xi = 2b^2.
$$

При $\mu=0$ распределение Лапласа представляет собой экспоненциальное распределение, плотность которого симметрично отражена на отрицательную полуось: если $\xi \sim \mathrm{Laplace}(0, b)$, то $\vert \xi \vert \sim \mathrm{Exp}\big(\frac 1b\big)$. Распределение Лапласа похоже на нормальное и отличается от него немного более тяжёлыми «хвостами» и тем, что его плотность теряет гладкость в нуле.

## Характеристики случайных величин

### Моменты

Если $n\in \mathbb N$, то **$n$-й момент** $\mu_n$ случайной величины $\xi$ равен $\mathbb E \xi^n$. В зависимости от типа случайной величины моменты вычисляются по-разному:

- $\mu_n = \sum\limits_k x_k^n \mathbb P(\xi = x_k)$, если $\xi$ принимает дискретные значения $x_1, x_2, \ldots, x_k, \ldots$;
- $\mu_n = \int\limits_{-\infty}^{+\infty} x^n p_\xi(x)\, dx$, если $\xi$ имеет плотность $p_\xi(x)$.

Первый момент $\mu_1$ — это в точности математическое ожидание (среднее) случайной величины $\xi$. Дисперсию тоже можно выразить через моменты:

$$
\mathbb V\xi = \mathbb E \xi^2 - \big(\mathbb E\xi\big)^2 = \mu_2 - \mu_1^2.
$$

Не у всех случайных величин есть конечные среднее и дисперсия. Например, **распределение Коши** (оно же [распределение Стьюдента](#распределение-стьюдента) с одной степенью свободы) имеет плотность $p(x) = \frac 1\pi \frac 1{1+x^2}$, и если мы попытаемся вычислить первые два момента, то получим расходящиеся интегралы

$$
  \frac 1\pi\int\limits_{-\infty}^{+\infty} \frac{x}{1+x^2} dx \text{ и }
  \frac 1\pi\int\limits_{-\infty}^{+\infty} \frac{x^2}{1+x^2} dx.
$$

**Упражнение**. Приведите пример дискретной случайной величины с бесконечным средним.

{% cut "Ответ" %}

Такое может быть только если случайная величина $\xi$ принимает счётное число значений. Пусть, например, $\mathbb P(\xi = k) = \frac 1{k(k+1)}$, $k\in \mathbb N$. Поскольку $\sum\limits_{k=1}^\infty \frac 1{k(k+1)} = 1$, это корректное распределение вероятностей. С другой стороны, $\mathbb E\xi = \sum\limits_{k=1}^\infty \frac 1{k+1}$, а это почти что гармонический ряд, который расходится.

{% endcut %}

### Свойства математического ожидания

1. Если $\xi = C$, то $\mathbb E \xi = C$.

2. $\mathbb E(a\xi + b \eta) = a\mathbb E \xi + b \mathbb E \eta$ (линейность).

3. Если $\xi \leqslant \eta$, то $\mathbb E \xi \leqslant \mathbb E \eta$ (монотонность).

4. $\mathbb E \mathbb I(A) = \mathbb P(A)$.

5. Если случайные величины $\xi$ и $\eta$ независимы, то $\mathbb E\xi\eta = \mathbb E\xi \mathbb E\eta$.

6. Если $\xi \geqslant 0$, то $\mathbb P(\xi \geqslant a) \leqslant \frac{\mathbb E \xi}{a}$ (**неравенство Маркова**).

7. Если функция $f$ выпукла вниз, то $f(\mathbb E \xi) \leqslant \mathbb E(f(\xi))$ (**неравенство Йенсена**).

**Law of the unconscious statistician (LOTUS)**

Если случайная величина $\eta$ получена применением некоторой детерминированной функцией из случайной величины $\xi$, $\eta = g(\xi)$, то

- $\mathbb E\eta = \sum\limits_k g(x_k) \mathbb P(\xi = x_k)$, если $\xi$ дискретна;
- $\mathbb E\eta = \int\limits_{-\infty}^{+\infty} g(x) p_{\xi}(x)\,dx$, если $\xi$ непрерывна.

### Дисперсия и ковариация

**Ковариация** случайных величин $\xi$ и $\eta$ определяется по формуле

$$
\mathrm{cov}(\xi,\eta) = \mathbb E((\xi - \mathbb E\xi) \cdot (\eta - \mathbb E \eta)) = \mathbb E(\xi \cdot\eta) - \mathbb E\xi \cdot \mathbb E\eta
$$

В частности, $\mathrm{cov}(\xi,\xi) = \mathbb V \xi$. На практике часто применяют **коэффициент корреляции**, который получается нормированием ковариации:

$$
  \mathrm{corr}(\xi, \eta) = \frac{\mathrm{cov}(\xi,\eta)}{\sqrt{\mathbb V \xi}\sqrt{\mathbb V \eta}}.
$$

Коэффициент корреляции всегда принимает значения из отрезка $[-1;1]$. Если $\mathrm{corr}(\xi, \eta) = 0$, то случайные величины $\xi$ и $\eta$ называют **некоррелированными**.

**Свойства дисперсии и ковариации**

1. $\mathbb V \xi \geqslant 0$, причём $\mathbb V \xi = 0 \iff \exists a\in\mathbb R \colon \mathbb P(\xi = a) = 1$.

2. $\mathbb V (a\xi) = a^2 \mathbb V \xi$, $\mathbb V(\xi + a) = \mathbb V\xi$.

3. $\mathrm{cov}(\xi, \eta) = \mathrm{cov}(\eta, \xi)$, $\mathrm{cov}(a\xi, b\eta) = ab\mathrm{cov}(\xi, \eta)$.

4. $\mathbb V(\xi + \eta) = \mathbb V \xi + \mathbb V \eta + 2\mathrm{cov}(\xi, \eta)$.

5. Если случайные величины $\xi$ и $\eta$ независимы, то $\mathrm{cov}(\xi, \eta) = 0$ и $\mathbb V(\xi + \eta) =  \mathbb V\xi + \mathbb V\eta$.

6. $\mathbb P(\vert\xi - \mathbb E\xi\vert \geqslant a) \leqslant \frac{\mathbb V \xi}{a^2}$ (**неравенство Чебышева**).

### Функции распределения и плотности

Случайная величина $\xi \colon \Omega \to \mathbb{R}$ является числовой функцией, заданной на пространстве элементарных событий; однако, больший интерес обычно представляет порождаемое ею распределение вероятностей. В дискретном случае достаточно задать вероятности отдельных значений $\mathbb{P}(\xi = x_i)$; для непрерывных же случайных величин на помощь приходят функция распределения и функция плотности.

**Функцией распределения** (**cumulative distribution function**, **cdf**) случайной величины $\xi$ называется функция
$$F_\xi(x) = \mathbb{P}(\xi \leqslant x).$$

Свойства функции распределения $F_\xi$:

- $F_\xi(-\infty) = 0$, $F_\xi(+\infty) = 1$;
- функция $F_\xi$ неубывающая;
- функция $F_\xi$ непрерывна справа: $\lim\limits_{h \to 0+ } F_\xi(x + h) = F_\xi(x)$;
- $\mathbb{P}(a < \xi \leqslant b) = F_\xi(b) - F_\xi(a)$.

Любая дискретная случайная величина имеет ступенчатую функцию распределения. К примеру, вот как выглядит график функции $F_\xi$ для $\xi \sim \mathrm{Bin}(10, 0.5)$:

<iframe src="https://yastatic.net/s3/education-portal/media/slider_binomial_cdf_21ed33b291_9b31c0ed41.html" width=100% height=500 frameborder=0></iframe>

Если непрерывная случайная величина $\xi$ имеет непрерывную плотность $p_\xi(x)$, то

$$
  F_\xi(x) - F_\xi(a) = \int\limits_a^x p_\xi(t)\, dt,
$$

откуда следует, что $F'_\xi(x) = p_\xi(x)$. В типичных случаях непрерывная случайная величина имеет гладкую возрастающую функцию распределения с двумя горизонтальными асимптотами. Вот примеры графиков функций распределения гауссовских случайных величин:

<br><br>

{% cut "Замечание о плотностях дискретных случайных величин" %}

Дискретные случайные величины не имеют плотности в описанном выше смысле. Например, возьмем $\xi$ – выпавшее число на идеальной кости. Тогда $p_\xi$ равна 0 везде кроме 1, 2, 3, 4, 5, 6. При этом вероятность выпасть 1 равна $\tfrac16$, то есть

$$
\int\limits_{1 - \varepsilon}^{1 + \varepsilon} p_\xi(x)\, dx = \frac16
$$

для любого малого $\varepsilon > 0$. Среди обычных функций мы такой плотности не найдём, однако, её можно выразить в терминах <a href="https://ru.wikipedia.org/wiki/%D0%9E%D0%B1%D0%BE%D0%B1%D1%89%D1%91%D0%BD%D0%BD%D0%B0%D1%8F_%D1%84%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D1%8F">обобщенных функций</a>:

$$
p_\xi(x) = \frac 16 \sum\limits_{k=1}^6 \delta(x-k),
$$

где $\delta(x)$ – **дельта-функция Дирака**, обладающая свойством

$$
\int\limits_{-\infty}^{+\infty} \delta(x)f(x)\,dx = f(0)
$$

для любой непрерывной функции $f$. Отсюда, в частности, следует, что

$$
\int\limits_{x_0-\varepsilon}^{x_0 + \varepsilon} \delta(x - x_0)\, dx = 1.
$$

До какой-то степени $\delta(x)$ можно представлять себе как «функцию», равную $0$ везде, кроме $x=0$, а в нуле принимающую некоторое экзотическое «бесконечное» значение.

{% endcut %}

### Медиана и мода

Математическое ожидание — не единственная числовая метрика, с помощью которой можно пытаться охарактеризовать, чему равно в среднем значение случайной величины. **Медиана** разбивает вероятностную массу распределения на две равные части. Если случайная величина $\xi$ имеет плотность $p_\xi(x)$, то её медиана $m = \mathrm{med}\xi$ определяется из условия

$$
  \mathbb P(\xi \leqslant m) = \int\limits_{-\infty}^m p_\xi(x)\,dx =
  \int\limits_m^{+\infty} p_\xi(x)\,dx = \mathbb P(\xi \geqslant m) = \frac 12.
$$

В терминах функции распределения это означает, что $F_\xi(m) = 1 - F_\xi(m)$, или $F_\xi(m) = \frac 12$. В непрерывном случае функция распределения $F_\xi(x)$ строго возрастает, поэтому уравнение $F_\xi(m) = \frac 12$ имеет единственное решение. Для дискретных случайных величин это может быть не так, и поэтому в общем случае медиану определяют как число $m$, удовлетворяющее условиям

$$
  \mathbb P(\xi \leqslant m) \geqslant \frac 12, \quad \mathbb P(\xi \geqslant m) \geqslant \frac 12.
$$

Например, если $\xi \sim \mathrm{Bern}\big(\frac 12\big)$, то $\mathbb P(\xi = 0) = \mathbb P(\xi = 1) = \frac 12$, и поэтому любое число $m \in (0, 1)$ является медианой симметричного бернуллиевского распределения. Бесконечное количество медиан будет у всякой дискретной случайной величины $\xi$, для которой $F_\xi(x) = \frac 12$ на целом промежутке.

**Мода** распределения максимизирует его pmf или pdf:

$$
  \mathrm{mode}(\xi) = \mathrm{arg}\max\limits_k \mathbb P(\xi = k) \text{ или }
  \mathrm{mode}(\xi) = \mathrm{arg}\max\limits_x p_\xi(x).
$$

Мод у распределения может быть больше одной; самое вырожденное в этом смысле распределение — равномерное, каждая точка носителя является его модой. Если плотность случайной величины имеет единственную точку максимума, то она и является модой. Например:

- $\mathrm{mode}(\xi) = \mu$, если $\xi \sim \mathcal N(\mu, \sigma^2)$;
- $\mathrm{mode}(\xi) = 0$, если $\xi \sim \mathrm{Exp}(\lambda)$;
- мода t-распределения Стьюдента также равна нулю.

Все такие распределения **унимодальны**. Если плотность $p_\xi(x)$ имеет два или более максимума, то случайная величина $\xi$ называется **бимодальной** или **мультимодальной**.

![image1_4467544f1b_d8a3e2d26b.svg](https://yastatic.net/s3/education-portal/media/image1_4467544f1b_d8a3e2d26b_25c2eea842.svg)

Для симметричных распределений вроде нормального математическое ожидание, медиана и мода совпадают, однако, в общем случае это три различные меры типичного среднего значения случайной величины. Смысл каждой из этой мер наглядно демострирует следующая иллюстрация:

![mmm_4238140d9e_5437f403f1.svg](https://yastatic.net/s3/education-portal/media/mmm_4238140d9e_5437f403f1_1ab256fcba.svg)

**Упражнение**. Найдите среднее, медиану и моду экспоненциального распределения с параметром $\lambda$ и сравните их между собой.

{% cut "Ответ" %}

Если $\xi \sim \mathrm{Exp}(\lambda)$, то

$$
\mathbb E\xi = \frac 1\lambda > \mathrm{med}(\xi) = \frac{\ln 2}\lambda >\mathrm{mode}(\xi) = 0.
$$

{% endcut %}

### Классификация случайных величин

У внимательного читателя (отягощённого математическим образованием впридачу) может возникнуть вопрос: а все ли случайные величины относятся к дискретным или непрерывным? В буквально такой постановке ответ, конечно, отрицательный, поскольку можно получить гибридную случайную величину, сложив дискретную и непрерывную. Но, может быть, всякая случайная величина равна сумме непрерывной и дискретной компонент?

В терминах функций распределения этот вопрос можно переформулировать так: верно ли, что всякая монотонная функция $F \colon \mathbb R \to \mathbb [0, 1]$ может быть представлена в виде $F = F_{\mathrm{jump}} + F_{\mathrm{smooth}}$, где $F_{\mathrm{jump}}$ — неубывающая ступенчатая функция (функция скачков), а

$$
  F_{\mathrm{smooth}}(x) = \int\limits_{-\infty}^x p(t)\,dt
$$

— гладкая возрастающая функция, полученная интегрированием плотности?

{% cut "Ответ (не открывайте, если не хотите его знать)" %}

И здесь ответ отрицательный: существуют непрерывные монотонные функции вроде <a href="https://ru.wikipedia.org/wiki/%D0%9A%D0%B0%D0%BD%D1%82%D0%BE%D1%80%D0%BE%D0%B2%D0%B0_%D0%BB%D0%B5%D1%81%D1%82%D0%BD%D0%B8%D1%86%D0%B0">лестницы Кантора</a> с производной, равной нулю почти всюду. Поскольку $\int\limits_{-\infty}^x 0\,dt = 0$, лестница Кантора не может быть получена интегрированием никакой плотности. Случайные величины с такими функциями распределения, как лестница Кантора, называются **сингулярными**. Из <a href="https://en.wikipedia.org/wiki/Lebesgue's_decomposition_theorem">теоремы Лебега о декомпозиции</a> вытекает, что любая вероятностная мера на числовой прямой может быть представлена в виде суммы дискретной, абсолютно непрерывной (имеющей плотность) и сингулярной компонент.

{% endcut %}


  ## handbook

  Учебник по машинному обучению

  ## title

  Вероятностные распределения

  ## description

  *null*

- 
  ## path

  /handbook/ml/article/mnogomernye-raspredeleniya

  ## content

  До этого мы рассматривали только одномерные распределения вероятностей на числовой прямой. Однако ничто не мешает в качестве носителя $\Omega$ выбрать пространство более высокой размерности. И снова все представляющие практический интерес распределения делятся на два класса: дискретные и непрерывные.

## Дискретные многомерные распределения

Пусть, например, эксперимент состоит из двух фаз: сначала подбрасывается монетка, а затем кубик. Тогда вероятностная масса сосредоточена в точках $(i, j)$, $i=0, 1$, $1\leqslant j \leqslant 6$. Вероятность каждого исхода можно записать в виде таблицы

#|
||


|

«Неудача»

|

«Успех»

||
||

<img src="https://yastatic.net/s3/ml-handbook/admin/1_d6deb4a9a1.svg" width = 40px>

|

$\frac 1{12}$

|

$\frac 1{12}$

||
||

<img src="https://yastatic.net/s3/ml-handbook/admin/Dice_2_b_svg_6c29bed6c9.png" width = 40px>

|

$\frac 1{12}$

|

$\frac 1{12}$

||
||

<img src="https://yastatic.net/s3/ml-handbook/admin/Dice_3_b_svg_08ff3a5f26.png" width = 40px>

|

$\frac 1{12}$

|

$\frac 1{12}$

||
||

<img src="https://yastatic.net/s3/ml-handbook/admin/Dice_4_b_svg_e5bfd0baa3.png" width = 40px>

|

$\frac 1{12}$

|

$\frac 1{12}$

||
||

<img src="https://yastatic.net/s3/ml-handbook/admin/Dice_5_b_svg_55095e524d.png" width = 40px>

|

$\frac 1{12}$

|

$\frac 1{12}$

||
||

<img src="https://yastatic.net/s3/ml-handbook/admin/2_a00a98e5e6.svg" width = 40px>

|

$\frac 1{12}$

|

$\frac 1{12}$

||
|#

Результат подбрасывания монеты моделирует бернуллиевская случайная величина $\xi$, а результат броска кубика — равномерно распределённая на множестве $\{1,2,3,4,5,6\}$ случайная величина $\eta$. Содержимое таблицы вероятностей каждого исхода можно также представить матрицей

$$  P = 
  \overbrace{\left.\begin{pmatrix}
    \frac 1{12} & \frac 1{12} & \frac 1{12} & \frac 1{12} & \frac 1{12} & \frac 1{12} \\
    \frac 1{12} & \frac 1{12} & \frac 1{12} & \frac 1{12} & \frac 1{12} & \frac 1{12} 
  \end{pmatrix}\right\}}^\eta  \xi,
$$

которая задаёт **совместное распределение** случайных величин $\xi$ и $\eta$: $\mathbb P(\xi = i, \eta = j) = P_{ij}$. Пару случайных величин $(\xi, \eta)$ в таком контексте называют также **случайным вектором**.

Элементы матрицы $P$ не обязаны совпадать; например, монета может быть несимметричной с вероятностью «успеха» $p$, и тогда таблица вероятностей примет вид

#|
||


|

«Неудача»

|

«Успех»

||
||

<img src="https://yastatic.net/s3/ml-handbook/admin/1_d6deb4a9a1.svg" width = 40px>

|

$\frac {1-p}{6}$

|

$\frac p{6}$

||
||

<img src="https://yastatic.net/s3/ml-handbook/admin/Dice_2_b_svg_6c29bed6c9.png" width = 40px>

|

$\frac {1-p}{6}$

|

$\frac p{6}$

||
||

<img src="https://yastatic.net/s3/ml-handbook/admin/Dice_3_b_svg_08ff3a5f26.png" width = 40px>

|

$\frac {1-p}{6}$

|

$\frac p{6}$

||
||

<img src="https://yastatic.net/s3/ml-handbook/admin/Dice_4_b_svg_e5bfd0baa3.png" width = 40px>

|

$\frac {1-p}{6}$

|

$\frac p{6}$

||
||

<img src="https://yastatic.net/s3/ml-handbook/admin/Dice_5_b_svg_55095e524d.png" width = 40px>

|

$\frac {1-p}{6}$

|

$\frac p{6}$

||
||

<img src="https://yastatic.net/s3/ml-handbook/admin/2_a00a98e5e6.svg" width = 40px>

|

$\frac {1-p}{6}$

|

$\frac p{6}$

||
|#

**Контрольный вопрос**. Какая таблица вероятностей соответствует эксперименту, в котором результат подбрасывания монеты «портит» кубик следующим образом: на нём могут равновероятно выпасть только значения $1$ или $2$ в случае «неудачи» и $4$, $5$ или $6$ в случае «успеха»?

{% cut "Ответ" %}

#|
||


|

«Неудача»

|

«Успех»

||
||

<img src="https://yastatic.net/s3/ml-handbook/admin/1_d6deb4a9a1.svg" width = 40px>

|
$\frac 1{4}$

|
$0$

||
||

<img src="https://yastatic.net/s3/ml-handbook/admin/Dice_2_b_svg_6c29bed6c9.png" width = 40px>

|
$\frac 1{4}$

|
$0$

||
||

<img src="https://yastatic.net/s3/ml-handbook/admin/Dice_3_b_svg_08ff3a5f26.png" width = 40px>

|
$0$

|
$0$

||
||

<img src="https://yastatic.net/s3/ml-handbook/admin/Dice_4_b_svg_e5bfd0baa3.png" width = 40px>

|
$0$

|
$\frac 1{6}$

||
||

<img src="https://yastatic.net/s3/ml-handbook/admin/Dice_5_b_svg_55095e524d.png" width = 40px>

|
$0$

|
$\frac 1{6}$

||
||

<img src="https://yastatic.net/s3/ml-handbook/admin/2_a00a98e5e6.svg" width = 40px>

|
$0$

|
$\frac 1{6}$

||
|#

{% endcut %}

В общем случае дискретное $n$-мерное распределение задаётся многомерным тензором из неотрицательных чисел $p_{i_1\ldots i_n}$, суммирующихся в единицу. Такие тензоры используются для задания совместного распределения вероятностей случайного вектора $(\xi_1, \ldots, \xi_n)$ из дискретных случайных величин:

$$ \mathbb P(\xi_1 = i_1, \xi_2 = i_2, \ldots, \xi_n = i_n) = p_{i_1i_2\ldots i_n}.
$$

## Непрерывные многомерные распределения

Непрерывное распределение на плоскости задаётся плотностью $p(x, y) \geqslant 0$; при этом вероятность события $A\subset \mathbb R^2$ равна

$$  \mathbb P(A) = \iint\limits_{A} p(x, y)\,dxdy
$$

при условии, что этот интеграл имеет смысл. Простейший пример — равномерное распределение на единичном квадрате $[0,1]^2$: его плотность равна $\mathbb I_{[0, 1]^2}(x, y)$, и

$$\mathbb P(A) = \iint \limits_{A} dxdy = \vert A\vert \text{ для } A\subset [0,1]^2.
$$

Именно так на единичном квадрате формально определяется геометрическая вероятность.

Плотность непрерывного распределения в $\mathbb R^n$ является неотрицательной функцией вида $p(x_1, \ldots, x_n)$ со свойством

$$  \int_{\mathbb R^n} p(x_1, \ldots, x_n)\,dx_1\ldots dx_n = 1.
$$

Говорят, что случайный вектор $\boldsymbol \xi = (\xi_1, \ldots, \xi_n)$ имеет **совместную плотность** $p_{\boldsymbol \xi}(x_1, \ldots, x_n)$, если

$$  \mathbb P(\boldsymbol \xi \in A) = \int\limits_A p(x_1, \ldots, x_n)\,dx_1\ldots dx_n
$$

для всех достаточно «хороших» ([измеримых по Лебегу](https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%80%D0%B0_%D0%9B%D0%B5%D0%B1%D0%B5%D0%B3%D0%B0)) множеств $A \subset \mathbb R^n$.

## Маргинальные распределения

Из совместного распределения можно получить распределение в пространстве меньшей размерности путём суммирования или интегрирования по части переменных. Например, если матрица $P_{ij}$ задаёт совместное распределение случайных величин $\xi$ и $\eta$, $P_{ij} = \mathbb P(\xi = i, \eta = j)$, то каждый из наборов чисел

$$  q_i = \sum\limits_j P_{ij}, \quad r_j = \sum\limits_i P_{ij},
$$

неотрицателен и суммируется в единицу:

$$  \sum\limits_i q_i = \sum\limits_j r_j = \sum\limits_{i, j} P_{ij} = 1.
$$

Таким образом, числа $\{q_i\}$ и $\{r_j\}$ задают некоторые распределения вероятностей, называемые **маргинальными**.

**Упражнение**. Найдите маргинальные распределения, если совместное распределение задано матрицей

$$\text{а) }
\begin{pmatrix}
    \frac {1-p}6 & \frac {1-p}6 & \frac {1-p}6 & \frac {1-p}6 & \frac {1-p}6 & \frac {1-p}6 \\
    \frac p6 & \frac p6 & \frac p6 & \frac p6 & \frac p6 & \frac p6 
  \end{pmatrix};\quad
$$

$$\text{б) }
  \begin{pmatrix}
    \frac 1{4} & \frac 14 & 0 & 0 & 0 & 0 \\
    \frac 16& \frac 16& \frac 16 & 0 & 0 & 0 \\
  \end{pmatrix}.
$$

{% cut "Ответ" %}

Суммируя столбцы этих матриц, получаем вероятности $(1-p, p)$ в случае а) и $\big(\frac 12, \frac 12\big)$ в случае б). Если же суммировать строки, то получаются наборы

$$\text{а) } \Big(\frac 16, \frac 16,\frac 16,\frac 16,\frac 16,\frac 16 \Big);\quad
\text{б) } \Big(\frac 5{12}, \frac 5{12},\frac 16,0,0,0 \Big).
$$

Заметим, что в п. а) после маргинализации получились в точности распределения вероятностей компонент случайного вектора $(\xi, \eta)$ из приведённого выше [примера](https://education.yandex.ru/handbook/ml/article/mnogomernye-raspredeleniya#diskretnye-mnogomernye-raspredeleniya). Это следствие [независимости](https://education.yandex.ru/handbook/ml/article/mnogomernye-raspredeleniya#nezavisimost-sluchajnyh-velichin) случайных величин $\xi$ и $\eta$.

{% endcut %}

В непрерывном случае ситуация похожая: если случайный вектор имеет совместную плотность $p(x,y)$, то функции

$$  q(x) = \int\limits_{-\infty}^\infty p(x, y)\,dy, \quad r(y) = \int\limits_{-\infty}^\infty p(x, y)\,dx
$$

являются плотностями маргинальных распределений.

Для $n$-мерных распределений можно находить маргинальные распределения, суммируя или интегрируя по любым наборам переменных с индексами $1\leqslant i_1 < i_2 < \ldots < i_k \leqslant n$; в результате получится маргинальное распределение по оставшимся $n-k$ переменным.

## Независимость случайных величин

Случайные величины $\xi$ и $\eta$ называются **независимыми**, если совместное распределение случайного вектора $(\xi, \eta)$ распадается на произведение одномерных. Точнее говоря,

* дискретные случайные величины $\xi$ и $\eta$ независимы, если $\mathbb P(\xi = x_i, \eta = y_j) = \mathbb P(\xi = x_i)\mathbb P(\eta = y_j)$ для всех возможных $x_i$ и $y_j$;
* непрерывные случайные величины $\xi$ и $\eta$ независимы, если их совместная плотность
  $p(x, y) = p_\xi(x)p_\eta(y)$.

Если случайные величины $\xi$ и $\eta$ независимы, то распределение каждой из них является маргинальным распределением их совместного распределения, поскольку

$$  \sum\limits_i\mathbb P(\xi = x_i)\mathbb P(\eta = y_j) = \mathbb P(\eta = y_j),
$$

$$  \sum\limits_j\mathbb P(\xi = x_i)\mathbb P(\eta = y_j) = \mathbb P(\xi = x_i),
$$

и

$$  \int\limits_{-\infty}^{+\infty} p_\xi(x)p_\eta(y) dx = p_\eta(y),
$$

$$  \int\limits_{-\infty}^{+\infty} p_\xi(x)p_\eta(y) dy = p_\xi(x).
$$

Случайные величины $(\xi_1, \ldots, \xi_n)$  **независимы в совокупности**, если их совместное распределение (совместная плотность) распадается в произведение одномерных распределений (плотностей).

**Пример**. Рассмотрим $n$ гауссовских случайных величин $\xi_k \sim \mathcal N(\mu_k, \sigma_k^2)$ с плотностями

$$  p_{\xi_k}(x_k) = \frac 1{\sqrt{2\pi}\sigma_k} e^{-\frac{(x_k - \mu_k)^2}{2\sigma_k^2}}.
$$

Совместную плотность случайного вектора $\boldsymbol \xi = (\xi_1, \ldots, \xi_n)$ определим как произведение плотностей его компонент:

$$  p_{\boldsymbol \xi}(x_1, \ldots, x_n) = p_{\xi_1}(x_1)\ldots p_{\xi_n}(x_n) = \frac 1{(2\pi)^{n/2}\sigma_1\ldots\sigma_n} e^{-\frac 12\sum\limits_{k=1}^n \frac{(x_k - \mu_k)^2}{\sigma_k^2}}.
$$

Случайный вектор $\boldsymbol \xi$ с такой плотностью имеет **многомерное нормальное (гауссовское) распределение** c независимыми в совокупности компонентами. Любое маргинальное распределение случайного вектора $\boldsymbol \xi$ обладает плотностью того же вида, и поэтому также является гауссовским.

## Характеристики случайных векторов

Математическое ожидание случайного вектора $\boldsymbol \xi = (\xi_1, \ldots, \xi_n)$ является вектором той же размерности и вычисляется покомпонентно:

$$  \mathbb E \boldsymbol \xi = (\mathbb E \xi_1, \ldots, \mathbb E\xi_n).
$$

Каждая компонента случайного вектора — это обычная случайная величина, и её среднее можно вычислить стандартными методами:

* $\mathbb E\xi_k = \sum\limits_{i_1, \ldots, i_n} i_k p_{i_1\ldots i_n}$ в дискретном случае;
* $\mathbb{E}\xi_k=\int\limits_{\mathbb{R}^n}x_kp(x_1,\ldots,x_n),dx_1\ldots dx_n$ в непрерывном случае.

Математическое ожидание перестановочно с линейным преобразованием случайного вектора: $\mathbb E(\boldsymbol{C\xi}) = \boldsymbol C \mathbb E \boldsymbol \xi$, где $\boldsymbol C$ — фиксированная матрица.

Вместо дисперсии у случайного вектора $\boldsymbol \xi = (\xi_1, \ldots, \xi_n)$ есть **матрица ковариаций**:

$$  \mathbb V \boldsymbol \xi = \mathrm{cov}(\boldsymbol \xi, \boldsymbol \xi) = \mathbb E\big(\boldsymbol \xi - \mathbb E\boldsymbol \xi\big)\big(\boldsymbol \xi - \mathbb E\boldsymbol \xi\big)^T.
$$

Матрица ковариаций симметрична и состоит из попарных ковариаций компонент случайного вектора $\boldsymbol \xi$:

$$  \mathrm{cov}(\boldsymbol \xi, \boldsymbol \xi)_{ij} = \mathrm{cov}(\xi_i, \xi_j).
$$

**Упражнение**. Докажите, что ковариационная матрица любого случайного вектора неотрицательно определена.

{% cut "Решение (не открывайте сразу, сначала попробуйте решить самостоятельно)" %}

Пользуясь линейностью математического ожидания, получаем

$$    \boldsymbol x^T\mathrm{cov}(\boldsymbol\xi, \boldsymbol\xi)\boldsymbol x = \mathbb{E}\boldsymbol x^T(\boldsymbol\xi - \mathbb{E}\boldsymbol\xi)(\boldsymbol\xi - \mathbb{E}\boldsymbol\xi)^T\boldsymbol x =
$$

$$    =\mathbb{E}\left(\boldsymbol x^T\boldsymbol\xi - \mathbb{E}(\boldsymbol x^T\boldsymbol\xi)\right)\cdot\left(\boldsymbol x^T\boldsymbol\xi - \mathbb{E}(\boldsymbol x^T\boldsymbol\xi)\right)^T = \mathrm{cov}\left(\boldsymbol x^T\boldsymbol\xi, \boldsymbol x^T\boldsymbol\xi\right)=\mathbb{V}(\boldsymbol x^T\boldsymbol\xi)\geqslant 0.
$$

{% endcut %}

Если случайные величины $\xi_1, \ldots, \xi_n$ независимы в совокупности, то $\mathrm{cov}(\xi_i, \xi_j) = 0$, и ковариационая матрица случайного вектора $\boldsymbol \xi = (\xi_1, \ldots, \xi_n)$ диагональна:

$$\mathrm{cov}(\boldsymbol \xi , \boldsymbol \xi) = \mathrm{diag}\{\mathbb V \xi_1, \ldots, \mathbb V \xi_n\}.
$$

Например, матрица ковариации гауссовского случайного вектора $\boldsymbol \xi$ с плотностью

$$p_{\boldsymbol \xi}(x_1, \ldots, x_n) = \frac 1{(2\pi)^{n/2}\sigma_1\ldots\sigma_n} e^{-\frac 12\sum\limits_{k=1}^n \frac{(x_k - \mu_k)^2}{\sigma_k^2}} = \prod\limits_{k=1}^n \frac 1{\sqrt{2\pi}\sigma_k} e^{-\frac{(x_k - \mu_k)^2}{2\sigma_k^2}}
$$

равна $\mathrm{diag}\{\sigma_1^2, \ldots, \sigma_n^2\}$, поскольку компоненты вектора $\boldsymbol \xi$ независимы в совокупности и имеют нормальное распределение $\mathcal N(\mu_k, \sigma_k^2)$.

Аналогом ковариации в многомерном случае служит матрица ковариаций между случайными векторами $\boldsymbol \xi = (\xi_1, \ldots, \xi_n)$ и $\boldsymbol \eta = (\eta_1, \ldots, \eta_n)$:

$$  \mathrm{cov}(\boldsymbol \xi, \boldsymbol \eta) = \mathbb E\big(\boldsymbol \xi - \mathbb E\boldsymbol \xi\big)\big(\boldsymbol \eta - \mathbb E\boldsymbol \eta\big)^T.
$$

Матрицу ковариаций можно также вычислить по формуле

$$  \mathrm{cov}(\boldsymbol \xi, \boldsymbol \eta) = \mathbb E\boldsymbol \xi\boldsymbol \eta^T - \mathbb E\boldsymbol \xi(\mathbb E\boldsymbol \eta)^T.
$$

**Упражнение**. Пусть случайный вектор $\boldsymbol \eta$ получен из случайного вектора $\boldsymbol \xi$ линейным преобразованием: $\boldsymbol \eta = \boldsymbol {C\xi}$. Как связаны между собой их ковариационные матрицы?

{% cut "Решение (не открывайте сразу, сначала попробуйте решить самостоятельно)" %}

Распишем по определению:

$$  \mathrm{cov}(\boldsymbol{C\xi}, \boldsymbol{C\xi}) = \mathbb{E}\big(\boldsymbol{C\xi} - \mathbb{E}(\boldsymbol{C\xi})\big)\big(\boldsymbol{C\xi} - \mathbb{E}(\boldsymbol{C\xi})\big)^T =
$$

$$  =\mathbb{E}\boldsymbol C(\boldsymbol \xi - \mathbb{E}\boldsymbol \xi)(\boldsymbol \xi - \mathbb{E}\boldsymbol \xi)^T\boldsymbol C^T = \boldsymbol C\mathrm{cov}(\boldsymbol \xi, \boldsymbol \xi)\boldsymbol C^T.
$$

{% endcut %}

## Преобразования плотностей случайных векторов

Нередко приходится иметь дело не с самими случайными векторами, а с функциями от них. Но как найти плотность случайного вектора $\boldsymbol \eta = g(\boldsymbol \xi)$, зная плотность $p_{\boldsymbol \xi}(\boldsymbol x)$?

Предположим, что $g \colon \mathbb R^n \to \mathbb R^n$ — гладкая обратимая функция. Тогда для измеримого $A\subset \mathbb R^n$ имеем

$$\mathbb{P}(\boldsymbol{\eta}\in A)=\mathbb{P}\big(g(\boldsymbol{\xi})\in A\big)=\mathbb{P}\big(\boldsymbol{\xi}\in g^{-1}(A)\big)=\int\limits_{g^{-1}(A)}p_{\boldsymbol{\xi}}(x)d\boldsymbol{x}
$$

Чтобы перейти к интегралу по $A$, сделаем замену переменной $\boldsymbol x = g^{-1}(\boldsymbol z)$. По формуле замены координат в кратном интеграле получаем

$$\int\limits_{g^{-1}(A)}p_{\boldsymbol \xi}(\boldsymbol x)d\boldsymbol x = \int\limits_{A}p_{\boldsymbol \xi}(g^{-1}(\boldsymbol z))\vert \det J(\boldsymbol z) \vert d\boldsymbol z,
$$

где $\det J(\boldsymbol z)$ – якобиан преобразования $g^{-1}(\boldsymbol z)$, т.е. определитель матрицы Якоби $J(\boldsymbol z) = \frac{\partial g^{-1}(\boldsymbol z)}{\partial \boldsymbol z}$.
Таким образом,

$$p_{\boldsymbol \eta}(\boldsymbol z) = p_{\boldsymbol \xi}(g^{-1}(\boldsymbol z))\vert \det J(\boldsymbol z)\vert.
$$

**Упражнение**. Пусть $\boldsymbol \xi$ – случайный вектор с плотностью $p_{\boldsymbol \xi}(\boldsymbol x)$. Какова плотность случайного вектора $\boldsymbol\eta = \boldsymbol\mu + \boldsymbol{C\xi}$, где $\boldsymbol \mu$ – постоянный вектор, а $\boldsymbol C$ – постоянная обратимая матрица?

{% cut "Решение (не открывайте сразу, сначала попробуйте решить самостоятельно)" %}

В данном случае $g(\boldsymbol x) = \boldsymbol\mu + \boldsymbol{Cx}$, $g^{-1}(\boldsymbol z) = \boldsymbol C^{-1}(\boldsymbol z - \boldsymbol\mu)$. Матрица Якоби преобразования $g^{-1}$ равна $\boldsymbol C^{-1}$. Следовательно,

$$  p_{\boldsymbol \eta}(\boldsymbol z) = \frac1{\vert\det(\boldsymbol C)\vert}p_{\boldsymbol \xi}(\boldsymbol C^{-1}(\boldsymbol z - \boldsymbol\mu)).
$$

{% endcut %}

## Распределение суммы независимых случайных величин

В дискретном случае найти распределение суммы двух независимых случайных величин несложно. В самом деле,

$$\mathbb{P}(\xi + \eta = k) = \sum_{i}\mathbb{P}(\xi + \eta = k, \eta = i)=
\sum_{i}\mathbb{P}(\xi = k - i, \eta = i).
$$

В силу независимости случайных величин $\xi$ и $\eta$ последняя сумма равна

$$\sum_{i}\mathbb{P}(\xi = k-i)\mathbb{P}(\eta = i).
$$

Полученная формула называется **формулой свёртки**.

Пусть теперь $\xi_1$ и $\xi_2$ – независимые непрерывные случайные величины с плотностями $p_{\xi_1}(x)$ и $p_{\xi_2}(x)$ соответственно. Сам собой напрашивается аналог формулы свёртки с плотностями вместо вероятностей, но чтобы достаточно строго вывести его и не запутаться, мы немного схитрим. А именно, мы рассмотрим случайный вектор $\boldsymbol\xi = (\xi_1, \xi_2)^T$ и его (обратимое!) преобразование

$$g(\boldsymbol\xi) = \begin{pmatrix}\xi_1 + \xi_2\\ \xi_2\end{pmatrix} = \begin{pmatrix}1 & 1\\ 0& 1\end{pmatrix}\boldsymbol \xi =: \boldsymbol\eta = \begin{pmatrix}\eta_1 \\ \eta_2\end{pmatrix}.
$$

Обратное к нему будет иметь вид

$$h(\boldsymbol \eta) = \begin{pmatrix}1 & -1\\ 0 & 1\end{pmatrix}\boldsymbol\eta = \begin{pmatrix} \eta_1 - \eta_2\\ \eta_2\end{pmatrix}
$$

Тогда по правилу преобразования плотности

$$p_{\boldsymbol\eta}(\boldsymbol z) = \underbrace{\left|\text{det}\begin{pmatrix}1 & -1 \\ 0 & 1\end{pmatrix}\right|}_{=1}p_{\boldsymbol\xi}\left(z_1 - z_2, z_2\right) =
p_{\xi_1}(z_1 - z_2)p_{\xi_2}(z_2),
$$

где в последнем равенстве мы воспользовались независимостью $\xi_1$ и $\xi_2$. Распределение случайной величины $\eta_1 = \xi_1+\xi_2$ – это маргинальное распределение, которое вычисляется следующим образом:

$$p_{\eta_1}(y) = \int\limits_{-\infty}^{+\infty}p_{\xi_1}(y - x)p_{\xi_2}(x)dx.
$$

Эта формула также называется **формулой свёртки**.

## Примеры многомерных распределений

Рассмотрим несколько популярных распределений случайных векторов.

### Мультиномиальное распределение

Биномиальное распределение $\mathrm{Bin}(n, p)$ моделирует $n$-кратное подбрасывание монеты с вероятностями «успеха» $p$ и «неудачи» $q = 1-p$. **Мультиномиальное распределение** обобщает этот эксперимент: теперь подбрасывается кубик с $k\geqslant 2$ гранями, и вероятность выпадения $i$-й грани равна $p_i$, $\sum\limits_{i=1}^k p_i = 1$. Обозначим через $\xi_i$ количество выпадений $i$-й грани в серии из $n$ бросков. Тогда случайный вектор $\boldsymbol \xi = (\xi_1, \ldots, \xi_k)$ имеет мультиномиальное распределение, при котором

$$  \mathbb P(\xi_1 = m_1, \ldots, \xi_k = m_k) = \frac{n!}{m_1!\cdot \ldots \cdot m_k!} p_1^{m_1}\cdot \ldots \cdot p_k^{m_k},
$$

$$  \sum\limits_{i = 1}^k m_i = n.
$$

При $n=1$ мультиномиальное распределение превращается в **категориальное**, известное также под названием **multinoulli**. Категориальное распределение моделирует случайный выбор одного из $k$ классов с заданными вероятностями $(p_1, \ldots, p_k)$.

### Многомерное нормальное распределение

**Многомерное нормальное (гауссовское) распределение** задаётся функцией плотности

$$p(\boldsymbol x) = \frac1{(2\pi)^{n/2}\sqrt{\det\boldsymbol\Sigma}}\exp\left(-\frac12(\boldsymbol x - \boldsymbol\mu)^T\boldsymbol\Sigma^{-1}(\boldsymbol x - \boldsymbol\mu)\right),
$$

где $\boldsymbol x, \boldsymbol \mu\in\mathbb{R}^n$, $\boldsymbol\Sigma$ — невырожденная симметричная матрица размера $n\times n$. Такое распределение обозначается $\mathcal{N}(\boldsymbol\mu, \boldsymbol\Sigma)$.

Если случайный вектор $\boldsymbol \xi \sim \mathcal{N}(\boldsymbol\mu, \boldsymbol\Sigma)$, то $\mathbb E\boldsymbol \xi =\boldsymbol \mu$, $\mathrm{cov}(\boldsymbol \xi, \boldsymbol \xi ) = \boldsymbol \Sigma$; таким образом, параметры гауссовского распределения — это его среднее и матрица ковариаций.

**Упражнение.** Пусть $\boldsymbol \xi \sim \mathcal{N}(\boldsymbol\mu, \boldsymbol\Sigma)$ и $\boldsymbol \eta =  \boldsymbol{A\xi} + \boldsymbol b$. Докажите, что $\boldsymbol \eta \sim \mathcal{N}(\boldsymbol{A\mu} + \boldsymbol b, \boldsymbol{A\Sigma A}^T)$.

{% cut "Решение (не открывайте сразу, сначала попробуйте решить самостоятельно)" %}

Если бы нам стало известно, что вектор $\boldsymbol \eta$ гауссовский, то мы нашли бы его параметры по стандартным формулам:

$$  \mathbb E\boldsymbol \eta = \mathbb E (\boldsymbol{A\xi} + \boldsymbol b) =  \boldsymbol A\mathbb E \boldsymbol \xi + \boldsymbol b = \boldsymbol{A\mu} + \boldsymbol b,
$$

$$  \mathrm{cov}(\boldsymbol \eta, \boldsymbol \eta) = \mathrm{cov}(\boldsymbol{A\xi} + \boldsymbol b, \boldsymbol{A\xi} + \boldsymbol b) = \boldsymbol A \mathrm{cov}(\boldsymbol \xi, \boldsymbol \xi) \boldsymbol A^T = \boldsymbol{A\Sigma A}^T.
$$

Решим задачу честно в предположении, что матрица $\boldsymbol A$ квадратная и невырожденная. Для этого воспользуемся формулой плотности линейного преобразования случайного вектора:

$$p_{\boldsymbol \eta}(\boldsymbol z) = \frac1{\vert\!\det(\boldsymbol A)\vert}p_{\boldsymbol \xi}(\boldsymbol A^{-1}(\boldsymbol z - \boldsymbol b)) =
$$

$$=
\frac1{(2\pi)^{n/2}\sqrt{\det\boldsymbol\Sigma}\vert\det(\boldsymbol A)\vert}\exp\left(-\frac12(\boldsymbol A^{-1}\boldsymbol z - \boldsymbol A^{-1}\boldsymbol b - \boldsymbol\mu)^T\boldsymbol\Sigma^{-1}(\boldsymbol A^{-1}\boldsymbol z - \boldsymbol A^{-1}\boldsymbol b  - \boldsymbol\mu)\right) =
$$

$$=
\frac1{(2\pi)^{n/2}\sqrt{\det\boldsymbol{A\Sigma A}^T}}\exp\left(-\frac12(\boldsymbol z - \boldsymbol b - \boldsymbol{A\mu})^T \boldsymbol A^{-T}\boldsymbol\Sigma^{-1}\boldsymbol A^{-1}(\boldsymbol z - \boldsymbol b  - \boldsymbol{A\mu})\right).
$$

В полученном выражении нетрудно узнать плотность гауссовского распределения $\mathcal{N}(\boldsymbol{A\mu} + \boldsymbol b, \boldsymbol{A\Sigma A}^T)$.

Заметим, что утверждение сохраняет силу и для случая прямоугольной матрицы $\boldsymbol A$ размера $m\times n$, где $n$ — размерность случайного вектора $\boldsymbol \xi$.

{% endcut %}

Важный частный случай случайного гауссовского вектора с независимыми компонентами был рассмотрен в примере из секции про <a href="https://education.yandex.ru/handbook/ml/article/mnogomernye-raspredeleniya#nezavisimost-sluchajnyh-velichin">независимость случайных величин</a>. Такое распределение получается, если матрица $\boldsymbol\Sigma$ диагональна, $\boldsymbol\Sigma = \mathrm{diag}\{\sigma_1^2, \ldots, \sigma_n^2\}$. Тогда $\sqrt{\det \boldsymbol\Sigma} = \sigma_1 \ldots \sigma_n$, $\boldsymbol\Sigma^{-1} = \mathrm{diag}\big\{\frac1{\sigma_1^2}, \ldots, \frac 1{\sigma_n^2}\big\}$, и поэтому

$$-\frac12(\boldsymbol x - \boldsymbol\mu)^T\boldsymbol\Sigma^{-1}(\boldsymbol x - \boldsymbol\mu) = -\frac 12 \sum\limits_{k=1}^n \frac{(x_k-\mu_k)^2}{\sigma_k^2}.
$$

Отсюда снова получаем формулу совместной плотности

$$  p_{\boldsymbol \xi}(\boldsymbol x) = \frac 1{(2\pi)^{n/2}\sigma_1\ldots\sigma_n} e^{-\frac 12\sum\limits_{k=1}^n \frac{(x_k - \mu_k)^2}{\sigma_k^2}},
$$

которую можно переписать в виде

$$  \prod\limits_{k=1}^n \frac 1{\sqrt{2\pi}\sigma_k} e^{-\frac{(x_k - \mu_k)^2}{2\sigma_k^2}} = \prod\limits_{k=1}^n p_{\xi_k}(x_k),
  \xi_k \sim \mathcal N(\xi_k, \sigma_k^2),
$$

откуда следует независимость в совокупности компонент вектора $\boldsymbol \xi$.

Если ковариационная матрица $\boldsymbol \Sigma$ не является диагональной, то отдельные компоненты случайного вектора $\boldsymbol \xi \sim \mathcal{N}(\boldsymbol\mu, \boldsymbol\Sigma)$ зависимы. Тем не менее, всегда найдётся линейное (и даже ортогональное) преобразование, которое превратит вектор $\boldsymbol \xi$ в гауссовский вектор с независимыми компонентами. Для этого достаточно найти ортогональную матрицу $\boldsymbol Q$ со свойством

$$\boldsymbol Q \boldsymbol \Sigma \boldsymbol Q^T = \mathrm{diag}\big\{\sigma_1^2,\ldots,\sigma_n^2\big\},
$$

и далее воспользоваться формулой плотности линейного преобразования гауссовского вектора.

По тем же соображениям облако точек, сгенерированных из распределения $\mathcal{N}(\boldsymbol\mu, \boldsymbol\Sigma)$, будет напоминать эллипсоид с полуосями, пропорциональными вектору $(\sigma_1^2,\ldots,\sigma_n^2)$. Линии уровня плотности $p(\boldsymbol x)$ задаются уравнениями вида $p(\boldsymbol x) = C$, а такое равенство эквивалентно квадратичной форме

$$(\boldsymbol x - \boldsymbol\mu)^T\boldsymbol\Sigma^{-1}(\boldsymbol x - \boldsymbol\mu) = C_1,
$$

где $C$ и $C_1$ – некоторые константы. С помощью описанной выше ортогональной замены эта квадратичная форма может быть приведена к главным осям:

$$  \boldsymbol z^T \boldsymbol\Lambda^{-1} \boldsymbol z = C_2, \quad \boldsymbol \Lambda = \mathrm{diag}\big\{\sigma_1^2,\ldots,\sigma_n^2\big\};
$$

в координатах это выглядит как

$$  \sum\limits_{l=1}^n \frac{z_k^2}{\sigma_k^2} = C_2.
$$

Мы получили практически каноническое уравнение $n$-мерного эллипсоида. В $\mathbb R^2$ это будут эллипсы, сплюснутые тем сильнее, чем дальше от единицы отношение $\kappa = \frac{\sigma_1}{\sigma_2}$ собственных значений матрицы $\boldsymbol \Sigma$.

<iframe src="https://yastatic.net/s3/ml-handbook/admin/slider_gaussian_2d_044a5877b0.html" width=100% height=500 frameborder=0></iframe>

Нормальным будет и всякое маргинальное распределение многомерного гауссовского вектора.

**Упражнение**. Пусть случайный вектор $\boldsymbol \xi = (\boldsymbol\xi_1, \boldsymbol \xi_2)$ имеет гауссовское распределение с параметрами

$$  \boldsymbol \mu = \begin{pmatrix}\boldsymbol \mu_1 \\ \boldsymbol \mu_2 \end{pmatrix}, \quad
  \boldsymbol \Sigma = \begin{pmatrix}
  \boldsymbol \Sigma_{11} & \boldsymbol\Sigma_{12} \\ \boldsymbol\Sigma_{12}^T & \boldsymbol\Sigma_{22}  \end{pmatrix},
$$

где $\boldsymbol \xi_1, \boldsymbol \mu_1 \in \mathbb R^k$, $\boldsymbol \xi_2, \boldsymbol \mu_2 \in \mathbb R^{n-k}$, $\boldsymbol \Sigma_{11} \in \mathrm{Mat}_{k\times k}$, $\boldsymbol \Sigma_{12} \in \mathrm{Mat}_{k\times (n-k)}$, $\boldsymbol \Sigma_{22} \in \mathrm{Mat}_{(n-k)\times (n-k)}$.
Докажите, что случайный вектор $\boldsymbol\xi_1$, полученный маргинализацией по компонентам вектора $\boldsymbol\xi_2$, является гауссовским с параметрами $\boldsymbol \\mu_1$ и $\boldsymbol \Sigma_{11}$.

{% cut "Решение (не открывайте сразу, сначала попробуйте решить самостоятельно)" %}

Существует прямое и довольно утомительное решение с многочисленными матричными манипуляциями. Мы поступим хитрее: рассмотрим маргинализацию как линейное преобразование

$$  \boldsymbol\xi_1 =  \boldsymbol A\boldsymbol\xi, \text{ где } \boldsymbol A = \begin{pmatrix}\boldsymbol I_{k} & \boldsymbol 0_{k\times(n-k)}\end{pmatrix} \in \mathrm{Mat}_{k\times n},
$$

и воспользуемся результатом предыдущего упражнения. Имеем $\boldsymbol A\boldsymbol\mu = \boldsymbol I_{k}\boldsymbol \mu_1 =  \boldsymbol \mu_1$, $\boldsymbol A \boldsymbol \Sigma \boldsymbol A^T  = \boldsymbol I_{k} \boldsymbol \Sigma_{11}\boldsymbol I_{k}^T = \boldsymbol \Sigma_{11}$, и поэтому
$\boldsymbol\xi_1 \sim \mathcal N(\boldsymbol \mu_1, \boldsymbol \Sigma_{11})$.

{% endcut %}

### Распределение Дирихле

**Распределение Дирихле** сосредоточено на $K$-мерном симплексе

$$\{(x_1,\ldots,x_K)\colon x_1 + \ldots + x_K = 1,\; x_i\geqslant 0\}.
$$

Плотность распределения Дирихле $\mathrm{Dir}(\boldsymbol \alpha)$ равна

$$p(x_1,\ldots,x_K) = \frac1{B(\boldsymbol \alpha)}\prod_{i=1}^Kx_i^{\alpha_i - 1},
$$

где $\boldsymbol\alpha = (\alpha_1,\ldots,\alpha_K)$ – вектор положительных параметров, а $B(\boldsymbol\alpha) = \frac{\prod_i\Gamma(\alpha_i)}{\Gamma(\sum_i\alpha_i)}$ – многомерная бета-функция. Если $\boldsymbol \xi \sim \mathrm{Dir}(\boldsymbol \alpha)$,
то

$$  \mathbb E \boldsymbol \xi =\frac{\boldsymbol \alpha}{\alpha_0}, \quad 
  \mathrm{cov} (\xi_i, \xi_j)=\frac{\alpha_0 \delta_{ij} - \alpha_i\alpha_j}{\alpha_0^2(\alpha_0 + 1)}, \quad \alpha_0 = \sum\limits_{k=1}^K \alpha_k.
$$

{% cut "Иллюстрация распределения Дирихле с помощью схемы Пойя" %}

Пусть у нас есть $K$ категорий и на них задано вероятностное распределение

$$\boldsymbol q^{(1)} = \frac{\boldsymbol\alpha}{\alpha_0} = \left(\frac{\alpha_1}{\alpha_0},\ldots,\frac{\alpha_K}{\alpha_0}\right),
$$

где $\alpha_0 = \sum\limits_{i=1}^K\alpha_i$. Это корректное распределение вероятностей, так как его компоненты неотрицательны и в сумме дают $1$. Будем производить следующий процесс:

* В первый момент генерируем одну из категорий с помощью распределения $\boldsymbol q^{(1)}$; допустим, выпала $i_1$-я. Обновляем вероятностное распределение на категориях, прибавив единицу к $i_1$-й компоненте вектора $\boldsymbol\alpha$; получаем вектор $\boldsymbol\alpha^{(2)}$.
* На $n$-м шаге генерируем одну из категорий с помощью распределения $\boldsymbol q^{(n)} = \frac{\boldsymbol \alpha^{(n)}}{\sum\limits_i\alpha^{(n)}_i}$. Допустим, выпала $i_n$-я. Обновляем вероятностное распределение на категориях, прибавив единицу к $i_n$-й компоненте вектора $\boldsymbol\alpha^{(n)}$; получаем вектор $\boldsymbol\alpha^{(n+1)}$.

Можно доказать, что вектор $\lim\limits_{n\to\infty} \boldsymbol q^{(n)}$ подчиняется распределению Дирихле $\mathrm{Dir}(\boldsymbol \alpha)$.

Чтобы стало чуть понятнее, проследим, что будет при различных $\boldsymbol\alpha$.

* Если $\boldsymbol\alpha = (10,10,10)$, то прибавление единицы будет не так сильно смещать вероятности, и дальше мы будем продолжать генерировать категорию из распределения, близкого к равномерному. Скорее всего, в пределе мы будем получать что-то, близкое к $(\frac13, \frac13,\frac13)$.
* Если $\boldsymbol\alpha = (1,1,20)$, то почти наверняка мы будем генерить третью категорию, причём со всё большей вероятностью (ведь при этом мы будем увеличивать $\alpha^{(n)}_3$), то есть в пределе будет (почти $0$, почти $0$, почти $1$).
* Если $\boldsymbol\alpha = (0.1,0.1,0.1)$, то та категория, которую мы сгенерировали на первом шаге, сразу вырвется вперёд и скорее всего будет доминировать в дальнейшем. Таким образом, нам следует ожидать в пределе векторов, в которых одна из компонент почти $1$, а остальные почти $0$. Важным отличием от предыдущего варианта является то, что здесь почти $1$ может быть в любой компоненте.
* Если $\boldsymbol\alpha = (1,1,1)$, то соответствующее распределение Дирихле будет равномерным.

Также вам может оказаться полезна визуализация плотности этого распределения при разных $\boldsymbol\alpha$:

<figure>
<img src="https://yastatic.net/s3/ml-handbook/admin/page1_1500px_Dirichlet_pdf_961f276adf.jpg" loading="lazy" decoding="async" alt="">
<figcaption>
  <a href="https://en.wikipedia.org/wiki/File:Dirichlet.pdf">ссылка на источник картинки</a>
</figcaption>
</figure>

{% endcut %}

  ## handbook

  Учебник по машинному обучению

  ## title

  Многомерные распределения

  ## description

  *null*

- 
  ## path

  /handbook/ml/article/nezavisimost-i-uslovnye-raspredeleniya-veroyatnostej

  ## content

  В этом параграфе описываются, пожалуй, главные фичи теории вероятностей: независимые события и условные вероятности. Эти концепции имеют большое прикладное значение, да и с теоретической точки зрения главным образом благодаря им теория вероятностей выделяется в отдельную ветвь математики.

## Условная вероятность

Условная вероятность возникает при ответе на вопрос о том, каковы шансы события $A$ при условии,что случилось событие $B$, и обозначается $\mathbb P(A\vert B)$.

**Пример**. Согласно исследованиям, в среднем $5\%$ пациентов испытывают приступы кашля в течение дня, однако среди курильщиков доля кашляющих составляет $40\%$. То есть (безусловная) вероятность $\mathbb P(\text{кашляет}) = 0.05$ при добавлении обусловливания может существенно измениться: $\mathbb P(\text{кашляет}\vert\text{курит}) = 0.4$.

**Упражнение**. Известно, что в семье два ребёнка, причём один из них мальчик. Какова вероятность, что другой ребёнок тоже мальчик?

{% cut "Ответ (не открывайте сразу, попробуйте сначала решить самостоятельно)" %}

Как ни странно, ответ вовсе не $50\%$. Пол новорождённого ребёнка можно приближённо считать результатом испытания Бернулли с вероятностью успеха $\frac 12$.
Из четырёх возможных вариантов ММ, МД, ДМ, ДД условию удовлетворяют только первые три, и лишь в одном случае из этих трёх второй ребёнок тоже мальчик. Поэтому правильный ответ — $\frac 13$.

Добавляя формализма, обозначим

$$  A = \{\text{хотя бы один ребёнок — мальчик}\}
$$

$$  B = \{\text{мальчики оба ребёнка}\},
$$

и тогда условная вероятность $\mathbb P(B\vert A)$ вычисляется по формуле

$$  \mathbb P(B\vert A) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(A)} = \frac{1/4}{3/4} = \frac 13.
$$

{% endcut %}

В общем случае условная вероятность $\mathbb P(B\vert A)$ при $\mathbb P(A) \ne 0$ полагается равной

$$  \mathbb{P}(B \vert A) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(A)}.
$$

В зависимости от соотношения событий $A$ и $B$ условная вероятность $\mathbb{P}(B \vert A)$ может принимать разные значения, например:

* если $A\cap B = \varnothing$, то событие $A$ исключает реализацию события $B$, и $\mathbb{P}(B \vert A) = 0$;
* если $A \subset B$, то событие $A$ гарантирует осуществление события $B$, и $\mathbb{P}(B \vert A) = 1$.

Разумеется, чаще всего события $A$ и $B$ соотносятся между собой более хитрым образом, и значение условной вероятности $\mathbb{P}(B \vert A)$ находится строго между $0$ и $1$.

## Формула полной вероятности

Пусть пространство $\Omega$ разбивается на попарно несовместные события $B_1, B_2, \dots, B_n$:

$$  \Omega = B_1 \cup \ldots \cup B_n, \quad B_i \cap B_j = \varnothing \text{ при } i\ne j.
$$

Тогда

$$A = A\cap\Omega = (A\cap B_1) \cup \ldots \cup (A\cap B_n);
$$

отсюда по свойству конечной аддитивности находим, что

$$  \mathbb P(A) = \mathbb{P}(A \cap B_1)  + \ldots + \mathbb{P}(A \cap B_n).
$$

Переходя к условным вероятностям, получаем **формулу полной вероятности**:

$$  \mathbb{P}(A) = \sum\limits_{k=1}^n \mathbb{P}(A \vert B_k) \mathbb{P}(B_k).
$$

**Пример**. Среди населения $33.7\%$ имеют первую группу крови, $37.5\%$ — вторую, $20.9\%$ — третью, $7.9\%$ — четвёртую. При переливании крови надо учитывать группы крови донора и рецепиента:

* реципиенту с четвёртой группой крови можно перелить кровь любой группы;
* реципиентам со второй и третьей группами можно перелить кровь той же группы или первой;
* реципиентам с первой группой крови можно перелить только кровь первой группы.

С какой вероятностью допустимо переливание в случайно взятой паре донор—реципиент?

**Решение**. Пусть событие $A$ состоит в том, что переливание возможно, а событие $B_k$ — в том, что донор имеет группу $k$. По формуле полной вероятности

$$  \mathbb P(A) = \mathbb P(A\vert B_1) \mathbb P(B_1) + \mathbb P(A\vert B_2) \mathbb P(B_2) +
  \mathbb P(A\vert B_3) \mathbb P(B_3) + \mathbb P(A\vert B_4) \mathbb P(B_4).
$$

Вероятности $\mathbb P(B_k)$ даны в условии, оттуда же находим, что

$$  \mathbb P(A\vert B_1) = 1,
$$

$$  \mathbb P(A\vert B_2) =  \mathbb P(B_2) + \mathbb P(B_4),
$$

$$  P(A\vert B_3) =  \mathbb P(B_3) + \mathbb P(B_4),
$$

$$  \mathbb P(A\vert B_4) = \mathbb P(B_4). 
$$

Подставляя численные значения, получаем

$$  \mathbb P(A) = 0.337 + (0.375+0.079)\cdot 0.375 + (0.209+0.079)\cdot 0.209 + 0.079^2 = 0.573683.
$$

**Упражнение**. Решите предыдущий пример, выбирая в качестве разбиения набор событий $C_k$, каждое из которых заключается в том, что реципиент имеет группу $k$.

{% cut "Ответ" %}

По той же формуле полной вероятности получаем, что

$$  \mathbb P(A) = \mathbb P(A\vert С_1) \mathbb P(С_1) + \mathbb P(A\vert С_2) \mathbb P(С_2) +
  \mathbb P(A\vert С_3) \mathbb P(С_3) + \mathbb P(A\vert С_4) \mathbb P(С_4).
$$

Ясно, что $\mathbb P(C_k) = \mathbb P(B_k)$; далее из условия находим, что

$$  \mathbb P(A\vert С_1) = \mathbb P(C_1),
$$

$$  \mathbb P(A\vert C_2) =  \mathbb P(C_1) + \mathbb P(C_2),
$$

$$  P(A\vert C_3) =  \mathbb P(C_1) + \mathbb P(C_3),
$$

$$  \mathbb P(A\vert C_4) = 1. 
$$

Подставляя численные значения, получаем тот же ответ $\mathbb P(A) = 0.573683$.

{% endcut %}

Формула полной вероятности легко обобщается на случай счётного числа попарно несовместных событий $B_k$, а также на случай обусловливания по некоторому событию $C$, например:

$$  \mathbb{P}(A\vert C) = \sum\limits_n \mathbb{P}(A \vert B_n, C) \mathbb{P}(B_n \vert C).
$$

## Формула Байеса

Заметим, что вероятность $\mathbb{P}(A \cap B)$ можно записать двумя способами

$$  \mathbb{P}(B \vert A)\mathbb{P}(A) = \mathbb{P}(A \cap B) = \mathbb{P}(A \vert B)\mathbb{P}(B).
$$

Оставим $\mathbb{P}(B \vert A)$ в левой части и получим формулу Байеса.

**Формула Байеса**. Для любых событий $A$, $B$ c положительной вероятностью

$$  \mathbb{P}(B \vert A) = \frac{\mathbb{P}(A \vert B)\mathbb{P}(B)}{\mathbb{P}(A)}.
$$

Для вычисления знаменателя в формуле Байеса часто используется формула полной вероятности.

**Упражнение**. Среди определенной группы людей вероятность некоторой болезни 0.02. Тест, позволяющий выявить болезнь, несовершенен. На больном он дает позитивный результат в 98 случаях из 100, и, кроме того, он дает позитивный результат в 4 случаях из 100 на здоровом. Найдите вероятность того, что человек, на котором тест дал положительный результат, действительно болен.

{% cut "Ответ (не открывайте сразу, попробуйте сначала решить самостоятельно)" %}

По формуле Байеса,

$$  \mathbb{P}(\text{болен}\vert\text{+}) = \frac{\mathbb{P}(\text{+}\vert\text{болен}) \mathbb{P}(\text{болен})}{\mathbb{P}(\text{+})}.
$$

По условию, $\mathbb{P}(\text{+} \vert \text{болен}) = 0.98$, $\mathbb{P}(\text{болен}) = 0.02$. Чтобы посчитать вероятность теста быть положительньным, применим формулу полной вероятности:

$$  \mathbb{P}(\text{+}) = \mathbb{P}(\text{+} \vert \text{болен}) \mathbb{P}(\text{болен}) + \mathbb{P}(\text{+} \vert \text{здоров})\mathbb{P}(\text{здоров}) = 0.98 \cdot 0.02 + 0.04 \cdot 0.98 = 0.98 \cdot 0.06.
$$

Тогда по формуле Байеса

$$  \mathbb{P}(\text{болен} \vert \text{+}) = \frac{0.98 \cdot 0.02}{0.98 \cdot 0.06} = \frac13.
$$

Получается, что точность теста очень низка — всего лишь около 1 из 3. Это происходит, потому что больные люди встречаются редко (2 из 100), и эта частота сравнима с долей ошибок I и II рода — 0.02 и 0.04.

{% endcut %}

Для непрерывного случая тоже есть своя формула полной вероятности, см. раздел про [условную вероятность](https://education.yandex.ru/handbook/ml/article/nezavisimost-i-uslovnye-raspredeleniya-veroyatnostej#uslovnye-raspredeleniya).

## Независимые события

События $A$ и $B$ называются **независимыми**, если
$\mathbb{P}(A \vert B) = \mathbb{P}(A)$, то есть информация о реализации события $B$ никак не влияет на вероятность события $A$.

По определению условной вероятности независимость событий $A$ и $B$ эквивалентна тому, что

$$  \mathbb{P}(A \cap B) = \mathbb{P}(A) \mathbb{P}(B).
$$

Последнее равенство годится для определения независмости событий $A$ и $B$ даже в том случае, если \$\\mathbb{P}(A) = 0 \$ или $\mathbb{P}(B) = 0$.

**Пример**. В полной колоде карт находится $52$ карты: $4$ масти от двойки до туза. Вероятность вытащить туза равна $\mathbb P(\mathrm{Ace}) = \frac 4{52} = \frac 1{13}$, карту пиковой масти — $\mathbb P(\spadesuit) = \frac {13}{52} = \frac 1{4}$. Эти события независимы, поскольку в пересечении этих событий лежит ровно одна карта — туз пик, вероятность появления которого равна $\frac 1{52} = \frac 1{13} \cdot \frac 14 = \mathbb P(\mathrm{Ace})\mathbb P(\spadesuit)$.

Пусть теперь вытаскивается сразу две карты. Зависимы ли события «вытащены две карты пиковой масти» и «вытащены туз и король»? Посчитаем:

$$  \mathbb P(\spadesuit \spadesuit) = \frac{\binom{13}2}{\binom{52}2} = \frac{13\cdot 12}{52\cdot 51} = \frac 1{17},
$$

$$  \mathbb P(\mathrm{AK}) = \frac{16}{\binom{52}2} = \frac{32}{52\cdot 51} = \frac 8{663}.
$$

Вероятность вытащить туза и короля пик равна $\frac 1{\binom{52}2} = \frac 1{1326}\approx 0.00075$, что отличается от $\mathbb P(\spadesuit \spadesuit)\mathbb P(\mathrm{AK})  = \frac 8{11271} \approx 0.00071$. Таким образом, эти события зависимы.

События $A_1, \ldots, A_n$ **попарно независимы**, если $\mathbb{P}(A_i \cap A_j) = \mathbb{P}(A_i) \mathbb{P}(A_j)$ при $i \ne j$. Эти же события **независимы в совокупности**, если

$$  \mathbb P\big(A_{i_1}\cap \ldots \cap A_{i_m}\big) = \prod\limits_{k=1}^m \mathbb P(A_{i_k})
$$

$$  \text{ для любого набора индексов } 1\leqslant i_1 < \ldots < i_m\leqslant n.
$$

**Упражнение**. Приведите пример попарно независимых событий $A_1$, $A_2$, $A_3$, не являющихся независимыми в совокупности.

{% cut "Ответ (не открывайте сразу, попробуйте сначала решить самостоятельно)" %}

Раскрасим тетраэдр в три цвета следующим образом: одна грань красная (R), вторая — зелёная (G), третья — синяя (B), а четвёртая содержит все три цвета. События $R$, $G$, $B$ состоят в том, что при случайном броске на нижней грани тетраэдра есть соответствующий цвет.
Тогда

$$  \mathbb P(R) = \mathbb P(G) = \mathbb P(B) = \frac 12,
$$

$$  \mathbb P(R \cap G) = \mathbb P(R \cap B) = \mathbb P(G\cap B) = \frac 14,
$$

что влечёт попарную независимость событий $R$, $G$, $B$. Однако $\mathbb P(R \cap G \cap B) = \frac 14$, что не равно $\mathbb P(R)\mathbb P(G)\mathbb P(B) = \frac 18$, поэтому эти события не являются независимыми  совокупности.

{% endcut %}

Определение [независимости случайных величин](https://education.yandex.ru/handbook/ml/article/mnogomernye-raspredeleniya#nezavisimost-sluchajnyh-velichin) из предыдущего параграфа полностью согласуется с только что введённым определением независимых событий. Например, для случая дискретных случайных величин $\xi$ и $\eta$ обозначим

$$  A_i = \mathbb P(\xi = x_i), \quad B_j = \mathbb P(\eta = y_j);
$$

тогда $\mathbb P(\xi = x_i, \eta = y_j) = \mathbb P(A_i \cap B_j)$, и поэтому независимость случайных величин $\xi$ и $\eta$ эквивалентна независимости событий $A_i$ и $B_j$ для всевозможных значений $i$ и $j$.

{% cut "Замечание о статистической независимости" %}

Математический термин «независимость» подразумевает **статистическую** (или **стохастическую**) независимость, которая может не вполне совпадать по смыслу с интуитивным значением этого термина. Например, если вы два раза подкидываете симметричную монетку, то **статистически** результат первого броска никак не влияет на результат второго броска. Но так ли это с философской точки зрения? Вот представим две ситуации:

1. вы бросили монетку, быстро подняли с пола, и снова бросили;

2. монетка при первом броске укатилась далеко под диван, и вы полчаса ворочали мебель, прежде чем произвести второе испытание.

Весьма вероятно, что столь досадное происшествие после первого броска могло существенно повлиять на ваше физическое и моральное состояние. И уж точно второй бросок в ситуациях (1) и (2) вы бы совершили совершенно по-разному, что вполне могло отразиться на его результате.

Однако в математике подобным метафизическим измышлениям нет места. С абстрактным понятием независимости гораздо проще работать, поскольку оно игнорирует замысловатые причинно-следственные связи и прочие несущественные детали. В модели независимых испытаний Бернулли каждое следующее испытание статистически никак не зависит от предыдущих. Что бы с вами не происходило, шансы во втором броске — 50 на 50, именно об этом говорит нам независимость испытаний Бернулли с вероятностью успеха $\frac 12$, не больше и не меньше.

{% endcut %}

## Условная независимость

Бывает так, что зависимые события $A$ и $B$ становятся независимыми при выполнении некоторого третьего события $C$. Более формально, события $A$ и $B$ **условно независимы** по отношению к событию $C$, если $\mathbb P(C) > 0$ и

$$  \mathbb P(A \vert B, C) = \mathbb P(A\vert C).
$$

Поскольку

$$  \mathbb P(A \vert B, C)  = \frac{\mathbb P(A \cap B \cap C)}{\mathbb P(B \cap C)}, \quad
  \mathbb P(A \vert C)  = \frac{\mathbb P(A \cap C)}{\mathbb P(C)},
$$

то условная независимость событий $A$ и $B$ эквивалетна равенству

$$  \frac{\mathbb P(A \cap B \cap C)}{\mathbb P(C)} =  \frac{\mathbb P(A \cap C)}{\mathbb P(C)} \cdot  \frac{\mathbb P(B \cap C)}{\mathbb P(C)},
$$

а это, в свою очередь, означает, что

$$  \mathbb P(A \cap B\vert C) = \mathbb P(A\vert C) \mathbb P(B\vert C).
$$

Таким образом, вероятность произведения условно независимых событий равна произведению условных вероятностей. Эта формула полностью аналогична формуле $\mathbb P(A\cap B) = \mathbb P(A)\mathbb P(B)$ для (безусловно) независимых событий.

**Пример** (цепь Маркова). Последовательность событий $S_0, S_1, S_2, \ldots, S_t, \ldots$ называется **марковской цепью**, если выполняется **марковское свойство**

$$\mathbb P (S_{t+1} \vert S_t, S_{t-1}, \ldots, S_0) = \mathbb P(S_{t+1} \vert S_t), \quad t \in \mathbb N \cup \{0\}.
$$

В марковском свойстве заложен следующий смысл: в каждый момент времени $t$ «будущее» $S_{t+1}$ зависит только от «настоящего» $S_t$, но не зависит от «прошлого»

$$P_t = S_{t-1} \cap \ldots \cap S_0.
$$

Итак, цепь Маркова характеризуется равенством $\mathbb P(S_{t+1} \vert P_t, S_t) = \mathbb P(S_{t+1} \vert S_t)$, которое означает, что события $S_{t+1}$ и $P_t$ условно независимы по отношению к событию $S_t$.

## Условные распределения

Пусть $\xi$ и $\eta$ — дискретные случайные величины и $\mathbb P(\eta = y) > 0$. По аналогии с условными вероятностями **условное распределение** случайной величины $\xi$ при условии, что значение случайной величины $\eta$ равно $y$, определяется по формуле

$$  \mathbb P(\xi = x_i \vert \eta = y) = \frac{\mathbb P(\xi = x_i , \eta = y)}{\mathbb P(\eta = y)}.
$$

Это действительно распределение вероятностей, поскольку $\mathbb P(\xi = x_i \vert \eta = y) \geqslant 0$ и

$$\sum\limits_{i}\mathbb P(\xi = x_i \vert \eta = y) = \frac 1{\mathbb P(\eta = y)} \sum\limits_{i}\mathbb P(\xi = x_i , \eta = y) = 1.
$$

В непрерывном случае условное распределение задаётся **условной плотностью**

$$  p_{\xi\vert \eta}(x\vert y) = \frac{p(x, y)}{p_\eta(y)},
$$

где $p(x, y)$ — совместная плотность случайных величин $\xi$ и $\eta$. И снова проведением маргинализации по $x$ убеждаемся в том, что с нормировкой всё в порядке:

$$  \int\limits_{-\infty}^{+\infty} p_{\xi\vert \eta}(x\vert y)\,dx = \frac 1{p_\eta(y)}\int\limits_{-\infty}^{+\infty} p(x, y)\,dx = \frac {p_\eta(y)}{p_\eta(y)} = 1.
$$

Поскольку $\int\limits_{-\infty}^{+\infty} p(x, y)\,dy = p_\xi(x)$, из формулы условной плотности получаем непрерывный аналог **формулы полной вероятности**:

$$  p_\xi(x) = \int\limits_{\mathbb{R}} p_{\xi \mid \eta}(x\vert y) p_\eta(y) dy.
$$

**Пример**. Выберем случайное число $x\in \big[\tfrac12, 1\big]$, а затем — случайное число $y \in [0, x]$. Как распределена случайная величина $y$?

Переформулируем задачу: известно, что $\xi \sim U\big[\tfrac12, 1\big]$ и $\eta \vert \xi \sim U[0, x]$. Требуется найти плотность случайной величины $\eta$. Имеем

$$  p_\xi(x) = 2\mathbb I_{\big[\tfrac12, 1\big]}(x), \quad
  p_{\eta\mid\xi}(y\vert x) = \frac 1x\mathbb I_{[0, x]}(y).
$$

Применяя формулу полной вероятности, находим

$$  p_\eta(y) = \int\limits_{1/2}^1 \frac 2x \mathbb I[y \leqslant x]\, dx =
    \begin{cases}
      2 \ln2, & 0 \leqslant y < \tfrac12, \\
      -2 \ln{y}, & \tfrac12 \leqslant y \leqslant 1.
    \end{cases}
$$

**Упражнение**. Пусть случайные величины $\xi_k \sim \mathrm{Exp}(\lambda_k)$, $k=1, \ldots, n$, независимы в совокупности. Чему равна вероятность $\mathbb P\big(\xi_k = \min \{\xi_1, \ldots, \xi_n \}\big)$?

<br>

{% cut "Ответ (не открывайте сразу, попробуйте сначала решить самостоятельно)" %}

Обозначим $\eta = \operatorname*{argmin}\limits_{1\leqslant k \leqslant n} \{\xi_k\}$. Требуется найти $\mathbb P(\eta = k)$. По формуле полной вероятности имеем

$$\mathbb P(\eta = k) = \int\limits_0^{+\infty} p_{\eta\vert \xi_k}(\eta = k \vert x) p_{\xi_k}(x)\,dx.
$$

Далее, $p_{\xi_k}(x) = \lambda_k e^{-\lambda_k x}$, $x\geqslant 0$,

$$  p_{\eta\vert \xi_k}(\eta = k \vert x) = \mathbb P(\xi_i > x,  i \ne k) = \prod\limits_{i\ne k} e^{-\lambda_i x}.
$$

Таким образом,

$$\mathbb P(\eta = k) = \int\limits_0^{+\infty} \lambda_k e^{-\lambda_k x}\prod\limits_{i\ne k} e^{-\lambda_i x} \, dx = \lambda_k \int\limits_0^{+\infty} \exp\Big(-\sum\limits_{i=1}^n \lambda_i x\Big)\,dx= \frac{\lambda_k}{\lambda_1 + \ldots + \lambda_n}.
$$

Условные распределения случайных векторов определяется аналогично с поправкой на возросшее число аргументов: в этом случае $x$ и $y$ уже не числа, а вектора тех же размерностей, что и сами случайные вектора.

{% endcut %}

## Условные математические ожидания

**Условное математическое ожидание** $\mathbb E(\xi\vert\eta = y)$ отвечает на вопрос «чему равно среднее значение случайной величины $\xi$ при условии, что $\eta = y$?».
Имея в распоряжении матрицу условного дискретного распределения $\mathbb P(\xi = x_i\vert \eta = y_j)$ или условную плотность $p_{\xi\vert \eta}(x\vert y)$, условное математическое ожидание можно вычислить следующим образом:

* $\mathbb E(\xi\vert\eta)\equiv \mathbb E(\xi\vert\eta=y) = \sum\limits_i x_i \mathbb P(\xi = x_i\vert \eta = y)$ в дискретном случае;
* $\mathbb E(\xi\vert\eta) \equiv \mathbb E(\xi\vert\eta=y) = \int\limits_{\mathbb R} x p_{\xi\vert \eta}(x\vert y)\,dx$ для непрерывных $\xi$ и $\eta$.

Важно отметить, что после суммирования или интегрирования по переменной $x$ в формуле условного математического ожидания остаются зависимость от $y$. Таким образом, в отличие от обычного среднего, которое является просто числом, условное ожидание представляет собой случайную величину $\zeta = \mathbb E(\xi\vert\eta=y)$, поскольку его значение зависит от случайного значения $\eta = y$.

**Свойства условного математического ожидания**

1. $\mathbb E(a\xi_1 + b \xi_2 \vert \eta) = a\mathbb E (\xi_1\vert \eta) + b \mathbb E (\xi_2 \vert\eta)$ (линейность).

2. Если $\xi_1 \leqslant \xi_2$, то $\mathbb E (\xi_1\vert \eta) \leqslant \mathbb E (\xi_2\vert \eta)$ (монотонность).

3. Если случайные величины $\xi$ и $\eta$ независимы, то $\mathbb E(\xi\vert\eta) = \mathbb E\xi$.

4. $\mathbb E(g(\eta) \xi\vert\eta) = g(\eta) \mathbb E(\xi\vert \eta)$.

5. $\mathbb E\big(\mathbb E(\xi\vert \eta)\big) = \mathbb E\xi$ (**law of total expectation**).

**Упражнение.** Prove the law of total expectation.

{% cut "Ответ (не открывайте сразу, попробуйте сначала решить самостоятельно)" %}

Пусть $\zeta = \mathbb E(\xi \vert \eta)$. Начнём с дискретного случая:

$$  \mathbb E\zeta = \sum\limits_j \mathbb E(\xi\vert\eta = y_j)\mathbb P(\eta = y_j) = 
  \sum\limits_j\sum\limits_i x_i \mathbb P(\xi = x_i\vert \eta = y_j)\mathbb P(\eta = y_j)=
$$

$$  =\sum\limits_i x_i \sum\limits_j\mathbb P(\xi = x_i, \eta = y_j) 
  =\sum\limits_i x_i \mathbb P(\xi = x_i) = \mathbb E\xi.
$$

В непрерывном случае вместо сумм потребуется переставить местами интегралы. Это позволяет сделать [теорема Фубини](https://en.wikipedia.org/wiki/Fubini%27s_theorem) о сведении двойного интеграла к повторному:

$$  \mathbb E\zeta = \int\limits_{-\infty}^{+\infty}\mathbb E(\xi\vert\eta = y) p_\eta(y)\,dy =
  \int\limits_{-\infty}^{+\infty} p_\eta(y)\,dy \int\limits_{-\infty}^{+\infty}x p_{\xi\vert\eta}(x\vert y)\,dx =
$$

$$  = \int\limits_{-\infty}^{+\infty} x\,dx \int\limits_{-\infty}^{+\infty} p(x, y)\,dy  = 
  \int\limits_{-\infty}^{+\infty} xp_\xi(x) \,dx = \mathbb E\xi.
$$

{% endcut %}

**Условная дисперсия** определяется по формуле

$$  \mathbb V(\xi \vert \eta) = \mathbb E\big((\xi - \mathbb E(\xi\vert \eta))^2 \vert \eta\big) = \mathbb E(\xi^2 \vert \eta) - \big(\mathbb E(\xi \vert \eta))^2.
$$

Справедливо равенство $\mathbb V \xi = \mathbb E\big(\mathbb V(\xi\vert \eta)\big) + \mathbb V\big(\mathbb E(\xi\vert \eta)\big)$ (**law of total variance**).

## Регрессия

В машинном обучении часто встречается задача **регрессии**, в которой требуется восстановить зависимость $Y = f(X)$ при наличии выборки

$$  (X_1, Y_1), \ldots, (X_n, Y_n) 
$$

из некоторого неизвестного распределения с совместной плотностью $p(x, y)$. Стандартный способ решения задачи регресии — минимизация среднего значения **функции потерь** $\mathcal L(Y, f(X))$:

$$  \mathbb E \big[\mathcal L(Y, f(X))\big] = \iint\limits_{\mathbb R^2} \mathcal L(y, f(x)) p(x, y) \,dxdy \to \min.
$$

В качестве функции потерь на одном объекте $(x, y)$ в задаче регрессии обычно выбирают квадратичную функцию: $\mathcal L(y, f(x)) = (y-f(x))^2$. Тогда

$$  \mathbb E \big[\mathcal L(Y, f(X))\big] = \iint\limits_{\mathbb R^2} \mathcal (y-f(x))^2 p(x, y) \,dxdy;
$$

для минимизации этого функционала применим немножко вариационного исчисления и продифференцируем по функции $f(x)$. Получим

$$2\iint\limits_{\mathbb R^2} (f(x)-y) p(x, y) \,dxdy = 0,
$$

откуда

$$  f(x) = \frac 1{p(x)} \int\limits_{-\infty}^{+\infty} yp(x, y)\,dy = \int\limits_{-\infty}^{+\infty} yp_{Y\vert X}(y \vert x)\,dy = \mathbb E(Y\vert X = x).
$$

Полученное условное математическое ожидание, называемое **функцией регрессии**, показывает, чему в среднем равно значение зависимой переменной $Y$ при условии, что $X=x$.

  ## handbook

  Учебник по машинному обучению

  ## title

  Независимость и условные распределения вероятностей

  ## description

  Независимость и условные распределения вероятностей

- 
  ## path

  /handbook/ml/article/parametricheskie-ocenki

  ## content

  Различные типы распределений, описанные в предыдущих параграфах, применяются в качестве теоретических моделей в задачах, связанных со случайностью и неопределённостью. Однако на практике далеко не всегда ясно, какое именно распределение моделирует имеющиеся в наличии данные. А если из каких-либо соображений тип распределения всё же установлен, то следующая задача — оценить параметры этого распределения, например, среднее и/или дисперсию в случае гауссовского распределения $\mathcal N(\mu, \sigma^2)$.

Подобными обратными по отношению к теории вероятностей задачами занимается **математическая статистика**. Типичный пример статистической задачи: по числовой выборке $X_1, \ldots, X_n$ оценить параметры распределения, из которого они были получены. Обычно предполагается, что выборка **i.i.d.** (**i**ndependent and **i**dentically **d**istributed), то есть представляет собой независимые реализации случайной величины с одним и тем же распределением. Параметр этого определения $\theta$ может быть числом или вектором; оценку этого параметра по выборке $X_1, \ldots, X_n$ обычно обозначают $\widehat \theta(X_1, \ldots, X_n)$ или просто $\widehat \theta$.

## Предельные теоремы

Как правило, чем больше размер выборки, тем более информативны параметрические оценки вида $\widehat \theta(X_1, \ldots, X_n)$. Теоретические свойства таких оценок при $n\to\infty$ устанавливаются с помощью предельных теорем теории вероятностей.

### Закон больших чисел

Внимательный читатель мог обратить внимание, что в ряде примеров из предыдущих параграфов параметры некоторых распределений почему-то молчаливо подменялись средними значениями. Так мы поступили в задаче о показе рекламы, взяв в качестве параметра пуассоновского распределение среднее количество кликов пользователей. Фактически мы оценили неизвестный параметра $\lambda$ средним по выборке:

$$\widehat\lambda = \frac 1n \sum\limits_{k=1}^n X_k.
$$

В общем-то это кажется логичным, поскольку $\lambda = \mathbb E\xi$, если $\xi \sim \mathrm{Pois}(\lambda)$. Однако у такой оценки есть также мощное теоретическое обоснование.

**Теорема (Закон больших чисел, ЗБЧ)**. Пусть $X_1, X_2, \dots$ – последовательность попарно независимых одинаково распределенных случайных величин с конечным математическим ожиданием $\mu$. Тогда для любого $\varepsilon > 0$

$$  \lim\limits_{n \to \infty} \mathbb{P}(|\overline{X}_n - \mu| > \varepsilon) = 0, \text{ где } \overline{X}_n = \frac{1}{n}\sum\limits_{k = 1}^n X_k.
$$

Таким образом, чем больше размер выборки $n$, тем менее вероятно отклонение **выборочного среднего** $\overline{X}_n$ от истинного среднего $\mu$ на любое число $\varepsilon > 0$.

Закон больших чисел особенно легко обосновать для случая конечных дисперсий: $\mathbb V X_k = \sigma^2 < +\infty$. Имеем

$$    \mathbb E \overline{X}_n = \frac 1n \sum\limits_{k=1}^n \mathbb EX_k = \mu, \quad
    \mathbb V \overline{X}_n = \frac 1{n^2} \sum\limits_{k=1}^n \mathbb VX_k = \frac{\sigma^2}n.
$$

Отсюда видно, что $\lim\limits_{n\to\infty} \mathbb V \overline{X}_n = 0$, поэтому при больших $n$ распределение случайной величины $\mathbb V \overline{X}_n$ всё больше похоже на распределение, сосредоточенное в одно лишь точке $\mu$. Формально же утверждение ЗБЧ получается с помощью неравенства Чебышева:

$$    \mathbb{P}\big(\vert \overline{X}_n - \mu\vert > \varepsilon\big) \leqslant \frac{\mathbb{V} \overline{X}_n}{\varepsilon} = \frac{\sigma^2}{n \varepsilon} \to 0, \quad n\to\infty.
$$

Закон больших чисел допускает следующее усиление.

**Теорема (Усиленный закон больших чисел, УЗБЧ)**. Пусть $X_1, X_2, \dots$ – последовательность попарно независимых одинаково распределенных случайных величин с конечным математическим ожиданием $\mu$. Тогда выборочное среднее $\overline{X}_n$ **почти наверное** сходится к $\mu$, т.е.
$\mathbb P\big(\lim\limits_{n\to\infty}\overline{X}_n = \mu\big) = 1$.

{% cut "Замечание о типах сходимостей случайных величин" %}

Последовательность случайных величин $(X_n)$, $n\in\mathbb N$, сходится к случайной величине $X$

1. **по распределению**, $X_n \stackrel{D}{\to} X$, если $F_{X_n}(x) \to F_X(x)$;
2. **по вероятности**, $X_n \stackrel{P}{\to} X$, если $\lim\limits_{n \to \infty} \mathbb{P}(\vert X_n - X\vert > \varepsilon) = 0$ для любого $\varepsilon > 0$;
3. **почти наверное**, $X_n \stackrel{\text{п.н.}}{\to} X$, если $\mathbb P\big(\lim\limits_{n\to\infty}X_n = X\big) = 1$;
4. **в среднем квадратичном**, $X_n \stackrel{L_2}{\to} X$, если $\lim\limits_{n \to \infty} \mathbb E(X_n - X)^2=0$.

Известно, что

* из сходимости по вероятности вытекает сходимость по распределению, $X_n \stackrel{P}{\to} X  \implies X_n \stackrel{D}{\to} X$;
* из сходимость почти наверное следует сходимость по вероятности, $X_n \stackrel{\text{п.н.}}{\to} X  \implies X_n \stackrel{P}{\to} X$;
* сходимость в среднем квадратичном влечёт сходимость по вероятности, $X_n \stackrel{L_2}{\to} X  \implies X_n \stackrel{P}{\to} X$.

А вот из сходимости по вероятности, вообще говоря, не следует сходимость почти наверное (контрпример можно посмотреть [здесь](https://math.stackexchange.com/questions/2479440/convergence-in-probability-does-not-imply-almost-sure-convergence)).

Закон больших чисел утверждает, что выборочное среднее сходится по вероятности к истинному среднему. А согласно УЗБЧ имеет место более сильный тип сходимости — почти наверное.

{% endcut %}

### Теорема Муавра-Лапласа

[Доска Гальтона](https://ru.wikipedia.org/wiki/%D0%94%D0%BE%D1%81%D0%BA%D0%B0_%D0%93%D0%B0%D0%BB%D1%8C%D1%82%D0%BE%D0%BD%D0%B0) иллюстрирует биномиальное распределение. До поворота на ее дне лежит множество маленьких шариков. Сразу после переворота шарики проходят через 10 рядов гладких круглых препятствий. Преодоление каждого препятствия можно рассматривать как испытание Бернулли: с равными вероятностями шарик может пойти как налево, так и направо. Поэтому финальное положение шарика в одной из 10 корзин является приблизительной реализацией биномиального распределения $\mathrm{Bin}(10, 0.5)$.

<iframe width="100%" height="400" src="https://frontend.vh.yandex.ru/player/4a4648c43887178282364bf5d057236f?from=partner&mute=1&autoplay=0&tv=0&no_ad=false&loop=false&play_on_visible=true&branding=0" allow="autoplay; fullscreen; accelerometer; gyroscope; picture-in-picture; encrypted-media" frameborder="0" scrolling="no" allowfullscreen></iframe>

Уже при $n=10$ биномиальное распределение напоминает нормальное. И действительно, чем больше $n$, тем лучше дискретная случайная величина $\xi \sim \mathrm{Bin}(n, p)$ аппроксимируется непрерывной гауссианой $\mathcal N\big(np, np(1-p)\big)$.

**Теорема Муавра-Лапласа**. Пусть $\xi \sim \mathrm{Bin}(n, p)$, $q=1-p$, тогда

$$    \lim\limits_{n\to\infty} \mathbb P\Big(a < \frac{\xi - np}{\sqrt{npq}} \leqslant b\Big) = \frac 1{\sqrt{2\pi}} \int\limits_a^b e^{-\frac{x^2}2}\,dx.
$$

Из теоремы Муавра-Лапласа вытекает, что при больших $n$ вероятность попадания биномиальной случайной величины $\xi \sim \mathrm{Bin}(n, p)$ в заданный интервал можно оценить как

$$    \mathbb P(A < \xi \leqslant B) \approx \Phi\Big(\frac{B - np}{\sqrt{npq}}\Big) - \Phi\Big(\frac{A - np}{\sqrt{npq}}\Big).
$$

где $\Phi(z)$ — функция распределения стандартного нормального распределения.

### Центральная предельная теорема

При выводе закона больших чисел мы видели, что выборочное среднее $\overline X_n$ имеет среднее $\mu$ и дисперсию $\frac{\sigma^2} n$. Но как именно выглядит распределение случайной величины $\overline X_n$ при увеличении $n$? Оказывается, что оно становится всё больше похоже на $\mathcal N\big(\mu, \frac{\sigma^2} n\big)$. Вот как, например, выглядят нормализованные гистограммы $5000$ выборочных средних, построенных по i.i.d. выборкам $X_1, \ldots, X_n  \sim \mathrm{Bin}(30, 0.3)$ для разных значений $n$:

<iframe src="https://yastatic.net/s3/ml-handbook/admin/binomials_CLT_b73d4eacfb.html" width=100% height=500 frameborder=0></iframe>

Эти гистограммы и впрямь очень напоминают гауссианы, и это прямое следствие следующей теоремы.

**Центральная предельная теорема, ЦПТ**. Пусть $X_1, X_2, \dots$ – последовательность попарно независимых одинаково распределенных случайных величин с конечным математическим ожиданием $\mu$ и дисперсией $\sigma^2$. Тогда

$$  Z_n := \frac{\sqrt n(\overline X_n - \mu)}{\sigma} \approx \mathcal N(0, 1) \text{ при } n \gg 1. 
$$

Точнее говоря, $\lim\limits_{n\to\infty}\mathbb P(Z_n \leqslant z) = \Phi(z)$. Таким образом, случайная величина $Z_n$ *сходится по распределению* к $\mathcal N(0,1)$: $Z_n \stackrel{D}{\to} \mathcal N(0, 1)$.

Если применить центральную предельную теорему к бернуллиевским случайным величинам с вероятностью успеха $p$, то вновь получим теорему Муавра-Лапласа.

## Свойства параметрических оценок

Оценивать параметры можно по-разному, хочется делать это хорошо. Ценные свойства оценок, которые обычно желательны – это **несмещенность** и **состоятельность**.

### Несмещённость

Каждый элемент i.i.d выборки $X_1, \ldots, X_n$ можно рассматривать как значение случайной величины из некоторого распределения с неизвестным параметром $\theta$.
А раз так, то всякую оценку этого параметра $\widehat\theta(X_1, \dots, X_n)$ также можно считать случайной величиной, у которой можно пытаться вычислять математическое ожидание, например.

Оценка $\widehat\theta(X_1, \dots, X_n)$ параметра $\theta$ называется **несмещенной**, если $\mathbb{E}\widehat\theta = \theta$. Несмещённость оценки означает, что она в среднем будет равна истинному значению параметра.

Интуитивно можно представлять себе несмещённость следующим образом: если мы нагенерим большое количество выборок $X_1^{(i)}, X_2^{(i)}, \dots, X_n^{(i)}$, $1\leqslant i \leqslant N$, и для каждой посчитаем оценку $\widehat \theta^{(i)}$, то в среднем получится более или менее истинное значение параметра $\theta$: $\frac 1N\sum\limits_{i=1}^N \widehat \theta^{(i)} \approx \theta$.

Простейший пример несмещённой оценки среднего значения $\theta$ даёт выборочное среднее $\overline{X}n = \frac{1}{n}\sum\limits^n X_k$, поскольку

$$    \mathbb E \overline{X}_n = \frac 1n \sum\limits_{k = 1}^n \mathbb E X_k = \frac 1n\cdot n\theta = \theta.
$$

**Медианой** выборки $X_1, \ldots, X_n$ называется средний член **вариационного ряда**, состоящего из отсортированных по возрастанию элементов выборки:

$$    X_{(1)} \leqslant X_{(2)} \leqslant \ldots \leqslant X_{(n)}.
$$

Если $n$ нечётно, $n=2m+1$, то есть ровно один элемент в середине вариационного ряда, именно он называется медианой: $\mathrm{med}(X_1,\ldots, X_n) = X_{(m)} = X_{\big(\frac{n+1}2\big)}$. При чётном $n=2m$ в качестве медианы берут среднее двух центральных элементов вариационного ряда:

$$\mathrm{med}(X_1,\ldots, X_n) = \frac 12(X_{(m)}+ X_{(m+1)}) = \frac 12 \big(X_{(\frac n2)} + X_{(\frac n2 + 1)}\big).
$$

**Упражнение.**  Дана i.i.d. выборка $X_1, \ldots, X_n$ из равномерного распределения $U[0,2\theta]$. Докажите, что выборочная медиана даёт несмещённую оценку медианы распределения $U[0,2\theta]$.

{% cut "Решение (не открывайте сразу, попробуйте сначала решить самостоятельно)" %}

Если $\xi \sim U[0,2\theta]$, то $\mathbb E\xi = \mathrm{med}(\xi) = \theta$. В секции про [бета-распределение](https://education.yandex.ru/handbook/ml/article/veroyatnostnye-raspredeleniya#nepreryvnye-raspredeleniya) была найдена плотность $k$-й порядковой статистики, посчитанной по выборке из равномерного распределения на $[0,1]$:

$$  p(x) = \frac{n!}{(k-1)!(n-k)!} x^{k-1}(1-x)^{n-k}, \quad 0 \leqslant x \leqslant 1.
$$

Чтобы получить отсюда плотность $k$-й порядковой статистики $X_{(k)}$ для нашей выборки из $U[0, 2\theta]$, сделаем линейную замену $t = 2\theta x$. Тогда

$$  p_{X_{(k)}}(t) = \frac 1{2\theta}\frac{n!}{(k-1)!(n-k)!} \Big(\frac t{2\theta}\Big)^{k-1}\Big(1-\frac t{2\theta}\Big)^{n-k},
$$

$$    0 \leqslant t \leqslant 2\theta.
$$

Рассмотрим два случая. Если $n = 2m+1$, то выборочная медиана равна $X_{(m+1)}$, и

$$    \mathbb E X_{(m+1)} = \frac{(2m+1)!}{m!(m+1)!}\int\limits_0^{2\theta} \Big(\frac t{2\theta}\Big)^{m+1} (1-\frac t{2\theta}\Big)^m\,dt.
$$

Возвращаясь к переменной $x= \frac t{2\theta}$, находим

$$    \mathbb E X_{(m+1)} = 2\theta\frac{(2m)!}{m!(m-1)!}\int\limits_0^1 x^{m+1} (1-x)^m\,dx =
$$

$$    = 2\theta\frac{(2m+1)!}{m!(m+1)!} B(m+2, m+1)
    =2\theta\frac{(2m+1)!}{m! m!} \frac{(m+1)!m!}{(2m+2)!} = \theta. 
$$

Если же $n = 2m$, то нам потребуется найти $\mathbb E \big(\frac 12(X_{(m)} + X_{(m+1)})\big)$. Используя ту же самую замену $x= \frac t{2\theta}$, получаем

$$    \mathbb E X_{(m)} = 2\theta\frac{(2m)!}{m!(m-1)!}\int\limits_0^1 x^{m} (1-x)^m\,dx =
$$

$$    =2\theta\frac{(2m)!}{m!(m-1)!} B(m+1, m+1) 
    =2\theta\frac{(2m)!}{m!(m-1)!}\frac{m!m!}{(2m+1)!} = \frac {2\theta m}{2m+1};
$$

$$    \mathbb E X_{(m+1)} = 2\theta\frac{(2m)!}{m!(m-1)!}\int\limits_0^1 x^{m+1} (1-x)^{m-1}\,dx =
$$

$$    =2\theta\frac{(2m)!}{m!(m-1)!} B(m+2, m) 
    =2\theta\frac{(2m)!}{m!(m-1)!}\frac{(m+1)!(m-1)!}{(2m+1)!} = \frac {2\theta(m+1)}{2m+1}.
$$

Следовательно,

$$\mathbb E \Big(\frac 12(X_{(m)} + X_{(m+1)})\Big) = \frac 12\Big(\mathbb E X_{(m)} + \mathbb EX_{(m+1)}\Big) = \theta \Big(\frac m{2m+1} + \frac {m+1}{2m+1}\Big) = \theta.
$$

Итак, выборочная медиана — несмещённая оценка как медианы, так и среднего распределения $U[0,2\theta]$.

{% endcut %}

В некоторых случаях оценка $\widehat\theta_n = \widehat\theta(X_1, \dots, X_n)$ смещена, но с ростом $n$ это смещение нивелируется. Если $\lim\limits_{n\to\infty} \mathbb E\widehat\theta_n = \theta$, то оценка $\widehat\theta_n$ называется **асимптотически несмещённой**.

**Упражнение.** Пусть $X_1, \ldots, X_n \sim U[0, \theta]$ — i.i.d. выборка. Оценим параметр $\theta$ как максимальное значение выборки:

$$\widehat \theta_n = X_{(n)} = \max\{X_1, \ldots, X_n\}.
$$

Является ли эта оценка несмещённой? Асимптотически несмещённой?

{% cut "Решение (не открывайте сразу, попробуйте сначала решить самостоятельно)" %}

В силу свойства i.i.d. для $0\leqslant x \leqslant \theta$ имеем

$$\mathbb P(X_{(n)}\leqslant x) = \mathbb P(X_1 \leqslant x, \ldots, X_n \leqslant x) = 
\prod\limits_{k=1}^n \mathbb P(X_1 \leqslant x) = \Big(\frac x\theta\Big)^n.
$$

Следовательно, плотность случайной величины $\widehat \theta_n = X_{(n)}$ равна $n\frac {x^{n-1}}{\theta^n}$, и поэтому

$$    \mathbb E \widehat \theta_n = \frac n{\theta^n} \int\limits_0^\theta x^n\,dx = \frac {n\theta}{n+1}.
$$

Отсюда видно, что оценка смещённая. Однако $\lim\limits_{n\to\infty} \frac {n\theta}{n+1} = \theta$, так что оценка $\widehat \theta_n$ асимптотически несмещённая. Чтобы получить несмещённость в чистом виде, можно взять оценку $\tilde \theta_n = \frac{n+1}n X_{(n)}$.

{% endcut %}

### Состоятельность

Оценка $\widehat\theta_n = \widehat\theta(X_1, \dots, X_n)$ называется **состоятельной**, если она сходится по вероятности к $\theta$, $\widehat\theta_n \stackrel{P}{\to} \theta$, то есть

$$  \lim\limits_{n \to \infty} \mathbb{P}(|\widehat\theta_n - \theta| > \varepsilon) = 0 \text{ для любого } \varepsilon > 0.
$$

Cостоятельность означает, что с ростом размера выборки всё менее вероятны хоть сколько нибудь значимые отклонения оценки от истинного значения параметра.

Если i.i.d. выборка $X_1, \ldots, X_n$ получена из распределения с конечным математическим ожиданием $\theta$, то в силу закона больших чисел выборочное среднее $\overline{X}_n$ является состоятельной оценкой для $\theta$.

Состоятельность оценки – независимое от несмещенности свойство: оценки могут быть состоятельными, но не несмещенными и наоборот. Например, оценка $
\widehat \theta_n = X_{(n)}$ из предыдущего упражнения оказалась смещённой, однако, она состоятельна:

$$    \mathbb P(\vert X_{(n)} - \theta\vert > \varepsilon) = \mathbb P(X_{(n)} < \theta + \varepsilon) = 
$$

$$    = \Big(\frac{\theta - \varepsilon}{\theta}\Big)^n = \Big(1 - \frac \varepsilon\theta\Big)^n \to 0, n\to\infty.
$$

**Упражнение**. Приведите пример несмещённой оценки, не являющейся состоятельной.

Имея i.i.d. выборку $X_1, \ldots, X_n$ из невырожденного распределения с конечным средним $\theta$, оценим это среднее как $\widehat \theta = X_1$. Эта оценка, очевидно, несмещённая: $\mathbb E \widehat \theta = \mathbb EX_1 = \theta$. Состоятельной, однако, она не является, ведь выражение

$$    \mathbb P(\vert \widehat \theta - \theta\vert > \varepsilon) = \mathbb P(\vert X_1 - \theta\vert > \varepsilon)
$$

никоим образом не зависит от $n$. Следовательно, состоятельность оценки $\widehat \theta$ означала бы, что $\mathbb P(\vert X_1 - \theta\vert > \varepsilon)=0$ для любого $\varepsilon >0$. Такое возможно только для вырожденного распределения, сосредоточенного в одной лишь точке $\theta$: $\mathbb P(X_1 = \theta) = 1$.

### Bias-variance decomposition

**Смещение** (**bias**) оценки $\widehat{\theta}\equiv\widehat{\theta}_n = \widehat{\theta}(X_1,\ldots,X_n)$ определяется как

$$\mathrm{bias}(\widehat{\theta}) = \mathbb{E}\widehat{\theta} - \theta.
$$

Смещение показывает, насколько оценка в среднем отклоняется от истинного значения. Оценка $\widehat{\theta}_n$

* несмещённая, если $\mathrm{bias}(\widehat{\theta}_n) = 0$;
* асимптотически несмещённая, если $\lim\limits_{n\to\infty}\mathrm{bias}(\widehat{\theta}_n) = 0$.

**Среднеквадратичной ошибкой** (**mean squared error**, **MSE**) оценки называется величина

$$\mathrm{MSE}(\widehat{\theta}) = \mathbb{E}(\widehat{\theta} - \theta)^2.
$$

Смещение, дисперсия и среднеквадратичная ошибка связаны между собой следующим соотношением (**bias-variance decomposition**):

$$\mathrm{MSE}(\widehat{\theta}) = \text{bias}^2(\widehat{\theta}) + \mathbb{V}(\widehat{\theta}).
$$

{% cut "Доказательство" %}

Имеем

$$\mathrm{MSE}(\widehat{\theta}) = \mathbb{E}(\widehat{\theta} - \theta)^2 =
\mathbb{E}\big(\widehat{\theta} - \mathbb E\widehat\theta + \mathbb E\widehat\theta  - \theta\big)^2=
$$

$$=\mathbb{E}\big(\widehat{\theta} - \mathbb E\widehat\theta\big)^2 + 2\mathbb{E}(\widehat{\theta} - \mathbb E\widehat\theta)(\mathbb E\widehat\theta - \theta) + 
\mathbb{E}\big(\mathbb E\widehat\theta - \theta\big)^2 =
$$

$$  = \mathbb{V}(\widehat{\theta})  + 2\big(\mathbb{E}\widehat{\theta} - \mathbb E\widehat\theta\big)\big(\mathbb E\widehat\theta - \theta\big) + \mathrm{bias}^2(\widehat{\theta}).
$$

Среднее слагаемое здесь равно нулю, откуда и вытекает доказываемое равенство.

{% endcut %}

**Упражнение**. Докажите, что оценка $\widehat{\theta}_n$ состоятельная, если она асимптотически несмещённая и $\lim\limits_{n\to\infty}\mathbb{V}(\widehat{\theta}_n) = 0$.

{% cut "Решение (не открывайте сразу, попробуйте сначала решить самостоятельно)" %}

С помощью неравенства Маркова получаем, что

$$\mathbb{P}\big(\vert\widehat{\theta}_n - \theta\vert > \varepsilon\big)\leqslant  \mathbb{P}\big((\widehat{\theta}_n - \theta)^2 \geqslant \varepsilon^2\big)\leqslant \frac{\mathbb{E}(\widehat{\theta}_n - \theta)^2}{\varepsilon^2} = \frac{\mathrm{MSE}(\widehat\theta_n)}{\varepsilon^2}.
$$

По условию оба слагаемых в формуле bias-variance decomposition стремятся к нулю,

$$\mathrm{MSE}(\widehat\theta_n) = \mathrm{bias}^2(\widehat{\theta}_n) + \mathbb{V}(\widehat{\theta}_n) \to 0, \quad n\to \infty,
$$

и поэтому $\lim\limits_{n\to\infty}\mathbb{P}\big(\vert\widehat{\theta}_n - \theta\vert > \varepsilon\big) = 0$ при любом фиксированном $\varepsilon > 0$.

{% endcut %}

Таким образом, если $\lim\limits_{n\to\infty}\mathrm{MSE}(\widehat{\theta}_n) = 0$, то оценка $\widehat{\theta}_n$ параметра $\theta$ асимптотически несмещённая и состоятельная.

### Асимптотическая нормальность

**Стандартным отклонением** оценки $\widehat{\theta}_n$ параметра $\theta$ называется корень из дисперсии:

$$    \mathrm{se}(\widehat{\theta}_n) = \sqrt{\mathbb V \widehat{\theta}_n}.
$$

Оценка $\widehat{\theta}_n$ **асимптотически нормальна**, если $\frac{\widehat{\theta}_n - \theta}{\mathrm{se}(\widehat{\theta}_n)} \stackrel{D}{\to} \mathcal N(0,1)$, т.е.

$$    \lim\limits_{n\to\infty}\mathbb P\Big(\frac{\widehat{\theta}_n - \theta}{\mathrm{se}(\widehat{\theta}_n)} \leqslant z\Big) = \mathbb \Phi(z).
$$

Согласно центральной предельной теореме выборочное среднее i.i.d. выборки из распределения с конечными средним $\mu$ и дисперсией $\sigma^2$ является асимптотически нормальной оценкой параметра $\mu$.

### Эффективность

Пусть $\widehat{\theta}$ и $\tilde{\theta}$ — несмещённые оценки параметра $\theta$. Оценка $\widehat{\theta}$ **эффективнее** оценки $\tilde{\theta}$, если $\mathbb V\widehat{\theta} < \mathbb V\tilde{\theta}$. Такое определение эффективности вполне логично, ведь чем меньше дисперсия несмещённой оценки, тем меньше у неё шансов удалиться куда-то далеко от истинного значения параметра.

**Пример**. Пусть $X_1, \ldots, X_n$ — i.i.d. выборка из распределения $U[0, 2\theta]$. Какая оценка параметра $\theta$ эффективнее: выборочное среднее или медиана?

Несмещённость оценок $\widehat{\theta} = \overline X _n$ и $\tilde{\theta} = \mathrm{med}(X_1, \ldots, X_n)$ уже была показана выше.

Найдём дисперсию наших оценок. Диспресия случайной величины $\xi \sim U[0, 2\theta]$ равна $\mathbb V \xi = \frac{\theta^2}{3}$, следовательно, $\mathbb V \widehat \theta = \frac{\theta^2}{3n}$.

Найти дисперсию медианы несколько сложнее. Ограничимся случаем $n = 2m+1$. Тогда $\tilde{\theta} = X_{(m+1)}$, и

$$    \mathbb E\tilde{\theta}^2 =  \mathbb E X_{(m+1)} = \frac 1{2\theta}\frac{(2m+1)!}{(m!)^2} \int\limits_0^{2\theta} x^2 \Big(\frac x{2\theta}\Big)^{m}\Big(1-\frac x{2\theta}\Big)^{m}\,dx.
$$

С помощью замены $t = \frac x{2\theta}$ отсюда находим, что

$$\mathbb E\tilde{\theta}^2 = \frac{(2m+1)!}{(m!)^2}\int\limits_0^1 4\theta^2t^{m+2}(1-t)^m\,
$$

$$dt =4\theta^2 \frac{(2m+1)!}{(m!)^2} B(m+3, m+1) =
$$

$$=4\theta^2 \frac{(2m+1)!}{(m!)^2} \frac{(m+2)!m!}{(2m+3)!} = 2\theta^2 \frac{m+2}{2m+3} = \theta^2 + \frac{\theta^2}{n+3}.
$$

Следовательно, $\mathbb V\tilde{\theta} = \frac{\theta^2}{n+3}$, что при $n>1$ больше, чем
$\mathbb V \widehat \theta = \frac{\theta^2}{3n}$, так что выборочное среднее эффективнее
медианы (примерно в $\sqrt 3$ раз при больших $n$, если считать по отношению стандартных отклонений).

Несмотря на то что в плане эффективности среднее оказалось предпочтительнее в этом примере,
в статистике медиану любят за бОльшую устойчивость к выбросам.

Ниже приведён scatter-plot, по которому можно наглядно оценить меру разброса среднего и медианы выборки из равномерного распределения на отрезке $[0, 2\theta]$ для $\theta = 5$. Для построения этого графика были взяты $200$ i.i.d. выборок из $U[0, 10]$ размера $n=10, 100, 1000, 10000$, и для каждого $n$ посчитаны выборочное среднее и медиана. Эти статистики и задают координаты точки на графике. Разумеется, чем больше значение $n$, тем кучнее локализованы точки вокруг среднего значения $\theta = 5$, совпадающего в данном случае с медианой. Как видно, облако точек сосредоточено вдоль прямой $y = \theta + \sqrt 3(x - \theta)$.

<iframe src="https://yastatic.net/s3/ml-handbook/admin/uniform_stats_cf66cbd1c0.html" width=100% height=500 frameborder=0></iframe>

### Выборочная дисперсия

Как мы уже убедились, выборочное среднее $\overline{X}_n = \frac{1}{n}\sum\limits_{k = 1}^n X_k$ представляет собой несмещённую и состоятельную оценку для математического ожидания. Можно ли то же самое сказать про **выборочную дисперсию**

$$    \overline S_n = \frac{1}{n} \sum\limits_{k = 1}^n (X_k - \overline{X}_n)^2
$$

в предположении, что i.i.d. выборка $X_1, \ldots, X_n$ состоит из реализаций случайной величины $\xi$ с конечными моментами $\mathbb E\xi = \theta_1$ и $\mathbb E\xi^2 = \theta_2$?

Прежде всего раскроем скобки и перепишем $\overline S_n$ в виде

$$    \overline S_n  = \frac{1}{n} \sum\limits_{k = 1}^n \big(X_k^2 - 2X_k \overline{X}_n + (\overline{X}_n)^2\big) = 
$$

$$    = \frac{1}{n} \sum\limits_{k = 1}^n X_k^2 - 2 (\overline{X}_n)^2 + (\overline{X}_n)^2 = 
    \overline{X^2}_n - (\overline{X}_n)^2,
$$

где $\overline{X^2}_n  = \frac 1n\sum\limits_{k = 1}^n X_k^2$ — выборочное среднее, построенное по выборке $X_1^2, \ldots, X_n^2$. Оно несмещённое, поэтому $\mathbb E \overline{X^2}_n  = \theta_2$. Заметим также, что

$$    (\overline{X}_n)^2 = \frac 1{n^2} \Big(\sum\limits_{k=1}^n X_k\Big)^2 =
    \frac 1{n^2} \sum\limits_{k=1}^n X_k^2 + \frac 2{n^2} \sum\limits_{1\leqslant i < j\leqslant n} X_iX_j,
$$

откуда в силу независимости $X_i$ и $X_j$ при $i\ne j$ получаем

$$    \mathbb E(\overline{X}_n)^2 = \frac 1n \mathbb E\overline{X^2}_n + \frac 2{n^2} \sum\limits_{1\leqslant i < j\leqslant n} \mathbb E X_i \mathbb E X_j=
    \frac{\theta_2}n + \frac{n-1}n\theta_1^2.
$$

Итак,

$$    \mathbb E\overline S_n = \theta_2- \frac {\theta_2}n - \frac{n-1}n\theta_1^2 = \frac{n-1}n \mathbb V\xi.
$$

Таким образом, оценка дисперсии $\overline S_n$ смещённая (хотя и асимптотически несмещённая). По этой причине для оценки дисперсии часто используют аналогичную несмещённую оценку

$$    \overline \sigma_n = \frac n{n-1}\overline S_n = \frac{1}{n-1} \sum\limits_{k = 1}^n (X_k - \overline{X}_n)^2,
$$

которую также называют выборочной дисперсией.

Обоснуем теперь состоятельность оценки $\overline S_n = \overline{X^2}_n - (\overline{X}_n)^2$. Согласно закону больших чисел $\overline{X^2}_n \stackrel{P}{\to} \theta_2$,
$\overline{X}_n \stackrel{P}{\to} \theta_1$. Здесь нам потребуется пара свойств сходимости по вероятности.

**Упражнение**. Пусть $\xi_n \stackrel{P}{\to} \xi$, $\eta_n \stackrel{P}{\to} \eta$. Докажите, что $\xi_n + \eta_n\stackrel{P}{\to} \xi + \eta$.

{% cut "Решение (не открывайте сразу, попробуйте сначала решить самостоятельно)" %}

Зафиксируем некоторое $\varepsilon > 0$. Поскольку $\vert \xi_n - \xi\vert + \vert \eta_n - \eta\vert \geqslant \vert \xi_n + \eta_n - \xi - \eta\vert$, то

$$    \mathbb P\big(\vert \xi_n + \eta_n - \xi - \eta\vert > \varepsilon\big) \leqslant
    \mathbb P\big(\vert \xi_n - \xi\vert  + \vert \eta_n - \eta\vert  > \varepsilon\big).
$$

Далее, если $\vert \xi_n - \xi\vert + \vert \eta_n - \eta\vert  > \varepsilon$, то выполняется хотя бы одно из неравенств $\vert \xi_n - \xi\vert >\frac \varepsilon 2$ и $\vert \eta_n - \eta\vert >\frac \varepsilon 2$. Следовательно,

$$    \mathbb P\big(\vert \xi_n + \eta_n - \xi - \eta\vert > \varepsilon\big) \leqslant \mathbb P\Big(\vert \xi_n - \xi\vert >\frac \varepsilon 2\Big) + \mathbb P\Big(\vert \eta_n - \eta\vert >\frac \varepsilon 2\Big).
$$

Но последние две вероятности стремятся к нулю, так как $\xi_n \stackrel{P}{\to} \xi$ и $\eta_n \stackrel{P}{\to} \eta$. Следовательно, последовательность случайных величин $\xi_n + \eta_n$ сходится по вероятности к $\xi + \eta$.

{% endcut %}

**Упражнение**. Пусть $\xi_n \stackrel{P}{\to} \xi$. Докажите, что $\xi_n^2 \stackrel{P}{\to} \xi^2$.

{% cut "Решение (не открывайте сразу, попробуйте сначала решить самостоятельно)" %}

Пусть $0 < \varepsilon < 1$ и $\delta > 0$. Выберем число $M>1$ так, что $\mathbb P(\vert\xi\vert > M) < \delta$. Если $\vert \xi_n - \xi\vert \leqslant \frac\varepsilon M$ и $\vert \xi \vert \leqslant M$, то

$$    \vert \xi_n^2 - \xi^2\vert = \vert \xi_n - \xi\vert \cdot\vert \xi_n + \xi\vert = \vert \xi_n - \xi\vert \cdot\vert \xi_n - \xi + 2\xi\vert  \leqslant
$$

$$    \leqslant\frac\varepsilon M(\vert \xi_n - \xi\vert + 2\vert \xi\vert) \leqslant \frac\varepsilon M(1+ 2M) < 3\varepsilon.
$$

Следовательно,

$$    \mathbb P\big(\vert \xi_n^2 - \xi^2\vert > 3\varepsilon\big) \leqslant  \mathbb P \Big(\vert \xi_n - \xi\vert > \frac \varepsilon M\Big) + \mathbb P(\vert \xi\vert > M) < 2\delta.
$$

Последнее неравенство выполняется для всех достаточно больших $n$, при которых первое слагаемое меньше $\delta$; этого же всегда можно достичь за счёт увеличения $n$, поскольку по условию $\xi_n \stackrel{P}{\to} \xi$. В силу произвольности $\delta$ отсюда заключаем, что

$$\lim\limits_{n\to\infty} \mathbb P\big(\vert \xi_n^2 - \xi^2\vert > 3\varepsilon\big) = 0,
$$

то есть последовательность $\xi_n^2$ сходится по вероятности к случайной величине $\xi^2$.

{% endcut %}

Пользуясь результатами этих упражнений, заключаем, что
$\big(\overline{X}_n\big)^2 \stackrel{P}{\to} \theta_1^2$ и
$\overline{S}_n \stackrel{P}{\to} \theta_2 - \theta_1^2 = \mathbb V\xi$, и, стало быть, оценка $\overline S_n$ состоятельна.

## Методы оценки параметров

До этого мы обсуждали разные приятные свойства оценок, а теперь рассмотрим некоторые методы, позволяющие систематически получать по выборке оценки параметров с нужными свойствами.

### Метод моментов

Пусть выборка $X_1, \ldots, X_n$ получена сэмплированием из некоторого семейства распределений $F_{\boldsymbol \theta}(x)$ с параметрами $\boldsymbol \theta = (\theta_1, \ldots, \theta_m)$. **Метод моментов** для оценки этих параметров заключается в приравнивании выборочных моментов

$$    \overline{X^k}_n = \frac 1n\sum\limits_{j=1}^n X_j^k
$$

к теоретическим

$$ \alpha_k(\boldsymbol \theta) = \int\limits_{-\infty}^{+\infty} x^k dF_{\boldsymbol \theta}(x).
$$

Решая полученную систему уравнений $\alpha_k(\boldsymbol \theta) = \overline{X^k}_n$, $1\leqslant k \leqslant m$, находим оценки параметров $\widehat \theta_k$.

**Пример**. Оценим параметры нормального распределения $\mathcal{N}(\mu, \sigma^2)$ с помощью метода моментов.

{% cut "Попробуйте сделать сами, прежде чем смотреть решение." %}

Теоретические моменты равны

$$\alpha_1 = \mu,\quad\alpha_2 = \sigma^2 + \mu^2.
$$

Запишем систему:

$$\mu = \overline X_n,
$$

$$\sigma^2 + \mu^2 = \overline {X^2}_n.
$$

Из неё очевидным образом находим $\widehat \mu =  \overline X_n$,

$$\widehat{\sigma^2} =  \overline {X^2}_n - \big( \overline X_n\big)^2=
\frac1n \sum\limits_{k=1}^n\big(X_k -  \overline X_n \big)^2.
$$

Как видно, оценки по методу моментов в данном случае совпадают с выборочными средним и дисперсией.

{% endcut %}

**Упражнение**. Оцените по методу моментов параметры $a$ и $b$ для выборки $X_1, \ldots, X_n$ из $U[a, b]$.

{% cut "Ответ" %}

Решая систему уравнений $\alpha_1 = \frac{a+b}2 = \overline X_n$, $\alpha_2 = \frac{a^2+ab + b^2}3 = \overline{X^2}_n$, находим

$$    \widehat a =  \overline X_n - \sqrt{3 (\overline {X^2}_n -  \overline X_n^2)}, \quad
    \widehat b =  \overline X_n + \sqrt{3 (\overline {X^2}_n -  \overline X_n^2)}
$$

Таким образом, согласно методу моментов оценки для границ отрезка отстоят от выборочного среднего на выборочное стандартное отклонение, помноженное на $\sqrt 3$.

{% endcut %}

При некоторых условиях на регулярность семейства распределений $F_{\boldsymbol \theta}(x)$ оценка по методу моментов получается состоятельной и асимптотически нормальной.

### Метод максимального правдоподбия

Пусть, как обычно, выборка $X_1, \ldots, X_n \sim F_\theta(x)$.
**Правдоподобие** (**функция правдоподобия**, **likelihood**) выборки $X_1,\ldots, \ldots X_n$ — это просто её совместная pmf или pdf. Вне зависимости от типа распределения будем обозначать правдоподобие как

$$\mathcal L(\theta) \equiv L(X_1, \ldots, X_n \vert \theta) = p(X_1, \ldots, X_n \vert \theta).
$$

Если выборка i.i.d., то функция правдоподобия распадается в произведение одномерных функций:

$$L(X_1, \ldots, X_n \vert \theta) = \prod\limits_{k=1}^n  p(X_k\vert \theta). 
$$

**Оценка максимального правдоподобия** (**maximum likelihood estimation**, **MLE**) максимизирует правдоподобие:

$$    \widehat \theta_{\mathrm{ML}} =  \arg \max\limits_{\theta} \mathcal L(\theta)
$$

Поскольку максимизировать сумму проще, чем произведение, обычно переходят к логарифму правдоподобия (**log-likelihood**). Это особенно удобно в случае i.i.d. выборки, тогда

$$    \widehat \theta_{\mathrm{ML}} =  \arg \max\limits_{\theta} \log \mathcal L(\theta) =
    \arg \max\limits_{\theta} \sum\limits_{k=1}^n \log p(X_k\vert \theta).
$$

**Пример**. В результате $n$ подбрасываний монеты выпало $k$ «орлов» и $n-k$ «решек».
Оценим вероятность выпадения «орла» методом максимального правдоподобия.

Пусть $p$ — вероятность выпадения «орла», тогда правдоподобие равно

$$\mathcal L(p)=p^k (1-p)^{n-k}.
$$

Дифференцируя логарифм правдоподобия

$$\log \mathcal L(p) = k\log p + (n-k)\log(1-p)
$$

и приравнивая к нулю производную, находим

$$    \frac kp = \frac{n-k}{1-p} \iff k(1-p) = (n-k)p \iff p = \frac kn.
$$

Нетрудно убедиться, что это точка максимума. Итак, оценка максимального правдоподобия $\widehat p_{\mathrm{ML}} = \frac kn$ вероятности «успеха» в схеме Бернулли вполне ожидаемо оказалась равна доле «успехов» в серии из $n$ испытаний.

**Упражнение**. Пусть i.i.d. выборка $X_1, \ldots, X_n$ взята из пуассоновского распределения с параметром $\lambda$. Найдите его оценку максимального правдоподобия.

{% cut "Ответ" %}

$$\widehat \lambda_{ML} = \overline X_n = \frac 1n \sum\limits_{k=1}^n X_k.
$$

{% endcut %}

Методом максимального правдоподобия можно оценить сразу несколько параметров.

**Пример**. Найдём MLE-оценки параметров распределения $\mathcal N(\mu, \tau)$ по i.i.d. выборке $X_1, \ldots, X_n$.

Запишем правдоподобие:

$$  \mathcal L(\mu, \tau) = \prod\limits_{k = 1}^n \frac{1}{\sqrt{2\pi\tau}} \exp{\frac{-(X_k - \mu)^2}{2\tau}}.
$$

Перейдём к log-likelihood:

$$  \log \mathcal L(\mu, \tau) = -\frac{n}{2}(\log{\tau} + \ln{2\pi}) - \frac{1}{2\tau} \sum\limits_{k = 1}^n (X_k - \mu)^2.
$$

Приравняем частные производные по $\mu$ и $\tau$ к нулю:

$$  \frac{\partial \log \mathcal L}{\partial \mu} = \frac{1}{\tau}\sum\limits_{k = 1}^N (X_k - \mu) = 0,
$$

$$  \frac{\partial \log \mathcal L}{\partial \tau} = -\frac{n}{\tau} + \frac{1}{\tau^2}\sum\limits_{k = 1}^n (X_k - \mu)^2 = 0,
$$

откуда $\widehat\mu_{\mathrm{ML}} = \overline{X}_n$ – выборочное среднее, $\widehat\tau_{\mathrm{ML}} = \frac{1}{n} \sum\limits_{k = 1}^n X_k^2 - (\overline{X}_n)^2$ – выборочная дисперсия.

**Упражнение**. Пусть i.i.d. выборка $X_1, \ldots, X_n \sim U[a, b]$. Найдите оценки максимального правдоподобия для параметров $a$ и $b$.

{% cut "Решение (не открывайте сразу, попробуйте сначала решить самостоятельно)" %}

Оказывается, при поиске MLE не всегда надо дифференцировать. Правдоподобие здесь имеет вид

$$    L(X_1, \ldots, X_n\vert a, b) = \frac 1{(b-a)^n}\prod\limits_{k=1}^n \mathbb I(X_k \in [a, b]).
$$

При фиксированных иксах и $b$ это выражение максимально при $a = X_{(1)}$: ведь если взять чуть больше, то произведение индикаторов обнулится, если меньше — то правдоподобие уменьшится за счёт увеличения $(b-a)^n$. По аналогичным соображениям $\widehat b_{\mathrm{ML}} = X_{(n)}$.

{% endcut %}

**Свойства оценки максимального правдоподобия**

* состоятельность: $\widehat \theta_{\mathrm{ML}} \stackrel{P}{\to} \theta$;
* инвариантность относительно параметризации: если $\widehat \theta_{\mathrm{ML}}$ — MLE-оценка для $\theta$, то $\varphi\left( \hat{\theta}_{ML} \right)$ — MLE-оценка для $\varphi(\theta)$;
* асимптотическая нормальность: $\frac{\widehat \theta_{\mathrm{ML}} - \theta}{\widehat{\mathrm{se}}} \stackrel{D}{\to} \mathcal N(0,1)$;
* асимптотическая оптимальность: при достаточно больших $n$ оценка
  $\widehat \theta_{\mathrm{ML}}$ имеет минимальную дисперсию.

  ## handbook

  Учебник по машинному обучению

  ## title

  Параметрические оценки

  ## description

  *null*

- 
  ## path

  /handbook/ml/article/entropiya-i-semejstvo-eksponencialnyh-raspredelenij

  ## content

  ## Энтропия

### Информативность наблюдений

Представим, что вы пронаблюдали значение $x$ некоторой случайной величины $\xi$. Как измерить количество информации $h(x)$, которое вы при этом получили? Следующие соображения кажутся в этом плане вполне естественными:

* чем выше вероятность $\mathbb P(\xi = x)$, тем более ожидаемо появление значения $x$ и, соответственно, менее информативно;
* и наоборот, наблюдение маловероятного значения $x$ обычно даёт обильную пищу для размышлений и повышает $h(x)$;
* при наблюдении двух независимых реализаций $x$ и $y$ случайной величины $\xi$ логично складывать полученную информацию: $h((x, y)) = h(x) + h(y)$.

Указанные соображения наводят на мысль, что информацию следует считать убывающей функцией от вероятности: $h(x) = g\big(\mathbb P(\xi = x)\big)$. Кроме того, функция $g$ должна превращать произведение в сумму, поскольку для независимых случайных величин $\xi$ и $\eta$
равенство $h((x, y)) = h(x) + h(y)$ влечёт

$$   g\big(\mathbb P(\xi = x, \eta=y)\big) = g\big(\mathbb P(\xi = x) \mathbb P(\eta = y)\big) = g\big(\mathbb P(\xi = x)\big) + g\big(\mathbb P(\eta = y)\big).
$$

На самом деле выбор тут небогат. Единственная непрерывная функция, обладающая такими свойствами, — это логарифм: $g(p) = -\log p$. Основание логарифма может быть любым числом больше единицы. Поскольку информацию измеряют в битах и байтах, в теории информации обычно предпочитают двоичные логарифмы. Однако для вычислений удобнее использовать натуральный логарифм, и по умолчанию мы будем подразумевать именно его (кстати, соответствующую единицу информации называют «нат»).

### Энтропия Шеннона

Среднее количество информации, которое несёт в себе значение дискретной случайной $\xi$ с распределением вероятностей $p_k= \mathbb P(\xi = k)$, вычисляется по формуле

$$    \mathbb H\xi = \mathbb E \big(g(p(\xi))\big) =  -\mathbb E \big(\log p(\xi)\big)=-\sum\limits_k p_k \log p_k.
$$

Это так называемая **энтропия** (**Шэннона**).

{% cut "Небольшое математическое замечание" %}

При вычислении энтропии регулярно можно встретить выражение $0\cdot \log 0$ с не вполне очевидным значением. Поскольку $\lim \limits_{t\to 0+} t\log t = 0$, по определению полагаем $0\cdot \log 0 = 0.$

{% endcut %}

**Пример**. Рассмотрим схему Бернулли с вероятностью «успеха» $p$. Энтропия её результата равна

$$\mathbb H\xi = -(1 - p)\log(1 - p) - p\log p, \quad \xi \sim \mathrm{Bern}(p).
$$

Давайте посмотрим на график этой функции:

![entropy](https://yastatic.net/s3/education-portal/media/entropy_bern_b21f03cc0e_d706952391.svg)

Минимальное значение (нулевое) энтропия принимает при $p = 0$ или $p=1$. Исход такого вырожденного эксперимента заранее известен, и чтобы сообщить кому-то о его результате,  достаточно $0$ бит информации. Иначе говоря, можно вообще ничего не передавать, и так всё предельно ясно.

Максимальное значение энтропии достигается в точке $\frac12$, что вполне соответствует тому, что при $p=\frac12$ предсказать исход эксперимента сложнее всего.

**Упражнение**. Найдите энтропию геометрического распределения с вероятностью «успеха» $p$: $\xi \sim \mathrm{Geom}(p)$,

$$\mathbb P(\xi = k) = p(1-p)^{k-1}, \quad k \in \mathbb N, \quad 0 < p \leqslant 1.
$$

{% cut "Решение (не открывайте сразу, попробуйте сначала решить самостоятельно)" %}

По определению энтропии имеем

$$\mathbb H\xi = -\sum\limits_{k=1}^\infty p(1-p)^{k-1} (\log p + (k-1)\log(1-p)) =
$$

$$= -\log p -p\log(1-p)\sum\limits_{k=1}^\infty k(1-p)^k.
$$

Дифференцированием геометрической прогрессии находим

$$\sum\limits_{k=1}^\infty kx^{k-1} = \sum\limits_{k=1}^\infty (x^k)' = \frac d{dx}\sum\limits_{k=0}^\infty x^k = \Big(\frac 1{1-x}\Big)' = \frac 1{(1-x)^2};
$$

подставляя сюда $x = 1-p$, окончательно получаем, что

$$    \mathbb H\xi = -\log p -p(1-p)\log(1-p)\cdot \frac 1{p^2} = -\log p - \frac{(1-p)\log(1-p)}p.
$$

Вспомним, что случайная величина $\xi\sim\mathrm{Geom}(p)$ равна количеству независимых испытаний с вероятностью «успеха» $p$ до появления первого «успеха». При $p=1$ энтропия $\mathbb H\xi$ минимальна и равна нулю, ведь в этом случае геометрическое распределение становится вырожденным: «успех» наступает сразу же с вероятностью $1$. А вот при $p\to 0+$ серия неудачных испытаний может быть сколь угодно длинной; распределение становится всё более неопределённым и «размазанным» по своему носителю, и его энтропия <br>стремится к $+\infty$. График энтропии как функции от $p$ выглядит так:

![entropy](https://yastatic.net/s3/education-portal/media/entropy_geom_2404990d13_1eb404572b.svg)

{% endcut %}

Следующие свойства энтропии дискретной случайной величины $\xi$ вытекают прямо из определения:

* неотрицательность: $\mathbb H\xi \geqslant 0$;
* $\mathbb H\xi = 0 \iff \mathbb P(\xi = a) = 1$ при некотором $a\in\mathbb R$ (нулевую энтропию имеют вырожденные распределения и только они);
* $\mathbb H\xi \leqslant \log n$, если случайная величина имеет конечный носитель мощности $n$.

Последнее свойство выводится из [неравенства Йенсена](https://ru.wikipedia.org/wiki/%D0%9D%D0%B5%D1%80%D0%B0%D0%B2%D0%B5%D0%BD%D1%81%D1%82%D0%B2%D0%BE_%D0%99%D0%B5%D0%BD%D1%81%D0%B5%D0%BD%D0%B0). Применяя его к выпуклой вверх логарифмической функции, с учётом нормировки условия $\sum\limits_{k=1}^n p_k = 1$ получаем

$$    -\sum\limits_{k=1}^n p_k\log p_k = \sum\limits_{k=1}^n p_k\log \frac 1p_k \leqslant
    \log\bigg(\sum\limits_{k=1}^n p_k\cdot \frac 1{p_k}\bigg) = \log n.
$$

**Вопрос на подумать**. Итак, всякое распределение с носителем $\{1, 2, \ldots, n\}$ имеет энтропию не больше $\log n$. А у какого распределения она в точности равна $\log n$?

{% cut "Ответ (не открывайте сразу, сначала подумайте самостоятельно)" %}

Такое произойдёт, если все вероятности $p_k = \frac 1n$, $1\leqslant k \leqslant n$, т.е. распределение равномерное.

{% endcut %}

### Дифференциальная энтропия

Чтобы вычислить энтропию непрерывной случайной величины $\xi$, надо, как водится, сумму заменить на интеграл, и получится формула **дифференциальной энтропии**:

$$    \mathbb H\xi = -\int p_{\xi}(x) \log p_{\xi}(x)\, dx.
$$

**Замечание.** В дальнейшем мы будем использовать одинаковый термин **энтропия** как для дискретных, так и для непрерывных случайных величин, для краткости опуская слово **дифференциальная** в последнем случае. Кроме того, энтропию распределения $p$, заданного через pmf или pdf, будем обозначать $\mathbb H[p]$. Такое обозначение позволяет избежать привязки к случайной величине там, где это излишне. Если $\xi \sim p(x)$, то обозначения $\mathbb H\xi$ и $\mathbb H[p]$ эквивалентны. Также отметим, что энтропию можно записать в виде математического ожидания:

$$  \mathbb H[p] = \mathbb E_{\xi \sim p(x)} \log\frac 1{p(\xi)}.
$$

**Пример**. Найдём энтропию нормального распределения $\mathcal N(\mu, \sigma^2)$. Его плотность равна $p(x) = \frac 1{\sqrt{2\pi} \sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$, следовательно,

$$\mathbb H[p] = \int\limits_{-\infty}^{+\infty}p(x)\Big(\frac 12\ln\big(2\pi\sigma^2\big) + \frac{(x-\mu)^2}{2\sigma^2}\Big)dx =
$$

$$= \frac 12\ln\big(2\pi\sigma^2\big)  + \frac 1{\sqrt{2\pi} \sigma}\int\limits_{-\infty}^{+\infty}\frac{(x-\mu)^2}{2\sigma^2}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\,dx.
$$

Делая в последнем интеграле замену $t = -\frac{(x-\mu)^2}{2\sigma^2}$, получаем, что

$$\mathbb H[p] = \frac 12\ln\big(2\pi\sigma^2\big) + \frac 1{\sqrt{2\pi}}\int\limits_{0}^{+\infty}\sqrt{2t}e^{-t}\,dt =\frac 12\ln\big(2\pi\sigma^2\big) + \frac{\Gamma\big(\frac 32\big)}{\sqrt \pi}.
$$

По свойству гамма-функции $\Gamma\big(\frac 32\big) = \frac 12 \Gamma\big(\frac 12\big) = \frac{\sqrt \pi}2$. Таким образом,

$$    \mathbb H[p] = \frac 12\ln\big(2\pi\sigma^2\big) + \frac 12.
$$

Как видно, энтропия гауссовского распределения $\mathcal N(\mu, \sigma^2)$ не ограничена ни сверху, ни снизу:

$$    \lim\limits_{\sigma\to +\infty} \frac 12\ln\big(2\pi\sigma^2\big) = +\infty, \quad
    \lim\limits_{\sigma\to 0+} \frac 12\ln\big(2\pi\sigma^2\big) = -\infty.
$$

И да, в отличие от энтропии дискретного распределения дифференциальная энтропия может быть отрицательной. Это связано с тем, что плотность может принимать значения больше единицы, и поэтому математическое ожидание её логарифма с обратным знаком может оказаться меньше нуля. В частности, с нормальным распределением так происходит, если $\sigma^2 < \frac 1{2\pi e}$.

**Упражнение**. Найдите энтропию показательного распределения $\mathrm{Exp}(\lambda)$.

{% cut "Решение (не открывайте сразу, попробуйте сначала решить самостоятельно)" %}

Плотность случайной величины $\xi \sim \mathrm{Exp}(\lambda)$ равна $p_{\xi}(x) = \lambda e^{-\lambda x}$, $x \geqslant 0$. Таким образом,

$$    \mathbb H\xi = \mathbb H[p_\xi] = \int\limits_{0}^{+\infty} p_{\xi}(x)(\lambda x - \log \lambda)\, dx =
    \int\limits_{0}^{+\infty} \lambda^2 xe^{-\lambda x}\,dx - \log\lambda.
$$

С помощью замены $t = \lambda x$ находим, что

$$    \mathbb H\xi = \int\limits_{0}^{+\infty} te^{-t}\,dt - \log\lambda = \Gamma(2) - \log\lambda = 1 - \log\lambda.
$$

{% endcut %}

### KL-дивергенция

В задачах машинного обучения истинное распределение $p(x)$, из которого приходят наблюдения, обычно неизвестно, и его пытаются приблизить распределением $q(x)$ из некоторого класса модельных распределений. **Дивергенция Кульбака-Лейблера** (**KL-дивергенция**, **относительная энтропия**) позволяет оценить расстояние между распределениями $p$ и $q$:

$$\mathbb{KL}(p\vert\vert q) = \sum\limits_k p_k\log\frac{p_k}{q_k}
$$

в дискретном случае и

$$\mathbb{KL}(p\vert\vert q) = \int p(x)\log \frac{p(x)}{q(x)}dx
$$

в непрерывном. KL-дивергенцию можно представить в виде разности:

$$\mathbb{KL}(p\vert\vert q) = \int p(x)\log \frac 1{q(x)}dx - \int p(x)\log\frac 1{p(x)}dx =
$$

$$=\underbrace{\mathbb E_{\xi \sim p(x)} \frac 1{\log q(\xi)}}_{\text{кросс-энтропия}} - \underbrace{\mathbb E_{\xi \sim p(x)} \frac1{\log p(\xi)}}_{\text{энтропия}}.
$$

Здесь вычитаемое – это уже знакомая нам энтропия распределения $p(x)$, которая показывает, сколько в среднем бит требуется, чтобы закодировать значение случайной величины $\xi \sim p(x)$. Уменьшаемое носит название **кросс-энтропии** распределений $p(x)$ и $q(x)$.
Кросс-энтропию можно интерпретировать как среднее число бит для кодирования значения случайной величины $\xi \sim p(x)$ алгоритмом, оптимизированным для кодирования случайной величины $\eta \sim q(x)$. Иными словами, дивергенция Кульбака-Лейблера говорит о том, насколько увеличится средняя длина кодов для значений $p$, если при настройке алгоритма кодирования вместо $p$ использовать $q$. Подробнее об этом вы можете почитать, например, в [данном посте](https://habr.com/ru/post/484756/).

Дивергенция Кульбака-Лейблера в некотором роде играет роль расстояния между распределениями. В частности, $\mathbb{KL}(p\vert\vert q)\geqslant 0$, причём дивергенция равна нулю, только если распределения совпадают почти всюду (для дискретных и непрерывных распределений это означает, что они просто тождественны). Но при этом она не является симметричной: вообще говоря, $\mathbb{KL}(p\vert\vert q)\ne \mathbb{KL}(q\vert\vert p)$.

**Упражнение**. Пользуясь неравенством $\ln (1+t) \leqslant t$, \$ t \> -1\$, докажите неотрицательность KL-дивергенции.

{% cut "Попробуйте доказать самостоятельно, прежде чем смотреть решение" %}

Указанное неравенство можно переписать в виде $-\ln t \geqslant t - 1$, $t > 0$, поэтому

$$  \mathbb{KL}(p\vert\vert q) = -\int p(x)\ln{\frac{q(x)}{p(x)}}dx \geqslant 
$$

$$  \geqslant \int p(x)\Big(\frac{q(x)}{p(x)} - 1\Big)dx = \int q(x)dx - \int p(x)dx = 0.
$$

{% endcut %}

**Пример.** С помощью KL-дивергенции измерим расстояние между двумя гауссианами $p(x) = \mathcal N(x \vert \mu_1, \sigma_1^2)$ и $q(x) = \mathcal N(x \vert \mu_2, \sigma_2^2)$.

Подставляя явные выражения для плотностей

$$p(x) = \frac 1{\sqrt{2\pi}\sigma_1} e^{-\frac{(x-\mu_1)^2}{2\sigma_1^2}}\text{ и }
q(x) = \frac 1{\sqrt{2\pi}\sigma_2} e^{-\frac{(x-\mu_2)^2}{2\sigma_2^2}},
$$

находим

$$\ln \frac{p(x)}{q(x)} = \ln \frac{\sigma_2}{\sigma_1} + \frac{(x-\mu_2)^2}{2\sigma_2^2} -\frac{(x-\mu_1)^2}{2\sigma_1^2} =
$$

$$=\ln \frac{\sigma_2}{\sigma_1} + \Big(\frac 1{2\sigma_2^2} - \frac 1{2\sigma_1^2}\Big)(x-\mu_1)^2 + \frac{\mu_1 - \mu_2}{\sigma_2^2}(x-\mu_1) + \frac{(\mu_1 - \mu_2)^2}{2\sigma_2^2}.
$$

Из свойств нормального распределения вытекает, что

$$    \int\limits_{-\infty}^{+\infty}p(x) (x-\mu_1)\,dx = 0, \quad
    \int\limits_{-\infty}^{+\infty}p(x)  (x-\mu_1)^2\,dx = \sigma_1^2.
$$

Таким образом,

$$\mathbb{KL}(p\vert\vert q) = \mathbb E_{\xi \sim p(x)} \ln \frac{p(\xi)}{q(\xi)} = \ln \frac{\sigma_2}{\sigma_1} + \frac{\sigma_1^2 + (\mu_1 - \mu_2)^2}{2\sigma_2^2} - \frac 12.
$$

Как и должно быть, полученное выражение равно нулю, если гауссианы совпадают. При равных дисперсиях $\sigma_1^2 = \sigma_2^2 = \sigma^2$ получаем, что $\mathbb{KL}(p\vert\vert q) = \frac{(\mu_1 - \mu_2)^2}{2\sigma^2}$. Это выражение остаётся прежним, если поменять местами $\mu_1$ и $\mu_2$, поэтому в этом случае $\mathbb{KL}(p\vert\vert q) = \mathbb{KL}(q\vert\vert p)$. Если же $\sigma_1 \ne \sigma_2$, то выражение для $\mathbb{KL}(q\vert\vert p)$ явно отличается от $\mathbb{KL}(p\vert\vert q)$, что лишний раз показывает несимметричность KL-дивергенции.

**Упражнение**. Найдите дивергенцию Кульбака-Лейблера двух показательных распределений $p(x) = \mathrm{Exp}(x \vert \lambda)$ и $q(x) = \mathrm{Exp}(x \vert \mu)$.

{% cut "Попробуйте решить самостоятельно, прежде чем смотреть решение" %}

Имеем $p(x) = \lambda e^{-\lambda x}$, $q(x) = \mu e^{-\mu x}$, $x\geqslant 0$, откуда

$$\ln \frac{p(x)}{q(x)} =  \ln \frac \lambda\mu + (\mu-\lambda)x.
$$

Следовательно,

$$  \mathbb{KL}(p\vert\vert q) = \ln \frac \lambda\mu + (\mu-\lambda)\int\limits_0^{+\infty} \lambda x e^{-\lambda x}\,dx =
$$

$$  =\ln\frac \lambda\mu + \frac{\mu-\lambda}{\lambda}\int\limits_0^{+\infty} te^{-t}dt = \ln \frac \lambda\mu + \frac \mu \lambda - 1.
$$

И здесь KL-дивергенция равна нулю при $\lambda = \mu$.

{% endcut %}

### Кросс-энтропия

При определении KL-дивергенции мы уже встречались с **кросс-энтропией**

$$    \mathbb H[p, q] = -\mathbb E_{\xi \sim p(x)} \log q(\xi)
$$

В зависимости от типа распределений кросс-энтропия вычисляется по формуле

$$    \mathbb H[p, q] = -\sum\limits_k p_k \log q_k \text{ или  }
    \mathbb H[p, q] = -\int\limits_{-\infty}^{+\infty} p(x) \log q(x)\, dx.
$$

Поскольку

$$    \mathbb{KL}(p\vert\vert q) = \mathbb H[p, q] - \mathbb H[p],
$$

задача минимизации KL-дивергенции между неизвестным распределением данных $p(x)$ и модельным распределением $q(x)$ эквивалентна задаче минимизации кросс-энтропии. Разница между ними равна энтропии распределения $p(x)$, которая, очевидно, не зависит от $q(x)$&nbsp.

В машинном обучении кросс-энтропию часто используют в качестве функции потерь в задаче классификации на $K>1$ классов. Истинное распределение на каждом обучающем объекте задаётся с помощью one hot encoding и является вырожденным:

$$\boldsymbol y = (y_1, \ldots, y_K), \quad y_k \in \{0,1\}, \quad \sum\limits_{k=1}^K y_k = 1.
$$

Классификатор обычно выдаёт вероятности принадлежности каждому из классов,

$$\boldsymbol{\widehat y} = (\widehat y_1, \ldots, \widehat y_K), \quad \widehat y_k = \mathbb P(\text{класс }k).
$$

Функция потерь на одном объекте полагается равной кросс-энтропии между истинным и предсказанным распределениями:

$$    \mathcal L(\boldsymbol y, \boldsymbol {\widehat y}) = -\sum\limits_{k=1}^K y_k \log \widehat y_k.
$$

И это вполне логично: чем ближе модельное распределение к истинному, тем меньше наши потери. В идеале $\mathcal L(\boldsymbol y, \boldsymbol {\widehat y}) = 0$, если $\boldsymbol y =  \boldsymbol {\widehat y}$.

Чтобы вычислить функцию потерь по обучающей выборке из $N$ объектов с метками $\boldsymbol y^{(i)}$, обычно берут усреднённную кросс-энтропию

$$    \frac 1N \sum\limits_{i=1}^N\mathcal L(\boldsymbol y^{(i)}, \boldsymbol{\widehat y}^{(i)})
    =-\frac 1N\sum\limits_{i=1}^N\sum\limits_{k=1}^K y^{(i)}_k \log \widehat y^{(i)}_k.
$$

## Принцип максимальной энтропии

В <a href='https://education.yandex.ru/handbook/ml/article/parametricheskie-ocenki'>параграфе про оценки параметров</a> были описаны различные свойства параметрических оценок и методы их получения, например, метод моментов или метод максимального правдоподобия. В принципе, если мы уже выбрали для наших данных $X_1, \ldots, X_n$ некоторое параметрическое семейство $F_\theta(x)$, моделирующее их распределение, восстановить его параметры чаще всего можно по выборочному среднему $\overline{X}_n = \frac{1}{n}\sum\limits_{k = 1}^n X_k$ и/или выборочной дисперсии $\overline S_n = \frac{1}{n} \sum\limits_{k = 1}^n (X_k - \overline{X}_n)^2$.

А теперь представим, что мы посчитали эти (или какие-то другие) статистики, а семейство распределений пока не выбрали. Как же совершить этот судьбоносный выбор? Давайте посмотрим на следующие три семейства и подумаем, в каком из них мы бы стали искать распределение, зная его истинные матожидание и дисперсию?

![Three](https://yastatic.net/s3/education-portal/media/Three_classes_aaffab9011_c0262c0989.webp)

Почему-то хочется сказать, что в первом. Почему? Второе не симметрично – но что нас может заставить подозревать, что интересующее нас распределение не симметрично? С третьим проблема в том, что, выбирая его, мы добавляем дополнительную информацию как минимум о том, что у распределения конечный носитель. А с чего бы? У нас такой инфомации вроде бы нет.

Общая идея такова: мы будем искать распределение, которое удовлетворяет только явно заданным нами ограничениям и не отражает никакого дополнительного знания о нём. Таким образом, искомое распределение должно обладать максимальной неопределённостью при заданных ограничениях, или, говоря более научно, иметь максимально возможную энтропию. В самом деле, энтропия выражает нашу меру незнания о том, как ведёт себя распределение, и чем она больше – тем более «‎произвольное» распределение, по крайней мере, в теории. В этом и заключается **принцип максимальной энтропии** для выбора модели машинного обучения.

Как мы уже видели выше, среди распределений с конечным носителем максимальную энтропию имеет равномерное распределение. Примеры геометрического и нормального распределения показывают, что энтропия распределений с бесконечным носителем (счётным или континуальным) может быть сколь угодно большой, и среди них нет какого-то одного распределения с максимальной энтропией. Однако в более узком классе распределений с фиксированным средним и/или дисперсией найти распределение с максимальной энтропией, как правило, можно.

**Пример**. Покажем, что среди распределений на множестве натуральных чисел $\mathbb N$ и математическим ожиданием $\mu > 1$ максимальную энтропию имеет геометрическое распределение.

Для минимизации энтропии $\mathbb H[p] = -\sum\limits_{n=1}^\infty p_n \log p_n$ с учётом ограничений

$$\sum\limits_{n=1}^\infty p_n = 1, \quad \sum\limits_{n=1}^\infty np_n = \mu
$$

воспользуемся [методом множителей Лагранжа](https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%BC%D0%BD%D0%BE%D0%B6%D0%B8%D1%82%D0%B5%D0%BB%D0%B5%D0%B9_%D0%9B%D0%B0%D0%B3%D1%80%D0%B0%D0%BD%D0%B6%D0%B0), согласно которому требуется минимизировать функцию Лагранжа

$$    \mathcal L(p, a, b) = \sum\limits_{n=1}^\infty p_n \log p_n - a\Big(\sum\limits_{n=1}^\infty p_n - 1\Big) - b\Big(\sum\limits_{n=1}^\infty np_n - \mu\Big).
$$

Приравняем к нулю частные производные по $p_n$:

$$    \frac{\partial \mathcal L}{\partial p_n} = 1 + \log p_n - a - bn = 0.
$$

Отсюда следует, что $p_n = e^{a-1 + bn} = \alpha \beta^n$, так что распределение действительно получается геометрическое. Параметры $\alpha$ и $\beta$ найдём из уравнений

$$    1 = \sum\limits_{n=1}^\infty p_n  = \sum\limits_{n=1}^\infty \alpha \beta^n = \frac{\alpha\beta}{1-\beta},
$$

$$    \mu = \sum\limits_{n=1}^\infty np_n  = \alpha \beta\sum\limits_{n=1}^\infty n \beta^{n-1} = \frac{\alpha\beta}{(1-\beta)^2}.
$$

Деля первое уравнение на второе, получаем $\frac 1 \mu = 1 - \beta$, или $\beta = 1 - \frac 1\mu$. Далее из первого уравнения находим $\alpha = \frac {1-\beta}{\beta} = \frac 1{\mu -1}$. Итак,

$$p_n = \frac 1{\mu -1} \Big(1 - \frac 1\mu\Big)^n = \frac 1\mu \Big(1 - \frac 1\mu\Big)^{n-1},
$$

а это и есть геометрическое распределение с параметром $\frac 1\mu$.

У непрерывных распределений возможны более интересные комбинации из ограничений на носитель и параметры. И конечно же, первую скрипку среди распределений с максимальной энтропией играет гауссовское распределение.

**Пример**. Докажем, что среди распределений на $\mathbb R$ c математическим ожиданием $\mu$ и дисперсией $\sigma^2$ наибольшую энтропию имеет нормальное распределение $\mathcal{N}(\mu,\sigma^2)$.

Пусть $p(x)$ – некоторое распределение со средним $\mu$ и дисперсией $\sigma^2$, $q(x)\sim\mathcal{N}(\mu, \sigma^2)$. Как было показано выше, $\mathbb H[q] = \frac12\log(2\pi\sigma^2) + \frac12$. Запишем дивергенцию Кульбака-Лейблера:

$$\mathbb KL(p\vert\vert q) = \int\limits_{-\infty}^{+\infty} p(x)\log{p(x)}dx - \int\limits_{-\infty}^{+\infty} p(x)\log{q(x)}dx =
$$

$$= -\mathbb H[p] - \int\limits_{-\infty}^{+\infty} p(x)\left(-\frac12\log(2\pi\sigma^2) - \frac 1{2\sigma^2}(x - \mu)^2\right)dx =
$$

$$= - \mathbb H[p] +\frac12\log(2\pi\sigma^2)\int\limits_{-\infty}^{+\infty} p(x)\,dx + \frac1{2\sigma^2}\underbrace{\int\limits_{-\infty}^{+\infty}(x - \mu)^2p(x)\,dx}_{=\mathbb{V}[p]=\sigma^2} =
$$

$$= - \mathbb H[p] + \frac12\log(2\pi\sigma^2) + \frac12 = \mathbb H[q] - \mathbb H[p].
$$

Так как KL-дивергенция всегда неотрицательна, получаем, что $\mathbb H[p]\leqslant \mathbb H[q]$ при любом распределении $p$, удовлетворяющем заданным ограничениям.

Можно показать, что максимальную энтропию среди многомерных распределений с вектором средних $\boldsymbol \mu$ и матрицей ковариаций $\boldsymbol \Sigma$ имеет также гауссовское распределение $\mathcal N(\boldsymbol \mu, \boldsymbol \Sigma)$.

**Упражнение**. Докажите, что среди распределений на отрезке $[a,b]$ максимальную энтропию имеет равномерное распределение $U[a,b]$.

{% cut "Попробуйте доказать самостоятельно, прежде чем смотреть решение." %}

Пусть $q(x) \sim U[a,b]$, тогда для произвольного распределения $p$ на отрезке $[a,b]$ имеем

$$  \mathbb H[p,q] = - \int\limits_a^b p(x)\log q(x)\,dx = \int\limits_a^b p(x) \log(b-a)\,dx = \log(b-a).
$$

В частности, отсюда следует, что $\mathbb H[q] = \log(b-a)$. Расписывая теперь KL-дивергенцию, получаем

$$\mathbb{KL}(p\vert\vert q) = \mathbb H[p,q] - \mathbb H[p] = \log(b-a) - \mathbb H[p] = \mathbb H[q] - \mathbb H[p] \geqslant 0,
$$

что и требовалось доказать.

{% endcut %}

**Упражнение**. Докажите, что среди распределений на промежутке $[0, +\infty)$ с математическим ожиданием $\lambda > 0$ максимальную энтропию имеет показательное распределение $\mathrm{Exp\big(\frac 1\lambda)}$.

{% cut "Попробуйте доказать самостоятельно, прежде чем смотреть решение." %}

Согласно результату одного из предыдущих упражнений $\mathbb H[q] = \log \lambda + 1$, если $q(x) \sim \mathrm{Exp}\big(\frac 1\lambda)$, т.е. $q(x) = \frac 1\lambda e^{-\frac x\lambda}$, $x\geqslant 0$. Далее, для произвольного распределения $p(x)$ на $[0, +\infty)$ со средним $\lambda$ имеем

$$\mathbb{KL}(p\vert\vert q) = \mathbb H[p,q] - \mathbb H[p] = - \int\limits_0^{+\infty} p(x)\Big(-\log\lambda - \frac x\lambda\Big) \,dx - \mathbb H[p]=
$$

$$=\log\lambda + \frac 1\lambda \int\limits_0^{+\infty} xp(x)\, dx  - \mathbb H[p] = \log\lambda + 1 - \mathbb H[p] =  \mathbb H[q]- \mathbb H[p] \geqslant 0.
$$

{% endcut %}

Как выяснилось, многие классические распределения имеют максимальную энтропию при весьма естественных ограничениях. Но как быть, если даны не эти конкретные, а какие-то другие ограничения? Есть ли какой-нибудь надёжный алгоритм вывода распределения с максимальной энтропией, позволяющий избежать случайных озарений и гаданий на кофейной гуще? Оказывается, что при некоторых не очень обременительных ограничениях ответ можно записать с помощью распределений экспоненциального класса.

## Экспоненциальное семейство распределений

Говорят, что параметрическое семейство распределений относится к **экспоненциальному классу**, если его pdf (или pmf) может быть представлена в виде

$$p(x\vert \boldsymbol\theta) = \frac{g(x)}{h(\boldsymbol\theta)}\exp(\boldsymbol \theta^T \boldsymbol u(x)) = g(x)\exp(\boldsymbol \theta^T \boldsymbol u(x) - A(\boldsymbol \theta)),
$$

где

* $\boldsymbol\theta \in\mathbb R^m$ – вектор **натуральных параметров** распределения;
* $g(x)$ — неотрицательная функция (**base measure**), часто равная единице;
* $h(\boldsymbol\theta) > 0$ — **нормализатор** (**partition**), обеспечивающий суммируемость pmf или интегрируемость pdf в единицу:

$$h(\boldsymbol\theta) = \int g(x)\exp(\boldsymbol \theta^T \boldsymbol u(x))dx;
$$

* $A(\boldsymbol \theta) = \ln h(\boldsymbol\theta)$ — **log-partition**;
* $\boldsymbol u(x)\in\mathbb R^m$ — вектор **достаточных статистик** распределения.

**Пример**. Покажем, что нормальное распределение $\mathcal N(x\vert \mu, \sigma^2)$ принадлежит экспоненциальному классу. Оно имеет два параметра, поэтому такую же размерность имеют $\boldsymbol \theta$ и вектор-функция $\boldsymbol u$.

Распишем плотность:

$$\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right) =
\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac 1{2\sigma^2}x^2 + \frac{\mu}{\sigma^2}x + \frac{\mu^2}{2\sigma^2}\right)=
\frac{\exp\left(-\frac1{2\sigma^2}x^2 + \frac{\mu}{\sigma^2}x\right)}{\sqrt{2\pi}\sigma\exp\left(-\frac{\mu^2}{2\sigma^2}\right)}.
$$

Положим $g(x) = \frac 1{\sqrt{2\pi}}$, $\boldsymbol u(x) = (x, x^2)$,

$$\boldsymbol \theta = (\theta_1, \theta_2),\quad \theta_1 = \frac{\mu}{\sigma^2},\quad \theta_2 = -\frac1{2\sigma^2}.
$$

Остаётся выразить функцию $h(\boldsymbol\theta) = \sigma\exp\left(-\frac{\mu^2}{2\sigma^2}\right)$ через $\theta_1$ и $\theta_2$.

**Упражнение**. Выразите partition $h(\boldsymbol\theta)$ и log-partition $A(\boldsymbol\theta)$ через $\boldsymbol\theta$ и запишите плотность нормального распределения в экспоненциальном виде.

{% cut "Ответ" %}

$$h(\boldsymbol\theta)=\sqrt{-2\theta_2} \exp\Big(\frac{\theta_1^2}{4\theta_2}\Big), \quad  A(\boldsymbol\theta) = -\frac{\theta_1^2}{4\theta_2} - \frac 12\ln(-2\theta_2),
$$

$$\mathcal N(x \vert \mu, \sigma^2) = \frac1{\sqrt {2\pi}}\cdot \frac 1{h(\boldsymbol\theta)}\exp(\boldsymbol\theta^T \boldsymbol u(x)) = \frac1{\sqrt {2\pi}} \exp(\boldsymbol \theta^T \boldsymbol u(x) - A(\boldsymbol \theta)).
$$

{% endcut %}

**Пример**. Покажем, что распределение Бернулли $\mathrm{Bern}(p)$ принадлежит экспоненциальному классу. Его pmf $\mathbb P(\xi =x \vert p)$ можно записать как

$$  p^x(1 - p)^{1 - x} = \exp\big(x\log{p} + (1 - x)\log(1 - p)\big)=
  (1-p) \exp\Big(x\log\frac p{1-p}\Big).
$$

Параметр здесь один, поэтому натуральный параметр $\theta$ тоже один: $\theta = \log\frac p{1-p}$. Такая функция от $p$ называется функцией логитов и активно участвует в построении модели [логистической регрессии](https://education.yandex.ru/handbook/ml/article/linear-models#linejnaya-klassifikaciya). Остальные функции положим равными $u(x) = x$, $g(x)=1$, $h(\theta) = \frac 1{1-p}$. Остаётся выразить partition через $\theta$:

$$  \log\frac p{1-p} = \log\Big(-1 + \frac 1{1-p}\Big) = \theta \iff \frac 1{1-p} = 1 + e^\theta. 
$$

Итак, $h(\theta) = 1+e^\theta$, и экспоненциальный вид распределения Бернулли записывается как

$$  \frac 1{1+e^\theta}\exp(\theta x) = \exp\big(\theta x - \log(1 + e^\theta)\big).
$$

**Вопрос на подумать**. Принадлежит ли к экспоненциальному классу семейство равномерных распределений на отрезках $U[a, b]$? Казалось бы, да, ведь

$$p(x) = \frac{1}{b - a}\mathbb{I}_{[a,b]}(x)\exp(0).
$$

В чём может быть подвох?

{% cut "Попробуйте определить сами, прежде чем смотреть ответ." %}

Нет, не принадлежит. Давайте вспомним, как звучало определение экспоненциального семейства. Возможно, вас удивило, что там было написано не «распределение относится», а «семейство распределений относится». Это важно: ведь семейство определяется именно различными значениями $\theta$, и если нас интересует семейство равномерных распределений на отрезках, определяемое параметрами $a$ и $b$, то они не могут быть в функции $g(x)$, они должны быть под экспонентой, а экспонента ни от чего не может быть равна индикатору.

При этом странное и не очень полезное семейство с нулём параметров, состоящее из одинокого распределения $U[0,1]$ можно считать относящимся к экспоненциальному классу: ведь для него формула

$$p(x) = \mathbb{I}_{[0,1]}(x)\exp(0)
$$

будет работать.

{% endcut %}

К экспоненциальным семействам относятся многие непрерывные и дискретные распределения из часто встречающихся в теории и на практике, в том числе

* нормальное $\mathcal N(\mu, \sigma^2)$;
* распределение Пуассона $\mathrm{Pois}(\lambda)$;
* экспоненциальное $\mathrm{Exp}(\lambda)$;
* биномиальное $\mathrm{Bin}(n, p)$;
* геометрическое $\mathrm{Geom}(p)$;
* бета-распределение;
* гамма-распределение;
* распределение Дирихле.

Как выглядят натуральные параметры, достаточные статистики и нормализаторы этих и других распределений из экспоненциального класса, можно посмотреть на [википедии](https://en.wikipedia.org/wiki/Exponential_family#Table_of_distributions).

К экспоненциальным семействам не относятся, например, равномерное распределение $U[a,b]$, $t$-распределение Стьюдента, распределение Коши, смесь нормальных распределений.

### Дифференцирование log-partition

Если распределение $p(x\vert \boldsymbol\theta)$ принадлежит экспоненциальному классу,

$$p(x\vert \boldsymbol\theta) = \frac{g(x)}{h(\boldsymbol\theta)}\exp(\boldsymbol \theta^T \boldsymbol u(x)) = g(x)\exp(\boldsymbol \theta^T \boldsymbol u(x) - A(\boldsymbol \theta)),
$$

то моменты его достаточных статистик $\boldsymbol u(x)$ могут быть получены дифференцированием функции $A(\boldsymbol \theta) = \log h(\boldsymbol\theta)$.

**Утверждение**. $\nabla_{\boldsymbol\theta}\log h(\boldsymbol \theta) = \mathbb{E}_{\xi \sim p(x\vert \boldsymbol \theta)} \boldsymbol u(\xi)$.

**Доказательство**.
По правилу дифференцирования сложной функции имеем

$$    \nabla_{\boldsymbol\theta}\log{h(\boldsymbol\theta)} = \frac{\nabla_{\boldsymbol\theta}{h(\boldsymbol\theta)}}{h(\boldsymbol\theta)}.
$$

Нормализатор $h(\boldsymbol\theta)$ записывается в виде интеграла

$$  h(\boldsymbol\theta) = \int g(x)\exp\left(\boldsymbol\theta^T\boldsymbol u(x)\right)dx,
$$

который мы продифференцируем внесением градиента внутрь под знак интеграла:

$$\nabla_{\boldsymbol\theta} h(\boldsymbol\theta) = \nabla_{\boldsymbol\theta} 
 \int g(x)\exp\left(\boldsymbol\theta^T\boldsymbol u(x)\right)dx =
$$

$$ =\int g(x)\nabla_{\boldsymbol\theta}  
 \exp\left(\boldsymbol\theta^T\boldsymbol u(x)\right)dx = 
 \int g(x)\boldsymbol u(x)
 \exp\big(\boldsymbol\theta^T\boldsymbol u(x)\big)dx.
$$

Таким образом,

$$  \frac{\nabla_{\boldsymbol\theta}{h(\boldsymbol\theta)}}{h(\boldsymbol\theta)}
    =\int \boldsymbol u(x)\underbrace{\frac{g(x)}{h(\boldsymbol \theta)}\exp(\boldsymbol \theta^T\boldsymbol u(x))}_{p(x\vert\boldsymbol \theta)}dx = \mathbb{E}_{\xi \sim p(x\vert \boldsymbol \theta)} \boldsymbol u(\xi).
$$

{% cut "Замечание о дифференцировании под знаком интегралом" %}

Использованный в доказательстве приём внесения градиента под знак интеграла называют также [правилом Лейбница](https://en.wikipedia.org/wiki/Leibniz_integral_rule). Этот же метод используется для почленного дифференцирования ряда, что может быть полезно в случае дискретного распределения $p(x\vert \boldsymbol\theta)$. В математическом анализе имеется ряд теорем, обеспечивающих применимость правила Лейбница, однако, мы не будем на них останавливаться. Будем считать, что все рассматриваемые экспоненциальные семейства таковы, что применение правила Лейбница законно.

{% endcut %}

Если $u_i(x) = x^i$, то в соответствии с только что доказанным частная производная $\frac{\partial A(\boldsymbol \theta)}{\partial \theta_i}$ даёт $i$-й момент распределения $p(x\vert \boldsymbol\theta)$.

**Упражнение**. Вычислите производные по натуральным параметрам от log-partition для распределения Бернулли $\mathrm{Bern}(x\vert p)$ и нормального распределения $\mathcal N(\mu, \sigma^2)$ и проверьте, что они совпадают со значениями соответствующих моментов.

{% cut "Решение" %}

Для $\xi \sim \mathrm{Bern}(x\vert p)$ имеем

$$  A(\theta) = \log(1 + e^\theta), \quad \frac 1{1-p} = 1 + e^\theta,
$$

поэтому

$$  A'(\theta) = \frac{e^\theta}{1+e^\theta} = 1 - \frac 1{1+e^\theta} = 1 -1 + p = p = \mathbb E\xi.
$$

Для гауссовской случайной величины $\eta\sim\mathcal N(x \vert\mu, \sigma^2)$ получаем

$$  A(\theta_1, \theta_2) =  -\frac{\theta_1^2}{4\theta_2} - \frac 12\ln(-2\theta_2),
$$

$$  \quad \theta_1 = \frac{\mu}{\sigma^2},
$$

$$  \quad \theta_2 = -\frac1{2\sigma^2},
$$

и, значит,

$$  \frac{\partial A}{\partial\theta_1} = -\frac{\theta_1}{2\theta_2} = \mu = \mathbb E\eta,
$$

$$  \frac{\partial A}{\partial\theta_2} = \frac{\theta_1^2}{4\theta_2^2} - \frac 1{2\theta_2} = \mu^2 + \sigma^2 = \mathbb E\eta^2.
$$

{% endcut %}

Кстати, можно продифференцировать ещё раз и доказать, что

$$  \nabla^2_{\boldsymbol\theta}\log{h(\boldsymbol\theta)} = \mathrm{cov}(\boldsymbol u(\xi), \boldsymbol u(\xi)).
$$

{% cut "Доказательство" %}

В предыдущий раз мы доказали, что

$$\nabla_{\boldsymbol\theta}A(\boldsymbol\theta) = \int \boldsymbol u(x)g(x)\exp(\boldsymbol \theta^T\boldsymbol u(x) - A(\boldsymbol \theta))\,dx.
$$

Теперь возьмём ещё раз градиент по $\boldsymbol \theta$ от этого выражения:

$$\nabla^2_{\boldsymbol\theta}A(\boldsymbol\theta) = \int \boldsymbol u(x)g(x)\nabla_{\boldsymbol\theta}\exp\big(\boldsymbol \theta^T\boldsymbol u(x) - A(\boldsymbol \theta)\big)\,dx
=
$$

$$= \int \boldsymbol u(x)g(x)\big(\boldsymbol u(x) - \nabla_{\boldsymbol\theta}A(\boldsymbol \theta)\big)^T \exp\big(\boldsymbol \theta^T\boldsymbol u(x) - A(\boldsymbol \theta)\big)\,dx=
$$

$$=\int \boldsymbol u(x)g(x)\big(\boldsymbol u(x) -  \mathbb{E} \boldsymbol u\big)^T \exp\big(\boldsymbol \theta^T\boldsymbol u(x) - A(\boldsymbol \theta)\big)\,dx =
$$

$$= \mathbb{E} \boldsymbol u \boldsymbol u^T - \mathbb{E} \boldsymbol u \big(\mathbb{E} \boldsymbol u\big)^T = \mathrm{cov}(\boldsymbol u(\xi), \boldsymbol u(\xi)).
$$

В последней выкладке для краткости обозначено $\mathbb E \boldsymbol u = \mathbb{E}_{\xi \sim p(x\vert \boldsymbol \theta)} \boldsymbol u(\xi)$.

{% endcut %}

### MLE для семейства из экспоненциального класса

Возможно, вас удивил странный и на первый взгляд не очень естественный вид $p(x\vert \boldsymbol \theta)$. Но всё не просто так: оказывается, что оценка максимального правдоподобия параметров распределений из экспоненциального класса устроена очень интригующе.

Запишем функцию правдоподобия i.i.d. выборки $x_1,\ldots,x_n$:

$$p(x_1, \ldots, x_n\vert\boldsymbol \theta) = h(\boldsymbol \theta)^{-n}\cdot\bigg(\prod_{i=1}^ng(x_i)\bigg)\cdot\exp\bigg(\boldsymbol \theta^T \sum_{i=1}^n \boldsymbol u(x_i)\bigg).
$$

Её логарифм равен

$$\log p(x_1, \ldots, x_n\vert\boldsymbol \theta) = -n\log{h(\boldsymbol \theta)} + \sum_{i=1}^n\log{g(x_i)} + \boldsymbol \theta^T\sum_{i=1}^n \boldsymbol u(x_i).
$$

Дифференцируя по $\boldsymbol \theta$, получаем

$$\nabla_{\boldsymbol\theta}\log p(x_1, \ldots, x_n\vert\boldsymbol\theta) = -n\nabla_{\boldsymbol\theta}\log{h(\boldsymbol\theta)} + \sum_{i=1}^n \boldsymbol u(x_i).
$$

Приравнивая $\nabla_{\boldsymbol\theta}\log p(x_1,\ldots, x_n\vert\boldsymbol\theta)$ к нулю и пользуясь равенством $\nabla_{\boldsymbol\theta}\log{h(\boldsymbol\theta)} = \mathbb{E}_{\xi \sim p(x\vert \boldsymbol \theta)} \boldsymbol u(\xi)$, находим

$$\mathbb{E}_{\xi \sim p(x\vert \boldsymbol \theta)} \boldsymbol u(\xi) = \frac1n \sum_{i=1}^n \boldsymbol u(x_i).
$$

Таким образом, теоретические матожидания всех компонент $u_i(\xi)$ должны совпадать с их эмпирическими оценками, а метод максимального правдоподобия совпадает с методом моментов для $\mathbb{E}u_i(\xi)$ в качестве моментов. И в следующем пункте выяснится, что распределения из экспоненциальных семейств обладают максимальной энтропией среди тех, что имеют заданные моменты $\mathbb{E}u_i(\xi)$.

### Теорема Купмана—Питмана—Дармуа

Теперь мы наконец готовы сформулировать одно из самых любопытных свойств семейств экспоненциального класса.

В следующей теореме мы опустим некоторые не очень обременительные условия регулярности. Просто считайте, что для хороших дискретных и абсолютно непрерывных распределений, с которыми вы в основном и будете сталкиваться, это так.

**Теорема**. Пусть параметр $\boldsymbol\theta\in R^m$ распределения $p_{\boldsymbol\theta}(x) = \frac1{h(\boldsymbol\theta)}\exp\big(\boldsymbol\theta^T\boldsymbol u(x)\big)$ выбран так, что

$$\mathbb{E}_{\xi \sim p_{\boldsymbol\theta}(x)} \boldsymbol u(\xi) = \boldsymbol\alpha
$$

для некоторого фиксированного $\boldsymbol \alpha \in\mathbb R^m$. Тогда распределение $p_{\boldsymbol\theta}(x)$ обладает наибольшей энтропией среди распределений $q$ с тем же носителем, для которых $\mathbb{E}_{\xi \sim q(x)} \boldsymbol u(\xi) = \boldsymbol\alpha$.

{% cut "Идея обоснования через оптимизацию." %}

Мы приведём рассуждение для дискретного случая; в абсолютно непрерывном рассуждения будут по сути теми же, только там придётся дифференцировать не по переменных, а по функциям, и мы решили не ввергать читателя в мир вариационного исчисления.

В дискретном случае у нас есть счётное семейство точек $x_1, x_2,\ldots$, и распределение определяется счётным набором вероятностей $p_i$ принимать значение $x_i$. Мы будем решать задачу

$$\begin{cases}
-\sum_j p_j\log{p_j}\longrightarrow\max,\\
\sum_jp_ju_i(x_j) = \alpha_i, i = 1,\ldots,m,\\
\sum_jp_j = 1,\\
p_j\geqslant 0.
\end{cases}
$$

Для решения этой оптимизационной задачи нам понадобится обобщение метода множителей Лагранжа, известное также как [теорема Каруша—Куна—Таккера](https://ru.wikipedia.org/wiki/%D0%A3%D1%81%D0%BB%D0%BE%D0%B2%D0%B8%D1%8F_%D0%9A%D0%B0%D1%80%D1%83%D1%88%D0%B0_%E2%80%94_%D0%9A%D1%83%D0%BD%D0%B0_%E2%80%94_%D0%A2%D0%B0%D0%BA%D0%BA%D0%B5%D1%80%D0%B0). В данном случае задача сводится к минимизации лагранжиана

$$\mathcal{L} = \sum_j p_j\log{p_j} + \sum_i\theta_i\left(\alpha_i - \sum_jp_ju_i(x_j)\right)+\theta_0\left(\sum_jp_j - 1\right) - \sum_j\lambda_jp_j
$$

при имеющихся ограничениях и условиях дополняющей нежёсткости $\lambda_jp_j = 0$.
Приравняем частные производные по $p_j$ к нулю:

$$\frac{\partial\mathcal{L}}{\partial p_j} = \log{p_j} + 1 - \sum_i\theta_iu_i(x_j) + \theta_0 - \lambda_j = 0.
$$

Отсюда получаем, что

$$p_j = \frac{\exp(\boldsymbol \theta^T \boldsymbol u(x_j))}{\exp\left(\lambda_j - \theta_0 - 1\right)}.
$$

Числитель уже ровно такой, как и должен быть у распределения из экспоненциального класса; разберёмся со знаменателем. Поскольку $p_j >0$ (ведь тут сплошные экспоненты), $\lambda_j = 0$ при всех $j$. Параметр $\theta_0$ находится из условия $\sum_jp_j = 1$, а точнее, выражается через остальные $\theta_i$, что позволяет записать знаменатель в виде $h(\boldsymbol \theta)$.

{% endcut %}

{% cut "Идея доказательства «в лоб»." %}

Как и следовало ожидать, оно ничем не отличается от того, как мы доказывали максимальность энтропии у равномерного или нормального распределения. Пусть $q(x)$ – ещё одно распределение, для которого

$$\int u_i(x)q(x)dx = \int u_i(x)p_{\boldsymbol\theta}(x)dx
$$

для всех $i = 1,\ldots,m$. Тогда

$$0\leqslant \mathbb{KL}(q\vert\vert p) = \int q(x)\log\left(\frac{q(x)}{p_{\boldsymbol\theta}(x)}\right)dx = 
$$

$$=\underbrace{\int q(x)\log{q(x)}dx}_{-\mathbb H[q]} - \int q(x)\log{p_{\boldsymbol\theta}(x)}dx=
$$

$$=-\mathbb H[q] - \int q(x)\left(-\log{h(\boldsymbol \theta)} + \sum_i\theta_iu_i(x)\right)dx =
$$

$$=-\mathbb H[q] - \log{h(\theta)}\underbrace{\int q(x)dx}_{=1=\int p_{\boldsymbol\theta}(x)dx} - \sum_i\theta_i\underbrace{\int q(x)u_i(x)dx}_{=\int p_{\boldsymbol\theta}(x)u_i(x)dx} =
$$

$$=-\mathbb H[q] - \int p(x)\left(-\log{h(\boldsymbol\theta)} + \sum_i\theta_iu_i(x)\right)dx =
$$

$$=-\mathbb H[q] + \int p_{\boldsymbol\theta}(x)\log{p_{\boldsymbol\theta}(x)}dx = -\mathbb H[q] + \mathbb H[p_{\boldsymbol\theta}].
$$

Таким образом, $\mathbb H[p_{\boldsymbol\theta}]\geqslant \mathbb H[q]$, что и требовалось доказать.

{% endcut %}

Выше мы уже находили обладающее максимальной энтропией распределение на множестве натуральных чисел с заданным математическим ожиданием $\mu>1$. Таковым оказалось геометрическое распределение $\mathrm{Geom}\big(\frac 1\mu\big)$.

Теорема Купмана—Питмана—Дармуа позволяет сделать это гораздо быстрее.
В данном случае у нас лишь одна функция $u_1(x) = x$, которая соответствует фиксации математического ожидания $\mathbb E\xi$. Искомое дискретное распределение имеет вид

$$p_k =\mathbb P(\xi = k) = \frac1{h(\theta)}\exp(\theta k) =  \frac1{h(\theta)} \big(e^{\theta}\big)^k.
$$

Это уже похоже на геометрическое распределение с параметром $p = 1 - e^{\theta}$. Его математическое ожидание равно $\frac 1p$, что по условию должно равняться $\mu$. Итак, наше распределение с максимальной этропией выглядит так:

$$p_k = \frac1{\mu}\left(1 - \frac1{\mu}\right)^{k-1},\quad k\in\mathbb N.
$$

**Пример**. Среди распределений на всей вещественной прямой с заданным математическим ожиданием $\mu$ найдём распределение с максимальной энтропией.

{% cut "А сможете ли вы его найти? Решение под катом." %}

Теория говорит нам, что его плотность должна иметь вид

$$p(x) = \frac1{h(\theta)}\exp\left(\theta x\right),
$$

но интеграла экспоненты не существует, то есть применение «в лоб» теоремы провалилось. И неспроста: если даже рассмотреть все нормально распределённые случайные величины со средним $\mu$, их энтропии, равные $\frac12 + \frac12\log(2\pi\sigma^2)$, не ограничены сверху, то есть величины с наибольшей энтропией не существует.

{% endcut %}

  ## handbook

  Учебник по машинному обучению

  ## title

  Энтропия и семейство экспоненциальных распределений

  ## description

  *null*

- 
  ## path

  /handbook/ml/article/pervie-shagi

  ## content

  ## Рабочее окружение для ML-специалиста

Грубо говоря, оно делится на две большие категории:

* Железо и вычислительные ресурсы для обучения моделей;
* Программы и библиотеки для работы с данными.

Начнём со второй категории.

Чтобы начать работу, нужно установить Python — именно этот язык программирования доминирует в индустрии, благодаря большому количеству библиотек и фреймворкам, предназначенным именно для машинного обучения — TensorFlow или PyTorch.

Фреймворк — слой абстракции над языком программирования, который облегчает разработку. Например — нам нужно сделать отверстие в доске. Эту задачу можно решить разными способами: закрутить и выкрутить шуруп, забить и вытащить гвоздь и так далее. А можно взять дрель и сверло. Дрель в этом примере и есть фреймворк.

PyTorch чаще выбирают для академических исследований — он более гибкий и больше подходит для экспериментов. TensorFlow — для продакшен-решений, поскольку он более подходит для масштабирования моделей.

Далее нужно установить библиотеки для Python. Продолжая строительную аналогию: библиотека — это насадка для дрели. То есть инструмент для конкретной задачи: можно установить сверло для дерева, для бетона, для металла, а можно коронку или щётку — зависит от задачи.

Чаще всего применяют:

* Scikit-learn — библиотека машинного обучения для классических алгоритмов: классификации, регрессии, ансамблей и других. О них мы подробнее поговорим далее в этом хендбуке.
* Pandas — библиотека для предварительной обработки данных, и работы с данными вообще. С её помощью можно загрузить датасет, обработать недостающие значения, закодировать категориальные переменные и многое другое.
* Matplotlib и Seaborn — библиотеки для создания визуализаций и графиков в Python.

После этого — выбрать IDE, то есть текстовый редактор для кода: Visual Studio Code, Jupyter, Sublime, PyCharm, и так далее.

Теоретически, всё это можно установить на домашний компьютер или ноутбук — именно так и делали ещё 15-20 лет назад. Но вам не хватит вычислительных ресурсов для обучения моделей, в первую очередь — объёма памяти GPU (видеокарты).

Даже для файнтюнинга небольших языковых моделей, таких как BERT, необходим графический процессор с минимум 16 Гб видеопамяти. Мало кто может позволить себе дома оборудование для обучения более сложных моделей.

Сейчас исследователи и студенты чаще берут вычислительные мощности в аренду. Тут есть два способа:

* арендовать устройство «в облаке» (эта модель называется IaaS),
* воспользоваться специальной платформой для ML (эта модель называется SaaS).

IaaS-сервис, грубо говоря, — очень мощный удалённый компьютер. Это значит, что прежде чем решать задачу на такой машине, её всё равно необходимо настроить: развернуть IDE, установить Python, фреймворки и библиотеки и многое другое. Это не всегда удобно: иногда хочется, чтобы всё работало «из коробки».

«Из коробки», как вы могли догадаться, работают SaaS-сервисы: они предоставляют полностью настроенные среды, готовые к немедленному использованию в решении задач.

Эти платформы обычно включают в себя:

* IDE или другие среды программирования, часто представленные в формате ноутбуков.
* Заранее настроенные рабочие окружения, оптимизированные для конкретной системы.
* Возможности для загрузки и хранения данных и файлов.
* Интеграцию с известными сервисами, такими как GitHub.

О них мы и поговорим далее. Но если вам ближе путь самурая — то вот несколько [IaaS](https://cloud.google.com/learn/what-is-iaas) [провайдеров](https://aws.amazon.com/ru/ai/machine-learning/). По ссылкам можно узнать, как развернуть окружение для ML в IaaS-сервисе.

## SaaS-платформы

Как мы уже выяснили, главное преимущество SaaS – это простота входа: вы получаете доступ к необходимым ресурсам без забот о их настройке и оптимизации, что позволяет быстро приступить к работе над ML-задачами.

К популярным SaaS-платформам относят:

* Google Colab
* Kaggle Notebooks
* AWS SageMaker
* Azure ML Studio
* Yandex DataSphere

Ниже мы собрали в таблицу их возможности, плюсы и минусы.

![яндекс иллюстрации](https://yastatic.net/s3/education-portal/media/yandeks_illyustraczii_tablicza_01_b7f613ce76.svg)

Далее мы расскажем, как решать ML-задачи на примере Yandex DataSphere. Но если вас заинтересовали другие платформы, то в конце параграфа будет список ссылок на руководства по работе с ними.

## Получение доступа и настройка DataSphere

Прежде чем мы начнём настройку — несколько важных моментов.

DataSphere — это платный сервис, но вы можете начать работу бесплатно, с помощью [тестового гранта](https://yandex.cloud/ru/docs/getting-started/usage-grant). Также у сервиса есть специальные гранты для учебных программ. Чтобы воспользоваться грантом, нужно попросить своего преподавателя заполнить [форму](https://cloud.yandex.ru/for-education-and-science/for-tutors), — это откроет доступ к сервису для всех студентов группы.

Отлично, теперь можем приступить к настройке. Для этого:

* Перейдите на сайт [DataSphere](https://datasphere.yandex.ru/)
* Нажмите большую синюю кнопку и авторизуйтесь в Яндекс ID
* Создайте сообщество и нажмите «Привязать платежный аккаунт»
  на появившемся красном дисклеймере.

В созданном сообществе вы сможете взаимодействовать со всеми важными сущностями в DataSphere. Теперь можно создать проект на вкладке «Проект».

![3.webp](https://yastatic.net/s3/education-portal/media/3_9a414bf739.webp)

В созданном проекте вы можете запустить JupyterLab:

![2.webp](https://yastatic.net/s3/education-portal/media/2_f09506b051.webp)

После незначительного ожидания откроется выбор среды исполнения. Там вы можете выбрать любой из примеров ноутбуков с различными снипеттами кода под разные задачи. Создадим новый пустой ноутбук, нажав “DataSphere Kernel”.

![1 (1).webp](https://yastatic.net/s3/education-portal/media/1_1_ca7b72bb0e.webp)

Теперь, в появившемся новом ноутбуке, если мы запустим любой код в одной из ячеек, вам будет предложено выбрать конфигурацию виртуального рабочего места (более подробно о доступных конфигурациях можно почитать [тут](https://cloud.yandex.ru/ru/docs/datasphere/concepts/configurations)).

После выделения ресурсов, которое тоже займет небольшое количество времени, все последующие выполнения ячеек будут происходить без выбора конфигурации.

Теперь, когда у нас всё готово — DataSphere настроена, ресурсы выделены, можем выполнить тестовую лабораторную работу!

## Лабораторная работа

В ней мы будем обучать генеративную трансформерную модель с помощью библиотеки `transformers`.

Сама работа находится в DataSphere — переходите по [ссылке](https://datasphere.yandex.cloud/import-ipynb?path=https://raw.githubusercontent.com/yandex-datasphere/sda-homeworks/main/train-trans/train-trans.ipynb), чтобы ознакомиться с заданием. А как закончите —  возвращайтесь, чтобы завершить урок.

Вот и всё! Если вы читаете эти строки, и у вас всё получилось — вы большой молодец. Если не получилось — ничего страшного, с первого раза мало у кого всё получается. Советуем вступить в [сообщество хендбука](https://t.me/MLhandbook) и попросить помощи или совета.

## Полезные ссылки

* [Руководство](https://colab.research.google.com/) по работе с Google Colab.
* [Гайд](https://www.kaggle.com/code/shagkala/a-complete-guide-for-beginners) для новичков по Kaggle Notebooks.
* [Руководство](https://docs.aws.amazon.com/sagemaker/latest/dg/onboard-quick-start.html) для AWS SageMaker.
* [Документация](https://learn.microsoft.com/en-us/azure/machine-learning/tutorial-azure-ml-in-a-day?view=azureml-api-2) по настройке Azure ML Studio
* [Статья](https://education.yandex.ru/journal/kak-ispolzovat-datasphere-vandnbspobrazovanii) про то, как используется DataSphere в образовании
* [Как](https://habr.com/ru/companies/yandex/articles/786560/) DataSphere помогает изучать снежных барсов

  ## handbook

  Учебник по машинному обучению

  ## title

  Первые шаги

  ## description

  В этой главе мы поговорим о рабочем окружении ML-специалиста — какие сервисы и библиотеки в него входят, как его развернуть, на что обратить внимание. 

А кроме того, в качестве быстрой практики обучим собственную модель генерировать ответы в стиле Льва Толстого.
