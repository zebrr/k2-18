"""
Unit —Ç–µ—Å—Ç—ã –¥–ª—è –º–æ–¥—É–ª—è slicer.py

–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –æ—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞:
- create_slug()
- preprocess_text()
- validate_config_parameters()
"""

import json

# –î–æ–±–∞–≤–ª—è–µ–º src –≤ path –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
import sys
import tempfile
import time
import unicodedata
from pathlib import Path

import pytest

sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from slicer import (
    InputError,
    create_slug,
    load_and_validate_file,
    preprocess_text,
    process_file,
    save_slice,
    slice_text_with_window,
    validate_config_parameters,
)
from utils.tokenizer import count_tokens


class TestCreateSlug:
    """–¢–µ—Å—Ç—ã –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ create_slug."""

    def test_cyrillic_transliteration(self):
        """–¢–µ—Å—Ç —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏–∏ –∫–∏—Ä–∏–ª–ª–∏—Ü—ã."""
        result = create_slug("–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –°—Ç—Ä—É–∫—Ç—É—Ä—ã.txt")
        assert result == "algoritmy_i_struktury"

    def test_english_with_spaces(self):
        """–¢–µ—Å—Ç –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å –ø—Ä–æ–±–µ–ª–∞–º–∏."""
        result = create_slug("My Course Chapter 1.md")
        assert result == "my_course_chapter_1"

    def test_hyphens_preserved(self):
        """–¢–µ—Å—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–µ—Ñ–∏—Å–æ–≤."""
        result = create_slug("python-basics.html")
        assert result == "python-basics"

    def test_extension_removal(self):
        """–¢–µ—Å—Ç —É–¥–∞–ª–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–π."""
        assert create_slug("test.json") == "test"
        assert create_slug("file.HTML") == "file"
        assert create_slug("document.MD") == "document"

    def test_complex_filename(self):
        """–¢–µ—Å—Ç —Å–ª–æ–∂–Ω–æ–≥–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞."""
        result = create_slug("–ì–ª–∞–≤–∞ 2. –û—Å–Ω–æ–≤—ã Python (—á–∞—Å—Ç—å-1).txt")
        assert result == "glava_2._osnovy_python_(chast'-1)"

    def test_special_characters(self):
        """–¢–µ—Å—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤."""
        result = create_slug("File@#$%^&*()Name.md")
        assert result == "file@#$%^&*()name"


class TestPreprocessText:
    """–¢–µ—Å—Ç—ã –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ preprocess_text."""

    def test_unicode_normalization(self):
        """–¢–µ—Å—Ç Unicode –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ NFC."""
        # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç —Å –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Å–∏–º–≤–æ–ª–∞–º–∏
        text_with_combining = "caf√©"  # e + ÃÅ (combining acute accent)
        result = preprocess_text(text_with_combining)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∞
        expected = unicodedata.normalize("NFC", text_with_combining)
        assert result == expected

    def test_script_tag_removal(self):
        """–¢–µ—Å—Ç —É–¥–∞–ª–µ–Ω–∏—è script —Ç–µ–≥–æ–≤."""
        html_with_script = """
        <html>
        <head>
        <script>alert('dangerous code');</script>
        </head>
        <body>
        <p>–ü–æ–ª–µ–∑–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç</p>
        </body>
        </html>
        """

        result = preprocess_text(html_with_script)

        # Script —Ç–µ–≥ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —É–¥–∞–ª–µ–Ω
        assert "<script>" not in result
        assert "alert('dangerous code');" not in result
        # –ü–æ–ª–µ–∑–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –æ—Å—Ç–∞—Ç—å—Å—è
        assert "–ü–æ–ª–µ–∑–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç" in result

    def test_style_tag_removal(self):
        """–¢–µ—Å—Ç —É–¥–∞–ª–µ–Ω–∏—è style —Ç–µ–≥–æ–≤."""
        html_with_style = """
        <html>
        <head>
        <style>
        body { background: red; }
        .danger { color: evil; }
        </style>
        </head>
        <body>
        <h1>–ó–∞–≥–æ–ª–æ–≤–æ–∫</h1>
        </body>
        </html>
        """

        result = preprocess_text(html_with_style)

        # Style —Ç–µ–≥ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —É–¥–∞–ª–µ–Ω
        assert "<style>" not in result
        assert "background: red" not in result
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –¥–æ–ª–∂–µ–Ω –æ—Å—Ç–∞—Ç—å—Å—è
        assert "–ó–∞–≥–æ–ª–æ–≤–æ–∫" in result

    def test_both_script_and_style_removal(self):
        """–¢–µ—Å—Ç —É–¥–∞–ª–µ–Ω–∏—è –∏ script, –∏ style —Ç–µ–≥–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ."""
        html_mixed = """
        <html>
        <head>
        <script src="evil.js"></script>
        <style>body { display: none; }</style>
        </head>
        <body>
        <h1>–ö–æ–Ω—Ç–µ–Ω—Ç</h1>
        <script>alert('popup');</script>
        </body>
        </html>
        """

        result = preprocess_text(html_mixed)

        assert "<script" not in result
        assert "<style>" not in result
        assert "evil.js" not in result
        assert "display: none" not in result
        assert "alert(" not in result
        assert "–ö–æ–Ω—Ç–µ–Ω—Ç" in result

    def test_plain_text_unchanged(self):
        """–¢–µ—Å—Ç —á—Ç–æ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç –æ—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π."""
        plain_text = """
        –≠—Ç–æ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç –±–µ–∑ HTML —Ç–µ–≥–æ–≤.
        
        –ì–ª–∞–≤–∞ 1. –í–≤–µ–¥–µ–Ω–∏–µ
        
        –ó–¥–µ—Å—å –µ—Å—Ç—å —Ä–∞–∑–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã: 123, abc, –∫–∏—Ä–∏–ª–ª–∏—Ü–∞.
        """

        result = preprocess_text(plain_text)

        # –¢–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –æ—Å—Ç–∞—Ç—å—Å—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ —Ç–µ–º –∂–µ (—Ç–æ–ª—å–∫–æ Unicode –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è)
        normalized_expected = unicodedata.normalize("NFC", plain_text)
        assert result == normalized_expected

    def test_html_without_script_style(self):
        """–¢–µ—Å—Ç HTML –±–µ–∑ script/style —Ç–µ–≥–æ–≤."""
        clean_html = """
        <html>
        <body>
        <h1>–ó–∞–≥–æ–ª–æ–≤–æ–∫</h1>
        <p>–ü–∞—Ä–∞–≥—Ä–∞—Ñ —Å <a href="link">—Å—Å—ã–ª–∫–æ–π</a></p>
        <div class="content">–ö–æ–Ω—Ç–µ–Ω—Ç</div>
        </body>
        </html>
        """

        result = preprocess_text(clean_html)

        # HTML —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–æ–ª–∂–Ω–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å—Å—è
        assert "<h1>" in result
        assert '<a href="link">' in result
        assert "–ó–∞–≥–æ–ª–æ–≤–æ–∫" in result
        assert "—Å—Å—ã–ª–∫–æ–π" in result

    def test_case_insensitive_tag_detection(self):
        """–¢–µ—Å—Ç –Ω–µ—á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫ —Ä–µ–≥–∏—Å—Ç—Ä—É —Ç–µ–≥–æ–≤."""
        html_upper = """
        <HTML>
        <SCRIPT>bad code</SCRIPT>
        <STYLE>bad style</STYLE>
        <BODY>Good content</BODY>
        </HTML>
        """

        result = preprocess_text(html_upper)

        assert "bad code" not in result
        assert "bad style" not in result
        assert "Good content" in result

    def test_invalid_input(self):
        """–¢–µ—Å—Ç –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –≤—Ö–æ–¥–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞."""
        with pytest.raises(ValueError, match="Input parameter must be a string"):
            preprocess_text(123)

        with pytest.raises(ValueError, match="Input parameter must be a string"):
            preprocess_text(None)


class TestValidateConfigParameters:
    """–¢–µ—Å—Ç—ã –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ validate_config_parameters."""

    def test_valid_config(self):
        """–¢–µ—Å—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏."""
        valid_config = {
            "slicer": {
                "max_tokens": 15000,
                "soft_boundary_max_shift": 500,
                "allowed_extensions": ["json", "txt", "md", "html"],
            }
        }

        # –ù–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –∏—Å–∫–ª—é—á–µ–Ω–∏–π
        validate_config_parameters(valid_config)

    def test_missing_slicer_section(self):
        """–¢–µ—Å—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è —Å–µ–∫—Ü–∏–∏ slicer."""
        config = {"other": {}}

        with pytest.raises(ValueError, match="Missing required parameter slicer.max_tokens"):
            validate_config_parameters(config)

    def test_missing_required_param(self):
        """–¢–µ—Å—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞."""
        config = {
            "slicer": {
                "max_tokens": 15000,
                # soft_boundary_max_shift –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
                "allowed_extensions": ["txt"],
            }
        }

        with pytest.raises(ValueError, match="Missing required parameter slicer.soft_boundary_max_shift"):
            validate_config_parameters(config)

    def test_invalid_max_tokens(self):
        """–¢–µ—Å—Ç –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ max_tokens."""
        config = {
            "slicer": {
                "max_tokens": -100,  # –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                "soft_boundary_max_shift": 500,
                "allowed_extensions": ["txt"],
            }
        }

        with pytest.raises(ValueError, match="must be a positive integer"):
            validate_config_parameters(config)


    def test_empty_allowed_extensions(self):
        """–¢–µ—Å—Ç –ø—É—Å—Ç–æ–≥–æ —Å–ø–∏—Å–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–π."""
        config = {
            "slicer": {
                "max_tokens": 15000,
                "soft_boundary_max_shift": 500,
                "allowed_extensions": [],  # –ü—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
            }
        }

        with pytest.raises(ValueError, match="must be a non-empty list"):
            validate_config_parameters(config)


class TestSliceTextWithWindow:
    """–¢–µ—Å—Ç—ã –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ slice_text_with_window."""

    def test_empty_text(self):
        """–¢–µ—Å—Ç –ø—É—Å—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."""
        result = slice_text_with_window("", 1000, True, 100)
        assert result == []

        result = slice_text_with_window("   ", 1000, True, 100)
        assert result == []

    def test_text_fits_in_one_slice(self):
        """–¢–µ—Å—Ç —Ç–µ–∫—Å—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–µ—â–∞–µ—Ç—Å—è –≤ –æ–¥–∏–Ω —Å–ª–∞–π—Å."""
        text = "–ö–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–ª–æ–≤."
        max_tokens = 1000

        result = slice_text_with_window(text, max_tokens, True, 100)

        assert len(result) == 1
        slice_text, start, end = result[0]
        assert slice_text == text
        assert start == 0
        assert end == count_tokens(text)

    def test_multiple_slices_no_overlap(self):
        """–¢–µ—Å—Ç –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–ª–∞–π—Å–æ–≤ –±–µ–∑ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–π."""
        # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π —Ç–æ—á–Ω–æ —Ä–∞–∑–¥–µ–ª–∏—Ç—Å—è –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ª–∞–π—Å–æ–≤
        text = "–°–ª–æ–≤–æ. " * 50  # –û–∫–æ–ª–æ 100 —Ç–æ–∫–µ–Ω–æ–≤
        max_tokens = 30

        result = slice_text_with_window(text, max_tokens, False, 0)

        # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ª–∞–π—Å–æ–≤
        assert len(result) > 1

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥—Ä–∞–Ω–∏—Ü—ã –∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤/–ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–π
        for i in range(len(result) - 1):
            current_slice = result[i]
            next_slice = result[i + 1]

            # –ö–æ–Ω–µ—Ü —Ç–µ–∫—É—â–µ–≥–æ —Å–ª–∞–π—Å–∞ = –Ω–∞—á–∞–ª–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ (–±–µ–∑ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–π)
            assert current_slice[2] == next_slice[1]

        # –ü–µ—Ä–≤—ã–π —Å–ª–∞–π—Å –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å 0
        assert result[0][1] == 0

        # –ü–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–∞–π—Å –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –Ω–∞ –æ–±—â–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ —Ç–æ–∫–µ–Ω–æ–≤
        total_tokens = count_tokens(text)
        assert result[-1][2] == total_tokens


    def test_no_token_loss(self):
        """–¢–µ—Å—Ç —á—Ç–æ —Ç–æ–∫–µ–Ω—ã –Ω–µ —Ç–µ—Ä—è—é—Ç—Å—è –ø—Ä–∏ —Ä–∞–∑–±–∏–µ–Ω–∏–∏."""
        text = "–¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —á—Ç–æ –≤—Å–µ —Ç–æ–∫–µ–Ω—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –ø—Ä–∏ —Ä–∞–∑–±–∏–µ–Ω–∏–∏ –Ω–∞ —Å–ª–∞–π—Å—ã."
        max_tokens = 15

        # –¢–µ—Å—Ç –±–µ–∑ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–π
        result_no_overlap = slice_text_with_window(text, max_tokens, True, 10)

        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ç–æ–∫–µ–Ω—ã –∏–∑ —Å–ª–∞–π—Å–æ–≤
        total_tokens_from_slices = 0
        for slice_text, start, end in result_no_overlap:
            total_tokens_from_slices += end - start

        original_tokens = count_tokens(text)
        assert total_tokens_from_slices == original_tokens

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –Ω–∞—á–∞–ª–æ –ø–µ—Ä–≤–æ–≥–æ —Å–ª–∞–π—Å–∞ = 0
        assert result_no_overlap[0][1] == 0

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∫–æ–Ω–µ—Ü –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–∞–π—Å–∞ = –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
        assert result_no_overlap[-1][2] == original_tokens

    def test_token_positions_correctness(self):
        """–¢–µ—Å—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –ø–æ–∑–∏—Ü–∏–π —Ç–æ–∫–µ–Ω–æ–≤."""
        text = "–û–¥–∏–Ω –¥–≤–∞ —Ç—Ä–∏ —á–µ—Ç—ã—Ä–µ –ø—è—Ç—å —à–µ—Å—Ç—å —Å–µ–º—å –≤–æ—Å–µ–º—å –¥–µ–≤—è—Ç—å –¥–µ—Å—è—Ç—å."
        max_tokens = 8

        result = slice_text_with_window(text, max_tokens, False, 0)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ slice_token_start inclusive, slice_token_end exclusive
        import tiktoken

        encoding = tiktoken.get_encoding("o200k_base")
        all_tokens = encoding.encode(text)

        for slice_text, start, end in result:
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω—ã –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ
            expected_slice_tokens = all_tokens[start:end]
            expected_slice_text = encoding.decode(expected_slice_tokens)

            assert slice_text == expected_slice_text

    def test_soft_boundaries_applied(self):
        """–¢–µ—Å—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è soft boundaries."""
        # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç —Å —è–≤–Ω—ã–º–∏ –≥—Ä–∞–Ω–∏—Ü–∞–º–∏
        text = """–ì–ª–∞–≤–∞ 1. –ü–µ—Ä–≤–∞—è –≥–ª–∞–≤–∞
        
        –≠—Ç–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–µ—Ä–≤–æ–π –≥–ª–∞–≤—ã —Å –¥–ª–∏–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º.
        
        –ì–ª–∞–≤–∞ 2. –í—Ç–æ—Ä–∞—è –≥–ª–∞–≤–∞
        
        –≠—Ç–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤—Ç–æ—Ä–æ–π –≥–ª–∞–≤—ã."""

        max_tokens = 30

        # –ë–µ–∑ soft boundaries
        result_hard = slice_text_with_window(text, max_tokens, False, 0)

        # –° soft boundaries
        result_soft = slice_text_with_window(text, max_tokens, True, 20)

        # Soft boundaries –º–æ–≥—É—Ç –¥–∞—Ç—å –¥—Ä—É–≥–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ
        # (–Ω–µ –≤—Å–µ–≥–¥–∞, –Ω–æ –≤ –æ–±—â–µ–º —Å–ª—É—á–∞–µ –≤–æ–∑–º–æ–∂–Ω–æ)
        # –ì–ª–∞–≤–Ω–æ–µ —á—Ç–æ –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è

        def get_total_tokens_from_slices(slices):
            return sum(slice_info[2] - slice_info[1] for slice_info in slices)

        total_original = count_tokens(text)
        assert get_total_tokens_from_slices(result_hard) == total_original
        assert get_total_tokens_from_slices(result_soft) == total_original

    def test_unicode_handling(self):
        """–¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ Unicode —Å–∏–º–≤–æ–ª–æ–≤."""
        text = "–ü—Ä–∏–≤–µ—Ç –º–∏—Ä! üåç –≠—Ç–æ —Ç–µ—Å—Ç —ç–º–æ–¥–∑–∏ –∏ —é–Ω–∏–∫–æ–¥–∞: √±√°√©√≠√≥√∫ √ß√±"
        max_tokens = 20

        result = slice_text_with_window(text, max_tokens, True, 5)

        # –û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - –Ω–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –∏—Å–∫–ª—é—á–µ–Ω–∏–π
        assert len(result) >= 1

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ Unicode —Å–∏–º–≤–æ–ª—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è
        combined_text = "".join(slice_text for slice_text, _, _ in result)

        # –í—Å–µ —Å–∏–º–≤–æ–ª—ã –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞ –¥–æ–ª–∂–Ω—ã –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å
        # (–ø–æ—Ä—è–¥–æ–∫ –º–æ–∂–µ—Ç —Å–ª–µ–≥–∫–∞ –æ—Ç–ª–∏—á–∞—Ç—å—Å—è –∏–∑-–∑–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏, –Ω–æ –æ—Å–Ω–æ–≤–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å)
        assert "–ü—Ä–∏–≤–µ—Ç" in combined_text
        assert "üåç" in combined_text


class TestLoadAndValidateFile:
    """–¢–µ—Å—Ç—ã –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ load_and_validate_file."""

    def test_valid_file_utf8(self):
        """–¢–µ—Å—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –≤–∞–ª–∏–¥–Ω–æ–≥–æ UTF-8 —Ñ–∞–π–ª–∞."""
        with tempfile.NamedTemporaryFile(
            mode="w", encoding="utf-8", suffix=".txt", delete=False
        ) as f:
            f.write("This is a test content.\n–°–æ–¥–µ—Ä–∂–∏–º–æ–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.")
            temp_path = Path(f.name)

        try:
            content = load_and_validate_file(temp_path, ["txt"])
            assert "This is a test content" in content
            assert "–°–æ–¥–µ—Ä–∂–∏–º–æ–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ" in content
        finally:
            temp_path.unlink()

    def test_valid_file_cp1251(self):
        """–¢–µ—Å—Ç –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞ –≤ –∫–æ–¥–∏—Ä–æ–≤–∫–µ CP1251."""
        with tempfile.NamedTemporaryFile(
            mode="w", encoding="cp1251", suffix=".txt", delete=False
        ) as f:
            f.write("–¢–µ–∫—Å—Ç –≤ –∫–æ–¥–∏—Ä–æ–≤–∫–µ CP1251")
            temp_path = Path(f.name)

        try:
            content = load_and_validate_file(temp_path, ["txt"])
            assert "–¢–µ–∫—Å—Ç –≤ –∫–æ–¥–∏—Ä–æ–≤–∫–µ CP1251" in content
        finally:
            temp_path.unlink()

    def test_empty_file_error(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ InputError –¥–ª—è –ø—É—Å—Ç–æ–≥–æ —Ñ–∞–π–ª–∞."""
        with tempfile.NamedTemporaryFile(mode="w", suffix=".txt", delete=False) as f:
            f.write("")
            temp_path = Path(f.name)

        try:
            with pytest.raises(InputError) as exc_info:
                load_and_validate_file(temp_path, ["txt"])
            assert "Empty file detected" in str(exc_info.value)
        finally:
            temp_path.unlink()

    def test_unsupported_extension_error(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ InputError –¥–ª—è –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–æ–≥–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è."""
        with tempfile.NamedTemporaryFile(mode="w", suffix=".xyz", delete=False) as f:
            f.write("Some content")
            temp_path = Path(f.name)

        try:
            with pytest.raises(InputError) as exc_info:
                load_and_validate_file(temp_path, ["txt", "md", "json"])
            assert "Unsupported file extension" in str(exc_info.value)
        finally:
            temp_path.unlink()

    def test_file_with_only_whitespace(self):
        """–§–∞–π–ª —Ç–æ–ª—å–∫–æ —Å –ø—Ä–æ–±–µ–ª–∞–º–∏ –¥–æ–ª–∂–µ–Ω –≤—ã–∑–≤–∞—Ç—å InputError."""
        with tempfile.NamedTemporaryFile(mode="w", suffix=".txt", delete=False) as f:
            f.write("   \n\t\n   ")
            temp_path = Path(f.name)

        try:
            with pytest.raises(InputError) as exc_info:
                load_and_validate_file(temp_path, ["txt"])
            assert "Empty file detected" in str(exc_info.value)
        finally:
            temp_path.unlink()


class TestSaveSlice:
    """–¢–µ—Å—Ç—ã –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ save_slice."""

    def test_save_slice_success(self):
        """–£—Å–ø–µ—à–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–ª–∞–π—Å–∞."""
        with tempfile.TemporaryDirectory() as temp_dir:
            output_dir = Path(temp_dir)
            slice_data = {
                "id": "slice_001",
                "order": 1,
                "source_file": "test.txt",
                "slug": "test",
                "text": "Test content",
                "slice_token_start": 0,
                "slice_token_end": 10,
            }

            save_slice(slice_data, output_dir)

            # Check file exists
            output_file = output_dir / "slice_001.slice.json"
            assert output_file.exists()

            # Check content
            with open(output_file, "r", encoding="utf-8") as f:
                saved_data = json.load(f)

            assert saved_data == slice_data

    def test_save_slice_creates_directory(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç."""
        with tempfile.TemporaryDirectory() as temp_dir:
            output_dir = Path(temp_dir) / "nested" / "dir"
            slice_data = {
                "id": "slice_002",
                "order": 2,
                "source_file": "test.txt",
                "slug": "test",
                "text": "Test content",
                "slice_token_start": 0,
                "slice_token_end": 10,
            }

            # Directory doesn't exist yet, but parent does
            output_dir.parent.mkdir(parents=True, exist_ok=True)

            # Note: save_slice expects directory to exist
            # Create directory first
            output_dir.mkdir(parents=True, exist_ok=True)

            save_slice(slice_data, output_dir)

            output_file = output_dir / "slice_002.slice.json"
            assert output_file.exists()

    def test_save_slice_utf8_content(self):
        """–ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ UTF-8 –∫–æ–Ω—Ç–µ–Ω—Ç–∞."""
        with tempfile.TemporaryDirectory() as temp_dir:
            output_dir = Path(temp_dir)
            slice_data = {
                "id": "slice_003",
                "order": 3,
                "source_file": "test.txt",
                "slug": "test",
                "text": "–¢–µ—Å—Ç UTF-8: ‰Ω†Â•Ω üåç √©√±",
                "slice_token_start": 0,
                "slice_token_end": 10,
            }

            save_slice(slice_data, output_dir)

            output_file = output_dir / "slice_003.slice.json"
            with open(output_file, "r", encoding="utf-8") as f:
                saved_data = json.load(f)

            assert saved_data["text"] == "–¢–µ—Å—Ç UTF-8: ‰Ω†Â•Ω üåç √©√±"


class TestProcessFile:
    """–¢–µ—Å—Ç—ã –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ process_file."""

    def test_process_valid_file(self):
        """–£—Å–ø–µ—à–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∞–ª–∏–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞."""
        with tempfile.NamedTemporaryFile(
            mode="w", encoding="utf-8", suffix=".txt", delete=False
        ) as f:
            f.write("This is a test content for processing. " * 10)
            temp_path = Path(f.name)

        config = {
            "slicer": {
                "max_tokens": 50,
                "soft_boundary": False,
                "soft_boundary_max_shift": 0,
                "allowed_extensions": ["txt"],
            }
        }

        try:
            slices, counter = process_file(temp_path, config, 1)

            assert len(slices) > 0
            assert counter > 1
            assert slices[0]["id"] == "slice_001"
            assert slices[0]["source_file"] == temp_path.name
            assert "test content" in slices[0]["text"]
        finally:
            temp_path.unlink()

    def test_process_empty_file_raises_input_error(self):
        """InputError –¥–ª—è –ø—É—Å—Ç–æ–≥–æ —Ñ–∞–π–ª–∞."""
        with tempfile.NamedTemporaryFile(mode="w", suffix=".txt", delete=False) as f:
            f.write("")
            temp_path = Path(f.name)

        config = {
            "slicer": {
                "max_tokens": 50,
                "soft_boundary": False,
                "soft_boundary_max_shift": 0,
                "allowed_extensions": ["txt"],
            }
        }

        try:
            with pytest.raises(InputError) as exc_info:
                process_file(temp_path, config, 1)
            assert "Empty file detected" in str(exc_info.value)
        finally:
            temp_path.unlink()

    def test_process_file_with_invalid_extension(self):
        """InputError –¥–ª—è –Ω–µ–≤–µ—Ä–Ω–æ–≥–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è."""
        with tempfile.NamedTemporaryFile(mode="w", suffix=".xyz", delete=False) as f:
            f.write("Some content")
            temp_path = Path(f.name)

        config = {
            "slicer": {
                "max_tokens": 50,
                "soft_boundary": False,
                "soft_boundary_max_shift": 0,
                "allowed_extensions": ["txt", "md"],
            }
        }

        try:
            with pytest.raises(InputError) as exc_info:
                process_file(temp_path, config, 1)
            assert "Unsupported file extension" in str(exc_info.value)
        finally:
            temp_path.unlink()


class TestPerformance:
    """–¢–µ—Å—Ç—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏."""

    @pytest.fixture
    def large_text(self):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –±–æ–ª—å—à–æ–π —Ç–µ–∫—Å—Ç –¥–ª—è —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (–æ–∫–æ–ª–æ 2MB)."""
        return "Test paragraph with some content. " * 50000  # ~2MB

    def test_large_file_performance(self, large_text):
        """–¢–µ—Å—Ç —á—Ç–æ –±–æ–ª—å—à–∏–µ —Ñ–∞–π–ª—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ."""
        # –î–æ–ª–∂–µ–Ω –∑–∞–≤–µ—Ä—à–∏—Ç—å—Å—è –±–µ–∑ –∑–∞–≤–∏—Å–∞–Ω–∏—è
        start_time = time.time()
        slices = slice_text_with_window(large_text, 5000, True, 500)
        elapsed = time.time() - start_time

        assert elapsed < 10.0  # –î–æ–ª–∂–µ–Ω –∑–∞–≤–µ—Ä—à–∏—Ç—å—Å—è –º–µ–Ω–µ–µ —á–µ–º –∑–∞ 10 —Å–µ–∫—É–Ω–¥
        assert len(slices) > 0

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –Ω–µ—Ç –ø—Ä–æ–±–µ–ª–æ–≤ –≤ –ø–æ–∫—Ä—ã—Ç–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤
        for i in range(1, len(slices)):
            assert slices[i][1] == slices[i-1][2]  # start == previous end

    def test_incremental_tokenization_consistency(self):
        """–¢–µ—Å—Ç —á—Ç–æ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–µ—Ç –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã."""
        # –¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç —Å –∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏ –≥—Ä–∞–Ω–∏—Ü–∞–º–∏
        test_text = "First paragraph.\n\nSecond paragraph.\n\nThird paragraph." * 100

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å –Ω–æ–≤—ã–º –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–º –ø–æ–¥—Ö–æ–¥–æ–º
        slices = slice_text_with_window(test_text, 100, True, 20)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –≥—Ä–∞–Ω–∏—Ü—ã –Ω–∞ —Ä–∞–∑—Ä—ã–≤–∞—Ö –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
        for slice_text, _, _ in slices:
            # –î–æ–ª–∂–Ω—ã –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–æ –∑–∞–∫–∞–Ω—á–∏–≤–∞—Ç—å—Å—è –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞
            if slice_text != test_text[-len(slice_text):]:  # –ï—Å–ª–∏ –Ω–µ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–∞–π—Å
                assert (
                    slice_text.endswith('.')
                    or slice_text.endswith('\n\n')
                    or slice_text.endswith('paragraph.\n')
                ), f"Unexpected boundary: ...{slice_text[-30:]}"

    def test_no_token_gaps(self):
        """–¢–µ—Å—Ç —á—Ç–æ –Ω–µ—Ç –ø—Ä–æ–±–µ–ª–æ–≤ –≤ —Ç–æ–∫–µ–Ω–∞—Ö –º–µ–∂–¥—É —Å–ª–∞–π—Å–∞–º–∏."""
        test_text = "This is a test. " * 1000

        slices = slice_text_with_window(test_text, 50, False, 0)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤
        prev_end = 0
        for _, start, end in slices:
            assert start == prev_end, f"Gap found: prev_end={prev_end}, start={start}"
            assert end > start, f"Invalid slice: start={start}, end={end}"
            prev_end = end

    def test_deterministic_boundaries(self):
        """–¢–µ—Å—Ç —á—Ç–æ –≥—Ä–∞–Ω–∏—Ü—ã –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω—ã –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—É—Å–∫–∞—Ö."""
        test_text = "Test sentence. Another sentence. " * 500

        # –ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫
        slices1 = slice_text_with_window(test_text, 100, True, 50)

        # –í—Ç–æ—Ä–æ–π –∑–∞–ø—É—Å–∫
        slices2 = slice_text_with_window(test_text, 100, True, 50)

        # –î–æ–ª–∂–Ω—ã –±—ã—Ç—å –∏–¥–µ–Ω—Ç–∏—á–Ω—ã
        assert len(slices1) == len(slices2)
        for (text1, start1, end1), (text2, start2, end2) in zip(slices1, slices2):
            assert text1 == text2
            assert start1 == start2
            assert end1 == end2

    def test_memory_efficiency(self, large_text):
        """–¢–µ—Å—Ç —á—Ç–æ –ø–∞–º—è—Ç—å –Ω–µ —Ä–∞—Å—Ç–µ—Ç –ª–∏–Ω–µ–π–Ω–æ —Å —Ä–∞–∑–º–µ—Ä–æ–º —Ñ–∞–π–ª–∞."""
        import tracemalloc

        # –ò–∑–º–µ—Ä—è–µ–º –ø–∞–º—è—Ç—å –¥–ª—è –º–∞–ª–µ–Ω—å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        small_text = "Test. " * 100
        tracemalloc.start()
        _ = slice_text_with_window(small_text, 50, True, 20)
        small_memory = tracemalloc.get_traced_memory()[1]  # peak
        tracemalloc.stop()

        # –ò–∑–º–µ—Ä—è–µ–º –ø–∞–º—è—Ç—å –¥–ª—è –±–æ–ª—å—à–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        tracemalloc.start()
        _ = slice_text_with_window(large_text, 5000, True, 500)
        large_memory = tracemalloc.get_traced_memory()[1]  # peak
        tracemalloc.stop()

        # –ü–∞–º—è—Ç—å –Ω–µ –¥–æ–ª–∂–Ω–∞ —Ä–∞—Å—Ç–∏ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ —Ä–∞–∑–º–µ—Ä—É
        # (large_text –≤ 500 —Ä–∞–∑ –±–æ–ª—å—à–µ, –Ω–æ –ø–∞–º—è—Ç—å –Ω–µ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –≤ 500 —Ä–∞–∑ –±–æ–ª—å—à–µ)
        memory_ratio = large_memory / small_memory
        size_ratio = len(large_text) / len(small_text)

        assert memory_ratio < size_ratio / 10, (
            f"Memory grew too much: {memory_ratio:.1f}x for {size_ratio:.1f}x size increase"
        )
